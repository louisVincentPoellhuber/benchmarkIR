{
    "settings": {
        "datapath":"/part/01/Tmp/lvpoellhuber/datasets/dprqa", 
        "q_model": "/part/01/Tmp/lvpoellhuber/models/backups/roberta_mlm_241019/roberta_model",
        "ctx_model": "/part/01/Tmp/lvpoellhuber/models/backups/roberta_mlm_241019/roberta_model",
        "save_path": "/part/01/Tmp/lvpoellhuber/models/custom_roberta/dpr",
        "tokenizer": "/part/01/Tmp/lvpoellhuber/models/backups/roberta_mlm_241019", 
        "checkpoint": null, 
        "accelerate": true
    },
    "preprocess_args": {
        "preprocess": false,
        "task": "dprqa", 
        "train_tokenizer": false, 
        "overwrite": false
    },
    "train_args": {
        "dataset":"/part/01/Tmp/lvpoellhuber/datasets/dprqa/train.pt", 
        "epochs": 5,
        "batch_size": 32, 
        "lr": 1e-5,
        "use_checkpoint": false, 
        "logging": true,
        "train": true, 
        "eval": false
    },
    "eval_args": {
        "eval": true,
        "dataset": null,
        "model": null
    },
    "config": {
        "vocab_size": 32,
        "max_position_embeddings": 514,
        "hidden_size": 768,
        "num_attention_heads": 12,
        "num_hidden_layers": 6,
        "type_vocab_size": 1,
        "attn_mechanism": "eager",
        "_attn_implementation_internal": "eager", 
        "train": true,
        "pretrained": false
    }
}
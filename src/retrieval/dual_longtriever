digraph {
	graph [size="1488.1499999999999,1488.1499999999999"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	140067040903568 [label="
 ()" fillcolor=darkolivegreen1]
	140067041701184 [label="NllLossBackward0
----------------------------------
ignore_index: 18446744073709551516
reduction   :                    1
self        :       [saved tensor]
target      :       [saved tensor]
total_weight:       [saved tensor]
weight      :                 None"]
	140067041701328 -> 140067041701184
	140067041701328 [label="LogSoftmaxBackward0
----------------------
dim   :              1
result: [saved tensor]"]
	140067041701472 -> 140067041701328
	140067041701472 [label="MmBackward0
--------------------------------
mat2            : [saved tensor]
mat2_sym_sizes  :       (768, 3)
mat2_sym_strides:      (1, 6912)
self            : [saved tensor]
self_sym_sizes  :       (3, 768)
self_sym_strides:      (1536, 1)"]
	140067041702096 -> 140067041701472
	140067041702096 [label="SqueezeBackward1
---------------------------
dim           :           1
self_sym_sizes: (3, 1, 768)"]
	140067041704976 -> 140067041702096
	140067041704976 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 1, 768)
start         :                   0
step          :                   1"]
	140067041703728 -> 140067041704976
	140067041703728 [label="SliceBackward0
---------------------------
dim           :           1
end           :           1
self_sym_sizes: (3, 2, 768)
start         :           0
step          :           1"]
	140067041700560 -> 140067041703728
	140067041700560 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:         (3, 2, 768)
start         :                   0
step          :                   1"]
	140067041700320 -> 140067041700560
	140067041700320 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140067041700704 -> 140067041700320
	140067041700704 [label="AddBackward0
------------
alpha: 1"]
	140067041700944 -> 140067041700704
	140067041700944 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041700608 -> 140067041700944
	140067041700608 [label="ViewBackward0
------------------------
self_sym_sizes: (6, 768)"]
	140067041699888 -> 140067041700608
	140067041699888 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (6, 3072)
mat1_sym_strides:      (3072, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (3072, 768)
mat2_sym_strides:      (1, 3072)"]
	140067041702000 -> 140067041699888
	140066794644304 [label="q_encoder.encoder.information_exchanging_layer.11.output.dense.bias
 (768)" fillcolor=lightblue]
	140066794644304 -> 140067041702000
	140067041702000 [label=AccumulateGrad]
	140067041700080 -> 140067041699888
	140067041700080 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 2, 3072)"]
	140067041700032 -> 140067041700080
	140067041700032 [label="GeluBackward0
---------------------------
approximate:           none
self       : [saved tensor]"]
	140067041699792 -> 140067041700032
	140067041699792 [label="ViewBackward0
-------------------------
self_sym_sizes: (6, 3072)"]
	140067041699264 -> 140067041699792
	140067041699264 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (6, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (768, 3072)
mat2_sym_strides:       (1, 768)"]
	140067041698928 -> 140067041699264
	140066794644224 [label="q_encoder.encoder.information_exchanging_layer.11.intermediate.dense.bias
 (3072)" fillcolor=lightblue]
	140066794644224 -> 140067041698928
	140067041698928 [label=AccumulateGrad]
	140067041698976 -> 140067041699264
	140067041698976 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140067041701904 -> 140067041698976
	140067041701904 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140067041698832 -> 140067041701904
	140067041698832 [label="AddBackward0
------------
alpha: 1"]
	140067041698112 -> 140067041698832
	140067041698112 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041697872 -> 140067041698112
	140067041697872 [label="ViewBackward0
------------------------
self_sym_sizes: (6, 768)"]
	140067041697680 -> 140067041697872
	140067041697680 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (6, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041697776 -> 140067041697680
	140066794643824 [label="q_encoder.encoder.information_exchanging_layer.11.attention.output.dense.bias
 (768)" fillcolor=lightblue]
	140066794643824 -> 140067041697776
	140067041697776 [label=AccumulateGrad]
	140067041698640 -> 140067041697680
	140067041698640 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140067041697488 -> 140067041698640
	140067041697488 [label="ViewBackward0
------------------------------
self_sym_sizes: (3, 2, 12, 64)"]
	140067041697584 -> 140067041697488
	140067041697584 [label=CloneBackward0]
	140067041696912 -> 140067041697584
	140067041696912 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041696720 -> 140067041696912
	140067041696720 [label="UnsafeViewBackward0
---------------------------
self_sym_sizes: (36, 2, 64)"]
	140067041696576 -> 140067041696720
	140067041696576 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140067041696192 -> 140067041696576
	140067041696192 [label="ViewBackward0
-----------------------------
self_sym_sizes: (3, 12, 2, 2)"]
	140067041695808 -> 140067041696192
	140067041695808 [label="ExpandBackward0
-----------------------------
self_sym_sizes: (3, 12, 2, 2)"]
	140067041695712 -> 140067041695808
	140067041695712 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041695424 -> 140067041695712
	140067041695424 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140067041695184 -> 140067041695424
	140067041695184 [label="AddBackward0
------------
alpha: 1"]
	140067041694992 -> 140067041695184
	140067041694992 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041695040 -> 140067041694992
	140067041695040 [label="UnsafeViewBackward0
--------------------------
self_sym_sizes: (36, 2, 2)"]
	140067041694464 -> 140067041695040
	140067041694464 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140067041694608 -> 140067041694464
	140067041694608 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 2, 64)"]
	140067041693936 -> 140067041694608
	140067041693936 [label=CloneBackward0]
	140067041693888 -> 140067041693936
	140067041693888 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 2, 64)"]
	140067041693504 -> 140067041693888
	140067041693504 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041693264 -> 140067041693504
	140067041693264 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140067041693120 -> 140067041693264
	140067041693120 [label="ViewBackward0
------------------------
self_sym_sizes: (6, 768)"]
	140067041692688 -> 140067041693120
	140067041692688 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (6, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041692592 -> 140067041692688
	140066794643264 [label="q_encoder.encoder.information_exchanging_layer.11.attention.self.query.bias
 (768)" fillcolor=lightblue]
	140066794643264 -> 140067041692592
	140067041692592 [label=AccumulateGrad]
	140067041692496 -> 140067041692688
	140067041692496 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140067041699024 -> 140067041692496
	140067041699024 [label="CatBackward0
------------
dim: 1"]
	140067041691824 -> 140067041699024
	140067041691824 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 1, 768)
start         :                   0
step          :                   1"]
	140067041692448 -> 140067041691824
	140067041692448 [label="SliceBackward0
---------------------------
dim           :           1
end           :           1
self_sym_sizes: (3, 2, 768)
start         :           0
step          :           1"]
	140067041690864 -> 140067041692448
	140067041690864 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:         (3, 2, 768)
start         :                   0
step          :                   1"]
	140067041691488 -> 140067041690864
	140067041691488 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140067041690288 -> 140067041691488
	140067041690288 [label="AddBackward0
------------
alpha: 1"]
	140067041691296 -> 140067041690288
	140067041691296 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067064880272 -> 140067041691296
	140067064880272 [label="ViewBackward0
------------------------
self_sym_sizes: (6, 768)"]
	140067041691152 -> 140067064880272
	140067041691152 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (6, 3072)
mat1_sym_strides:      (3072, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (3072, 768)
mat2_sym_strides:      (1, 3072)"]
	140067041689456 -> 140067041691152
	140066794643024 [label="q_encoder.encoder.information_exchanging_layer.10.output.dense.bias
 (768)" fillcolor=lightblue]
	140066794643024 -> 140067041689456
	140067041689456 [label=AccumulateGrad]
	140067041689408 -> 140067041691152
	140067041689408 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 2, 3072)"]
	140067041689264 -> 140067041689408
	140067041689264 [label="GeluBackward0
---------------------------
approximate:           none
self       : [saved tensor]"]
	140067041689360 -> 140067041689264
	140067041689360 [label="ViewBackward0
-------------------------
self_sym_sizes: (6, 3072)"]
	140067041688688 -> 140067041689360
	140067041688688 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (6, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (768, 3072)
mat2_sym_strides:       (1, 768)"]
	140067041688640 -> 140067041688688
	140066794642944 [label="q_encoder.encoder.information_exchanging_layer.10.intermediate.dense.bias
 (3072)" fillcolor=lightblue]
	140066794642944 -> 140067041688640
	140067041688640 [label=AccumulateGrad]
	140067041688592 -> 140067041688688
	140067041688592 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140067041691248 -> 140067041688592
	140067041691248 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140067041687920 -> 140067041691248
	140067041687920 [label="AddBackward0
------------
alpha: 1"]
	140067041687776 -> 140067041687920
	140067041687776 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041687344 -> 140067041687776
	140067041687344 [label="ViewBackward0
------------------------
self_sym_sizes: (6, 768)"]
	140067041687008 -> 140067041687344
	140067041687008 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (6, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041687200 -> 140067041687008
	140066794642544 [label="q_encoder.encoder.information_exchanging_layer.10.attention.output.dense.bias
 (768)" fillcolor=lightblue]
	140066794642544 -> 140067041687200
	140067041687200 [label=AccumulateGrad]
	140067041688448 -> 140067041687008
	140067041688448 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140067041686672 -> 140067041688448
	140067041686672 [label="ViewBackward0
------------------------------
self_sym_sizes: (3, 2, 12, 64)"]
	140067041686864 -> 140067041686672
	140067041686864 [label=CloneBackward0]
	140067041686240 -> 140067041686864
	140067041686240 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041686048 -> 140067041686240
	140067041686048 [label="UnsafeViewBackward0
---------------------------
self_sym_sizes: (36, 2, 64)"]
	140067041685952 -> 140067041686048
	140067041685952 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140067041685568 -> 140067041685952
	140067041685568 [label="ViewBackward0
-----------------------------
self_sym_sizes: (3, 12, 2, 2)"]
	140067041685088 -> 140067041685568
	140067041685088 [label="ExpandBackward0
-----------------------------
self_sym_sizes: (3, 12, 2, 2)"]
	140067041685040 -> 140067041685088
	140067041685040 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041684704 -> 140067041685040
	140067041684704 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140067041684560 -> 140067041684704
	140067041684560 [label="AddBackward0
------------
alpha: 1"]
	140067041684368 -> 140067041684560
	140067041684368 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041684464 -> 140067041684368
	140067041684464 [label="UnsafeViewBackward0
--------------------------
self_sym_sizes: (36, 2, 2)"]
	140067041683936 -> 140067041684464
	140067041683936 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140067041684032 -> 140067041683936
	140067041684032 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 2, 64)"]
	140067041683552 -> 140067041684032
	140067041683552 [label=CloneBackward0]
	140067041683408 -> 140067041683552
	140067041683408 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 2, 64)"]
	140067041683168 -> 140067041683408
	140067041683168 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041682928 -> 140067041683168
	140067041682928 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140067041682880 -> 140067041682928
	140067041682880 [label="ViewBackward0
------------------------
self_sym_sizes: (6, 768)"]
	140067041682496 -> 140067041682880
	140067041682496 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (6, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041682304 -> 140067041682496
	140066794642064 [label="q_encoder.encoder.information_exchanging_layer.10.attention.self.query.bias
 (768)" fillcolor=lightblue]
	140066794642064 -> 140067041682304
	140067041682304 [label=AccumulateGrad]
	140067041682256 -> 140067041682496
	140067041682256 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140067041687872 -> 140067041682256
	140067041687872 [label="CatBackward0
------------
dim: 1"]
	140067041681584 -> 140067041687872
	140067041681584 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 1, 768)
start         :                   0
step          :                   1"]
	140067041682112 -> 140067041681584
	140067041682112 [label="SliceBackward0
---------------------------
dim           :           1
end           :           1
self_sym_sizes: (3, 2, 768)
start         :           0
step          :           1"]
	140067041680912 -> 140067041682112
	140067041680912 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:         (3, 2, 768)
start         :                   0
step          :                   1"]
	140067041681344 -> 140067041680912
	140067041681344 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140067041680576 -> 140067041681344
	140067041680576 [label="AddBackward0
------------
alpha: 1"]
	140067041681296 -> 140067041680576
	140067041681296 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041680240 -> 140067041681296
	140067041680240 [label="ViewBackward0
------------------------
self_sym_sizes: (6, 768)"]
	140067041680000 -> 140067041680240
	140067041680000 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (6, 3072)
mat1_sym_strides:      (3072, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (3072, 768)
mat2_sym_strides:      (1, 3072)"]
	140067041680096 -> 140067041680000
	140066794641744 [label="q_encoder.encoder.information_exchanging_layer.9.output.dense.bias
 (768)" fillcolor=lightblue]
	140066794641744 -> 140067041680096
	140067041680096 [label=AccumulateGrad]
	140067041681056 -> 140067041680000
	140067041681056 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 2, 3072)"]
	140067041679904 -> 140067041681056
	140067041679904 [label="GeluBackward0
---------------------------
approximate:           none
self       : [saved tensor]"]
	140067041679856 -> 140067041679904
	140067041679856 [label="ViewBackward0
-------------------------
self_sym_sizes: (6, 3072)"]
	140067041679232 -> 140067041679856
	140067041679232 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (6, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (768, 3072)
mat2_sym_strides:       (1, 768)"]
	140067041679136 -> 140067041679232
	140066794641664 [label="q_encoder.encoder.information_exchanging_layer.9.intermediate.dense.bias
 (3072)" fillcolor=lightblue]
	140066794641664 -> 140067041679136
	140067041679136 [label=AccumulateGrad]
	140067041679088 -> 140067041679232
	140067041679088 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140067041681248 -> 140067041679088
	140067041681248 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140067041678368 -> 140067041681248
	140067041678368 [label="AddBackward0
------------
alpha: 1"]
	140067041678224 -> 140067041678368
	140067041678224 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041677840 -> 140067041678224
	140067041677840 [label="ViewBackward0
------------------------
self_sym_sizes: (6, 768)"]
	140067041677552 -> 140067041677840
	140067041677552 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (6, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041677696 -> 140067041677552
	140066794641264 [label="q_encoder.encoder.information_exchanging_layer.9.attention.output.dense.bias
 (768)" fillcolor=lightblue]
	140066794641264 -> 140067041677696
	140067041677696 [label=AccumulateGrad]
	140067041678896 -> 140067041677552
	140067041678896 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140067041677264 -> 140067041678896
	140067041677264 [label="ViewBackward0
------------------------------
self_sym_sizes: (3, 2, 12, 64)"]
	140067041677456 -> 140067041677264
	140067041677456 [label=CloneBackward0]
	140067041676832 -> 140067041677456
	140067041676832 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041676688 -> 140067041676832
	140067041676688 [label="UnsafeViewBackward0
---------------------------
self_sym_sizes: (36, 2, 64)"]
	140067041676640 -> 140067041676688
	140067041676640 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140067041676400 -> 140067041676640
	140067041676400 [label="ViewBackward0
-----------------------------
self_sym_sizes: (3, 12, 2, 2)"]
	140067041676112 -> 140067041676400
	140067041676112 [label="ExpandBackward0
-----------------------------
self_sym_sizes: (3, 12, 2, 2)"]
	140067041676064 -> 140067041676112
	140067041676064 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041675728 -> 140067041676064
	140067041675728 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140067041675536 -> 140067041675728
	140067041675536 [label="AddBackward0
------------
alpha: 1"]
	140067041675200 -> 140067041675536
	140067041675200 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041675344 -> 140067041675200
	140067041675344 [label="UnsafeViewBackward0
--------------------------
self_sym_sizes: (36, 2, 2)"]
	140067041674720 -> 140067041675344
	140067041674720 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140067041674912 -> 140067041674720
	140067041674912 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 2, 64)"]
	140067041674384 -> 140067041674912
	140067041674384 [label=CloneBackward0]
	140067041674336 -> 140067041674384
	140067041674336 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 2, 64)"]
	140067041674000 -> 140067041674336
	140067041674000 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041673760 -> 140067041674000
	140067041673760 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140067041673568 -> 140067041673760
	140067041673568 [label="ViewBackward0
------------------------
self_sym_sizes: (6, 768)"]
	140067041673376 -> 140067041673568
	140067041673376 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (6, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140066688027568 -> 140067041673376
	140066794640704 [label="q_encoder.encoder.information_exchanging_layer.9.attention.self.query.bias
 (768)" fillcolor=lightblue]
	140066794640704 -> 140066688027568
	140066688027568 [label=AccumulateGrad]
	140066688039808 -> 140067041673376
	140066688039808 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140067041678320 -> 140066688039808
	140067041678320 [label="CatBackward0
------------
dim: 1"]
	140066688038512 -> 140067041678320
	140066688038512 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 1, 768)
start         :                   0
step          :                   1"]
	140066688030016 -> 140066688038512
	140066688030016 [label="SliceBackward0
---------------------------
dim           :           1
end           :           1
self_sym_sizes: (3, 2, 768)
start         :           0
step          :           1"]
	140066688028528 -> 140066688030016
	140066688028528 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:         (3, 2, 768)
start         :                   0
step          :                   1"]
	140066688031024 -> 140066688028528
	140066688031024 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140066688027136 -> 140066688031024
	140066688027136 [label="AddBackward0
------------
alpha: 1"]
	140066688030640 -> 140066688027136
	140066688030640 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067973179104 -> 140066688030640
	140067973179104 [label="ViewBackward0
------------------------
self_sym_sizes: (6, 768)"]
	140066688029152 -> 140067973179104
	140066688029152 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (6, 3072)
mat1_sym_strides:      (3072, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (3072, 768)
mat2_sym_strides:      (1, 3072)"]
	140067042049856 -> 140066688029152
	140066794640464 [label="q_encoder.encoder.information_exchanging_layer.8.output.dense.bias
 (768)" fillcolor=lightblue]
	140066794640464 -> 140067042049856
	140067042049856 [label=AccumulateGrad]
	140067042049808 -> 140066688029152
	140067042049808 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 2, 3072)"]
	140067042049568 -> 140067042049808
	140067042049568 [label="GeluBackward0
---------------------------
approximate:           none
self       : [saved tensor]"]
	140067042049664 -> 140067042049568
	140067042049664 [label="ViewBackward0
-------------------------
self_sym_sizes: (6, 3072)"]
	140067042048656 -> 140067042049664
	140067042048656 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (6, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (768, 3072)
mat2_sym_strides:       (1, 768)"]
	140067042048560 -> 140067042048656
	140066794640384 [label="q_encoder.encoder.information_exchanging_layer.8.intermediate.dense.bias
 (3072)" fillcolor=lightblue]
	140066794640384 -> 140067042048560
	140067042048560 [label=AccumulateGrad]
	140067042048512 -> 140067042048656
	140067042048512 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140066688029968 -> 140067042048512
	140066688029968 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140067042047792 -> 140066688029968
	140067042047792 [label="AddBackward0
------------
alpha: 1"]
	140067042047648 -> 140067042047792
	140067042047648 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067042047264 -> 140067042047648
	140067042047264 [label="ViewBackward0
------------------------
self_sym_sizes: (6, 768)"]
	140067042047024 -> 140067042047264
	140067042047024 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (6, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067042047120 -> 140067042047024
	140066794639984 [label="q_encoder.encoder.information_exchanging_layer.8.attention.output.dense.bias
 (768)" fillcolor=lightblue]
	140066794639984 -> 140067042047120
	140067042047120 [label=AccumulateGrad]
	140067042048272 -> 140067042047024
	140067042048272 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140067042046688 -> 140067042048272
	140067042046688 [label="ViewBackward0
------------------------------
self_sym_sizes: (3, 2, 12, 64)"]
	140067042046784 -> 140067042046688
	140067042046784 [label=CloneBackward0]
	140067042046208 -> 140067042046784
	140067042046208 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067042046064 -> 140067042046208
	140067042046064 [label="UnsafeViewBackward0
---------------------------
self_sym_sizes: (36, 2, 64)"]
	140067042046016 -> 140067042046064
	140067042046016 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140067042045584 -> 140067042046016
	140067042045584 [label="ViewBackward0
-----------------------------
self_sym_sizes: (3, 12, 2, 2)"]
	140067042045296 -> 140067042045584
	140067042045296 [label="ExpandBackward0
-----------------------------
self_sym_sizes: (3, 12, 2, 2)"]
	140067042045248 -> 140067042045296
	140067042045248 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067042044864 -> 140067042045248
	140067042044864 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140067042044672 -> 140067042044864
	140067042044672 [label="AddBackward0
------------
alpha: 1"]
	140067042044432 -> 140067042044672
	140067042044432 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067042044528 -> 140067042044432
	140067042044528 [label="UnsafeViewBackward0
--------------------------
self_sym_sizes: (36, 2, 2)"]
	140067042043856 -> 140067042044528
	140067042043856 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140067042044000 -> 140067042043856
	140067042044000 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 2, 64)"]
	140067042043616 -> 140067042044000
	140067042043616 [label=CloneBackward0]
	140067042043520 -> 140067042043616
	140067042043520 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 2, 64)"]
	140067042043232 -> 140067042043520
	140067042043232 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067042042800 -> 140067042043232
	140067042042800 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140067042042704 -> 140067042042800
	140067042042704 [label="ViewBackward0
------------------------
self_sym_sizes: (6, 768)"]
	140067042042368 -> 140067042042704
	140067042042368 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (6, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067042042272 -> 140067042042368
	140066794639424 [label="q_encoder.encoder.information_exchanging_layer.8.attention.self.query.bias
 (768)" fillcolor=lightblue]
	140066794639424 -> 140067042042272
	140067042042272 [label=AccumulateGrad]
	140067042041936 -> 140067042042368
	140067042041936 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140067042047744 -> 140067042041936
	140067042047744 [label="CatBackward0
------------
dim: 1"]
	140067042041216 -> 140067042047744
	140067042041216 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 1, 768)
start         :                   0
step          :                   1"]
	140067042041888 -> 140067042041216
	140067042041888 [label="SliceBackward0
---------------------------
dim           :           1
end           :           1
self_sym_sizes: (3, 2, 768)
start         :           0
step          :           1"]
	140067042040736 -> 140067042041888
	140067042040736 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:         (3, 2, 768)
start         :                   0
step          :                   1"]
	140067042041024 -> 140067042040736
	140067042041024 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140067042040352 -> 140067042041024
	140067042040352 [label="AddBackward0
------------
alpha: 1"]
	140067042040976 -> 140067042040352
	140067042040976 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067042040064 -> 140067042040976
	140067042040064 [label="ViewBackward0
------------------------
self_sym_sizes: (6, 768)"]
	140067042039632 -> 140067042040064
	140067042039632 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (6, 3072)
mat1_sym_strides:      (3072, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (3072, 768)
mat2_sym_strides:      (1, 3072)"]
	140067042039872 -> 140067042039632
	140066794639184 [label="q_encoder.encoder.information_exchanging_layer.7.output.dense.bias
 (768)" fillcolor=lightblue]
	140066794639184 -> 140067042039872
	140067042039872 [label=AccumulateGrad]
	140067042040832 -> 140067042039632
	140067042040832 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 2, 3072)"]
	140067042039392 -> 140067042040832
	140067042039392 [label="GeluBackward0
---------------------------
approximate:           none
self       : [saved tensor]"]
	140067042039440 -> 140067042039392
	140067042039440 [label="ViewBackward0
-------------------------
self_sym_sizes: (6, 3072)"]
	140067042038624 -> 140067042039440
	140067042038624 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (6, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (768, 3072)
mat2_sym_strides:       (1, 768)"]
	140067042038432 -> 140067042038624
	140066794639104 [label="q_encoder.encoder.information_exchanging_layer.7.intermediate.dense.bias
 (3072)" fillcolor=lightblue]
	140066794639104 -> 140067042038432
	140067042038432 [label=AccumulateGrad]
	140067042038384 -> 140067042038624
	140067042038384 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140067042040928 -> 140067042038384
	140067042040928 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140067042037664 -> 140067042040928
	140067042037664 [label="AddBackward0
------------
alpha: 1"]
	140067042037472 -> 140067042037664
	140067042037472 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067042037088 -> 140067042037472
	140067042037088 [label="ViewBackward0
------------------------
self_sym_sizes: (6, 768)"]
	140067042036752 -> 140067042037088
	140067042036752 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (6, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067042036944 -> 140067042036752
	140066794638704 [label="q_encoder.encoder.information_exchanging_layer.7.attention.output.dense.bias
 (768)" fillcolor=lightblue]
	140066794638704 -> 140067042036944
	140067042036944 [label=AccumulateGrad]
	140067042038192 -> 140067042036752
	140067042038192 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140067042036608 -> 140067042038192
	140067042036608 [label="ViewBackward0
------------------------------
self_sym_sizes: (3, 2, 12, 64)"]
	140067042036656 -> 140067042036608
	140067042036656 [label=CloneBackward0]
	140067042036032 -> 140067042036656
	140067042036032 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067042035840 -> 140067042036032
	140067042035840 [label="UnsafeViewBackward0
---------------------------
self_sym_sizes: (36, 2, 64)"]
	140067042035792 -> 140067042035840
	140067042035792 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140067042035120 -> 140067042035792
	140067042035120 [label="ViewBackward0
-----------------------------
self_sym_sizes: (3, 12, 2, 2)"]
	140067042034736 -> 140067042035120
	140067042034736 [label="ExpandBackward0
-----------------------------
self_sym_sizes: (3, 12, 2, 2)"]
	140067042034592 -> 140067042034736
	140067042034592 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067042034304 -> 140067042034592
	140067042034304 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140067042034160 -> 140067042034304
	140067042034160 [label="AddBackward0
------------
alpha: 1"]
	140067042033728 -> 140067042034160
	140067042033728 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067042033824 -> 140067042033728
	140067042033824 [label="UnsafeViewBackward0
--------------------------
self_sym_sizes: (36, 2, 2)"]
	140067042167008 -> 140067042033824
	140067042167008 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140067042177520 -> 140067042167008
	140067042177520 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 2, 64)"]
	140067042165232 -> 140067042177520
	140067042165232 [label=CloneBackward0]
	140067042166432 -> 140067042165232
	140067042166432 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 2, 64)"]
	140067042165664 -> 140067042166432
	140067042165664 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067042165328 -> 140067042165664
	140067042165328 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140067042165184 -> 140067042165328
	140067042165184 [label="ViewBackward0
------------------------
self_sym_sizes: (6, 768)"]
	140067041525568 -> 140067042165184
	140067041525568 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (6, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041525328 -> 140067041525568
	140066794638144 [label="q_encoder.encoder.information_exchanging_layer.7.attention.self.query.bias
 (768)" fillcolor=lightblue]
	140066794638144 -> 140067041525328
	140067041525328 [label=AccumulateGrad]
	140067041525232 -> 140067041525568
	140067041525232 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140067042037616 -> 140067041525232
	140067042037616 [label="CatBackward0
------------
dim: 1"]
	140067041524512 -> 140067042037616
	140067041524512 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 1, 768)
start         :                   0
step          :                   1"]
	140067041525088 -> 140067041524512
	140067041525088 [label="SliceBackward0
---------------------------
dim           :           1
end           :           1
self_sym_sizes: (3, 2, 768)
start         :           0
step          :           1"]
	140067041523744 -> 140067041525088
	140067041523744 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:         (3, 2, 768)
start         :                   0
step          :                   1"]
	140067041524224 -> 140067041523744
	140067041524224 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140067041523360 -> 140067041524224
	140067041523360 [label="AddBackward0
------------
alpha: 1"]
	140067041524128 -> 140067041523360
	140067041524128 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041523072 -> 140067041524128
	140067041523072 [label="ViewBackward0
------------------------
self_sym_sizes: (6, 768)"]
	140067041522832 -> 140067041523072
	140067041522832 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (6, 3072)
mat1_sym_strides:      (3072, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (3072, 768)
mat2_sym_strides:      (1, 3072)"]
	140067041522928 -> 140067041522832
	140066794637904 [label="q_encoder.encoder.information_exchanging_layer.6.output.dense.bias
 (768)" fillcolor=lightblue]
	140066794637904 -> 140067041522928
	140067041522928 [label=AccumulateGrad]
	140067041523888 -> 140067041522832
	140067041523888 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 2, 3072)"]
	140067041522544 -> 140067041523888
	140067041522544 [label="GeluBackward0
---------------------------
approximate:           none
self       : [saved tensor]"]
	140067041522592 -> 140067041522544
	140067041522592 [label="ViewBackward0
-------------------------
self_sym_sizes: (6, 3072)"]
	140067041521872 -> 140067041522592
	140067041521872 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (6, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (768, 3072)
mat2_sym_strides:       (1, 768)"]
	140067041521824 -> 140067041521872
	140066794637824 [label="q_encoder.encoder.information_exchanging_layer.6.intermediate.dense.bias
 (3072)" fillcolor=lightblue]
	140066794637824 -> 140067041521824
	140067041521824 [label=AccumulateGrad]
	140067041521776 -> 140067041521872
	140067041521776 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140067041524080 -> 140067041521776
	140067041524080 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140067041521152 -> 140067041524080
	140067041521152 [label="AddBackward0
------------
alpha: 1"]
	140067041521008 -> 140067041521152
	140067041521008 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041520672 -> 140067041521008
	140067041520672 [label="ViewBackward0
------------------------
self_sym_sizes: (6, 768)"]
	140067041520384 -> 140067041520672
	140067041520384 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (6, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041520480 -> 140067041520384
	140066794637424 [label="q_encoder.encoder.information_exchanging_layer.6.attention.output.dense.bias
 (768)" fillcolor=lightblue]
	140066794637424 -> 140067041520480
	140067041520480 [label=AccumulateGrad]
	140067041521632 -> 140067041520384
	140067041521632 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140067041520096 -> 140067041521632
	140067041520096 [label="ViewBackward0
------------------------------
self_sym_sizes: (3, 2, 12, 64)"]
	140067041520192 -> 140067041520096
	140067041520192 [label=CloneBackward0]
	140067041519616 -> 140067041520192
	140067041519616 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041519376 -> 140067041519616
	140067041519376 [label="UnsafeViewBackward0
---------------------------
self_sym_sizes: (36, 2, 64)"]
	140067041519280 -> 140067041519376
	140067041519280 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140067041518896 -> 140067041519280
	140067041518896 [label="ViewBackward0
-----------------------------
self_sym_sizes: (3, 12, 2, 2)"]
	140067041518560 -> 140067041518896
	140067041518560 [label="ExpandBackward0
-----------------------------
self_sym_sizes: (3, 12, 2, 2)"]
	140067041518464 -> 140067041518560
	140067041518464 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041518128 -> 140067041518464
	140067041518128 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140067041517936 -> 140067041518128
	140067041517936 [label="AddBackward0
------------
alpha: 1"]
	140067041517648 -> 140067041517936
	140067041517648 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041517744 -> 140067041517648
	140067041517744 [label="UnsafeViewBackward0
--------------------------
self_sym_sizes: (36, 2, 2)"]
	140067041517120 -> 140067041517744
	140067041517120 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140067041517264 -> 140067041517120
	140067041517264 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 2, 64)"]
	140067041516832 -> 140067041517264
	140067041516832 [label=CloneBackward0]
	140067041516736 -> 140067041516832
	140067041516736 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 2, 64)"]
	140067041516496 -> 140067041516736
	140067041516496 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041516256 -> 140067041516496
	140067041516256 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140067041516160 -> 140067041516256
	140067041516160 [label="ViewBackward0
------------------------
self_sym_sizes: (6, 768)"]
	140067041515824 -> 140067041516160
	140067041515824 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (6, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041515632 -> 140067041515824
	140066794636864 [label="q_encoder.encoder.information_exchanging_layer.6.attention.self.query.bias
 (768)" fillcolor=lightblue]
	140066794636864 -> 140067041515632
	140067041515632 [label=AccumulateGrad]
	140067041515536 -> 140067041515824
	140067041515536 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140067041521056 -> 140067041515536
	140067041521056 [label="CatBackward0
------------
dim: 1"]
	140067041514960 -> 140067041521056
	140067041514960 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 1, 768)
start         :                   0
step          :                   1"]
	140067041515488 -> 140067041514960
	140067041515488 [label="SliceBackward0
---------------------------
dim           :           1
end           :           1
self_sym_sizes: (3, 2, 768)
start         :           0
step          :           1"]
	140067041514144 -> 140067041515488
	140067041514144 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:         (3, 2, 768)
start         :                   0
step          :                   1"]
	140067041514624 -> 140067041514144
	140067041514624 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140067041513856 -> 140067041514624
	140067041513856 [label="AddBackward0
------------
alpha: 1"]
	140067041514576 -> 140067041513856
	140067041514576 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041513424 -> 140067041514576
	140067041513424 [label="ViewBackward0
------------------------
self_sym_sizes: (6, 768)"]
	140067041513184 -> 140067041513424
	140067041513184 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (6, 3072)
mat1_sym_strides:      (3072, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (3072, 768)
mat2_sym_strides:      (1, 3072)"]
	140067041513280 -> 140067041513184
	140066794636624 [label="q_encoder.encoder.information_exchanging_layer.5.output.dense.bias
 (768)" fillcolor=lightblue]
	140066794636624 -> 140067041513280
	140067041513280 [label=AccumulateGrad]
	140067041514336 -> 140067041513184
	140067041514336 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 2, 3072)"]
	140067041512848 -> 140067041514336
	140067041512848 [label="GeluBackward0
---------------------------
approximate:           none
self       : [saved tensor]"]
	140067041512944 -> 140067041512848
	140067041512944 [label="ViewBackward0
-------------------------
self_sym_sizes: (6, 3072)"]
	140067041512080 -> 140067041512944
	140067041512080 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (6, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (768, 3072)
mat2_sym_strides:       (1, 768)"]
	140067041511984 -> 140067041512080
	140066794636544 [label="q_encoder.encoder.information_exchanging_layer.5.intermediate.dense.bias
 (3072)" fillcolor=lightblue]
	140066794636544 -> 140067041511984
	140067041511984 [label=AccumulateGrad]
	140067041511936 -> 140067041512080
	140067041511936 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140067041514480 -> 140067041511936
	140067041514480 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140067041511360 -> 140067041514480
	140067041511360 [label="AddBackward0
------------
alpha: 1"]
	140067041511264 -> 140067041511360
	140067041511264 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041510976 -> 140067041511264
	140067041510976 [label="ViewBackward0
------------------------
self_sym_sizes: (6, 768)"]
	140067041510640 -> 140067041510976
	140067041510640 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (6, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041510784 -> 140067041510640
	140066794636144 [label="q_encoder.encoder.information_exchanging_layer.5.attention.output.dense.bias
 (768)" fillcolor=lightblue]
	140066794636144 -> 140067041510784
	140067041510784 [label=AccumulateGrad]
	140067041511744 -> 140067041510640
	140067041511744 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140067041510448 -> 140067041511744
	140067041510448 [label="ViewBackward0
------------------------------
self_sym_sizes: (3, 2, 12, 64)"]
	140067041510544 -> 140067041510448
	140067041510544 [label=CloneBackward0]
	140067041509872 -> 140067041510544
	140067041509872 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041509584 -> 140067041509872
	140067041509584 [label="UnsafeViewBackward0
---------------------------
self_sym_sizes: (36, 2, 64)"]
	140067041509536 -> 140067041509584
	140067041509536 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140066879635216 -> 140067041509536
	140066879635216 [label="ViewBackward0
-----------------------------
self_sym_sizes: (3, 12, 2, 2)"]
	140066879633440 -> 140066879635216
	140066879633440 [label="ExpandBackward0
-----------------------------
self_sym_sizes: (3, 12, 2, 2)"]
	140066879632336 -> 140066879633440
	140066879632336 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140066879635120 -> 140066879632336
	140066879635120 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140066879634832 -> 140066879635120
	140066879634832 [label="AddBackward0
------------
alpha: 1"]
	140066879634592 -> 140066879634832
	140066879634592 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140066879634640 -> 140066879634592
	140066879634640 [label="UnsafeViewBackward0
--------------------------
self_sym_sizes: (36, 2, 2)"]
	140066879634016 -> 140066879634640
	140066879634016 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140066879634112 -> 140066879634016
	140066879634112 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 2, 64)"]
	140066879633584 -> 140066879634112
	140066879633584 [label=CloneBackward0]
	140066879633536 -> 140066879633584
	140066879633536 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 2, 64)"]
	140066879633200 -> 140066879633536
	140066879633200 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140066879632720 -> 140066879633200
	140066879632720 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140066879632672 -> 140066879632720
	140066879632672 [label="ViewBackward0
------------------------
self_sym_sizes: (6, 768)"]
	140066879632096 -> 140066879632672
	140066879632096 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (6, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140066879631904 -> 140066879632096
	140066794635584 [label="q_encoder.encoder.information_exchanging_layer.5.attention.self.query.bias
 (768)" fillcolor=lightblue]
	140066794635584 -> 140066879631904
	140066879631904 [label=AccumulateGrad]
	140066879631856 -> 140066879632096
	140066879631856 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140067041511312 -> 140066879631856
	140067041511312 [label="CatBackward0
------------
dim: 1"]
	140066879630848 -> 140067041511312
	140066879630848 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 1, 768)
start         :                   0
step          :                   1"]
	140066879631472 -> 140066879630848
	140066879631472 [label="SliceBackward0
---------------------------
dim           :           1
end           :           1
self_sym_sizes: (3, 2, 768)
start         :           0
step          :           1"]
	140066879629840 -> 140066879631472
	140066879629840 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:         (3, 2, 768)
start         :                   0
step          :                   1"]
	140066879630608 -> 140066879629840
	140066879630608 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140066879629504 -> 140066879630608
	140066879629504 [label="AddBackward0
------------
alpha: 1"]
	140066879630320 -> 140066879629504
	140066879630320 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140066879629168 -> 140066879630320
	140066879629168 [label="ViewBackward0
------------------------
self_sym_sizes: (6, 768)"]
	140066879628832 -> 140066879629168
	140066879628832 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (6, 3072)
mat1_sym_strides:      (3072, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (3072, 768)
mat2_sym_strides:      (1, 3072)"]
	140066879628976 -> 140066879628832
	140066495932256 [label="q_encoder.encoder.information_exchanging_layer.4.output.dense.bias
 (768)" fillcolor=lightblue]
	140066495932256 -> 140066879628976
	140066879628976 [label=AccumulateGrad]
	140066879630128 -> 140066879628832
	140066879630128 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 2, 3072)"]
	140066879628640 -> 140066879630128
	140066879628640 [label="GeluBackward0
---------------------------
approximate:           none
self       : [saved tensor]"]
	140066879628688 -> 140066879628640
	140066879628688 [label="ViewBackward0
-------------------------
self_sym_sizes: (6, 3072)"]
	140066879627488 -> 140066879628688
	140066879627488 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (6, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (768, 3072)
mat2_sym_strides:       (1, 768)"]
	140066879627248 -> 140066879627488
	140066495938496 [label="q_encoder.encoder.information_exchanging_layer.4.intermediate.dense.bias
 (3072)" fillcolor=lightblue]
	140066495938496 -> 140066879627248
	140066879627248 [label=AccumulateGrad]
	140066879627056 -> 140066879627488
	140066879627056 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140066879630272 -> 140066879627056
	140066879630272 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140066879626432 -> 140066879630272
	140066879626432 [label="AddBackward0
------------
alpha: 1"]
	140066879626336 -> 140066879626432
	140066879626336 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140066879625808 -> 140066879626336
	140066879625808 [label="ViewBackward0
------------------------
self_sym_sizes: (6, 768)"]
	140066879625472 -> 140066879625808
	140066879625472 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (6, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140066879625664 -> 140066879625472
	140066495938096 [label="q_encoder.encoder.information_exchanging_layer.4.attention.output.dense.bias
 (768)" fillcolor=lightblue]
	140066495938096 -> 140066879625664
	140066879625664 [label=AccumulateGrad]
	140066879626912 -> 140066879625472
	140066879626912 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140066879625136 -> 140066879626912
	140066879625136 [label="ViewBackward0
------------------------------
self_sym_sizes: (3, 2, 12, 64)"]
	140066879625184 -> 140066879625136
	140066879625184 [label=CloneBackward0]
	140066879624176 -> 140066879625184
	140066879624176 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140066879623984 -> 140066879624176
	140066879623984 [label="UnsafeViewBackward0
---------------------------
self_sym_sizes: (36, 2, 64)"]
	140066879623936 -> 140066879623984
	140066879623936 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140066879623600 -> 140066879623936
	140066879623600 [label="ViewBackward0
-----------------------------
self_sym_sizes: (3, 12, 2, 2)"]
	140066879623408 -> 140066879623600
	140066879623408 [label="ExpandBackward0
-----------------------------
self_sym_sizes: (3, 12, 2, 2)"]
	140066879623360 -> 140066879623408
	140066879623360 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140066879623072 -> 140066879623360
	140066879623072 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140066879622784 -> 140066879623072
	140066879622784 [label="AddBackward0
------------
alpha: 1"]
	140066879622544 -> 140066879622784
	140066879622544 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140066879622592 -> 140066879622544
	140066879622592 [label="UnsafeViewBackward0
--------------------------
self_sym_sizes: (36, 2, 2)"]
	140066879622160 -> 140066879622592
	140066879622160 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140066879622256 -> 140066879622160
	140066879622256 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 2, 64)"]
	140066879621488 -> 140066879622256
	140066879621488 [label=CloneBackward0]
	140066879621440 -> 140066879621488
	140066879621440 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 2, 64)"]
	140066879620960 -> 140066879621440
	140066879620960 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140066879620768 -> 140066879620960
	140066879620768 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140066879620624 -> 140066879620768
	140066879620624 [label="ViewBackward0
------------------------
self_sym_sizes: (6, 768)"]
	140066879620192 -> 140066879620624
	140066879620192 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (6, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140066879619808 -> 140066879620192
	140066495937536 [label="q_encoder.encoder.information_exchanging_layer.4.attention.self.query.bias
 (768)" fillcolor=lightblue]
	140066495937536 -> 140066879619808
	140066879619808 [label=AccumulateGrad]
	140066879619712 -> 140066879620192
	140066879619712 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140066879626384 -> 140066879619712
	140066879626384 [label="CatBackward0
------------
dim: 1"]
	140066879619616 -> 140066879626384
	140066879619616 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 1, 768)
start         :                   0
step          :                   1"]
	140067041491744 -> 140066879619616
	140067041491744 [label="SliceBackward0
---------------------------
dim           :           1
end           :           1
self_sym_sizes: (3, 2, 768)
start         :           0
step          :           1"]
	140067041488480 -> 140067041491744
	140067041488480 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:         (3, 2, 768)
start         :                   0
step          :                   1"]
	140067041490928 -> 140067041488480
	140067041490928 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140067041486272 -> 140067041490928
	140067041486272 [label="AddBackward0
------------
alpha: 1"]
	140067041490880 -> 140067041486272
	140067041490880 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041485360 -> 140067041490880
	140067041485360 [label="ViewBackward0
------------------------
self_sym_sizes: (6, 768)"]
	140067041483632 -> 140067041485360
	140067041483632 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (6, 3072)
mat1_sym_strides:      (3072, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (3072, 768)
mat2_sym_strides:      (1, 3072)"]
	140067041484544 -> 140067041483632
	140066495937296 [label="q_encoder.encoder.information_exchanging_layer.3.output.dense.bias
 (768)" fillcolor=lightblue]
	140066495937296 -> 140067041484544
	140067041484544 [label=AccumulateGrad]
	140067041490208 -> 140067041483632
	140067041490208 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 2, 3072)"]
	140067041482912 -> 140067041490208
	140067041482912 [label="GeluBackward0
---------------------------
approximate:           none
self       : [saved tensor]"]
	140067041482720 -> 140067041482912
	140067041482720 [label="ViewBackward0
-------------------------
self_sym_sizes: (6, 3072)"]
	140067041480608 -> 140067041482720
	140067041480608 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (6, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (768, 3072)
mat2_sym_strides:       (1, 768)"]
	140067041479600 -> 140067041480608
	140066495937216 [label="q_encoder.encoder.information_exchanging_layer.3.intermediate.dense.bias
 (3072)" fillcolor=lightblue]
	140066495937216 -> 140067041479600
	140067041479600 [label=AccumulateGrad]
	140067041479936 -> 140067041480608
	140067041479936 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140067041490544 -> 140067041479936
	140067041490544 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140067041492704 -> 140067041490544
	140067041492704 [label="AddBackward0
------------
alpha: 1"]
	140067041492560 -> 140067041492704
	140067041492560 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041492272 -> 140067041492560
	140067041492272 [label="ViewBackward0
------------------------
self_sym_sizes: (6, 768)"]
	140067041491888 -> 140067041492272
	140067041491888 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (6, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041492080 -> 140067041491888
	140066495936816 [label="q_encoder.encoder.information_exchanging_layer.3.attention.output.dense.bias
 (768)" fillcolor=lightblue]
	140066495936816 -> 140067041492080
	140067041492080 [label=AccumulateGrad]
	140067041478256 -> 140067041491888
	140067041478256 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140067041491408 -> 140067041478256
	140067041491408 [label="ViewBackward0
------------------------------
self_sym_sizes: (3, 2, 12, 64)"]
	140067041491552 -> 140067041491408
	140067041491552 [label=CloneBackward0]
	140067041490832 -> 140067041491552
	140067041490832 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041490496 -> 140067041490832
	140067041490496 [label="UnsafeViewBackward0
---------------------------
self_sym_sizes: (36, 2, 64)"]
	140067041490400 -> 140067041490496
	140067041490400 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140067041489824 -> 140067041490400
	140067041489824 [label="ViewBackward0
-----------------------------
self_sym_sizes: (3, 12, 2, 2)"]
	140067041489488 -> 140067041489824
	140067041489488 [label="ExpandBackward0
-----------------------------
self_sym_sizes: (3, 12, 2, 2)"]
	140067041489440 -> 140067041489488
	140067041489440 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041489104 -> 140067041489440
	140067041489104 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140067041488816 -> 140067041489104
	140067041488816 [label="AddBackward0
------------
alpha: 1"]
	140067041488672 -> 140067041488816
	140067041488672 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041488720 -> 140067041488672
	140067041488720 [label="UnsafeViewBackward0
--------------------------
self_sym_sizes: (36, 2, 2)"]
	140067041488096 -> 140067041488720
	140067041488096 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140067041488288 -> 140067041488096
	140067041488288 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 2, 64)"]
	140067041487568 -> 140067041488288
	140067041487568 [label=CloneBackward0]
	140067041487520 -> 140067041487568
	140067041487520 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 2, 64)"]
	140067041487088 -> 140067041487520
	140067041487088 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041486800 -> 140067041487088
	140067041486800 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140067041486752 -> 140067041486800
	140067041486752 [label="ViewBackward0
------------------------
self_sym_sizes: (6, 768)"]
	140067041486128 -> 140067041486752
	140067041486128 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (6, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041485984 -> 140067041486128
	140066495936256 [label="q_encoder.encoder.information_exchanging_layer.3.attention.self.query.bias
 (768)" fillcolor=lightblue]
	140066495936256 -> 140067041485984
	140067041485984 [label=AccumulateGrad]
	140067041485936 -> 140067041486128
	140067041485936 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140067041492656 -> 140067041485936
	140067041492656 [label="CatBackward0
------------
dim: 1"]
	140067041485120 -> 140067041492656
	140067041485120 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 1, 768)
start         :                   0
step          :                   1"]
	140067041485792 -> 140067041485120
	140067041485792 [label="SliceBackward0
---------------------------
dim           :           1
end           :           1
self_sym_sizes: (3, 2, 768)
start         :           0
step          :           1"]
	140067041484592 -> 140067041485792
	140067041484592 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:         (3, 2, 768)
start         :                   0
step          :                   1"]
	140067041484880 -> 140067041484592
	140067041484880 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140067041484016 -> 140067041484880
	140067041484016 [label="AddBackward0
------------
alpha: 1"]
	140067041484832 -> 140067041484016
	140067041484832 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041483440 -> 140067041484832
	140067041483440 [label="ViewBackward0
------------------------
self_sym_sizes: (6, 768)"]
	140067041483248 -> 140067041483440
	140067041483248 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (6, 3072)
mat1_sym_strides:      (3072, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (3072, 768)
mat2_sym_strides:      (1, 3072)"]
	140067041483344 -> 140067041483248
	140066495936016 [label="q_encoder.encoder.information_exchanging_layer.2.output.dense.bias
 (768)" fillcolor=lightblue]
	140066495936016 -> 140067041483344
	140067041483344 [label=AccumulateGrad]
	140067041484688 -> 140067041483248
	140067041484688 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 2, 3072)"]
	140067041482768 -> 140067041484688
	140067041482768 [label="GeluBackward0
---------------------------
approximate:           none
self       : [saved tensor]"]
	140067041482960 -> 140067041482768
	140067041482960 [label="ViewBackward0
-------------------------
self_sym_sizes: (6, 3072)"]
	140067041481856 -> 140067041482960
	140067041481856 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (6, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (768, 3072)
mat2_sym_strides:       (1, 768)"]
	140067041481760 -> 140067041481856
	140066495935936 [label="q_encoder.encoder.information_exchanging_layer.2.intermediate.dense.bias
 (3072)" fillcolor=lightblue]
	140066495935936 -> 140067041481760
	140067041481760 [label=AccumulateGrad]
	140067041481664 -> 140067041481856
	140067041481664 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140067041484784 -> 140067041481664
	140067041484784 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140067041480944 -> 140067041484784
	140067041480944 [label="AddBackward0
------------
alpha: 1"]
	140067041480800 -> 140067041480944
	140067041480800 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041480272 -> 140067041480800
	140067041480272 [label="ViewBackward0
------------------------
self_sym_sizes: (6, 768)"]
	140067041479984 -> 140067041480272
	140067041479984 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (6, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041480128 -> 140067041479984
	140066495935536 [label="q_encoder.encoder.information_exchanging_layer.2.attention.output.dense.bias
 (768)" fillcolor=lightblue]
	140066495935536 -> 140067041480128
	140067041480128 [label=AccumulateGrad]
	140067041481568 -> 140067041479984
	140067041481568 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140067041479648 -> 140067041481568
	140067041479648 [label="ViewBackward0
------------------------------
self_sym_sizes: (3, 2, 12, 64)"]
	140067041479744 -> 140067041479648
	140067041479744 [label=CloneBackward0]
	140067041478976 -> 140067041479744
	140067041478976 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041478736 -> 140067041478976
	140067041478736 [label="UnsafeViewBackward0
---------------------------
self_sym_sizes: (36, 2, 64)"]
	140067041478688 -> 140067041478736
	140067041478688 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140067041478448 -> 140067041478688
	140067041478448 [label="ViewBackward0
-----------------------------
self_sym_sizes: (3, 12, 2, 2)"]
	140067041477920 -> 140067041478448
	140067041477920 [label="ExpandBackward0
-----------------------------
self_sym_sizes: (3, 12, 2, 2)"]
	140067041477824 -> 140067041477920
	140067041477824 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041477632 -> 140067041477824
	140067041477632 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140067041477248 -> 140067041477632
	140067041477248 [label="AddBackward0
------------
alpha: 1"]
	140067041477008 -> 140067041477248
	140067041477008 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067054845712 -> 140067041477008
	140067054845712 [label="UnsafeViewBackward0
--------------------------
self_sym_sizes: (36, 2, 2)"]
	140067054841008 -> 140067054845712
	140067054841008 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140067041476864 -> 140067054841008
	140067041476864 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 2, 64)"]
	140067042230224 -> 140067041476864
	140067042230224 [label=CloneBackward0]
	140067042229936 -> 140067042230224
	140067042229936 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 2, 64)"]
	140067042229600 -> 140067042229936
	140067042229600 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067042229408 -> 140067042229600
	140067042229408 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140067042229264 -> 140067042229408
	140067042229264 [label="ViewBackward0
------------------------
self_sym_sizes: (6, 768)"]
	140067042228928 -> 140067042229264
	140067042228928 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (6, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067042228784 -> 140067042228928
	140066495934976 [label="q_encoder.encoder.information_exchanging_layer.2.attention.self.query.bias
 (768)" fillcolor=lightblue]
	140066495934976 -> 140067042228784
	140067042228784 [label=AccumulateGrad]
	140067042228640 -> 140067042228928
	140067042228640 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140067041480848 -> 140067042228640
	140067041480848 [label="CatBackward0
------------
dim: 1"]
	140067042227968 -> 140067041480848
	140067042227968 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 1, 768)
start         :                   0
step          :                   1"]
	140067042228544 -> 140067042227968
	140067042228544 [label="SliceBackward0
---------------------------
dim           :           1
end           :           1
self_sym_sizes: (3, 2, 768)
start         :           0
step          :           1"]
	140067042227392 -> 140067042228544
	140067042227392 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:         (3, 2, 768)
start         :                   0
step          :                   1"]
	140067042227872 -> 140067042227392
	140067042227872 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140067042226960 -> 140067042227872
	140067042226960 [label="AddBackward0
------------
alpha: 1"]
	140067042227824 -> 140067042226960
	140067042227824 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067042226720 -> 140067042227824
	140067042226720 [label="ViewBackward0
------------------------
self_sym_sizes: (6, 768)"]
	140067042226240 -> 140067042226720
	140067042226240 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (6, 3072)
mat1_sym_strides:      (3072, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (3072, 768)
mat2_sym_strides:      (1, 3072)"]
	140067042226528 -> 140067042226240
	140066495934736 [label="q_encoder.encoder.information_exchanging_layer.1.output.dense.bias
 (768)" fillcolor=lightblue]
	140066495934736 -> 140067042226528
	140067042226528 [label=AccumulateGrad]
	140067042227536 -> 140067042226240
	140067042227536 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 2, 3072)"]
	140067042226048 -> 140067042227536
	140067042226048 [label="GeluBackward0
---------------------------
approximate:           none
self       : [saved tensor]"]
	140067042226096 -> 140067042226048
	140067042226096 [label="ViewBackward0
-------------------------
self_sym_sizes: (6, 3072)"]
	140067042225328 -> 140067042226096
	140067042225328 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (6, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (768, 3072)
mat2_sym_strides:       (1, 768)"]
	140067042225232 -> 140067042225328
	140066495934656 [label="q_encoder.encoder.information_exchanging_layer.1.intermediate.dense.bias
 (3072)" fillcolor=lightblue]
	140066495934656 -> 140067042225232
	140067042225232 [label=AccumulateGrad]
	140067042225136 -> 140067042225328
	140067042225136 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140067042227776 -> 140067042225136
	140067042227776 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140067042224464 -> 140067042227776
	140067042224464 [label="AddBackward0
------------
alpha: 1"]
	140067042224272 -> 140067042224464
	140067042224272 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067042223936 -> 140067042224272
	140067042223936 [label="ViewBackward0
------------------------
self_sym_sizes: (6, 768)"]
	140067042223696 -> 140067042223936
	140067042223696 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (6, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067042223792 -> 140067042223696
	140066495934256 [label="q_encoder.encoder.information_exchanging_layer.1.attention.output.dense.bias
 (768)" fillcolor=lightblue]
	140066495934256 -> 140067042223792
	140067042223792 [label=AccumulateGrad]
	140067042224992 -> 140067042223696
	140067042224992 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140067042223216 -> 140067042224992
	140067042223216 [label="ViewBackward0
------------------------------
self_sym_sizes: (3, 2, 12, 64)"]
	140067042223360 -> 140067042223216
	140067042223360 [label=CloneBackward0]
	140067042222880 -> 140067042223360
	140067042222880 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067042222496 -> 140067042222880
	140067042222496 [label="UnsafeViewBackward0
---------------------------
self_sym_sizes: (36, 2, 64)"]
	140067042222400 -> 140067042222496
	140067042222400 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140067042222160 -> 140067042222400
	140067042222160 [label="ViewBackward0
-----------------------------
self_sym_sizes: (3, 12, 2, 2)"]
	140067042221920 -> 140067042222160
	140067042221920 [label="ExpandBackward0
-----------------------------
self_sym_sizes: (3, 12, 2, 2)"]
	140067042221824 -> 140067042221920
	140067042221824 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067042221440 -> 140067042221824
	140067042221440 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140067042221296 -> 140067042221440
	140067042221296 [label="AddBackward0
------------
alpha: 1"]
	140067042221056 -> 140067042221296
	140067042221056 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067042221152 -> 140067042221056
	140067042221152 [label="UnsafeViewBackward0
--------------------------
self_sym_sizes: (36, 2, 2)"]
	140067042220576 -> 140067042221152
	140067042220576 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140067042220768 -> 140067042220576
	140067042220768 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 2, 64)"]
	140067042220240 -> 140067042220768
	140067042220240 [label=CloneBackward0]
	140067042220192 -> 140067042220240
	140067042220192 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 2, 64)"]
	140067042219904 -> 140067042220192
	140067042219904 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067042219568 -> 140067042219904
	140067042219568 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140067042219424 -> 140067042219568
	140067042219424 [label="ViewBackward0
------------------------
self_sym_sizes: (6, 768)"]
	140067042218944 -> 140067042219424
	140067042218944 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (6, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067042218752 -> 140067042218944
	140066495933696 [label="q_encoder.encoder.information_exchanging_layer.1.attention.self.query.bias
 (768)" fillcolor=lightblue]
	140066495933696 -> 140067042218752
	140067042218752 [label=AccumulateGrad]
	140067042218608 -> 140067042218944
	140067042218608 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140067042224416 -> 140067042218608
	140067042224416 [label="CatBackward0
------------
dim: 1"]
	140067042218032 -> 140067042224416
	140067042218032 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 1, 768)
start         :                   0
step          :                   1"]
	140067042218464 -> 140067042218032
	140067042218464 [label="SliceBackward0
---------------------------
dim           :           1
end           :           1
self_sym_sizes: (3, 2, 768)
start         :           0
step          :           1"]
	140067042217264 -> 140067042218464
	140067042217264 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:         (3, 2, 768)
start         :                   0
step          :                   1"]
	140067042217744 -> 140067042217264
	140067042217744 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140067042216928 -> 140067042217744
	140067042216928 [label="AddBackward0
------------
alpha: 1"]
	140067042217648 -> 140067042216928
	140067042217648 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067042216400 -> 140067042217648
	140067042216400 [label="ViewBackward0
------------------------
self_sym_sizes: (6, 768)"]
	140067042216160 -> 140067042216400
	140067042216160 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (6, 3072)
mat1_sym_strides:      (3072, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (3072, 768)
mat2_sym_strides:      (1, 3072)"]
	140067042216304 -> 140067042216160
	140066495933456 [label="q_encoder.encoder.information_exchanging_layer.0.output.dense.bias
 (768)" fillcolor=lightblue]
	140066495933456 -> 140067042216304
	140067042216304 [label=AccumulateGrad]
	140067042217456 -> 140067042216160
	140067042217456 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 2, 3072)"]
	140067042215872 -> 140067042217456
	140067042215872 [label="GeluBackward0
---------------------------
approximate:           none
self       : [saved tensor]"]
	140067042215968 -> 140067042215872
	140067042215968 [label="ViewBackward0
-------------------------
self_sym_sizes: (6, 3072)"]
	140067042215344 -> 140067042215968
	140067042215344 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (6, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (768, 3072)
mat2_sym_strides:       (1, 768)"]
	140067042215296 -> 140067042215344
	140066495933376 [label="q_encoder.encoder.information_exchanging_layer.0.intermediate.dense.bias
 (3072)" fillcolor=lightblue]
	140066495933376 -> 140067042215296
	140067042215296 [label=AccumulateGrad]
	140067042215248 -> 140067042215344
	140067042215248 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140067042217600 -> 140067042215248
	140067042217600 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140067042214576 -> 140067042217600
	140067042214576 [label="AddBackward0
------------
alpha: 1"]
	140067042214336 -> 140067042214576
	140067042214336 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140077287889680 -> 140067042214336
	140077287889680 [label="ViewBackward0
------------------------
self_sym_sizes: (6, 768)"]
	140067042215056 -> 140077287889680
	140067042215056 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (6, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067042214000 -> 140067042215056
	140066495932976 [label="q_encoder.encoder.information_exchanging_layer.0.attention.output.dense.bias
 (768)" fillcolor=lightblue]
	140066495932976 -> 140067042214000
	140067042214000 [label=AccumulateGrad]
	140067042214048 -> 140067042215056
	140067042214048 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140067053402288 -> 140067042214048
	140067053402288 [label="ViewBackward0
------------------------------
self_sym_sizes: (3, 2, 12, 64)"]
	140067053396720 -> 140067053402288
	140067053396720 [label=CloneBackward0]
	140067053400080 -> 140067053396720
	140067053400080 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067042098576 -> 140067053400080
	140067042098576 [label="UnsafeViewBackward0
---------------------------
self_sym_sizes: (36, 2, 64)"]
	140067042098480 -> 140067042098576
	140067042098480 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140067042098048 -> 140067042098480
	140067042098048 [label="ViewBackward0
-----------------------------
self_sym_sizes: (3, 12, 2, 2)"]
	140067042097808 -> 140067042098048
	140067042097808 [label="ExpandBackward0
-----------------------------
self_sym_sizes: (3, 12, 2, 2)"]
	140067042097760 -> 140067042097808
	140067042097760 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067042097376 -> 140067042097760
	140067042097376 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140067042096944 -> 140067042097376
	140067042096944 [label="AddBackward0
------------
alpha: 1"]
	140067042096752 -> 140067042096944
	140067042096752 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067042096800 -> 140067042096752
	140067042096800 [label="UnsafeViewBackward0
--------------------------
self_sym_sizes: (36, 2, 2)"]
	140067042096272 -> 140067042096800
	140067042096272 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140067042096368 -> 140067042096272
	140067042096368 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 2, 64)"]
	140067042095984 -> 140067042096368
	140067042095984 [label=CloneBackward0]
	140067042095936 -> 140067042095984
	140067042095936 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 2, 64)"]
	140067042095600 -> 140067042095936
	140067042095600 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067042095360 -> 140067042095600
	140067042095360 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140067042095312 -> 140067042095360
	140067042095312 [label="ViewBackward0
------------------------
self_sym_sizes: (6, 768)"]
	140067042095072 -> 140067042095312
	140067042095072 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (6, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067042094928 -> 140067042095072
	140066495932496 [label="q_encoder.encoder.information_exchanging_layer.0.attention.self.query.bias
 (768)" fillcolor=lightblue]
	140066495932496 -> 140067042094928
	140067042094928 [label=AccumulateGrad]
	140067042094832 -> 140067042095072
	140067042094832 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140067042214432 -> 140067042094832
	140067042214432 [label="CatBackward0
------------
dim: 1"]
	140067042094304 -> 140067042214432
	140067042094304 [label="RepeatBackward0
---------------------------
repeats       :   (3, 1, 1)
self_sym_sizes: (1, 1, 768)"]
	140067042094784 -> 140067042094304
	140067042094784 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (1, 1, 768)
start         :                   0
step          :                   1"]
	140067042093920 -> 140067042094784
	140067042093920 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:         (1, 1, 768)
start         :                   0
step          :                   1"]
	140067042093200 -> 140067042093920
	140067042093200 [label="UnsqueezeBackward0
------------------
dim: 0"]
	140067042094064 -> 140067042093200
	140066489389200 [label="q_encoder.doc_embeddings
 (1, 768)" fillcolor=lightblue]
	140066489389200 -> 140067042094064
	140067042094064 [label=AccumulateGrad]
	140067042094496 -> 140067042214432
	140067042094496 [label=CloneBackward0]
	140067042094208 -> 140067042094496
	140067042094208 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 1, 768)
start         :                   0
step          :                   1"]
	140067042092672 -> 140067042094208
	140067042092672 [label="SelectBackward0
-------------------------------
dim           :               2
index         :               1
self_sym_sizes: (3, 1, 11, 768)"]
	140067042092288 -> 140067042092672
	140067042092288 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:     (3, 1, 11, 768)
start         :                   0
step          :                   1"]
	140067042093584 -> 140067042092288
	140067042093584 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:     (3, 1, 11, 768)
start         :                   0
step          :                   1"]
	140067042092000 -> 140067042093584
	140067042092000 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 11, 768)"]
	140067042092528 -> 140067042092000
	140067042092528 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140067042093440 -> 140067042092528
	140067042093440 [label="AddBackward0
------------
alpha: 1"]
	140067042092144 -> 140067042093440
	140067042092144 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067042091472 -> 140067042092144
	140067042091472 [label="ViewBackward0
-------------------------
self_sym_sizes: (33, 768)"]
	140067042091184 -> 140067042091472
	140067042091184 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :     (33, 3072)
mat1_sym_strides:      (3072, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (3072, 768)
mat2_sym_strides:      (1, 3072)"]
	140067042091328 -> 140067042091184
	140066489271792 [label="q_encoder.encoder.text_encoding_layer.0.output.dense.bias
 (768)" fillcolor=lightblue]
	140066489271792 -> 140067042091328
	140067042091328 [label=AccumulateGrad]
	140067042093728 -> 140067042091184
	140067042093728 [label="ViewBackward0
-----------------------------
self_sym_sizes: (3, 11, 3072)"]
	140067042090992 -> 140067042093728
	140067042090992 [label="GeluBackward0
---------------------------
approximate:           none
self       : [saved tensor]"]
	140067042091040 -> 140067042090992
	140067042091040 [label="ViewBackward0
--------------------------
self_sym_sizes: (33, 3072)"]
	140067042090320 -> 140067042091040
	140067042090320 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (33, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (768, 3072)
mat2_sym_strides:       (1, 768)"]
	140067042090272 -> 140067042090320
	140066489085824 [label="q_encoder.encoder.text_encoding_layer.0.intermediate.dense.bias
 (3072)" fillcolor=lightblue]
	140066489085824 -> 140067042090272
	140067042090272 [label=AccumulateGrad]
	140067042090224 -> 140067042090320
	140067042090224 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 11, 768)"]
	140067042091760 -> 140067042090224
	140067042091760 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140067042089456 -> 140067042091760
	140067042089456 [label="AddBackward0
------------
alpha: 1"]
	140067042089264 -> 140067042089456
	140067042089264 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067042089072 -> 140067042089264
	140067042089072 [label="ViewBackward0
-------------------------
self_sym_sizes: (33, 768)"]
	140067042088784 -> 140067042089072
	140067042088784 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (33, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067042088928 -> 140067042088784
	140066489442912 [label="q_encoder.encoder.text_encoding_layer.0.attention.output.dense.bias
 (768)" fillcolor=lightblue]
	140066489442912 -> 140067042088928
	140067042088928 [label=AccumulateGrad]
	140067042089984 -> 140067042088784
	140067042089984 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 11, 768)"]
	140067042088592 -> 140067042089984
	140067042088592 [label="ViewBackward0
-------------------------------
self_sym_sizes: (3, 11, 12, 64)"]
	140067042088640 -> 140067042088592
	140067042088640 [label=CloneBackward0]
	140067042087968 -> 140067042088640
	140067042087968 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067042087776 -> 140067042087968
	140067042087776 [label="UnsafeViewBackward0
----------------------------
self_sym_sizes: (36, 11, 64)"]
	140067042087728 -> 140067042087776
	140067042087728 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140067042087056 -> 140067042087728
	140067042087056 [label="ViewBackward0
-------------------------------
self_sym_sizes: (3, 12, 11, 11)"]
	140067042086768 -> 140067042087056
	140067042086768 [label="ExpandBackward0
-------------------------------
self_sym_sizes: (3, 12, 11, 11)"]
	140067042086672 -> 140067042086768
	140067042086672 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067042086384 -> 140067042086672
	140067042086384 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140067042086096 -> 140067042086384
	140067042086096 [label="AddBackward0
------------
alpha: 1"]
	140067042085856 -> 140067042086096
	140067042085856 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067042086000 -> 140067042085856
	140067042086000 [label="UnsafeViewBackward0
----------------------------
self_sym_sizes: (36, 11, 11)"]
	140067042085376 -> 140067042086000
	140067042085376 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140067042085472 -> 140067042085376
	140067042085472 [label="UnsafeViewBackward0
-------------------------------
self_sym_sizes: (3, 12, 11, 64)"]
	140067042085088 -> 140067042085472
	140067042085088 [label=CloneBackward0]
	140067042085040 -> 140067042085088
	140067042085040 [label="ExpandBackward0
-------------------------------
self_sym_sizes: (3, 12, 11, 64)"]
	140067042084512 -> 140067042085040
	140067042084512 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067042084320 -> 140067042084512
	140067042084320 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 11, 768)"]
	140067042084272 -> 140067042084320
	140067042084272 [label="ViewBackward0
-------------------------
self_sym_sizes: (33, 768)"]
	140067042083840 -> 140067042084272
	140067042083840 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (33, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067042083648 -> 140067042083840
	140066489622816 [label="q_encoder.encoder.text_encoding_layer.0.attention.self.query.bias
 (768)" fillcolor=lightblue]
	140066489622816 -> 140067042083648
	140067042083648 [label=AccumulateGrad]
	140067042083552 -> 140067042083840
	140067042083552 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 11, 768)"]
	140067042089408 -> 140067042083552
	140067042089408 [label="CatBackward0
------------
dim: 1"]
	140067042099152 -> 140067042089408
	140067042099152 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067042099104 -> 140067042099152
	140067042099104 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140067042083408 -> 140067042099104
	140067042083408 [label="AddBackward0
------------
alpha: 1"]
	140066907257248 -> 140067042083408
	140066907257248 [label="AddBackward0
------------
alpha: 1"]
	140066907257488 -> 140066907257248
	140066907257488 [label="EmbeddingBackward0
------------------------------------
indices             : [saved tensor]
padding_idx         :              0
scale_grad_by_freq  :          False
sparse              :          False
weight_sym_argsize_0:          30522"]
	140066907254560 -> 140066907257488
	140067041395888 [label="q_encoder.embeddings.word_embeddings.weight
 (30522, 768)" fillcolor=lightblue]
	140067041395888 -> 140066907254560
	140066907254560 [label=AccumulateGrad]
	140066907257824 -> 140066907257248
	140066907257824 [label="EmbeddingBackward0
------------------------------------------
indices             :       [saved tensor]
padding_idx         : 18446744073709551615
scale_grad_by_freq  :                False
sparse              :                False
weight_sym_argsize_0:                    2"]
	140066907255232 -> 140066907257824
	140067041400848 [label="q_encoder.embeddings.token_type_embeddings.weight
 (2, 768)" fillcolor=lightblue]
	140067041400848 -> 140066907255232
	140066907255232 [label=AccumulateGrad]
	140066907256384 -> 140067042083408
	140066907256384 [label="EmbeddingBackward0
------------------------------------------
indices             :       [saved tensor]
padding_idx         : 18446744073709551615
scale_grad_by_freq  :                False
sparse              :                False
weight_sym_argsize_0:                  512"]
	140066907253456 -> 140066907256384
	140067041395168 [label="q_encoder.embeddings.position_embeddings.weight
 (512, 768)" fillcolor=lightblue]
	140067041395168 -> 140066907253456
	140066907253456 [label=AccumulateGrad]
	140066907258256 -> 140067042099104
	140066489450432 [label="q_encoder.embeddings.LayerNorm.weight
 (768)" fillcolor=lightblue]
	140066489450432 -> 140066907258256
	140066907258256 [label=AccumulateGrad]
	140066907258064 -> 140067042099104
	140066731344912 [label="q_encoder.embeddings.LayerNorm.bias
 (768)" fillcolor=lightblue]
	140066731344912 -> 140066907258064
	140066907258064 [label=AccumulateGrad]
	140067042082880 -> 140067042083840
	140067042082880 [label=TBackward0]
	140067042083072 -> 140067042082880
	140066489534256 [label="q_encoder.encoder.text_encoding_layer.0.attention.self.query.weight
 (768, 768)" fillcolor=lightblue]
	140066489534256 -> 140067042083072
	140067042083072 [label=AccumulateGrad]
	140067042085616 -> 140067042085376
	140067042085616 [label="UnsafeViewBackward0
-------------------------------
self_sym_sizes: (3, 12, 64, 11)"]
	140067042085184 -> 140067042085616
	140067042085184 [label=CloneBackward0]
	140067042084176 -> 140067042085184
	140067042084176 [label="ExpandBackward0
-------------------------------
self_sym_sizes: (3, 12, 64, 11)"]
	140067042083936 -> 140067042084176
	140067042083936 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551615
dim1: 18446744073709551614"]
	140067042083360 -> 140067042083936
	140067042083360 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067042085712 -> 140067042083360
	140067042085712 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 11, 768)"]
	140066907254704 -> 140067042085712
	140066907254704 [label="ViewBackward0
-------------------------
self_sym_sizes: (33, 768)"]
	140066907254320 -> 140066907254704
	140066907254320 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (33, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140066907251536 -> 140066907254320
	140066488843184 [label="q_encoder.encoder.text_encoding_layer.0.attention.self.key.bias
 (768)" fillcolor=lightblue]
	140066488843184 -> 140066907251536
	140066907251536 [label=AccumulateGrad]
	140066907248752 -> 140066907254320
	140066907248752 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 11, 768)"]
	140067042089408 -> 140066907248752
	140066907257776 -> 140066907254320
	140066907257776 [label=TBackward0]
	140066907249424 -> 140066907257776
	140066488836464 [label="q_encoder.encoder.text_encoding_layer.0.attention.self.key.weight
 (768, 768)" fillcolor=lightblue]
	140066488836464 -> 140066907249424
	140066907249424 [label=AccumulateGrad]
	140067042087152 -> 140067042087728
	140067042087152 [label="UnsafeViewBackward0
-------------------------------
self_sym_sizes: (3, 12, 11, 64)"]
	140067042086864 -> 140067042087152
	140067042086864 [label=CloneBackward0]
	140067042086048 -> 140067042086864
	140067042086048 [label="ExpandBackward0
-------------------------------
self_sym_sizes: (3, 12, 11, 64)"]
	140067042085520 -> 140067042086048
	140067042085520 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067042084704 -> 140067042085520
	140067042084704 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 11, 768)"]
	140067042082928 -> 140067042084704
	140067042082928 [label="ViewBackward0
-------------------------
self_sym_sizes: (33, 768)"]
	140067042087008 -> 140067042082928
	140067042087008 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (33, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140066907248080 -> 140067042087008
	140066488842864 [label="q_encoder.encoder.text_encoding_layer.0.attention.self.value.bias
 (768)" fillcolor=lightblue]
	140066488842864 -> 140066907248080
	140066907248080 [label=AccumulateGrad]
	140066907251824 -> 140067042087008
	140066907251824 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 11, 768)"]
	140067042089408 -> 140066907251824
	140066907256096 -> 140067042087008
	140066907256096 [label=TBackward0]
	140066907245920 -> 140066907256096
	140066489394000 [label="q_encoder.encoder.text_encoding_layer.0.attention.self.value.weight
 (768, 768)" fillcolor=lightblue]
	140066489394000 -> 140066907245920
	140066907245920 [label=AccumulateGrad]
	140067042088256 -> 140067042088784
	140067042088256 [label=TBackward0]
	140067042088544 -> 140067042088256
	140066488831184 [label="q_encoder.encoder.text_encoding_layer.0.attention.output.dense.weight
 (768, 768)" fillcolor=lightblue]
	140066488831184 -> 140067042088544
	140067042088544 [label=AccumulateGrad]
	140067042089408 -> 140067042089456
	140067042089792 -> 140067042091760
	140066489436352 [label="q_encoder.encoder.text_encoding_layer.0.attention.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	140066489436352 -> 140067042089792
	140067042089792 [label=AccumulateGrad]
	140067042089120 -> 140067042091760
	140066489434192 [label="q_encoder.encoder.text_encoding_layer.0.attention.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	140066489434192 -> 140067042089120
	140067042089120 [label=AccumulateGrad]
	140067042089600 -> 140067042090320
	140067042089600 [label=TBackward0]
	140067042089216 -> 140067042089600
	140066489085664 [label="q_encoder.encoder.text_encoding_layer.0.intermediate.dense.weight
 (3072, 768)" fillcolor=lightblue]
	140066489085664 -> 140067042089216
	140067042089216 [label=AccumulateGrad]
	140067042090560 -> 140067042091184
	140067042090560 [label=TBackward0]
	140067042090848 -> 140067042090560
	140066489443552 [label="q_encoder.encoder.text_encoding_layer.0.output.dense.weight
 (768, 3072)" fillcolor=lightblue]
	140066489443552 -> 140067042090848
	140067042090848 [label=AccumulateGrad]
	140067042091760 -> 140067042093440
	140067042092240 -> 140067042092528
	140066489086064 [label="q_encoder.encoder.text_encoding_layer.0.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	140066489086064 -> 140067042092240
	140067042092240 [label=AccumulateGrad]
	140067042091568 -> 140067042092528
	140066489085904 [label="q_encoder.encoder.text_encoding_layer.0.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	140066489085904 -> 140067042091568
	140067042091568 [label=AccumulateGrad]
	140067042094400 -> 140067042095072
	140067042094400 [label=TBackward0]
	140067042094112 -> 140067042094400
	140066495932416 [label="q_encoder.encoder.information_exchanging_layer.0.attention.self.query.weight
 (768, 768)" fillcolor=lightblue]
	140066495932416 -> 140067042094112
	140067042094112 [label=AccumulateGrad]
	140067042096560 -> 140067042096272
	140067042096560 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 64, 2)"]
	140067042096176 -> 140067042096560
	140067042096176 [label=CloneBackward0]
	140067042095264 -> 140067042096176
	140067042095264 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 64, 2)"]
	140067042095120 -> 140067042095264
	140067042095120 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551615
dim1: 18446744073709551614"]
	140067042092864 -> 140067042095120
	140067042092864 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067042092096 -> 140067042092864
	140067042092096 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140067042094736 -> 140067042092096
	140067042094736 [label="ViewBackward0
------------------------
self_sym_sizes: (6, 768)"]
	140067042091664 -> 140067042094736
	140067042091664 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (6, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067042091376 -> 140067042091664
	140066495932656 [label="q_encoder.encoder.information_exchanging_layer.0.attention.self.key.bias
 (768)" fillcolor=lightblue]
	140066495932656 -> 140067042091376
	140067042091376 [label=AccumulateGrad]
	140067042091232 -> 140067042091664
	140067042091232 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140067042214432 -> 140067042091232
	140067042096704 -> 140067042091664
	140067042096704 [label=TBackward0]
	140067042088832 -> 140067042096704
	140066495932576 [label="q_encoder.encoder.information_exchanging_layer.0.attention.self.key.weight
 (768, 768)" fillcolor=lightblue]
	140066495932576 -> 140067042088832
	140067042088832 [label=AccumulateGrad]
	140067042098144 -> 140067042098480
	140067042098144 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 2, 64)"]
	140067042097904 -> 140067042098144
	140067042097904 [label=CloneBackward0]
	140067042097040 -> 140067042097904
	140067042097040 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 2, 64)"]
	140067042096512 -> 140067042097040
	140067042096512 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067042095744 -> 140067042096512
	140067042095744 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140067042094448 -> 140067042095744
	140067042094448 [label="ViewBackward0
------------------------
self_sym_sizes: (6, 768)"]
	140067042092432 -> 140067042094448
	140067042092432 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (6, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067042089312 -> 140067042092432
	140066495932816 [label="q_encoder.encoder.information_exchanging_layer.0.attention.self.value.bias
 (768)" fillcolor=lightblue]
	140066495932816 -> 140067042089312
	140067042089312 [label=AccumulateGrad]
	140067042094640 -> 140067042092432
	140067042094640 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140067042214432 -> 140067042094640
	140067042098000 -> 140067042092432
	140067042098000 [label=TBackward0]
	140067042089888 -> 140067042098000
	140066495932736 [label="q_encoder.encoder.information_exchanging_layer.0.attention.self.value.weight
 (768, 768)" fillcolor=lightblue]
	140066495932736 -> 140067042089888
	140067042089888 [label=AccumulateGrad]
	140067053394368 -> 140067042215056
	140067053394368 [label=TBackward0]
	140067053398352 -> 140067053394368
	140066495932896 [label="q_encoder.encoder.information_exchanging_layer.0.attention.output.dense.weight
 (768, 768)" fillcolor=lightblue]
	140066495932896 -> 140067053398352
	140067053398352 [label=AccumulateGrad]
	140067042214432 -> 140067042214576
	140067042214768 -> 140067042217600
	140066495933136 [label="q_encoder.encoder.information_exchanging_layer.0.attention.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	140066495933136 -> 140067042214768
	140067042214768 [label=AccumulateGrad]
	140067042214096 -> 140067042217600
	140066495933056 [label="q_encoder.encoder.information_exchanging_layer.0.attention.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	140066495933056 -> 140067042214096
	140067042214096 [label=AccumulateGrad]
	140067042214624 -> 140067042215344
	140067042214624 [label=TBackward0]
	140067042214192 -> 140067042214624
	140066495933296 [label="q_encoder.encoder.information_exchanging_layer.0.intermediate.dense.weight
 (3072, 768)" fillcolor=lightblue]
	140066495933296 -> 140067042214192
	140067042214192 [label=AccumulateGrad]
	140067042215488 -> 140067042216160
	140067042215488 [label=TBackward0]
	140067042215776 -> 140067042215488
	140066495930976 [label="q_encoder.encoder.information_exchanging_layer.0.output.dense.weight
 (768, 3072)" fillcolor=lightblue]
	140066495930976 -> 140067042215776
	140067042215776 [label=AccumulateGrad]
	140067042217600 -> 140067042216928
	140067042216832 -> 140067042217744
	140066495933616 [label="q_encoder.encoder.information_exchanging_layer.0.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	140066495933616 -> 140067042216832
	140067042216832 [label=AccumulateGrad]
	140067042216496 -> 140067042217744
	140066495933536 [label="q_encoder.encoder.information_exchanging_layer.0.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	140066495933536 -> 140067042216496
	140067042216496 [label=AccumulateGrad]
	140067042218272 -> 140067042224416
	140067042218272 [label=CloneBackward0]
	140067042217984 -> 140067042218272
	140067042217984 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 1, 768)
start         :                   0
step          :                   1"]
	140067042217120 -> 140067042217984
	140067042217120 [label="SelectBackward0
-------------------------------
dim           :               2
index         :               1
self_sym_sizes: (3, 1, 11, 768)"]
	140067042214240 -> 140067042217120
	140067042214240 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:     (3, 1, 11, 768)
start         :                   0
step          :                   1"]
	140067053397488 -> 140067042214240
	140067053397488 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:     (3, 1, 11, 768)
start         :                   0
step          :                   1"]
	140067042216352 -> 140067053397488
	140067042216352 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 11, 768)"]
	140067042214720 -> 140067042216352
	140067042214720 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140067042216592 -> 140067042214720
	140067042216592 [label="AddBackward0
------------
alpha: 1"]
	140067042215632 -> 140067042216592
	140067042215632 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067042098864 -> 140067042215632
	140067042098864 [label="ViewBackward0
-------------------------
self_sym_sizes: (33, 768)"]
	140067042092960 -> 140067042098864
	140067042092960 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :     (33, 3072)
mat1_sym_strides:      (3072, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (3072, 768)
mat2_sym_strides:      (1, 3072)"]
	140067042098336 -> 140067042092960
	140066489085984 [label="q_encoder.encoder.text_encoding_layer.1.output.dense.bias
 (768)" fillcolor=lightblue]
	140066489085984 -> 140067042098336
	140067042098336 [label=AccumulateGrad]
	140067042096320 -> 140067042092960
	140067042096320 [label="ViewBackward0
-----------------------------
self_sym_sizes: (3, 11, 3072)"]
	140067042090800 -> 140067042096320
	140067042090800 [label="GeluBackward0
---------------------------
approximate:           none
self       : [saved tensor]"]
	140067042091712 -> 140067042090800
	140067042091712 [label="ViewBackward0
--------------------------
self_sym_sizes: (33, 3072)"]
	140067042089024 -> 140067042091712
	140067042089024 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (33, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (768, 3072)
mat2_sym_strides:       (1, 768)"]
	140067042087872 -> 140067042089024
	140066489087104 [label="q_encoder.encoder.text_encoding_layer.1.intermediate.dense.bias
 (3072)" fillcolor=lightblue]
	140066489087104 -> 140067042087872
	140067042087872 [label=AccumulateGrad]
	140067042088064 -> 140067042089024
	140067042088064 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 11, 768)"]
	140067042097856 -> 140067042088064
	140067042097856 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140067042086432 -> 140067042097856
	140067042086432 [label="AddBackward0
------------
alpha: 1"]
	140067042088352 -> 140067042086432
	140067042088352 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067042087536 -> 140067042088352
	140067042087536 [label="ViewBackward0
-------------------------
self_sym_sizes: (33, 768)"]
	140066907247648 -> 140067042087536
	140066907247648 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (33, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140066907243952 -> 140066907247648
	140066489086704 [label="q_encoder.encoder.text_encoding_layer.1.attention.output.dense.bias
 (768)" fillcolor=lightblue]
	140066489086704 -> 140066907243952
	140066907243952 [label=AccumulateGrad]
	140066907249136 -> 140066907247648
	140066907249136 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 11, 768)"]
	140066907246880 -> 140066907249136
	140066907246880 [label="ViewBackward0
-------------------------------
self_sym_sizes: (3, 11, 12, 64)"]
	140066907247456 -> 140066907246880
	140066907247456 [label=CloneBackward0]
	140066907256624 -> 140066907247456
	140066907256624 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140066907245440 -> 140066907256624
	140066907245440 [label="UnsafeViewBackward0
----------------------------
self_sym_sizes: (36, 11, 64)"]
	140066907244048 -> 140066907245440
	140066907244048 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140066907242848 -> 140066907244048
	140066907242848 [label="ViewBackward0
-------------------------------
self_sym_sizes: (3, 12, 11, 11)"]
	140066907243568 -> 140066907242848
	140066907243568 [label="ExpandBackward0
-------------------------------
self_sym_sizes: (3, 12, 11, 11)"]
	140066907254032 -> 140066907243568
	140066907254032 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140066907258832 -> 140066907254032
	140066907258832 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140066907258352 -> 140066907258832
	140066907258352 [label="AddBackward0
------------
alpha: 1"]
	140066907258304 -> 140066907258352
	140066907258304 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140066907258160 -> 140066907258304
	140066907258160 [label="UnsafeViewBackward0
----------------------------
self_sym_sizes: (36, 11, 11)"]
	140066907257872 -> 140066907258160
	140066907257872 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140066907257728 -> 140066907257872
	140066907257728 [label="UnsafeViewBackward0
-------------------------------
self_sym_sizes: (3, 12, 11, 64)"]
	140066907257680 -> 140066907257728
	140066907257680 [label=CloneBackward0]
	140066907257200 -> 140066907257680
	140066907257200 [label="ExpandBackward0
-------------------------------
self_sym_sizes: (3, 12, 11, 64)"]
	140066907257152 -> 140066907257200
	140066907257152 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140066907256912 -> 140066907257152
	140066907256912 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 11, 768)"]
	140066907256576 -> 140066907256912
	140066907256576 [label="ViewBackward0
-------------------------
self_sym_sizes: (33, 768)"]
	140066907256336 -> 140066907256576
	140066907256336 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (33, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140066907256000 -> 140066907256336
	140066489086224 [label="q_encoder.encoder.text_encoding_layer.1.attention.self.query.bias
 (768)" fillcolor=lightblue]
	140066489086224 -> 140066907256000
	140066907256000 [label=AccumulateGrad]
	140066907256240 -> 140066907256336
	140066907256240 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 11, 768)"]
	140067042085136 -> 140066907256240
	140067042085136 [label="ViewBackward0
-------------------------------
self_sym_sizes: (3, 1, 11, 768)"]
	140066907255760 -> 140067042085136
	140066907255760 [label="AsStridedBackward0
------------------------------------
size          :      (3, 1, 11, 768)
storage_offset:                    0
stride        : (8448, 8448, 768, 1)"]
	140066907255328 -> 140066907255760
	140066907255328 [label=CopySlices]
	140067042092528 -> 140066907255328
	140066907255088 -> 140066907255328
	140066907255088 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 1, 768)
start         :                   0
step          :                   1"]
	140066907255280 -> 140066907255088
	140066907255280 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:         (3, 2, 768)
start         :                   1
step          :                   1"]
	140066907254080 -> 140066907255280
	140066907254080 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:         (3, 2, 768)
start         :                   0
step          :                   1"]
	140067042217744 -> 140066907254080
	140066907257536 -> 140066907256336
	140066907257536 [label=TBackward0]
	140066907255952 -> 140066907257536
	140066489086144 [label="q_encoder.encoder.text_encoding_layer.1.attention.self.query.weight
 (768, 768)" fillcolor=lightblue]
	140066489086144 -> 140066907255952
	140066907255952 [label=AccumulateGrad]
	140066907258208 -> 140066907257872
	140066907258208 [label="UnsafeViewBackward0
-------------------------------
self_sym_sizes: (3, 12, 64, 11)"]
	140066907258112 -> 140066907258208
	140066907258112 [label=CloneBackward0]
	140066907256528 -> 140066907258112
	140066907256528 [label="ExpandBackward0
-------------------------------
self_sym_sizes: (3, 12, 64, 11)"]
	140066907256864 -> 140066907256528
	140066907256864 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551615
dim1: 18446744073709551614"]
	140066907255136 -> 140066907256864
	140066907255136 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140066907253936 -> 140066907255136
	140066907253936 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 11, 768)"]
	140066907256144 -> 140066907253936
	140066907256144 [label="ViewBackward0
-------------------------
self_sym_sizes: (33, 768)"]
	140066907255712 -> 140066907256144
	140066907255712 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (33, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140066907253792 -> 140066907255712
	140066489086384 [label="q_encoder.encoder.text_encoding_layer.1.attention.self.key.bias
 (768)" fillcolor=lightblue]
	140066489086384 -> 140066907253792
	140066907253792 [label=AccumulateGrad]
	140066907253696 -> 140066907255712
	140066907253696 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 11, 768)"]
	140067042085136 -> 140066907253696
	140066907257344 -> 140066907255712
	140066907257344 [label=TBackward0]
	140066907253552 -> 140066907257344
	140066489086304 [label="q_encoder.encoder.text_encoding_layer.1.attention.self.key.weight
 (768, 768)" fillcolor=lightblue]
	140066489086304 -> 140066907253552
	140066907253552 [label=AccumulateGrad]
	140066907243664 -> 140066907244048
	140066907243664 [label="UnsafeViewBackward0
-------------------------------
self_sym_sizes: (3, 12, 11, 64)"]
	140066907249760 -> 140066907243664
	140066907249760 [label=CloneBackward0]
	140066907258784 -> 140066907249760
	140066907258784 [label="ExpandBackward0
-------------------------------
self_sym_sizes: (3, 12, 11, 64)"]
	140066907258592 -> 140066907258784
	140066907258592 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140066907257104 -> 140066907258592
	140066907257104 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 11, 768)"]
	140066907255616 -> 140066907257104
	140066907255616 [label="ViewBackward0
-------------------------
self_sym_sizes: (33, 768)"]
	140066907255040 -> 140066907255616
	140066907255040 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (33, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140066907253984 -> 140066907255040
	140066489086544 [label="q_encoder.encoder.text_encoding_layer.1.attention.self.value.bias
 (768)" fillcolor=lightblue]
	140066489086544 -> 140066907253984
	140066907253984 [label=AccumulateGrad]
	140066907255808 -> 140066907255040
	140066907255808 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 11, 768)"]
	140067042085136 -> 140066907255808
	140066907242608 -> 140066907255040
	140066907242608 [label=TBackward0]
	140066907253264 -> 140066907242608
	140066489086464 [label="q_encoder.encoder.text_encoding_layer.1.attention.self.value.weight
 (768, 768)" fillcolor=lightblue]
	140066489086464 -> 140066907253264
	140066907253264 [label=AccumulateGrad]
	140066907252160 -> 140066907247648
	140066907252160 [label=TBackward0]
	140066907247264 -> 140066907252160
	140066489086624 [label="q_encoder.encoder.text_encoding_layer.1.attention.output.dense.weight
 (768, 768)" fillcolor=lightblue]
	140066489086624 -> 140066907247264
	140066907247264 [label=AccumulateGrad]
	140067042085136 -> 140067042086432
	140067042088400 -> 140067042097856
	140066489086864 [label="q_encoder.encoder.text_encoding_layer.1.attention.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	140066489086864 -> 140067042088400
	140067042088400 [label=AccumulateGrad]
	140067042086240 -> 140067042097856
	140066489086784 [label="q_encoder.encoder.text_encoding_layer.1.attention.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	140066489086784 -> 140067042086240
	140067042086240 [label=AccumulateGrad]
	140067042089696 -> 140067042089024
	140067042089696 [label=TBackward0]
	140067042084368 -> 140067042089696
	140066489087024 [label="q_encoder.encoder.text_encoding_layer.1.intermediate.dense.weight
 (3072, 768)" fillcolor=lightblue]
	140066489087024 -> 140067042084368
	140067042084368 [label=AccumulateGrad]
	140067042098912 -> 140067042092960
	140067042098912 [label=TBackward0]
	140067042090416 -> 140067042098912
	140066489086944 [label="q_encoder.encoder.text_encoding_layer.1.output.dense.weight
 (768, 3072)" fillcolor=lightblue]
	140066489086944 -> 140067042090416
	140067042090416 [label=AccumulateGrad]
	140067042097856 -> 140067042216592
	140067042215392 -> 140067042214720
	140066489087344 [label="q_encoder.encoder.text_encoding_layer.1.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	140066489087344 -> 140067042215392
	140067042215392 [label=AccumulateGrad]
	140067042215584 -> 140067042214720
	140066489087184 [label="q_encoder.encoder.text_encoding_layer.1.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	140066489087184 -> 140067042215584
	140067042215584 [label=AccumulateGrad]
	140067042218080 -> 140067042218944
	140067042218080 [label=TBackward0]
	140067053399888 -> 140067042218080
	140066495933216 [label="q_encoder.encoder.information_exchanging_layer.1.attention.self.query.weight
 (768, 768)" fillcolor=lightblue]
	140066495933216 -> 140067053399888
	140067053399888 [label=AccumulateGrad]
	140067042220960 -> 140067042220576
	140067042220960 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 64, 2)"]
	140067042220384 -> 140067042220960
	140067042220384 [label=CloneBackward0]
	140067042219328 -> 140067042220384
	140067042219328 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 64, 2)"]
	140067042218992 -> 140067042219328
	140067042218992 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551615
dim1: 18446744073709551614"]
	140067042216976 -> 140067042218992
	140067042216976 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067042218416 -> 140067042216976
	140067042218416 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140067042214960 -> 140067042218416
	140067042214960 [label="ViewBackward0
------------------------
self_sym_sizes: (6, 768)"]
	140067042213952 -> 140067042214960
	140067042213952 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (6, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067042221008 -> 140067042213952
	140066495933936 [label="q_encoder.encoder.information_exchanging_layer.1.attention.self.key.bias
 (768)" fillcolor=lightblue]
	140066495933936 -> 140067042221008
	140067042221008 [label=AccumulateGrad]
	140067042096032 -> 140067042213952
	140067042096032 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140067042224416 -> 140067042096032
	140067042097136 -> 140067042213952
	140067042097136 [label=TBackward0]
	140067042083456 -> 140067042097136
	140066495933776 [label="q_encoder.encoder.information_exchanging_layer.1.attention.self.key.weight
 (768, 768)" fillcolor=lightblue]
	140066495933776 -> 140067042083456
	140067042083456 [label=AccumulateGrad]
	140067042222208 -> 140067042222400
	140067042222208 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 2, 64)"]
	140067042222016 -> 140067042222208
	140067042222016 [label=CloneBackward0]
	140067042221200 -> 140067042222016
	140067042221200 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 2, 64)"]
	140067042220864 -> 140067042221200
	140067042220864 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067042220048 -> 140067042220864
	140067042220048 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140067042218128 -> 140067042220048
	140067042218128 [label="ViewBackward0
------------------------
self_sym_sizes: (6, 768)"]
	140067042216256 -> 140067042218128
	140067042216256 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (6, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067042230176 -> 140067042216256
	140066495934096 [label="q_encoder.encoder.information_exchanging_layer.1.attention.self.value.bias
 (768)" fillcolor=lightblue]
	140066495934096 -> 140067042230176
	140067042230176 [label=AccumulateGrad]
	140067042218368 -> 140067042216256
	140067042218368 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140067042224416 -> 140067042218368
	140067042222112 -> 140067042216256
	140067042222112 [label=TBackward0]
	140067042095456 -> 140067042222112
	140066495934016 [label="q_encoder.encoder.information_exchanging_layer.1.attention.self.value.weight
 (768, 768)" fillcolor=lightblue]
	140066495934016 -> 140067042095456
	140067042095456 [label=AccumulateGrad]
	140067042222976 -> 140067042223696
	140067042222976 [label=TBackward0]
	140067042223168 -> 140067042222976
	140066495934176 [label="q_encoder.encoder.information_exchanging_layer.1.attention.output.dense.weight
 (768, 768)" fillcolor=lightblue]
	140066495934176 -> 140067042223168
	140067042223168 [label=AccumulateGrad]
	140067042224416 -> 140067042224464
	140067042224704 -> 140067042227776
	140066495934416 [label="q_encoder.encoder.information_exchanging_layer.1.attention.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	140066495934416 -> 140067042224704
	140067042224704 [label=AccumulateGrad]
	140067042223984 -> 140067042227776
	140066495934336 [label="q_encoder.encoder.information_exchanging_layer.1.attention.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	140066495934336 -> 140067042223984
	140067042223984 [label=AccumulateGrad]
	140067042224512 -> 140067042225328
	140067042224512 [label=TBackward0]
	140067042224032 -> 140067042224512
	140066495934576 [label="q_encoder.encoder.information_exchanging_layer.1.intermediate.dense.weight
 (3072, 768)" fillcolor=lightblue]
	140066495934576 -> 140067042224032
	140067042224032 [label=AccumulateGrad]
	140067042225568 -> 140067042226240
	140067042225568 [label=TBackward0]
	140067042225952 -> 140067042225568
	140066495933856 [label="q_encoder.encoder.information_exchanging_layer.1.output.dense.weight
 (768, 3072)" fillcolor=lightblue]
	140066495933856 -> 140067042225952
	140067042225952 [label=AccumulateGrad]
	140067042227776 -> 140067042226960
	140067042226912 -> 140067042227872
	140066495934896 [label="q_encoder.encoder.information_exchanging_layer.1.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	140066495934896 -> 140067042226912
	140067042226912 [label=AccumulateGrad]
	140067042226672 -> 140067042227872
	140066495934816 [label="q_encoder.encoder.information_exchanging_layer.1.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	140066495934816 -> 140067042226672
	140067042226672 [label=AccumulateGrad]
	140067042228352 -> 140067041480848
	140067042228352 [label=CloneBackward0]
	140067042228064 -> 140067042228352
	140067042228064 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 1, 768)
start         :                   0
step          :                   1"]
	140067042227200 -> 140067042228064
	140067042227200 [label="SelectBackward0
-------------------------------
dim           :               2
index         :               1
self_sym_sizes: (3, 1, 11, 768)"]
	140067042224176 -> 140067042227200
	140067042224176 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:     (3, 1, 11, 768)
start         :                   0
step          :                   1"]
	140067042226576 -> 140067042224176
	140067042226576 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:     (3, 1, 11, 768)
start         :                   0
step          :                   1"]
	140067042222832 -> 140067042226576
	140067042222832 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 11, 768)"]
	140067042223072 -> 140067042222832
	140067042223072 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140067042224896 -> 140067042223072
	140067042224896 [label="AddBackward0
------------
alpha: 1"]
	140067042223888 -> 140067042224896
	140067042223888 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067042226816 -> 140067042223888
	140067042226816 [label="ViewBackward0
-------------------------
self_sym_sizes: (33, 768)"]
	140067042217840 -> 140067042226816
	140067042217840 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :     (33, 3072)
mat1_sym_strides:      (3072, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (3072, 768)
mat2_sym_strides:      (1, 3072)"]
	140067042092576 -> 140067042217840
	140066489085504 [label="q_encoder.encoder.text_encoding_layer.2.output.dense.bias
 (768)" fillcolor=lightblue]
	140066489085504 -> 140067042092576
	140067042092576 [label=AccumulateGrad]
	140067042090704 -> 140067042217840
	140067042090704 [label="ViewBackward0
-----------------------------
self_sym_sizes: (3, 11, 3072)"]
	140067042097424 -> 140067042090704
	140067042097424 [label="GeluBackward0
---------------------------
approximate:           none
self       : [saved tensor]"]
	140067042219712 -> 140067042097424
	140067042219712 [label="ViewBackward0
--------------------------
self_sym_sizes: (33, 3072)"]
	140067042225856 -> 140067042219712
	140067042225856 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (33, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (768, 3072)
mat2_sym_strides:       (1, 768)"]
	140066907245008 -> 140067042225856
	140066489088384 [label="q_encoder.encoder.text_encoding_layer.2.intermediate.dense.bias
 (3072)" fillcolor=lightblue]
	140066489088384 -> 140066907245008
	140066907245008 [label=AccumulateGrad]
	140066907246592 -> 140067042225856
	140066907246592 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 11, 768)"]
	140067042222352 -> 140066907246592
	140067042222352 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140066907245104 -> 140067042222352
	140066907245104 [label="AddBackward0
------------
alpha: 1"]
	140066907256960 -> 140066907245104
	140066907256960 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140066907254512 -> 140066907256960
	140066907254512 [label="ViewBackward0
-------------------------
self_sym_sizes: (33, 768)"]
	140066907253168 -> 140066907254512
	140066907253168 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (33, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140066907251248 -> 140066907253168
	140066489087984 [label="q_encoder.encoder.text_encoding_layer.2.attention.output.dense.bias
 (768)" fillcolor=lightblue]
	140066489087984 -> 140066907251248
	140066907251248 [label=AccumulateGrad]
	140066907254896 -> 140066907253168
	140066907254896 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 11, 768)"]
	140066907252880 -> 140066907254896
	140066907252880 [label="ViewBackward0
-------------------------------
self_sym_sizes: (3, 11, 12, 64)"]
	140066907253072 -> 140066907252880
	140066907253072 [label=CloneBackward0]
	140066907254224 -> 140066907253072
	140066907254224 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140066907252544 -> 140066907254224
	140066907252544 [label="UnsafeViewBackward0
----------------------------
self_sym_sizes: (36, 11, 64)"]
	140066907252352 -> 140066907252544
	140066907252352 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140066907252112 -> 140066907252352
	140066907252112 [label="ViewBackward0
-------------------------------
self_sym_sizes: (3, 12, 11, 11)"]
	140066907251968 -> 140066907252112
	140066907251968 [label="ExpandBackward0
-------------------------------
self_sym_sizes: (3, 12, 11, 11)"]
	140066907251680 -> 140066907251968
	140066907251680 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140066907251488 -> 140066907251680
	140066907251488 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140066907251152 -> 140066907251488
	140066907251152 [label="AddBackward0
------------
alpha: 1"]
	140066907251056 -> 140066907251152
	140066907251056 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140066907250960 -> 140066907251056
	140066907250960 [label="UnsafeViewBackward0
----------------------------
self_sym_sizes: (36, 11, 11)"]
	140066907250768 -> 140066907250960
	140066907250768 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140066907250720 -> 140066907250768
	140066907250720 [label="UnsafeViewBackward0
-------------------------------
self_sym_sizes: (3, 12, 11, 64)"]
	140066907250672 -> 140066907250720
	140066907250672 [label=CloneBackward0]
	140066907250432 -> 140066907250672
	140066907250432 [label="ExpandBackward0
-------------------------------
self_sym_sizes: (3, 12, 11, 64)"]
	140066907250288 -> 140066907250432
	140066907250288 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140066907249952 -> 140066907250288
	140066907249952 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 11, 768)"]
	140066907249808 -> 140066907249952
	140066907249808 [label="ViewBackward0
-------------------------
self_sym_sizes: (33, 768)"]
	140066907249664 -> 140066907249808
	140066907249664 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (33, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140066907249328 -> 140066907249664
	140066489087504 [label="q_encoder.encoder.text_encoding_layer.2.attention.self.query.bias
 (768)" fillcolor=lightblue]
	140066489087504 -> 140066907249328
	140066907249328 [label=AccumulateGrad]
	140066907249568 -> 140066907249664
	140066907249568 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 11, 768)"]
	140066907255904 -> 140066907249568
	140066907255904 [label="ViewBackward0
-------------------------------
self_sym_sizes: (3, 1, 11, 768)"]
	140066907248992 -> 140066907255904
	140066907248992 [label="AsStridedBackward0
------------------------------------
size          :      (3, 1, 11, 768)
storage_offset:                    0
stride        : (8448, 8448, 768, 1)"]
	140066907248848 -> 140066907248992
	140066907248848 [label=CopySlices]
	140067042214720 -> 140066907248848
	140066907248608 -> 140066907248848
	140066907248608 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 1, 768)
start         :                   0
step          :                   1"]
	140066907248704 -> 140066907248608
	140066907248704 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:         (3, 2, 768)
start         :                   1
step          :                   1"]
	140066907247936 -> 140066907248704
	140066907247936 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:         (3, 2, 768)
start         :                   0
step          :                   1"]
	140067042227872 -> 140066907247936
	140066907250624 -> 140066907249664
	140066907250624 [label=TBackward0]
	140066907249232 -> 140066907250624
	140066489087424 [label="q_encoder.encoder.text_encoding_layer.2.attention.self.query.weight
 (768, 768)" fillcolor=lightblue]
	140066489087424 -> 140066907249232
	140066907249232 [label=AccumulateGrad]
	140066907251008 -> 140066907250768
	140066907251008 [label="UnsafeViewBackward0
-------------------------------
self_sym_sizes: (3, 12, 64, 11)"]
	140066907250912 -> 140066907251008
	140066907250912 [label=CloneBackward0]
	140066907249712 -> 140066907250912
	140066907249712 [label="ExpandBackward0
-------------------------------
self_sym_sizes: (3, 12, 64, 11)"]
	140066907249904 -> 140066907249712
	140066907249904 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551615
dim1: 18446744073709551614"]
	140066907248656 -> 140066907249904
	140066907248656 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140066907247744 -> 140066907248656
	140066907247744 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 11, 768)"]
	140066907249472 -> 140066907247744
	140066907249472 [label="ViewBackward0
-------------------------
self_sym_sizes: (33, 768)"]
	140066907248944 -> 140066907249472
	140066907248944 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (33, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140066907247504 -> 140066907248944
	140066489087664 [label="q_encoder.encoder.text_encoding_layer.2.attention.self.key.bias
 (768)" fillcolor=lightblue]
	140066489087664 -> 140066907247504
	140066907247504 [label=AccumulateGrad]
	140066907247408 -> 140066907248944
	140066907247408 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 11, 768)"]
	140066907255904 -> 140066907247408
	140066907250480 -> 140066907248944
	140066907250480 [label=TBackward0]
	140066907247312 -> 140066907250480
	140066489087584 [label="q_encoder.encoder.text_encoding_layer.2.attention.self.key.weight
 (768, 768)" fillcolor=lightblue]
	140066489087584 -> 140066907247312
	140066907247312 [label=AccumulateGrad]
	140066907252448 -> 140066907252352
	140066907252448 [label="UnsafeViewBackward0
-------------------------------
self_sym_sizes: (3, 12, 11, 64)"]
	140066907252064 -> 140066907252448
	140066907252064 [label=CloneBackward0]
	140066907251440 -> 140066907252064
	140066907251440 [label="ExpandBackward0
-------------------------------
self_sym_sizes: (3, 12, 11, 64)"]
	140066907251296 -> 140066907251440
	140066907251296 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140066907250240 -> 140066907251296
	140066907250240 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 11, 768)"]
	140066907248896 -> 140066907250240
	140066907248896 [label="ViewBackward0
-------------------------
self_sym_sizes: (33, 768)"]
	140066907248560 -> 140066907248896
	140066907248560 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (33, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140066907247840 -> 140066907248560
	140066489087824 [label="q_encoder.encoder.text_encoding_layer.2.attention.self.value.bias
 (768)" fillcolor=lightblue]
	140066489087824 -> 140066907247840
	140066907247840 [label=AccumulateGrad]
	140066907249088 -> 140066907248560
	140066907249088 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 11, 768)"]
	140066907255904 -> 140066907249088
	140066907251776 -> 140066907248560
	140066907251776 [label=TBackward0]
	140066907246832 -> 140066907251776
	140066489087744 [label="q_encoder.encoder.text_encoding_layer.2.attention.self.value.weight
 (768, 768)" fillcolor=lightblue]
	140066489087744 -> 140066907246832
	140066907246832 [label=AccumulateGrad]
	140066907254752 -> 140066907253168
	140066907254752 [label=TBackward0]
	140066907253024 -> 140066907254752
	140066489087904 [label="q_encoder.encoder.text_encoding_layer.2.attention.output.dense.weight
 (768, 768)" fillcolor=lightblue]
	140066489087904 -> 140066907253024
	140066907253024 [label=AccumulateGrad]
	140066907255904 -> 140066907245104
	140066907247216 -> 140067042222352
	140066489088144 [label="q_encoder.encoder.text_encoding_layer.2.attention.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	140066489088144 -> 140066907247216
	140066907247216 [label=AccumulateGrad]
	140066907258400 -> 140067042222352
	140066489088064 [label="q_encoder.encoder.text_encoding_layer.2.attention.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	140066489088064 -> 140066907258400
	140066907258400 [label=AccumulateGrad]
	140066907250192 -> 140067042225856
	140066907250192 [label=TBackward0]
	140066907254656 -> 140066907250192
	140066489088304 [label="q_encoder.encoder.text_encoding_layer.2.intermediate.dense.weight
 (3072, 768)" fillcolor=lightblue]
	140066489088304 -> 140066907254656
	140066907254656 [label=AccumulateGrad]
	140067042086816 -> 140067042217840
	140067042086816 [label=TBackward0]
	140067042220624 -> 140067042086816
	140066489088224 [label="q_encoder.encoder.text_encoding_layer.2.output.dense.weight
 (768, 3072)" fillcolor=lightblue]
	140066489088224 -> 140067042220624
	140067042220624 [label=AccumulateGrad]
	140067042222352 -> 140067042224896
	140067042222736 -> 140067042223072
	140066489088624 [label="q_encoder.encoder.text_encoding_layer.2.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	140066489088624 -> 140067042222736
	140067042222736 [label=AccumulateGrad]
	140067042228496 -> 140067042223072
	140066489088464 [label="q_encoder.encoder.text_encoding_layer.2.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	140066489088464 -> 140067042228496
	140067042228496 [label=AccumulateGrad]
	140067042228160 -> 140067042228928
	140067042228160 [label=TBackward0]
	140067042085424 -> 140067042228160
	140066495934496 [label="q_encoder.encoder.information_exchanging_layer.2.attention.self.query.weight
 (768, 768)" fillcolor=lightblue]
	140066495934496 -> 140067042085424
	140067042085424 [label=AccumulateGrad]
	140067041476720 -> 140067054841008
	140067041476720 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 64, 2)"]
	140067042230080 -> 140067041476720
	140067042230080 [label=CloneBackward0]
	140067042229216 -> 140067042230080
	140067042229216 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 64, 2)"]
	140067042229024 -> 140067042229216
	140067042229024 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551615
dim1: 18446744073709551614"]
	140067042227056 -> 140067042229024
	140067042227056 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067042225760 -> 140067042227056
	140067042225760 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140067042223024 -> 140067042225760
	140067042223024 [label="ViewBackward0
------------------------
self_sym_sizes: (6, 768)"]
	140067042224608 -> 140067042223024
	140067042224608 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (6, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067042225424 -> 140067042224608
	140066495935216 [label="q_encoder.encoder.information_exchanging_layer.2.attention.self.key.bias
 (768)" fillcolor=lightblue]
	140066495935216 -> 140067042225424
	140067042225424 [label=AccumulateGrad]
	140067042221584 -> 140067042224608
	140067042221584 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140067041480848 -> 140067042221584
	140067042230032 -> 140067042224608
	140067042230032 [label=TBackward0]
	140067042220288 -> 140067042230032
	140066495935056 [label="q_encoder.encoder.information_exchanging_layer.2.attention.self.key.weight
 (768, 768)" fillcolor=lightblue]
	140066495935056 -> 140067042220288
	140067042220288 [label=AccumulateGrad]
	140067041478496 -> 140067041478688
	140067041478496 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 2, 64)"]
	140067054845328 -> 140067041478496
	140067054845328 [label=CloneBackward0]
	140067041477392 -> 140067054845328
	140067041477392 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 2, 64)"]
	140067041477152 -> 140067041477392
	140067041477152 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041478352 -> 140067041477152
	140067041478352 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140067042228256 -> 140067041478352
	140067042228256 [label="ViewBackward0
------------------------
self_sym_sizes: (6, 768)"]
	140067042226384 -> 140067042228256
	140067042226384 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (6, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067042221344 -> 140067042226384
	140066495935376 [label="q_encoder.encoder.information_exchanging_layer.2.attention.self.value.bias
 (768)" fillcolor=lightblue]
	140066495935376 -> 140067042221344
	140067042221344 [label=AccumulateGrad]
	140067042228448 -> 140067042226384
	140067042228448 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140067041480848 -> 140067042228448
	140067042229840 -> 140067042226384
	140067042229840 [label=TBackward0]
	140067042223744 -> 140067042229840
	140066495935296 [label="q_encoder.encoder.information_exchanging_layer.2.attention.self.value.weight
 (768, 768)" fillcolor=lightblue]
	140066495935296 -> 140067042223744
	140067042223744 [label=AccumulateGrad]
	140067041479312 -> 140067041479984
	140067041479312 [label=TBackward0]
	140067041479504 -> 140067041479312
	140066495935456 [label="q_encoder.encoder.information_exchanging_layer.2.attention.output.dense.weight
 (768, 768)" fillcolor=lightblue]
	140066495935456 -> 140067041479504
	140067041479504 [label=AccumulateGrad]
	140067041480848 -> 140067041480944
	140067041481232 -> 140067041484784
	140066495935696 [label="q_encoder.encoder.information_exchanging_layer.2.attention.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	140066495935696 -> 140067041481232
	140067041481232 [label=AccumulateGrad]
	140067041480368 -> 140067041484784
	140066495935616 [label="q_encoder.encoder.information_exchanging_layer.2.attention.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	140066495935616 -> 140067041480368
	140067041480368 [label=AccumulateGrad]
	140067041480992 -> 140067041481856
	140067041480992 [label=TBackward0]
	140067041480464 -> 140067041480992
	140066495935856 [label="q_encoder.encoder.information_exchanging_layer.2.intermediate.dense.weight
 (3072, 768)" fillcolor=lightblue]
	140066495935856 -> 140067041480464
	140067041480464 [label=AccumulateGrad]
	140067041482048 -> 140067041483248
	140067041482048 [label=TBackward0]
	140067041482672 -> 140067041482048
	140066495935136 [label="q_encoder.encoder.information_exchanging_layer.2.output.dense.weight
 (768, 3072)" fillcolor=lightblue]
	140066495935136 -> 140067041482672
	140067041482672 [label=AccumulateGrad]
	140067041484784 -> 140067041484016
	140067041483872 -> 140067041484880
	140066495936176 [label="q_encoder.encoder.information_exchanging_layer.2.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	140066495936176 -> 140067041483872
	140067041483872 [label=AccumulateGrad]
	140067041483536 -> 140067041484880
	140066495936096 [label="q_encoder.encoder.information_exchanging_layer.2.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	140066495936096 -> 140067041483536
	140067041483536 [label=AccumulateGrad]
	140067041485312 -> 140067041492656
	140067041485312 [label=CloneBackward0]
	140067041485072 -> 140067041485312
	140067041485072 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 1, 768)
start         :                   0
step          :                   1"]
	140067041484352 -> 140067041485072
	140067041484352 [label="SelectBackward0
-------------------------------
dim           :               2
index         :               1
self_sym_sizes: (3, 1, 11, 768)"]
	140067041480656 -> 140067041484352
	140067041480656 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:     (3, 1, 11, 768)
start         :                   0
step          :                   1"]
	140067041483392 -> 140067041480656
	140067041483392 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:     (3, 1, 11, 768)
start         :                   0
step          :                   1"]
	140067041479072 -> 140067041483392
	140067041479072 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 11, 768)"]
	140067041479456 -> 140067041479072
	140067041479456 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140067041481424 -> 140067041479456
	140067041481424 [label="AddBackward0
------------
alpha: 1"]
	140067041480176 -> 140067041481424
	140067041480176 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041483776 -> 140067041480176
	140067041483776 [label="ViewBackward0
-------------------------
self_sym_sizes: (33, 768)"]
	140067041476960 -> 140067041483776
	140067041476960 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :     (33, 3072)
mat1_sym_strides:      (3072, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (3072, 768)
mat2_sym_strides:      (1, 3072)"]
	140067041477680 -> 140067041476960
	140066489087264 [label="q_encoder.encoder.text_encoding_layer.3.output.dense.bias
 (768)" fillcolor=lightblue]
	140066489087264 -> 140067041477680
	140067041477680 [label=AccumulateGrad]
	140067041482528 -> 140067041476960
	140067041482528 [label="ViewBackward0
-----------------------------
self_sym_sizes: (3, 11, 3072)"]
	140067042228016 -> 140067041482528
	140067042228016 [label="GeluBackward0
---------------------------
approximate:           none
self       : [saved tensor]"]
	140066907254368 -> 140067042228016
	140066907254368 [label="ViewBackward0
--------------------------
self_sym_sizes: (33, 3072)"]
	140066907247024 -> 140066907254368
	140066907247024 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (33, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (768, 3072)
mat2_sym_strides:       (1, 768)"]
	140066907252496 -> 140066907247024
	140066489089664 [label="q_encoder.encoder.text_encoding_layer.3.intermediate.dense.bias
 (3072)" fillcolor=lightblue]
	140066489089664 -> 140066907252496
	140066907252496 [label=AccumulateGrad]
	140066907252592 -> 140066907247024
	140066907252592 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 11, 768)"]
	140067041478640 -> 140066907252592
	140067041478640 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140066907251632 -> 140067041478640
	140066907251632 [label="AddBackward0
------------
alpha: 1"]
	140066907250048 -> 140066907251632
	140066907250048 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140066907251920 -> 140066907250048
	140066907251920 [label="ViewBackward0
-------------------------
self_sym_sizes: (33, 768)"]
	140066907246784 -> 140066907251920
	140066907246784 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (33, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140066907258016 -> 140066907246784
	140066489089264 [label="q_encoder.encoder.text_encoding_layer.3.attention.output.dense.bias
 (768)" fillcolor=lightblue]
	140066489089264 -> 140066907258016
	140066907258016 [label=AccumulateGrad]
	140066907248464 -> 140066907246784
	140066907248464 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 11, 768)"]
	140066907246544 -> 140066907248464
	140066907246544 [label="ViewBackward0
-------------------------------
self_sym_sizes: (3, 11, 12, 64)"]
	140066907246736 -> 140066907246544
	140066907246736 [label=CloneBackward0]
	140066907247888 -> 140066907246736
	140066907247888 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140066907246208 -> 140066907247888
	140066907246208 [label="UnsafeViewBackward0
----------------------------
self_sym_sizes: (36, 11, 64)"]
	140066907246016 -> 140066907246208
	140066907246016 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140066907245776 -> 140066907246016
	140066907245776 [label="ViewBackward0
-------------------------------
self_sym_sizes: (3, 12, 11, 11)"]
	140066907245632 -> 140066907245776
	140066907245632 [label="ExpandBackward0
-------------------------------
self_sym_sizes: (3, 12, 11, 11)"]
	140066907245248 -> 140066907245632
	140066907245248 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140066907245056 -> 140066907245248
	140066907245056 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140066907244480 -> 140066907245056
	140066907244480 [label="AddBackward0
------------
alpha: 1"]
	140066907244384 -> 140066907244480
	140066907244384 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140066907244288 -> 140066907244384
	140066907244288 [label="UnsafeViewBackward0
----------------------------
self_sym_sizes: (36, 11, 11)"]
	140066907243856 -> 140066907244288
	140066907243856 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140066907243712 -> 140066907243856
	140066907243712 [label="UnsafeViewBackward0
-------------------------------
self_sym_sizes: (3, 12, 11, 64)"]
	140066907243520 -> 140066907243712
	140066907243520 [label=CloneBackward0]
	140066907243280 -> 140066907243520
	140066907243280 [label="ExpandBackward0
-------------------------------
self_sym_sizes: (3, 12, 11, 64)"]
	140066907243232 -> 140066907243280
	140066907243232 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140066907242896 -> 140066907243232
	140066907242896 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 11, 768)"]
	140066907242704 -> 140066907242896
	140066907242704 [label="ViewBackward0
-------------------------
self_sym_sizes: (33, 768)"]
	140066907243472 -> 140066907242704
	140066907243472 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (33, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041738560 -> 140066907243472
	140066489088784 [label="q_encoder.encoder.text_encoding_layer.3.attention.self.query.bias
 (768)" fillcolor=lightblue]
	140066489088784 -> 140067041738560
	140067041738560 [label=AccumulateGrad]
	140067041738704 -> 140066907243472
	140067041738704 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 11, 768)"]
	140066907249184 -> 140067041738704
	140066907249184 [label="ViewBackward0
-------------------------------
self_sym_sizes: (3, 1, 11, 768)"]
	140067041738176 -> 140066907249184
	140067041738176 [label="AsStridedBackward0
------------------------------------
size          :      (3, 1, 11, 768)
storage_offset:                    0
stride        : (8448, 8448, 768, 1)"]
	140067041738032 -> 140067041738176
	140067041738032 [label=CopySlices]
	140067042223072 -> 140067041738032
	140067041737792 -> 140067041738032
	140067041737792 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 1, 768)
start         :                   0
step          :                   1"]
	140067041737936 -> 140067041737792
	140067041737936 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:         (3, 2, 768)
start         :                   1
step          :                   1"]
	140067041737072 -> 140067041737936
	140067041737072 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:         (3, 2, 768)
start         :                   0
step          :                   1"]
	140067041484880 -> 140067041737072
	140067041733904 -> 140066907243472
	140067041733904 [label=TBackward0]
	140067041738512 -> 140067041733904
	140066489088704 [label="q_encoder.encoder.text_encoding_layer.3.attention.self.query.weight
 (768, 768)" fillcolor=lightblue]
	140066489088704 -> 140067041738512
	140067041738512 [label=AccumulateGrad]
	140066907244336 -> 140066907243856
	140066907244336 [label="UnsafeViewBackward0
-------------------------------
self_sym_sizes: (3, 12, 64, 11)"]
	140066907244144 -> 140066907244336
	140066907244144 [label=CloneBackward0]
	140066907242656 -> 140066907244144
	140066907242656 [label="ExpandBackward0
-------------------------------
self_sym_sizes: (3, 12, 64, 11)"]
	140066907243376 -> 140066907242656
	140066907243376 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551615
dim1: 18446744073709551614"]
	140067041737840 -> 140066907243376
	140067041737840 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041736880 -> 140067041737840
	140067041736880 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 11, 768)"]
	140067041738656 -> 140067041736880
	140067041738656 [label="ViewBackward0
-------------------------
self_sym_sizes: (33, 768)"]
	140067041737984 -> 140067041738656
	140067041737984 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (33, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041736832 -> 140067041737984
	140066489088944 [label="q_encoder.encoder.text_encoding_layer.3.attention.self.key.bias
 (768)" fillcolor=lightblue]
	140066489088944 -> 140067041736832
	140067041736832 [label=AccumulateGrad]
	140067041736736 -> 140067041737984
	140067041736736 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 11, 768)"]
	140066907249184 -> 140067041736736
	140067041738080 -> 140067041737984
	140067041738080 [label=TBackward0]
	140067041736640 -> 140067041738080
	140066489088864 [label="q_encoder.encoder.text_encoding_layer.3.attention.self.key.weight
 (768, 768)" fillcolor=lightblue]
	140066489088864 -> 140067041736640
	140067041736640 [label=AccumulateGrad]
	140066907246064 -> 140066907246016
	140066907246064 [label="UnsafeViewBackward0
-------------------------------
self_sym_sizes: (3, 12, 11, 64)"]
	140066907245728 -> 140066907246064
	140066907245728 [label=CloneBackward0]
	140066907244864 -> 140066907245728
	140066907244864 [label="ExpandBackward0
-------------------------------
self_sym_sizes: (3, 12, 11, 64)"]
	140066907244816 -> 140066907244864
	140066907244816 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140066907243136 -> 140066907244816
	140066907243136 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 11, 768)"]
	140066907242800 -> 140066907243136
	140066907242800 [label="ViewBackward0
-------------------------
self_sym_sizes: (33, 768)"]
	140066907245344 -> 140066907242800
	140066907245344 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (33, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041736976 -> 140066907245344
	140066489089104 [label="q_encoder.encoder.text_encoding_layer.3.attention.self.value.bias
 (768)" fillcolor=lightblue]
	140066489089104 -> 140067041736976
	140067041736976 [label=AccumulateGrad]
	140067041738272 -> 140066907245344
	140067041738272 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 11, 768)"]
	140066907249184 -> 140067041738272
	140067041737648 -> 140066907245344
	140067041737648 [label=TBackward0]
	140067041736544 -> 140067041737648
	140066489089024 [label="q_encoder.encoder.text_encoding_layer.3.attention.self.value.weight
 (768, 768)" fillcolor=lightblue]
	140066489089024 -> 140067041736544
	140067041736544 [label=AccumulateGrad]
	140066907248368 -> 140066907246784
	140066907248368 [label=TBackward0]
	140066907246640 -> 140066907248368
	140066489089184 [label="q_encoder.encoder.text_encoding_layer.3.attention.output.dense.weight
 (768, 768)" fillcolor=lightblue]
	140066489089184 -> 140066907246640
	140066907246640 [label=AccumulateGrad]
	140066907249184 -> 140066907251632
	140066907252784 -> 140067041478640
	140066489089424 [label="q_encoder.encoder.text_encoding_layer.3.attention.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	140066489089424 -> 140066907252784
	140066907252784 [label=AccumulateGrad]
	140066907251200 -> 140067041478640
	140066489089344 [label="q_encoder.encoder.text_encoding_layer.3.attention.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	140066489089344 -> 140066907251200
	140066907251200 [label=AccumulateGrad]
	140066907254608 -> 140066907247024
	140066907254608 [label=TBackward0]
	140066907248224 -> 140066907254608
	140066489089584 [label="q_encoder.encoder.text_encoding_layer.3.intermediate.dense.weight
 (3072, 768)" fillcolor=lightblue]
	140066489089584 -> 140066907248224
	140066907248224 [label=AccumulateGrad]
	140067042229456 -> 140067041476960
	140067042229456 [label=TBackward0]
	140066907246976 -> 140067042229456
	140066489089504 [label="q_encoder.encoder.text_encoding_layer.3.output.dense.weight
 (768, 3072)" fillcolor=lightblue]
	140066489089504 -> 140066907246976
	140066907246976 [label=AccumulateGrad]
	140067041478640 -> 140067041481424
	140067041478880 -> 140067041479456
	140066489089904 [label="q_encoder.encoder.text_encoding_layer.3.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	140066489089904 -> 140067041478880
	140067041478880 [label=AccumulateGrad]
	140067041485696 -> 140067041479456
	140066489089744 [label="q_encoder.encoder.text_encoding_layer.3.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	140066489089744 -> 140067041485696
	140067041485696 [label=AccumulateGrad]
	140067041485168 -> 140067041486128
	140067041485168 [label=TBackward0]
	140067041485024 -> 140067041485168
	140066495935776 [label="q_encoder.encoder.information_exchanging_layer.3.attention.self.query.weight
 (768, 768)" fillcolor=lightblue]
	140066495935776 -> 140067041485024
	140067041485024 [label=AccumulateGrad]
	140067041488576 -> 140067041488096
	140067041488576 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 64, 2)"]
	140067041487856 -> 140067041488576
	140067041487856 [label=CloneBackward0]
	140067041486704 -> 140067041487856
	140067041486704 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 64, 2)"]
	140067041486224 -> 140067041486704
	140067041486224 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551615
dim1: 18446744073709551614"]
	140067041483296 -> 140067041486224
	140067041483296 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041479408 -> 140067041483296
	140067041479408 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140067041481040 -> 140067041479408
	140067041481040 [label="ViewBackward0
------------------------
self_sym_sizes: (6, 768)"]
	140067041478064 -> 140067041481040
	140067041478064 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (6, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067042221968 -> 140067041478064
	140066495936496 [label="q_encoder.encoder.information_exchanging_layer.3.attention.self.key.bias
 (768)" fillcolor=lightblue]
	140066495936496 -> 140067042221968
	140067042221968 [label=AccumulateGrad]
	140067041477104 -> 140067041478064
	140067041477104 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140067041492656 -> 140067041477104
	140067041481904 -> 140067041478064
	140067041481904 [label=TBackward0]
	140067041477968 -> 140067041481904
	140066495936336 [label="q_encoder.encoder.information_exchanging_layer.3.attention.self.key.weight
 (768, 768)" fillcolor=lightblue]
	140066495936336 -> 140067041477968
	140067041477968 [label=AccumulateGrad]
	140067041489920 -> 140067041490400
	140067041489920 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 2, 64)"]
	140067041489728 -> 140067041489920
	140067041489728 [label=CloneBackward0]
	140067041488768 -> 140067041489728
	140067041488768 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 2, 64)"]
	140067041488432 -> 140067041488768
	140067041488432 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041487328 -> 140067041488432
	140067041487328 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140067041485264 -> 140067041487328
	140067041485264 [label="ViewBackward0
------------------------
self_sym_sizes: (6, 768)"]
	140067041480032 -> 140067041485264
	140067041480032 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (6, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041488624 -> 140067041480032
	140066495936656 [label="q_encoder.encoder.information_exchanging_layer.3.attention.self.value.bias
 (768)" fillcolor=lightblue]
	140066495936656 -> 140067041488624
	140067041488624 [label=AccumulateGrad]
	140067041485456 -> 140067041480032
	140067041485456 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140067041492656 -> 140067041485456
	140067041489776 -> 140067041480032
	140067041489776 [label=TBackward0]
	140066907252832 -> 140067041489776
	140066495936576 [label="q_encoder.encoder.information_exchanging_layer.3.attention.self.value.weight
 (768, 768)" fillcolor=lightblue]
	140066495936576 -> 140066907252832
	140066907252832 [label=AccumulateGrad]
	140067041491072 -> 140067041491888
	140067041491072 [label=TBackward0]
	140067041491264 -> 140067041491072
	140066495936736 [label="q_encoder.encoder.information_exchanging_layer.3.attention.output.dense.weight
 (768, 768)" fillcolor=lightblue]
	140066495936736 -> 140067041491264
	140067041491264 [label=AccumulateGrad]
	140067041492656 -> 140067041492704
	140067041478016 -> 140067041490544
	140066495936976 [label="q_encoder.encoder.information_exchanging_layer.3.attention.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	140066495936976 -> 140067041478016
	140067041478016 [label=AccumulateGrad]
	140067041492320 -> 140067041490544
	140066495936896 [label="q_encoder.encoder.information_exchanging_layer.3.attention.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	140066495936896 -> 140067041492320
	140067041492320 [label=AccumulateGrad]
	140067041492944 -> 140067041480608
	140067041492944 [label=TBackward0]
	140067041492368 -> 140067041492944
	140066495937136 [label="q_encoder.encoder.information_exchanging_layer.3.intermediate.dense.weight
 (3072, 768)" fillcolor=lightblue]
	140066495937136 -> 140067041492368
	140067041492368 [label=AccumulateGrad]
	140067041481088 -> 140067041483632
	140067041481088 [label=TBackward0]
	140067041482432 -> 140067041481088
	140066495936416 [label="q_encoder.encoder.information_exchanging_layer.3.output.dense.weight
 (768, 3072)" fillcolor=lightblue]
	140066495936416 -> 140067041482432
	140067041482432 [label=AccumulateGrad]
	140067041490544 -> 140067041486272
	140067041486368 -> 140067041490928
	140066495937456 [label="q_encoder.encoder.information_exchanging_layer.3.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	140066495937456 -> 140067041486368
	140067041486368 [label=AccumulateGrad]
	140067041485552 -> 140067041490928
	140066495937376 [label="q_encoder.encoder.information_exchanging_layer.3.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	140066495937376 -> 140067041485552
	140067041485552 [label=AccumulateGrad]
	140066879619376 -> 140066879626384
	140066879619376 [label=CloneBackward0]
	140067041491504 -> 140066879619376
	140067041491504 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 1, 768)
start         :                   0
step          :                   1"]
	140067041487664 -> 140067041491504
	140067041487664 [label="SelectBackward0
-------------------------------
dim           :               2
index         :               1
self_sym_sizes: (3, 1, 11, 768)"]
	140067041492416 -> 140067041487664
	140067041492416 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:     (3, 1, 11, 768)
start         :                   0
step          :                   1"]
	140067041484928 -> 140067041492416
	140067041484928 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:     (3, 1, 11, 768)
start         :                   0
step          :                   1"]
	140067041490976 -> 140067041484928
	140067041490976 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 11, 768)"]
	140067041491216 -> 140067041490976
	140067041491216 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140067041476768 -> 140067041491216
	140067041476768 [label="AddBackward0
------------
alpha: 1"]
	140067041492176 -> 140067041476768
	140067041492176 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041485648 -> 140067041492176
	140067041485648 [label="ViewBackward0
-------------------------
self_sym_sizes: (33, 768)"]
	140067041484208 -> 140067041485648
	140067041484208 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :     (33, 3072)
mat1_sym_strides:      (3072, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (3072, 768)
mat2_sym_strides:      (1, 3072)"]
	140067041483056 -> 140067041484208
	140066495922256 [label="q_encoder.encoder.text_encoding_layer.4.output.dense.bias
 (768)" fillcolor=lightblue]
	140066495922256 -> 140067041483056
	140067041483056 [label=AccumulateGrad]
	140067041488192 -> 140067041484208
	140067041488192 [label="ViewBackward0
-----------------------------
self_sym_sizes: (3, 11, 3072)"]
	140067041486896 -> 140067041488192
	140067041486896 [label="GeluBackward0
---------------------------
approximate:           none
self       : [saved tensor]"]
	140066907250816 -> 140067041486896
	140066907250816 [label="ViewBackward0
--------------------------
self_sym_sizes: (33, 3072)"]
	140066907248176 -> 140066907250816
	140066907248176 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (33, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (768, 3072)
mat2_sym_strides:       (1, 768)"]
	140066907246112 -> 140066907248176
	140066495923136 [label="q_encoder.encoder.text_encoding_layer.4.intermediate.dense.bias
 (3072)" fillcolor=lightblue]
	140066495923136 -> 140066907246112
	140066907246112 [label=AccumulateGrad]
	140066907246256 -> 140066907248176
	140066907246256 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 11, 768)"]
	140067041490304 -> 140066907246256
	140067041490304 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140066907245200 -> 140067041490304
	140066907245200 [label="AddBackward0
------------
alpha: 1"]
	140066907245536 -> 140066907245200
	140066907245536 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041737264 -> 140066907245536
	140067041737264 [label="ViewBackward0
-------------------------
self_sym_sizes: (33, 768)"]
	140067041736448 -> 140067041737264
	140067041736448 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (33, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041737456 -> 140067041736448
	140066495922736 [label="q_encoder.encoder.text_encoding_layer.4.attention.output.dense.bias
 (768)" fillcolor=lightblue]
	140066495922736 -> 140067041737456
	140067041737456 [label=AccumulateGrad]
	140067041737600 -> 140067041736448
	140067041737600 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 11, 768)"]
	140067041736256 -> 140067041737600
	140067041736256 [label="ViewBackward0
-------------------------------
self_sym_sizes: (3, 11, 12, 64)"]
	140067041736400 -> 140067041736256
	140067041736400 [label=CloneBackward0]
	140067041737168 -> 140067041736400
	140067041737168 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041735920 -> 140067041737168
	140067041735920 [label="UnsafeViewBackward0
----------------------------
self_sym_sizes: (36, 11, 64)"]
	140067041735680 -> 140067041735920
	140067041735680 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140067041735584 -> 140067041735680
	140067041735584 [label="ViewBackward0
-------------------------------
self_sym_sizes: (3, 12, 11, 11)"]
	140067041735440 -> 140067041735584
	140067041735440 [label="ExpandBackward0
-------------------------------
self_sym_sizes: (3, 12, 11, 11)"]
	140067041735200 -> 140067041735440
	140067041735200 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041734864 -> 140067041735200
	140067041734864 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140067041734624 -> 140067041734864
	140067041734624 [label="AddBackward0
------------
alpha: 1"]
	140067041734528 -> 140067041734624
	140067041734528 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041734336 -> 140067041734528
	140067041734336 [label="UnsafeViewBackward0
----------------------------
self_sym_sizes: (36, 11, 11)"]
	140067041734144 -> 140067041734336
	140067041734144 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140067041734048 -> 140067041734144
	140067041734048 [label="UnsafeViewBackward0
-------------------------------
self_sym_sizes: (3, 12, 11, 64)"]
	140067041733808 -> 140067041734048
	140067041733808 [label=CloneBackward0]
	140067041733472 -> 140067041733808
	140067041733472 [label="ExpandBackward0
-------------------------------
self_sym_sizes: (3, 12, 11, 64)"]
	140067041733424 -> 140067041733472
	140067041733424 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041733184 -> 140067041733424
	140067041733184 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 11, 768)"]
	140067041733040 -> 140067041733184
	140067041733040 [label="ViewBackward0
-------------------------
self_sym_sizes: (33, 768)"]
	140067041732800 -> 140067041733040
	140067041732800 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (33, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041732560 -> 140067041732800
	140066489088544 [label="q_encoder.encoder.text_encoding_layer.4.attention.self.query.bias
 (768)" fillcolor=lightblue]
	140066489088544 -> 140067041732560
	140067041732560 [label=AccumulateGrad]
	140067041732752 -> 140067041732800
	140067041732752 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 11, 768)"]
	140066907245824 -> 140067041732752
	140066907245824 [label="ViewBackward0
-------------------------------
self_sym_sizes: (3, 1, 11, 768)"]
	140067041732176 -> 140066907245824
	140067041732176 [label="AsStridedBackward0
------------------------------------
size          :      (3, 1, 11, 768)
storage_offset:                    0
stride        : (8448, 8448, 768, 1)"]
	140067041732032 -> 140067041732176
	140067041732032 [label=CopySlices]
	140067041479456 -> 140067041732032
	140067041731792 -> 140067041732032
	140067041731792 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 1, 768)
start         :                   0
step          :                   1"]
	140067041731936 -> 140067041731792
	140067041731936 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:         (3, 2, 768)
start         :                   1
step          :                   1"]
	140067041731120 -> 140067041731936
	140067041731120 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:         (3, 2, 768)
start         :                   0
step          :                   1"]
	140067041490928 -> 140067041731120
	140067041733712 -> 140067041732800
	140067041733712 [label=TBackward0]
	140067041732416 -> 140067041733712
	140066489089984 [label="q_encoder.encoder.text_encoding_layer.4.attention.self.query.weight
 (768, 768)" fillcolor=lightblue]
	140066489089984 -> 140067041732416
	140067041732416 [label=AccumulateGrad]
	140067041734432 -> 140067041734144
	140067041734432 [label="UnsafeViewBackward0
-------------------------------
self_sym_sizes: (3, 12, 64, 11)"]
	140067041734192 -> 140067041734432
	140067041734192 [label=CloneBackward0]
	140067041732992 -> 140067041734192
	140067041732992 [label="ExpandBackward0
-------------------------------
self_sym_sizes: (3, 12, 64, 11)"]
	140067041733136 -> 140067041732992
	140067041733136 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551615
dim1: 18446744073709551614"]
	140067041731840 -> 140067041733136
	140067041731840 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041731024 -> 140067041731840
	140067041731024 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 11, 768)"]
	140067041732704 -> 140067041731024
	140067041732704 [label="ViewBackward0
-------------------------
self_sym_sizes: (33, 768)"]
	140067041732128 -> 140067041732704
	140067041732128 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (33, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041730928 -> 140067041732128
	140066495922416 [label="q_encoder.encoder.text_encoding_layer.4.attention.self.key.bias
 (768)" fillcolor=lightblue]
	140066495922416 -> 140067041730928
	140067041730928 [label=AccumulateGrad]
	140067041730832 -> 140067041732128
	140067041730832 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 11, 768)"]
	140066907245824 -> 140067041730832
	140067041733568 -> 140067041732128
	140067041733568 [label=TBackward0]
	140067041730736 -> 140067041733568
	140066489089824 [label="q_encoder.encoder.text_encoding_layer.4.attention.self.key.weight
 (768, 768)" fillcolor=lightblue]
	140066489089824 -> 140067041730736
	140067041730736 [label=AccumulateGrad]
	140067041735776 -> 140067041735680
	140067041735776 [label="UnsafeViewBackward0
-------------------------------
self_sym_sizes: (3, 12, 11, 64)"]
	140067041735488 -> 140067041735776
	140067041735488 [label=CloneBackward0]
	140067041734816 -> 140067041735488
	140067041734816 [label="ExpandBackward0
-------------------------------
self_sym_sizes: (3, 12, 11, 64)"]
	140067041734768 -> 140067041734816
	140067041734768 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041733376 -> 140067041734768
	140067041733376 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 11, 768)"]
	140067041732080 -> 140067041733376
	140067041732080 [label="ViewBackward0
-------------------------
self_sym_sizes: (33, 768)"]
	140067041731648 -> 140067041732080
	140067041731648 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (33, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041731072 -> 140067041731648
	140066495922576 [label="q_encoder.encoder.text_encoding_layer.4.attention.self.value.bias
 (768)" fillcolor=lightblue]
	140066495922576 -> 140067041731072
	140067041731072 [label=AccumulateGrad]
	140067041732224 -> 140067041731648
	140067041732224 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 11, 768)"]
	140066907245824 -> 140067041732224
	140067041735344 -> 140067041731648
	140067041735344 [label=TBackward0]
	140067041730688 -> 140067041735344
	140066495922496 [label="q_encoder.encoder.text_encoding_layer.4.attention.self.value.weight
 (768, 768)" fillcolor=lightblue]
	140066495922496 -> 140067041730688
	140067041730688 [label=AccumulateGrad]
	140067041738464 -> 140067041736448
	140067041738464 [label=TBackward0]
	140067041736352 -> 140067041738464
	140066495922656 [label="q_encoder.encoder.text_encoding_layer.4.attention.output.dense.weight
 (768, 768)" fillcolor=lightblue]
	140066495922656 -> 140067041736352
	140067041736352 [label=AccumulateGrad]
	140066907245824 -> 140066907245200
	140066907246352 -> 140067041490304
	140066495922896 [label="q_encoder.encoder.text_encoding_layer.4.attention.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	140066495922896 -> 140066907246352
	140066907246352 [label=AccumulateGrad]
	140066907244768 -> 140067041490304
	140066495922816 [label="q_encoder.encoder.text_encoding_layer.4.attention.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	140066495922816 -> 140066907244768
	140066907244768 [label=AccumulateGrad]
	140066907248032 -> 140066907248176
	140066907248032 [label=TBackward0]
	140066907243088 -> 140066907248032
	140066495923056 [label="q_encoder.encoder.text_encoding_layer.4.intermediate.dense.weight
 (3072, 768)" fillcolor=lightblue]
	140066495923056 -> 140066907243088
	140066907243088 [label=AccumulateGrad]
	140067041488864 -> 140067041484208
	140067041488864 [label=TBackward0]
	140066907244672 -> 140067041488864
	140066495922976 [label="q_encoder.encoder.text_encoding_layer.4.output.dense.weight
 (768, 3072)" fillcolor=lightblue]
	140066495922976 -> 140066907244672
	140066907244672 [label=AccumulateGrad]
	140067041490304 -> 140067041476768
	140067041490784 -> 140067041491216
	140066495923376 [label="q_encoder.encoder.text_encoding_layer.4.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	140066495923376 -> 140067041490784
	140067041490784 [label=AccumulateGrad]
	140067041492800 -> 140067041491216
	140066495923216 [label="q_encoder.encoder.text_encoding_layer.4.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	140066495923216 -> 140067041492800
	140067041492800 [label=AccumulateGrad]
	140066879619184 -> 140066879620192
	140066879619184 [label=TBackward0]
	140066879619520 -> 140066879619184
	140066495937056 [label="q_encoder.encoder.information_exchanging_layer.4.attention.self.query.weight
 (768, 768)" fillcolor=lightblue]
	140066495937056 -> 140066879619520
	140066879619520 [label=AccumulateGrad]
	140066879622448 -> 140066879622160
	140066879622448 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 64, 2)"]
	140066879621680 -> 140066879622448
	140066879621680 [label=CloneBackward0]
	140066879620528 -> 140066879621680
	140066879620528 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 64, 2)"]
	140066879620240 -> 140066879620528
	140066879620240 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551615
dim1: 18446744073709551614"]
	140066879622496 -> 140066879620240
	140066879622496 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041491120 -> 140066879622496
	140067041491120 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140067041482192 -> 140067041491120
	140067041482192 [label="ViewBackward0
------------------------
self_sym_sizes: (6, 768)"]
	140067041489152 -> 140067041482192
	140067041489152 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (6, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140066907252736 -> 140067041489152
	140066495937776 [label="q_encoder.encoder.information_exchanging_layer.4.attention.self.key.bias
 (768)" fillcolor=lightblue]
	140066495937776 -> 140066907252736
	140066907252736 [label=AccumulateGrad]
	140066907246496 -> 140067041489152
	140066907246496 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140066879626384 -> 140066907246496
	140066907252208 -> 140067041489152
	140066907252208 [label=TBackward0]
	140066907246304 -> 140066907252208
	140066495937616 [label="q_encoder.encoder.information_exchanging_layer.4.attention.self.key.weight
 (768, 768)" fillcolor=lightblue]
	140066495937616 -> 140066907246304
	140066907246304 [label=AccumulateGrad]
	140066879623696 -> 140066879623936
	140066879623696 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 2, 64)"]
	140066879623504 -> 140066879623696
	140066879623504 [label=CloneBackward0]
	140066879622640 -> 140066879623504
	140066879622640 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 2, 64)"]
	140066879622352 -> 140066879622640
	140066879622352 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140066907244096 -> 140066879622352
	140066907244096 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140066879623552 -> 140066907244096
	140066879623552 [label="ViewBackward0
------------------------
self_sym_sizes: (6, 768)"]
	140066879621296 -> 140066879623552
	140066879621296 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (6, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041481136 -> 140066879621296
	140066495937936 [label="q_encoder.encoder.information_exchanging_layer.4.attention.self.value.bias
 (768)" fillcolor=lightblue]
	140066495937936 -> 140067041481136
	140067041481136 [label=AccumulateGrad]
	140067041491984 -> 140066879621296
	140067041491984 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140066879626384 -> 140067041491984
	140067041485744 -> 140066879621296
	140067041485744 [label=TBackward0]
	140067041484112 -> 140067041485744
	140066495937856 [label="q_encoder.encoder.information_exchanging_layer.4.attention.self.value.weight
 (768, 768)" fillcolor=lightblue]
	140066495937856 -> 140067041484112
	140067041484112 [label=AccumulateGrad]
	140066879624512 -> 140066879625472
	140066879624512 [label=TBackward0]
	140066879625088 -> 140066879624512
	140066495938016 [label="q_encoder.encoder.information_exchanging_layer.4.attention.output.dense.weight
 (768, 768)" fillcolor=lightblue]
	140066495938016 -> 140066879625088
	140066879625088 [label=AccumulateGrad]
	140066879626384 -> 140066879626432
	140066879626624 -> 140066879630272
	140066495938256 [label="q_encoder.encoder.information_exchanging_layer.4.attention.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	140066495938256 -> 140066879626624
	140066879626624 [label=AccumulateGrad]
	140066879625904 -> 140066879630272
	140066495938176 [label="q_encoder.encoder.information_exchanging_layer.4.attention.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	140066495938176 -> 140066879625904
	140066879625904 [label=AccumulateGrad]
	140066879626480 -> 140066879627488
	140066879626480 [label=TBackward0]
	140066879626096 -> 140066879626480
	140066495938416 [label="q_encoder.encoder.information_exchanging_layer.4.intermediate.dense.weight
 (3072, 768)" fillcolor=lightblue]
	140066495938416 -> 140066879626096
	140066879626096 [label=AccumulateGrad]
	140066879627872 -> 140066879628832
	140066879627872 [label=TBackward0]
	140066879628544 -> 140066879627872
	140066495937696 [label="q_encoder.encoder.information_exchanging_layer.4.output.dense.weight
 (768, 3072)" fillcolor=lightblue]
	140066495937696 -> 140066879628544
	140066879628544 [label=AccumulateGrad]
	140066879630272 -> 140066879629504
	140066879629408 -> 140066879630608
	140066794635504 [label="q_encoder.encoder.information_exchanging_layer.4.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	140066794635504 -> 140066879629408
	140066879629408 [label=AccumulateGrad]
	140066879629216 -> 140066879630608
	140066794635344 [label="q_encoder.encoder.information_exchanging_layer.4.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	140066794635344 -> 140066879629216
	140066879629216 [label=AccumulateGrad]
	140066879631232 -> 140067041511312
	140066879631232 [label=CloneBackward0]
	140066879630800 -> 140066879631232
	140066879630800 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 1, 768)
start         :                   0
step          :                   1"]
	140066879629648 -> 140066879630800
	140066879629648 [label="SelectBackward0
-------------------------------
dim           :               2
index         :               1
self_sym_sizes: (3, 1, 11, 768)"]
	140066879626144 -> 140066879629648
	140066879626144 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:     (3, 1, 11, 768)
start         :                   0
step          :                   1"]
	140066879629072 -> 140066879626144
	140066879629072 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:     (3, 1, 11, 768)
start         :                   0
step          :                   1"]
	140066879624224 -> 140066879629072
	140066879624224 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 11, 768)"]
	140066879625040 -> 140066879624224
	140066879625040 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140066879626768 -> 140066879625040
	140066879626768 [label="AddBackward0
------------
alpha: 1"]
	140066879625760 -> 140066879626768
	140066879625760 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140066879629360 -> 140066879625760
	140066879629360 [label="ViewBackward0
-------------------------
self_sym_sizes: (33, 768)"]
	140066879619280 -> 140066879629360
	140066879619280 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :     (33, 3072)
mat1_sym_strides:      (3072, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (3072, 768)
mat2_sym_strides:      (1, 3072)"]
	140067041482480 -> 140066879619280
	140066495923296 [label="q_encoder.encoder.text_encoding_layer.5.output.dense.bias
 (768)" fillcolor=lightblue]
	140066495923296 -> 140067041482480
	140067041482480 [label=AccumulateGrad]
	140067041480704 -> 140066879619280
	140067041480704 [label="ViewBackward0
-----------------------------
self_sym_sizes: (3, 11, 3072)"]
	140067041489536 -> 140067041480704
	140067041489536 [label="GeluBackward0
---------------------------
approximate:           none
self       : [saved tensor]"]
	140066879620816 -> 140067041489536
	140066879620816 [label="ViewBackward0
--------------------------
self_sym_sizes: (33, 3072)"]
	140066879628352 -> 140066879620816
	140066879628352 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (33, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (768, 3072)
mat2_sym_strides:       (1, 768)"]
	140067041735824 -> 140066879628352
	140066495924416 [label="q_encoder.encoder.text_encoding_layer.5.intermediate.dense.bias
 (3072)" fillcolor=lightblue]
	140066495924416 -> 140067041735824
	140067041735824 [label=AccumulateGrad]
	140067041735968 -> 140066879628352
	140067041735968 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 11, 768)"]
	140066879623888 -> 140067041735968
	140066879623888 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140067041734960 -> 140066879623888
	140067041734960 [label="AddBackward0
------------
alpha: 1"]
	140067041733280 -> 140067041734960
	140067041733280 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041735392 -> 140067041733280
	140067041735392 [label="ViewBackward0
-------------------------
self_sym_sizes: (33, 768)"]
	140067041730544 -> 140067041735392
	140067041730544 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (33, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041737504 -> 140067041730544
	140066495924016 [label="q_encoder.encoder.text_encoding_layer.5.attention.output.dense.bias
 (768)" fillcolor=lightblue]
	140066495924016 -> 140067041737504
	140067041737504 [label=AccumulateGrad]
	140067041731552 -> 140067041730544
	140067041731552 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 11, 768)"]
	140067041730400 -> 140067041731552
	140067041730400 [label="ViewBackward0
-------------------------------
self_sym_sizes: (3, 11, 12, 64)"]
	140067041730496 -> 140067041730400
	140067041730496 [label=CloneBackward0]
	140067041731168 -> 140067041730496
	140067041731168 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041730112 -> 140067041731168
	140067041730112 [label="UnsafeViewBackward0
----------------------------
self_sym_sizes: (36, 11, 64)"]
	140067041729680 -> 140067041730112
	140067041729680 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140067041729536 -> 140067041729680
	140067041729536 [label="ViewBackward0
-------------------------------
self_sym_sizes: (3, 12, 11, 11)"]
	140067041729344 -> 140067041729536
	140067041729344 [label="ExpandBackward0
-------------------------------
self_sym_sizes: (3, 12, 11, 11)"]
	140067041729152 -> 140067041729344
	140067041729152 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041729008 -> 140067041729152
	140067041729008 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140067041728576 -> 140067041729008
	140067041728576 [label="AddBackward0
------------
alpha: 1"]
	140067041728480 -> 140067041728576
	140067041728480 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041728288 -> 140067041728480
	140067041728288 [label="UnsafeViewBackward0
----------------------------
self_sym_sizes: (36, 11, 11)"]
	140067041728048 -> 140067041728288
	140067041728048 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140067041727952 -> 140067041728048
	140067041727952 [label="UnsafeViewBackward0
-------------------------------
self_sym_sizes: (3, 12, 11, 64)"]
	140067041727904 -> 140067041727952
	140067041727904 [label=CloneBackward0]
	140067041727568 -> 140067041727904
	140067041727568 [label="ExpandBackward0
-------------------------------
self_sym_sizes: (3, 12, 11, 64)"]
	140067041727472 -> 140067041727568
	140067041727472 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041727232 -> 140067041727472
	140067041727232 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 11, 768)"]
	140067041727040 -> 140067041727232
	140067041727040 [label="ViewBackward0
-------------------------
self_sym_sizes: (33, 768)"]
	140067041726848 -> 140067041727040
	140067041726848 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (33, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041726608 -> 140067041726848
	140066495923536 [label="q_encoder.encoder.text_encoding_layer.5.attention.self.query.bias
 (768)" fillcolor=lightblue]
	140066495923536 -> 140067041726608
	140067041726608 [label=AccumulateGrad]
	140067041726752 -> 140067041726848
	140067041726752 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 11, 768)"]
	140067041732320 -> 140067041726752
	140067041732320 [label="ViewBackward0
-------------------------------
self_sym_sizes: (3, 1, 11, 768)"]
	140067041726224 -> 140067041732320
	140067041726224 [label="AsStridedBackward0
------------------------------------
size          :      (3, 1, 11, 768)
storage_offset:                    0
stride        : (8448, 8448, 768, 1)"]
	140067041725984 -> 140067041726224
	140067041725984 [label=CopySlices]
	140067041491216 -> 140067041725984
	140067041725792 -> 140067041725984
	140067041725792 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 1, 768)
start         :                   0
step          :                   1"]
	140067041725936 -> 140067041725792
	140067041725936 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:         (3, 2, 768)
start         :                   1
step          :                   1"]
	140067041725216 -> 140067041725936
	140067041725216 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:         (3, 2, 768)
start         :                   0
step          :                   1"]
	140066879630608 -> 140067041725216
	140067041727760 -> 140067041726848
	140067041727760 [label=TBackward0]
	140067041726560 -> 140067041727760
	140066495923456 [label="q_encoder.encoder.text_encoding_layer.5.attention.self.query.weight
 (768, 768)" fillcolor=lightblue]
	140066495923456 -> 140067041726560
	140067041726560 [label=AccumulateGrad]
	140067041728432 -> 140067041728048
	140067041728432 [label="UnsafeViewBackward0
-------------------------------
self_sym_sizes: (3, 12, 64, 11)"]
	140067041728240 -> 140067041728432
	140067041728240 [label=CloneBackward0]
	140067041726992 -> 140067041728240
	140067041726992 [label="ExpandBackward0
-------------------------------
self_sym_sizes: (3, 12, 64, 11)"]
	140067041727136 -> 140067041726992
	140067041727136 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551615
dim1: 18446744073709551614"]
	140067041725888 -> 140067041727136
	140067041725888 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041725072 -> 140067041725888
	140067041725072 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 11, 768)"]
	140067041726704 -> 140067041725072
	140067041726704 [label="ViewBackward0
-------------------------
self_sym_sizes: (33, 768)"]
	140067041726128 -> 140067041726704
	140067041726128 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (33, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041724976 -> 140067041726128
	140066495923696 [label="q_encoder.encoder.text_encoding_layer.5.attention.self.key.bias
 (768)" fillcolor=lightblue]
	140066495923696 -> 140067041724976
	140067041724976 [label=AccumulateGrad]
	140067041724928 -> 140067041726128
	140067041724928 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 11, 768)"]
	140067041732320 -> 140067041724928
	140067041727616 -> 140067041726128
	140067041727616 [label=TBackward0]
	140067041724784 -> 140067041727616
	140066495923616 [label="q_encoder.encoder.text_encoding_layer.5.attention.self.key.weight
 (768, 768)" fillcolor=lightblue]
	140066495923616 -> 140067041724784
	140067041724784 [label=AccumulateGrad]
	140067041729776 -> 140067041729680
	140067041729776 [label="UnsafeViewBackward0
-------------------------------
self_sym_sizes: (3, 12, 11, 64)"]
	140067041729392 -> 140067041729776
	140067041729392 [label=CloneBackward0]
	140067041728816 -> 140067041729392
	140067041728816 [label="ExpandBackward0
-------------------------------
self_sym_sizes: (3, 12, 11, 64)"]
	140067041728720 -> 140067041728816
	140067041728720 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041727424 -> 140067041728720
	140067041727424 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 11, 768)"]
	140067041726032 -> 140067041727424
	140067041726032 [label="ViewBackward0
-------------------------
self_sym_sizes: (33, 768)"]
	140067041725744 -> 140067041726032
	140067041725744 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (33, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041725120 -> 140067041725744
	140066495923856 [label="q_encoder.encoder.text_encoding_layer.5.attention.self.value.bias
 (768)" fillcolor=lightblue]
	140066495923856 -> 140067041725120
	140067041725120 [label=AccumulateGrad]
	140067041726368 -> 140067041725744
	140067041726368 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 11, 768)"]
	140067041732320 -> 140067041726368
	140067041729200 -> 140067041725744
	140067041729200 [label=TBackward0]
	140067041724688 -> 140067041729200
	140066495923776 [label="q_encoder.encoder.text_encoding_layer.5.attention.self.value.weight
 (768, 768)" fillcolor=lightblue]
	140066495923776 -> 140067041724688
	140067041724688 [label=AccumulateGrad]
	140067041731504 -> 140067041730544
	140067041731504 [label=TBackward0]
	140067041730448 -> 140067041731504
	140066495923936 [label="q_encoder.encoder.text_encoding_layer.5.attention.output.dense.weight
 (768, 768)" fillcolor=lightblue]
	140066495923936 -> 140067041730448
	140067041730448 [label=AccumulateGrad]
	140067041732320 -> 140067041734960
	140067041736160 -> 140066879623888
	140066495924176 [label="q_encoder.encoder.text_encoding_layer.5.attention.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	140066495924176 -> 140067041736160
	140067041736160 [label=AccumulateGrad]
	140067041734720 -> 140066879623888
	140066495924096 [label="q_encoder.encoder.text_encoding_layer.5.attention.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	140066495924096 -> 140067041734720
	140067041734720 [label=AccumulateGrad]
	140067041737312 -> 140066879628352
	140067041737312 [label=TBackward0]
	140067041731456 -> 140067041737312
	140066495924336 [label="q_encoder.encoder.text_encoding_layer.5.intermediate.dense.weight
 (3072, 768)" fillcolor=lightblue]
	140066495924336 -> 140067041731456
	140067041731456 [label=AccumulateGrad]
	140067041491456 -> 140066879619280
	140067041491456 [label=TBackward0]
	140066879622208 -> 140067041491456
	140066495924256 [label="q_encoder.encoder.text_encoding_layer.5.output.dense.weight
 (768, 3072)" fillcolor=lightblue]
	140066495924256 -> 140066879622208
	140066879622208 [label=AccumulateGrad]
	140066879623888 -> 140066879626768
	140066879624080 -> 140066879625040
	140066495924656 [label="q_encoder.encoder.text_encoding_layer.5.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	140066495924656 -> 140066879624080
	140066879624080 [label=AccumulateGrad]
	140066879631376 -> 140066879625040
	140066495924496 [label="q_encoder.encoder.text_encoding_layer.5.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	140066495924496 -> 140066879631376
	140066879631376 [label=AccumulateGrad]
	140066879630992 -> 140066879632096
	140066879630992 [label=TBackward0]
	140067041487616 -> 140066879630992
	140066495938336 [label="q_encoder.encoder.information_exchanging_layer.5.attention.self.query.weight
 (768, 768)" fillcolor=lightblue]
	140066495938336 -> 140067041487616
	140067041487616 [label=AccumulateGrad]
	140066879634304 -> 140066879634016
	140066879634304 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 64, 2)"]
	140066879633824 -> 140066879634304
	140066879633824 [label=CloneBackward0]
	140066879632624 -> 140066879633824
	140066879632624 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 64, 2)"]
	140066879632288 -> 140066879632624
	140066879632288 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551615
dim1: 18446744073709551614"]
	140066879629552 -> 140066879632288
	140066879629552 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140066879628160 -> 140066879629552
	140066879628160 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140066879624944 -> 140066879628160
	140066879624944 [label="ViewBackward0
------------------------
self_sym_sizes: (6, 768)"]
	140066879626576 -> 140066879624944
	140066879626576 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (6, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140066879627536 -> 140066879626576
	140066794635824 [label="q_encoder.encoder.information_exchanging_layer.5.attention.self.key.bias
 (768)" fillcolor=lightblue]
	140066794635824 -> 140066879627536
	140066879627536 [label=AccumulateGrad]
	140066879623120 -> 140066879626576
	140066879623120 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140067041511312 -> 140066879623120
	140066879634400 -> 140066879626576
	140066879634400 [label=TBackward0]
	140066879621632 -> 140066879634400
	140066794635664 [label="q_encoder.encoder.information_exchanging_layer.5.attention.self.key.weight
 (768, 768)" fillcolor=lightblue]
	140066794635664 -> 140066879621632
	140066879621632 [label=AccumulateGrad]
	140066879635168 -> 140067041509536
	140066879635168 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 2, 64)"]
	140066879632960 -> 140066879635168
	140066879632960 [label=CloneBackward0]
	140066879634784 -> 140066879632960
	140066879634784 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 2, 64)"]
	140066879634160 -> 140066879634784
	140066879634160 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140066879633296 -> 140066879634160
	140066879633296 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140066879631136 -> 140066879633296
	140066879631136 [label="ViewBackward0
------------------------
self_sym_sizes: (6, 768)"]
	140066879628880 -> 140066879631136
	140066879628880 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (6, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140066879622880 -> 140066879628880
	140066794635984 [label="q_encoder.encoder.information_exchanging_layer.5.attention.self.value.bias
 (768)" fillcolor=lightblue]
	140066794635984 -> 140066879622880
	140066879622880 [label=AccumulateGrad]
	140066879631328 -> 140066879628880
	140066879631328 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140067041511312 -> 140066879631328
	140066879633008 -> 140066879628880
	140066879633008 [label=TBackward0]
	140066879625616 -> 140066879633008
	140066794635904 [label="q_encoder.encoder.information_exchanging_layer.5.attention.self.value.weight
 (768, 768)" fillcolor=lightblue]
	140066794635904 -> 140066879625616
	140066879625616 [label=AccumulateGrad]
	140067041510064 -> 140067041510640
	140067041510064 [label=TBackward0]
	140067041510304 -> 140067041510064
	140066794636064 [label="q_encoder.encoder.information_exchanging_layer.5.attention.output.dense.weight
 (768, 768)" fillcolor=lightblue]
	140066794636064 -> 140067041510304
	140067041510304 [label=AccumulateGrad]
	140067041511312 -> 140067041511360
	140067041511552 -> 140067041514480
	140066794636304 [label="q_encoder.encoder.information_exchanging_layer.5.attention.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	140066794636304 -> 140067041511552
	140067041511552 [label=AccumulateGrad]
	140067041511072 -> 140067041514480
	140066794636224 [label="q_encoder.encoder.information_exchanging_layer.5.attention.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	140066794636224 -> 140067041511072
	140067041511072 [label=AccumulateGrad]
	140067041511408 -> 140067041512080
	140067041511408 [label=TBackward0]
	140067041511120 -> 140067041511408
	140066794636464 [label="q_encoder.encoder.information_exchanging_layer.5.intermediate.dense.weight
 (3072, 768)" fillcolor=lightblue]
	140066794636464 -> 140067041511120
	140067041511120 [label=AccumulateGrad]
	140067041512368 -> 140067041513184
	140067041512368 [label=TBackward0]
	140067041512752 -> 140067041512368
	140066794635744 [label="q_encoder.encoder.information_exchanging_layer.5.output.dense.weight
 (768, 3072)" fillcolor=lightblue]
	140066794635744 -> 140067041512752
	140067041512752 [label=AccumulateGrad]
	140067041514480 -> 140067041513856
	140067041513712 -> 140067041514624
	140066794636784 [label="q_encoder.encoder.information_exchanging_layer.5.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	140066794636784 -> 140067041513712
	140067041513712 [label=AccumulateGrad]
	140067041513520 -> 140067041514624
	140066794636704 [label="q_encoder.encoder.information_exchanging_layer.5.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	140066794636704 -> 140067041513520
	140067041513520 [label=AccumulateGrad]
	140067041515200 -> 140067041521056
	140067041515200 [label=CloneBackward0]
	140067041514864 -> 140067041515200
	140067041514864 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 1, 768)
start         :                   0
step          :                   1"]
	140067041514048 -> 140067041514864
	140067041514048 [label="SelectBackward0
-------------------------------
dim           :               2
index         :               1
self_sym_sizes: (3, 1, 11, 768)"]
	140067041511168 -> 140067041514048
	140067041511168 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:     (3, 1, 11, 768)
start         :                   0
step          :                   1"]
	140067041513328 -> 140067041511168
	140067041513328 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:     (3, 1, 11, 768)
start         :                   0
step          :                   1"]
	140067041509968 -> 140067041513328
	140067041509968 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 11, 768)"]
	140067041509728 -> 140067041509968
	140067041509728 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140067041511696 -> 140067041509728
	140067041511696 [label="AddBackward0
------------
alpha: 1"]
	140067041513664 -> 140067041511696
	140067041513664 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140066879633152 -> 140067041513664
	140066879633152 [label="ViewBackward0
-------------------------
self_sym_sizes: (33, 768)"]
	140066879630704 -> 140066879633152
	140066879630704 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :     (33, 3072)
mat1_sym_strides:      (3072, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (3072, 768)
mat2_sym_strides:      (1, 3072)"]
	140066879635312 -> 140066879630704
	140066495922336 [label="q_encoder.encoder.text_encoding_layer.6.output.dense.bias
 (768)" fillcolor=lightblue]
	140066495922336 -> 140066879635312
	140066879635312 [label=AccumulateGrad]
	140066879634064 -> 140066879630704
	140066879634064 [label="ViewBackward0
-----------------------------
self_sym_sizes: (3, 11, 3072)"]
	140066879632816 -> 140066879634064
	140066879632816 [label="GeluBackward0
---------------------------
approximate:           none
self       : [saved tensor]"]
	140067041731312 -> 140066879632816
	140067041731312 [label="ViewBackward0
--------------------------
self_sym_sizes: (33, 3072)"]
	140067041736064 -> 140067041731312
	140067041736064 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (33, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (768, 3072)
mat2_sym_strides:       (1, 768)"]
	140067041729968 -> 140067041736064
	140066495925696 [label="q_encoder.encoder.text_encoding_layer.6.intermediate.dense.bias
 (3072)" fillcolor=lightblue]
	140066495925696 -> 140067041729968
	140067041729968 [label=AccumulateGrad]
	140067041730160 -> 140067041736064
	140067041730160 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 11, 768)"]
	140067041512176 -> 140067041730160
	140067041512176 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140067041729104 -> 140067041512176
	140067041729104 [label="AddBackward0
------------
alpha: 1"]
	140067041727376 -> 140067041729104
	140067041727376 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041729296 -> 140067041727376
	140067041729296 [label="ViewBackward0
-------------------------
self_sym_sizes: (33, 768)"]
	140067041724640 -> 140067041729296
	140067041724640 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (33, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041734240 -> 140067041724640
	140066495925296 [label="q_encoder.encoder.text_encoding_layer.6.attention.output.dense.bias
 (768)" fillcolor=lightblue]
	140066495925296 -> 140067041734240
	140067041734240 [label=AccumulateGrad]
	140067041725648 -> 140067041724640
	140067041725648 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 11, 768)"]
	140067041724352 -> 140067041725648
	140067041724352 [label="ViewBackward0
-------------------------------
self_sym_sizes: (3, 11, 12, 64)"]
	140067041724544 -> 140067041724352
	140067041724544 [label=CloneBackward0]
	140067041725312 -> 140067041724544
	140067041725312 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041723920 -> 140067041725312
	140067041723920 [label="UnsafeViewBackward0
----------------------------
self_sym_sizes: (36, 11, 64)"]
	140067041723728 -> 140067041723920
	140067041723728 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140067041723584 -> 140067041723728
	140067041723584 [label="ViewBackward0
-------------------------------
self_sym_sizes: (3, 12, 11, 11)"]
	140067041723488 -> 140067041723584
	140067041723488 [label="ExpandBackward0
-------------------------------
self_sym_sizes: (3, 12, 11, 11)"]
	140067041723248 -> 140067041723488
	140067041723248 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041723056 -> 140067041723248
	140067041723056 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140067041722768 -> 140067041723056
	140067041722768 [label="AddBackward0
------------
alpha: 1"]
	140067041722672 -> 140067041722768
	140067041722672 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041722528 -> 140067041722672
	140067041722528 [label="UnsafeViewBackward0
----------------------------
self_sym_sizes: (36, 11, 11)"]
	140067041722576 -> 140067041722528
	140067041722576 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140067041723392 -> 140067041722576
	140067041723392 [label="UnsafeViewBackward0
-------------------------------
self_sym_sizes: (3, 12, 11, 64)"]
	140067041607344 -> 140067041723392
	140067041607344 [label=CloneBackward0]
	140067041607008 -> 140067041607344
	140067041607008 [label="ExpandBackward0
-------------------------------
self_sym_sizes: (3, 12, 11, 64)"]
	140067041606864 -> 140067041607008
	140067041606864 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041606624 -> 140067041606864
	140067041606624 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 11, 768)"]
	140067041606528 -> 140067041606624
	140067041606528 [label="ViewBackward0
-------------------------
self_sym_sizes: (33, 768)"]
	140067041606288 -> 140067041606528
	140067041606288 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (33, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041606096 -> 140067041606288
	140066495924816 [label="q_encoder.encoder.text_encoding_layer.6.attention.self.query.bias
 (768)" fillcolor=lightblue]
	140066495924816 -> 140067041606096
	140067041606096 [label=AccumulateGrad]
	140067041606240 -> 140067041606288
	140067041606240 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 11, 768)"]
	140067041726416 -> 140067041606240
	140067041726416 [label="ViewBackward0
-------------------------------
self_sym_sizes: (3, 1, 11, 768)"]
	140067041605568 -> 140067041726416
	140067041605568 [label="AsStridedBackward0
------------------------------------
size          :      (3, 1, 11, 768)
storage_offset:                    0
stride        : (8448, 8448, 768, 1)"]
	140067041605232 -> 140067041605568
	140067041605232 [label=CopySlices]
	140066879625040 -> 140067041605232
	140067041604896 -> 140067041605232
	140067041604896 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 1, 768)
start         :                   0
step          :                   1"]
	140067041605136 -> 140067041604896
	140067041605136 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:         (3, 2, 768)
start         :                   1
step          :                   1"]
	140067041604224 -> 140067041605136
	140067041604224 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:         (3, 2, 768)
start         :                   0
step          :                   1"]
	140067041514624 -> 140067041604224
	140067041607248 -> 140067041606288
	140067041607248 [label=TBackward0]
	140067041605952 -> 140067041607248
	140066495924736 [label="q_encoder.encoder.text_encoding_layer.6.attention.self.query.weight
 (768, 768)" fillcolor=lightblue]
	140066495924736 -> 140067041605952
	140067041605952 [label=AccumulateGrad]
	140067041607536 -> 140067041722576
	140067041607536 [label="UnsafeViewBackward0
-------------------------------
self_sym_sizes: (3, 12, 64, 11)"]
	140067041607392 -> 140067041607536
	140067041607392 [label=CloneBackward0]
	140067041606336 -> 140067041607392
	140067041606336 [label="ExpandBackward0
-------------------------------
self_sym_sizes: (3, 12, 64, 11)"]
	140067041606576 -> 140067041606336
	140067041606576 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551615
dim1: 18446744073709551614"]
	140067041605040 -> 140067041606576
	140067041605040 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041604032 -> 140067041605040
	140067041604032 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 11, 768)"]
	140067041606192 -> 140067041604032
	140067041606192 [label="ViewBackward0
-------------------------
self_sym_sizes: (33, 768)"]
	140067041605424 -> 140067041606192
	140067041605424 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (33, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041603984 -> 140067041605424
	140066495924976 [label="q_encoder.encoder.text_encoding_layer.6.attention.self.key.bias
 (768)" fillcolor=lightblue]
	140066495924976 -> 140067041603984
	140067041603984 [label=AccumulateGrad]
	140067041603888 -> 140067041605424
	140067041603888 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 11, 768)"]
	140067041726416 -> 140067041603888
	140067041607152 -> 140067041605424
	140067041607152 [label=TBackward0]
	140067041603792 -> 140067041607152
	140066495924896 [label="q_encoder.encoder.text_encoding_layer.6.attention.self.key.weight
 (768, 768)" fillcolor=lightblue]
	140066495924896 -> 140067041603792
	140067041603792 [label=AccumulateGrad]
	140067041723776 -> 140067041723728
	140067041723776 [label="UnsafeViewBackward0
-------------------------------
self_sym_sizes: (3, 12, 11, 64)"]
	140067041723536 -> 140067041723776
	140067041723536 [label=CloneBackward0]
	140067041722960 -> 140067041723536
	140067041722960 [label="ExpandBackward0
-------------------------------
self_sym_sizes: (3, 12, 11, 64)"]
	140067041722912 -> 140067041722960
	140067041722912 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041723296 -> 140067041722912
	140067041723296 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 11, 768)"]
	140067041605280 -> 140067041723296
	140067041605280 [label="ViewBackward0
-------------------------
self_sym_sizes: (33, 768)"]
	140067041604800 -> 140067041605280
	140067041604800 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (33, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041604128 -> 140067041604800
	140066495925136 [label="q_encoder.encoder.text_encoding_layer.6.attention.self.value.bias
 (768)" fillcolor=lightblue]
	140066495925136 -> 140067041604128
	140067041604128 [label=AccumulateGrad]
	140067041605712 -> 140067041604800
	140067041605712 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 11, 768)"]
	140067041726416 -> 140067041605712
	140067041606816 -> 140067041604800
	140067041606816 [label=TBackward0]
	140067041603744 -> 140067041606816
	140066495925056 [label="q_encoder.encoder.text_encoding_layer.6.attention.self.value.weight
 (768, 768)" fillcolor=lightblue]
	140066495925056 -> 140067041603744
	140067041603744 [label=AccumulateGrad]
	140067041725600 -> 140067041724640
	140067041725600 [label=TBackward0]
	140067041724448 -> 140067041725600
	140066495925216 [label="q_encoder.encoder.text_encoding_layer.6.attention.output.dense.weight
 (768, 768)" fillcolor=lightblue]
	140066495925216 -> 140067041724448
	140067041724448 [label=AccumulateGrad]
	140067041726416 -> 140067041729104
	140067041730256 -> 140067041512176
	140066495925456 [label="q_encoder.encoder.text_encoding_layer.6.attention.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	140066495925456 -> 140067041730256
	140067041730256 [label=AccumulateGrad]
	140067041728624 -> 140067041512176
	140066495925376 [label="q_encoder.encoder.text_encoding_layer.6.attention.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	140066495925376 -> 140067041728624
	140067041728624 [label=AccumulateGrad]
	140067041731360 -> 140067041736064
	140067041731360 [label=TBackward0]
	140067041725552 -> 140067041731360
	140066495925616 [label="q_encoder.encoder.text_encoding_layer.6.intermediate.dense.weight
 (3072, 768)" fillcolor=lightblue]
	140066495925616 -> 140067041725552
	140067041725552 [label=AccumulateGrad]
	140066879630656 -> 140066879630704
	140066879630656 [label=TBackward0]
	140067041736208 -> 140066879630656
	140066495925536 [label="q_encoder.encoder.text_encoding_layer.6.output.dense.weight
 (768, 3072)" fillcolor=lightblue]
	140066495925536 -> 140067041736208
	140067041736208 [label=AccumulateGrad]
	140067041512176 -> 140067041511696
	140067041510208 -> 140067041509728
	140066495925936 [label="q_encoder.encoder.text_encoding_layer.6.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	140066495925936 -> 140067041510208
	140067041510208 [label=AccumulateGrad]
	140067041515392 -> 140067041509728
	140066495925776 [label="q_encoder.encoder.text_encoding_layer.6.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	140066495925776 -> 140067041515392
	140067041515392 [label=AccumulateGrad]
	140067041515056 -> 140067041515824
	140067041515056 [label=TBackward0]
	140067041514768 -> 140067041515056
	140066794636384 [label="q_encoder.encoder.information_exchanging_layer.6.attention.self.query.weight
 (768, 768)" fillcolor=lightblue]
	140066794636384 -> 140067041514768
	140067041514768 [label=AccumulateGrad]
	140067041517504 -> 140067041517120
	140067041517504 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 64, 2)"]
	140067041517024 -> 140067041517504
	140067041517024 [label=CloneBackward0]
	140067041516064 -> 140067041517024
	140067041516064 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 64, 2)"]
	140067041515872 -> 140067041516064
	140067041515872 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551615
dim1: 18446744073709551614"]
	140067041513232 -> 140067041515872
	140067041513232 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041510160 -> 140067041513232
	140067041510160 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140067041511504 -> 140067041510160
	140067041511504 [label="ViewBackward0
------------------------
self_sym_sizes: (6, 768)"]
	140067041510880 -> 140067041511504
	140067041510880 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (6, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041517600 -> 140067041510880
	140066794637104 [label="q_encoder.encoder.information_exchanging_layer.6.attention.self.key.bias
 (768)" fillcolor=lightblue]
	140066794637104 -> 140067041517600
	140067041517600 [label=AccumulateGrad]
	140067041512656 -> 140067041510880
	140067041512656 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140067041521056 -> 140067041512656
	140066879633680 -> 140067041510880
	140066879633680 [label=TBackward0]
	140066879634928 -> 140066879633680
	140066794636944 [label="q_encoder.encoder.information_exchanging_layer.6.attention.self.key.weight
 (768, 768)" fillcolor=lightblue]
	140066794636944 -> 140066879634928
	140066879634928 [label=AccumulateGrad]
	140067041519040 -> 140067041519280
	140067041519040 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 2, 64)"]
	140067041518752 -> 140067041519040
	140067041518752 [label=CloneBackward0]
	140067041517792 -> 140067041518752
	140067041517792 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 2, 64)"]
	140067041517408 -> 140067041517792
	140067041517408 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041516640 -> 140067041517408
	140067041516640 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140067041515152 -> 140067041516640
	140067041515152 [label="ViewBackward0
------------------------
self_sym_sizes: (6, 768)"]
	140066879623456 -> 140067041515152
	140066879623456 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (6, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041518848 -> 140066879623456
	140066794637264 [label="q_encoder.encoder.information_exchanging_layer.6.attention.self.value.bias
 (768)" fillcolor=lightblue]
	140066794637264 -> 140067041518848
	140067041518848 [label=AccumulateGrad]
	140067041515344 -> 140066879623456
	140067041515344 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140067041521056 -> 140067041515344
	140067041510688 -> 140066879623456
	140067041510688 [label=TBackward0]
	140067041730352 -> 140067041510688
	140066794637184 [label="q_encoder.encoder.information_exchanging_layer.6.attention.self.value.weight
 (768, 768)" fillcolor=lightblue]
	140066794637184 -> 140067041730352
	140067041730352 [label=AccumulateGrad]
	140067041519856 -> 140067041520384
	140067041519856 [label=TBackward0]
	140067041520048 -> 140067041519856
	140066794637344 [label="q_encoder.encoder.information_exchanging_layer.6.attention.output.dense.weight
 (768, 768)" fillcolor=lightblue]
	140066794637344 -> 140067041520048
	140067041520048 [label=AccumulateGrad]
	140067041521056 -> 140067041521152
	140067041521344 -> 140067041524080
	140066794637584 [label="q_encoder.encoder.information_exchanging_layer.6.attention.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	140066794637584 -> 140067041521344
	140067041521344 [label=AccumulateGrad]
	140067041520768 -> 140067041524080
	140066794637504 [label="q_encoder.encoder.information_exchanging_layer.6.attention.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	140066794637504 -> 140067041520768
	140067041520768 [label=AccumulateGrad]
	140067041521248 -> 140067041521872
	140067041521248 [label=TBackward0]
	140067041520864 -> 140067041521248
	140066794637744 [label="q_encoder.encoder.information_exchanging_layer.6.intermediate.dense.weight
 (3072, 768)" fillcolor=lightblue]
	140066794637744 -> 140067041520864
	140067041520864 [label=AccumulateGrad]
	140067041522064 -> 140067041522832
	140067041522064 [label=TBackward0]
	140067041522496 -> 140067041522064
	140066794637024 [label="q_encoder.encoder.information_exchanging_layer.6.output.dense.weight
 (768, 3072)" fillcolor=lightblue]
	140066794637024 -> 140067041522496
	140067041522496 [label=AccumulateGrad]
	140067041524080 -> 140067041523360
	140067041523264 -> 140067041524224
	140066794638064 [label="q_encoder.encoder.information_exchanging_layer.6.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	140066794638064 -> 140067041523264
	140067041523264 [label=AccumulateGrad]
	140067041523120 -> 140067041524224
	140066794637984 [label="q_encoder.encoder.information_exchanging_layer.6.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	140066794637984 -> 140067041523120
	140067041523120 [label=AccumulateGrad]
	140067041524752 -> 140067042037616
	140067041524752 [label=CloneBackward0]
	140067041524416 -> 140067041524752
	140067041524416 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 1, 768)
start         :                   0
step          :                   1"]
	140067041523648 -> 140067041524416
	140067041523648 [label="SelectBackward0
-------------------------------
dim           :               2
index         :               1
self_sym_sizes: (3, 1, 11, 768)"]
	140067041520960 -> 140067041523648
	140067041520960 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:     (3, 1, 11, 768)
start         :                   0
step          :                   1"]
	140067041523024 -> 140067041520960
	140067041523024 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:     (3, 1, 11, 768)
start         :                   0
step          :                   1"]
	140067041519760 -> 140067041523024
	140067041519760 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 11, 768)"]
	140067041520000 -> 140067041519760
	140067041520000 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140067041521536 -> 140067041520000
	140067041521536 [label="AddBackward0
------------
alpha: 1"]
	140067041520576 -> 140067041521536
	140067041520576 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041523168 -> 140067041520576
	140067041523168 [label="ViewBackward0
-------------------------
self_sym_sizes: (33, 768)"]
	140067041513952 -> 140067041523168
	140067041513952 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :     (33, 3072)
mat1_sym_strides:      (3072, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (3072, 768)
mat2_sym_strides:      (1, 3072)"]
	140067041522448 -> 140067041513952
	140066495924576 [label="q_encoder.encoder.text_encoding_layer.7.output.dense.bias
 (768)" fillcolor=lightblue]
	140066495924576 -> 140067041522448
	140067041522448 [label=AccumulateGrad]
	140067041517216 -> 140067041513952
	140067041517216 [label="ViewBackward0
-----------------------------
self_sym_sizes: (3, 11, 3072)"]
	140067041516304 -> 140067041517216
	140067041516304 [label="GeluBackward0
---------------------------
approximate:           none
self       : [saved tensor]"]
	140067041728096 -> 140067041516304
	140067041728096 [label="ViewBackward0
--------------------------
self_sym_sizes: (33, 3072)"]
	140067041725456 -> 140067041728096
	140067041725456 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (33, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (768, 3072)
mat2_sym_strides:       (1, 768)"]
	140067041723872 -> 140067041725456
	140066495926976 [label="q_encoder.encoder.text_encoding_layer.7.intermediate.dense.bias
 (3072)" fillcolor=lightblue]
	140066495926976 -> 140067041723872
	140067041723872 [label=AccumulateGrad]
	140067041724016 -> 140067041725456
	140067041724016 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 11, 768)"]
	140067041519232 -> 140067041724016
	140067041519232 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140067041723200 -> 140067041519232
	140067041723200 [label="AddBackward0
------------
alpha: 1"]
	140067041723680 -> 140067041723200
	140067041723680 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041605856 -> 140067041723680
	140067041605856 [label="ViewBackward0
-------------------------
self_sym_sizes: (33, 768)"]
	140067041603648 -> 140067041605856
	140067041603648 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (33, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041606672 -> 140067041603648
	140066495926576 [label="q_encoder.encoder.text_encoding_layer.7.attention.output.dense.bias
 (768)" fillcolor=lightblue]
	140066495926576 -> 140067041606672
	140067041606672 [label=AccumulateGrad]
	140067041604704 -> 140067041603648
	140067041604704 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 11, 768)"]
	140067041603456 -> 140067041604704
	140067041603456 [label="ViewBackward0
-------------------------------
self_sym_sizes: (3, 11, 12, 64)"]
	140067041603600 -> 140067041603456
	140067041603600 [label=CloneBackward0]
	140067041604320 -> 140067041603600
	140067041604320 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041603072 -> 140067041604320
	140067041603072 [label="UnsafeViewBackward0
----------------------------
self_sym_sizes: (36, 11, 64)"]
	140067041602832 -> 140067041603072
	140067041602832 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140067041602640 -> 140067041602832
	140067041602640 [label="ViewBackward0
-------------------------------
self_sym_sizes: (3, 12, 11, 11)"]
	140067041602544 -> 140067041602640
	140067041602544 [label="ExpandBackward0
-------------------------------
self_sym_sizes: (3, 12, 11, 11)"]
	140067041602352 -> 140067041602544
	140067041602352 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041602208 -> 140067041602352
	140067041602208 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140067041601872 -> 140067041602208
	140067041601872 [label="AddBackward0
------------
alpha: 1"]
	140067041601824 -> 140067041601872
	140067041601824 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041601632 -> 140067041601824
	140067041601632 [label="UnsafeViewBackward0
----------------------------
self_sym_sizes: (36, 11, 11)"]
	140067041601440 -> 140067041601632
	140067041601440 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140067041601392 -> 140067041601440
	140067041601392 [label="UnsafeViewBackward0
-------------------------------
self_sym_sizes: (3, 12, 11, 64)"]
	140067041601296 -> 140067041601392
	140067041601296 [label=CloneBackward0]
	140067041601104 -> 140067041601296
	140067041601104 [label="ExpandBackward0
-------------------------------
self_sym_sizes: (3, 12, 11, 64)"]
	140067041601056 -> 140067041601104
	140067041601056 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041600864 -> 140067041601056
	140067041600864 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 11, 768)"]
	140067041600624 -> 140067041600864
	140067041600624 [label="ViewBackward0
-------------------------
self_sym_sizes: (33, 768)"]
	140067041600432 -> 140067041600624
	140067041600432 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (33, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041600192 -> 140067041600432
	140066495926096 [label="q_encoder.encoder.text_encoding_layer.7.attention.self.query.bias
 (768)" fillcolor=lightblue]
	140066495926096 -> 140067041600192
	140067041600192 [label=AccumulateGrad]
	140067041600384 -> 140067041600432
	140067041600384 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 11, 768)"]
	140067041604368 -> 140067041600384
	140067041604368 [label="ViewBackward0
-------------------------------
self_sym_sizes: (3, 1, 11, 768)"]
	140067041599952 -> 140067041604368
	140067041599952 [label="AsStridedBackward0
------------------------------------
size          :      (3, 1, 11, 768)
storage_offset:                    0
stride        : (8448, 8448, 768, 1)"]
	140067041599760 -> 140067041599952
	140067041599760 [label=CopySlices]
	140067041509728 -> 140067041599760
	140067041599568 -> 140067041599760
	140067041599568 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 1, 768)
start         :                   0
step          :                   1"]
	140067041599712 -> 140067041599568
	140067041599712 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:         (3, 2, 768)
start         :                   1
step          :                   1"]
	140067041598896 -> 140067041599712
	140067041598896 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:         (3, 2, 768)
start         :                   0
step          :                   1"]
	140067041524224 -> 140067041598896
	140067041601248 -> 140067041600432
	140067041601248 [label=TBackward0]
	140067041600096 -> 140067041601248
	140066495926016 [label="q_encoder.encoder.text_encoding_layer.7.attention.self.query.weight
 (768, 768)" fillcolor=lightblue]
	140066495926016 -> 140067041600096
	140067041600096 [label=AccumulateGrad]
	140067041601776 -> 140067041601440
	140067041601776 [label="UnsafeViewBackward0
-------------------------------
self_sym_sizes: (3, 12, 64, 11)"]
	140067041601584 -> 140067041601776
	140067041601584 [label=CloneBackward0]
	140067041600528 -> 140067041601584
	140067041600528 [label="ExpandBackward0
-------------------------------
self_sym_sizes: (3, 12, 64, 11)"]
	140067041600768 -> 140067041600528
	140067041600768 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551615
dim1: 18446744073709551614"]
	140067041599664 -> 140067041600768
	140067041599664 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041598704 -> 140067041599664
	140067041598704 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 11, 768)"]
	140067041600288 -> 140067041598704
	140067041600288 [label="ViewBackward0
-------------------------
self_sym_sizes: (33, 768)"]
	140067041599856 -> 140067041600288
	140067041599856 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (33, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041598608 -> 140067041599856
	140066495926256 [label="q_encoder.encoder.text_encoding_layer.7.attention.self.key.bias
 (768)" fillcolor=lightblue]
	140066495926256 -> 140067041598608
	140067041598608 [label=AccumulateGrad]
	140067041598560 -> 140067041599856
	140067041598560 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 11, 768)"]
	140067041604368 -> 140067041598560
	140067041601152 -> 140067041599856
	140067041601152 [label=TBackward0]
	140067041598512 -> 140067041601152
	140066495926176 [label="q_encoder.encoder.text_encoding_layer.7.attention.self.key.weight
 (768, 768)" fillcolor=lightblue]
	140066495926176 -> 140067041598512
	140067041598512 [label=AccumulateGrad]
	140067041602928 -> 140067041602832
	140067041602928 [label="UnsafeViewBackward0
-------------------------------
self_sym_sizes: (3, 12, 11, 64)"]
	140067041602592 -> 140067041602928
	140067041602592 [label=CloneBackward0]
	140067041602064 -> 140067041602592
	140067041602064 [label="ExpandBackward0
-------------------------------
self_sym_sizes: (3, 12, 11, 64)"]
	140067041602016 -> 140067041602064
	140067041602016 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041601008 -> 140067041602016
	140067041601008 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 11, 768)"]
	140067041599808 -> 140067041601008
	140067041599808 [label="ViewBackward0
-------------------------
self_sym_sizes: (33, 768)"]
	140067041599520 -> 140067041599808
	140067041599520 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (33, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041598800 -> 140067041599520
	140066495926416 [label="q_encoder.encoder.text_encoding_layer.7.attention.self.value.bias
 (768)" fillcolor=lightblue]
	140066495926416 -> 140067041598800
	140067041598800 [label=AccumulateGrad]
	140067041600000 -> 140067041599520
	140067041600000 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 11, 768)"]
	140067041604368 -> 140067041600000
	140067041602400 -> 140067041599520
	140067041602400 [label=TBackward0]
	140067041598416 -> 140067041602400
	140066495926336 [label="q_encoder.encoder.text_encoding_layer.7.attention.self.value.weight
 (768, 768)" fillcolor=lightblue]
	140066495926336 -> 140067041598416
	140067041598416 [label=AccumulateGrad]
	140067041607632 -> 140067041603648
	140067041607632 [label=TBackward0]
	140067041603504 -> 140067041607632
	140066495926496 [label="q_encoder.encoder.text_encoding_layer.7.attention.output.dense.weight
 (768, 768)" fillcolor=lightblue]
	140066495926496 -> 140067041603504
	140067041603504 [label=AccumulateGrad]
	140067041604368 -> 140067041723200
	140067041724208 -> 140067041519232
	140066495926736 [label="q_encoder.encoder.text_encoding_layer.7.attention.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	140066495926736 -> 140067041724208
	140067041724208 [label=AccumulateGrad]
	140067041722864 -> 140067041519232
	140066495926656 [label="q_encoder.encoder.text_encoding_layer.7.attention.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	140066495926656 -> 140067041722864
	140067041722864 [label=AccumulateGrad]
	140067041725360 -> 140067041725456
	140067041725360 [label=TBackward0]
	140067041724304 -> 140067041725360
	140066495926896 [label="q_encoder.encoder.text_encoding_layer.7.intermediate.dense.weight
 (3072, 768)" fillcolor=lightblue]
	140066495926896 -> 140067041724304
	140067041724304 [label=AccumulateGrad]
	140067041517984 -> 140067041513952
	140067041517984 [label=TBackward0]
	140067041735632 -> 140067041517984
	140066495926816 [label="q_encoder.encoder.text_encoding_layer.7.output.dense.weight
 (768, 3072)" fillcolor=lightblue]
	140066495926816 -> 140067041735632
	140067041735632 [label=AccumulateGrad]
	140067041519232 -> 140067041521536
	140067041519568 -> 140067041520000
	140066495927216 [label="q_encoder.encoder.text_encoding_layer.7.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	140066495927216 -> 140067041519568
	140067041519568 [label=AccumulateGrad]
	140067041524992 -> 140067041520000
	140066495927056 [label="q_encoder.encoder.text_encoding_layer.7.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	140066495927056 -> 140067041524992
	140067041524992 [label=AccumulateGrad]
	140067041524560 -> 140067041525568
	140067041524560 [label=TBackward0]
	140067041524320 -> 140067041524560
	140066794637664 [label="q_encoder.encoder.information_exchanging_layer.7.attention.self.query.weight
 (768, 768)" fillcolor=lightblue]
	140066794637664 -> 140067041524320
	140067041524320 [label=AccumulateGrad]
	140067042176800 -> 140067042167008
	140067042176800 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 64, 2)"]
	140067042166768 -> 140067042176800
	140067042166768 [label=CloneBackward0]
	140067042165040 -> 140067042166768
	140067042165040 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 64, 2)"]
	140067042176464 -> 140067042165040
	140067042176464 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551615
dim1: 18446744073709551614"]
	140067041522880 -> 140067042176464
	140067041522880 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041519904 -> 140067041522880
	140067041519904 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140067041521296 -> 140067041519904
	140067041521296 [label="ViewBackward0
------------------------
self_sym_sizes: (6, 768)"]
	140067041518224 -> 140067041521296
	140067041518224 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (6, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041729632 -> 140067041518224
	140066794638384 [label="q_encoder.encoder.information_exchanging_layer.7.attention.self.key.bias
 (768)" fillcolor=lightblue]
	140066794638384 -> 140067041729632
	140067041729632 [label=AccumulateGrad]
	140067041730208 -> 140067041518224
	140067041730208 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140067042037616 -> 140067041730208
	140067041722432 -> 140067041518224
	140067041722432 [label=TBackward0]
	140067041516976 -> 140067041722432
	140066794638224 [label="q_encoder.encoder.information_exchanging_layer.7.attention.self.key.weight
 (768, 768)" fillcolor=lightblue]
	140066794638224 -> 140067041516976
	140067041516976 [label=AccumulateGrad]
	140067042035216 -> 140067042035792
	140067042035216 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 2, 64)"]
	140067042034928 -> 140067042035216
	140067042034928 [label=CloneBackward0]
	140067042033872 -> 140067042034928
	140067042033872 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 2, 64)"]
	140067042035024 -> 140067042033872
	140067042035024 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041724160 -> 140067042035024
	140067041724160 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140067042177232 -> 140067041724160
	140067042177232 [label="ViewBackward0
------------------------
self_sym_sizes: (6, 768)"]
	140067042166240 -> 140067042177232
	140067042166240 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (6, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041522304 -> 140067042166240
	140066794638544 [label="q_encoder.encoder.information_exchanging_layer.7.attention.self.value.bias
 (768)" fillcolor=lightblue]
	140066794638544 -> 140067041522304
	140067041522304 [label=AccumulateGrad]
	140067041520432 -> 140067042166240
	140067041520432 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140067042037616 -> 140067041520432
	140067041523408 -> 140067042166240
	140067041523408 [label=TBackward0]
	140067041525664 -> 140067041523408
	140066794638464 [label="q_encoder.encoder.information_exchanging_layer.7.attention.self.value.weight
 (768, 768)" fillcolor=lightblue]
	140066794638464 -> 140067041525664
	140067041525664 [label=AccumulateGrad]
	140067042036320 -> 140067042036752
	140067042036320 [label=TBackward0]
	140067042036560 -> 140067042036320
	140066794638624 [label="q_encoder.encoder.information_exchanging_layer.7.attention.output.dense.weight
 (768, 768)" fillcolor=lightblue]
	140066794638624 -> 140067042036560
	140067042036560 [label=AccumulateGrad]
	140067042037616 -> 140067042037664
	140067042038000 -> 140067042040928
	140066794638864 [label="q_encoder.encoder.information_exchanging_layer.7.attention.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	140066794638864 -> 140067042038000
	140067042038000 [label=AccumulateGrad]
	140067042037136 -> 140067042040928
	140066794638784 [label="q_encoder.encoder.information_exchanging_layer.7.attention.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	140066794638784 -> 140067042037136
	140067042037136 [label=AccumulateGrad]
	140067042037760 -> 140067042038624
	140067042037760 [label=TBackward0]
	140067042037280 -> 140067042037760
	140066794639024 [label="q_encoder.encoder.information_exchanging_layer.7.intermediate.dense.weight
 (3072, 768)" fillcolor=lightblue]
	140066794639024 -> 140067042037280
	140067042037280 [label=AccumulateGrad]
	140067042038816 -> 140067042039632
	140067042038816 [label=TBackward0]
	140067042039056 -> 140067042038816
	140066794638304 [label="q_encoder.encoder.information_exchanging_layer.7.output.dense.weight
 (768, 3072)" fillcolor=lightblue]
	140066794638304 -> 140067042039056
	140067042039056 [label=AccumulateGrad]
	140067042040928 -> 140067042040352
	140067042040304 -> 140067042041024
	140066794639344 [label="q_encoder.encoder.information_exchanging_layer.7.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	140066794639344 -> 140067042040304
	140067042040304 [label=AccumulateGrad]
	140067042040112 -> 140067042041024
	140066794639264 [label="q_encoder.encoder.information_exchanging_layer.7.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	140066794639264 -> 140067042040112
	140067042040112 [label=AccumulateGrad]
	140067042041360 -> 140067042047744
	140067042041360 [label=CloneBackward0]
	140067042041168 -> 140067042041360
	140067042041168 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 1, 768)
start         :                   0
step          :                   1"]
	140067042040592 -> 140067042041168
	140067042040592 [label="SelectBackward0
-------------------------------
dim           :               2
index         :               1
self_sym_sizes: (3, 1, 11, 768)"]
	140067042037376 -> 140067042040592
	140067042037376 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:     (3, 1, 11, 768)
start         :                   0
step          :                   1"]
	140067042039968 -> 140067042037376
	140067042039968 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:     (3, 1, 11, 768)
start         :                   0
step          :                   1"]
	140067042036128 -> 140067042039968
	140067042036128 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 11, 768)"]
	140067042036416 -> 140067042036128
	140067042036416 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140067042038144 -> 140067042036416
	140067042038144 [label="AddBackward0
------------
alpha: 1"]
	140067042037040 -> 140067042038144
	140067042037040 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067042165280 -> 140067042037040
	140067042165280 [label="ViewBackward0
-------------------------
self_sym_sizes: (33, 768)"]
	140067042165856 -> 140067042165280
	140067042165856 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :     (33, 3072)
mat1_sym_strides:      (3072, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (3072, 768)
mat2_sym_strides:      (1, 3072)"]
	140067042039008 -> 140067042165856
	140066495925856 [label="q_encoder.encoder.text_encoding_layer.8.output.dense.bias
 (768)" fillcolor=lightblue]
	140066495925856 -> 140067042039008
	140067042039008 [label=AccumulateGrad]
	140067042034208 -> 140067042165856
	140067042034208 [label="ViewBackward0
-----------------------------
self_sym_sizes: (3, 11, 3072)"]
	140067041512560 -> 140067042034208
	140067041512560 [label="GeluBackward0
---------------------------
approximate:           none
self       : [saved tensor]"]
	140067041518656 -> 140067041512560
	140067041518656 [label="ViewBackward0
--------------------------
self_sym_sizes: (33, 3072)"]
	140067041604464 -> 140067041518656
	140067041604464 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (33, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (768, 3072)
mat2_sym_strides:       (1, 768)"]
	140067041603024 -> 140067041604464
	140066495928256 [label="q_encoder.encoder.text_encoding_layer.8.intermediate.dense.bias
 (3072)" fillcolor=lightblue]
	140066495928256 -> 140067041603024
	140067041603024 [label=AccumulateGrad]
	140067041603120 -> 140067041604464
	140067041603120 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 11, 768)"]
	140067042035696 -> 140067041603120
	140067042035696 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140067041602304 -> 140067042035696
	140067041602304 [label="AddBackward0
------------
alpha: 1"]
	140067041600912 -> 140067041602304
	140067041600912 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041602448 -> 140067041600912
	140067041602448 [label="ViewBackward0
-------------------------
self_sym_sizes: (33, 768)"]
	140067041598368 -> 140067041602448
	140067041598368 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (33, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041604416 -> 140067041598368
	140066495927856 [label="q_encoder.encoder.text_encoding_layer.8.attention.output.dense.bias
 (768)" fillcolor=lightblue]
	140066495927856 -> 140067041604416
	140067041604416 [label=AccumulateGrad]
	140067041599424 -> 140067041598368
	140067041599424 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 11, 768)"]
	140067041598080 -> 140067041599424
	140067041598080 [label="ViewBackward0
-------------------------------
self_sym_sizes: (3, 11, 12, 64)"]
	140067041598272 -> 140067041598080
	140067041598272 [label=CloneBackward0]
	140067041598944 -> 140067041598272
	140067041598944 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041597600 -> 140067041598944
	140067041597600 [label="UnsafeViewBackward0
----------------------------
self_sym_sizes: (36, 11, 64)"]
	140067041597168 -> 140067041597600
	140067041597168 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140067041596976 -> 140067041597168
	140067041596976 [label="ViewBackward0
-------------------------------
self_sym_sizes: (3, 12, 11, 11)"]
	140067041596880 -> 140067041596976
	140067041596880 [label="ExpandBackward0
-------------------------------
self_sym_sizes: (3, 12, 11, 11)"]
	140067041596688 -> 140067041596880
	140067041596688 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041596448 -> 140067041596688
	140067041596448 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140067041596160 -> 140067041596448
	140067041596160 [label="AddBackward0
------------
alpha: 1"]
	140067041596064 -> 140067041596160
	140067041596064 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041595920 -> 140067041596064
	140067041595920 [label="UnsafeViewBackward0
----------------------------
self_sym_sizes: (36, 11, 11)"]
	140067041595632 -> 140067041595920
	140067041595632 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140067041595536 -> 140067041595632
	140067041595536 [label="UnsafeViewBackward0
-------------------------------
self_sym_sizes: (3, 12, 11, 64)"]
	140067041595440 -> 140067041595536
	140067041595440 [label=CloneBackward0]
	140067041595248 -> 140067041595440
	140067041595248 [label="ExpandBackward0
-------------------------------
self_sym_sizes: (3, 12, 11, 64)"]
	140067041595056 -> 140067041595248
	140067041595056 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041594720 -> 140067041595056
	140067041594720 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 11, 768)"]
	140067041594624 -> 140067041594720
	140067041594624 [label="ViewBackward0
-------------------------
self_sym_sizes: (33, 768)"]
	140067041594480 -> 140067041594624
	140067041594480 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (33, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041594288 -> 140067041594480
	140066495927376 [label="q_encoder.encoder.text_encoding_layer.8.attention.self.query.bias
 (768)" fillcolor=lightblue]
	140066495927376 -> 140067041594288
	140067041594288 [label=AccumulateGrad]
	140067041594432 -> 140067041594480
	140067041594432 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 11, 768)"]
	140067041600048 -> 140067041594432
	140067041600048 [label="ViewBackward0
-------------------------------
self_sym_sizes: (3, 1, 11, 768)"]
	140067041594000 -> 140067041600048
	140067041594000 [label="AsStridedBackward0
------------------------------------
size          :      (3, 1, 11, 768)
storage_offset:                    0
stride        : (8448, 8448, 768, 1)"]
	140067041593760 -> 140067041594000
	140067041593760 [label=CopySlices]
	140067041520000 -> 140067041593760
	140067041593520 -> 140067041593760
	140067041593520 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 1, 768)
start         :                   0
step          :                   1"]
	140067041593664 -> 140067041593520
	140067041593664 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:         (3, 2, 768)
start         :                   1
step          :                   1"]
	140067041592896 -> 140067041593664
	140067041592896 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:         (3, 2, 768)
start         :                   0
step          :                   1"]
	140067042041024 -> 140067041592896
	140067041595344 -> 140067041594480
	140067041595344 [label=TBackward0]
	140067041594240 -> 140067041595344
	140066495927296 [label="q_encoder.encoder.text_encoding_layer.8.attention.self.query.weight
 (768, 768)" fillcolor=lightblue]
	140066495927296 -> 140067041594240
	140067041594240 [label=AccumulateGrad]
	140067041596016 -> 140067041595632
	140067041596016 [label="UnsafeViewBackward0
-------------------------------
self_sym_sizes: (3, 12, 64, 11)"]
	140067041595776 -> 140067041596016
	140067041595776 [label=CloneBackward0]
	140067041594528 -> 140067041595776
	140067041594528 [label="ExpandBackward0
-------------------------------
self_sym_sizes: (3, 12, 64, 11)"]
	140067041594672 -> 140067041594528
	140067041594672 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551615
dim1: 18446744073709551614"]
	140067041593616 -> 140067041594672
	140067041593616 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041592704 -> 140067041593616
	140067041592704 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 11, 768)"]
	140067041594336 -> 140067041592704
	140067041594336 [label="ViewBackward0
-------------------------
self_sym_sizes: (33, 768)"]
	140067041593904 -> 140067041594336
	140067041593904 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (33, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041592656 -> 140067041593904
	140066495927536 [label="q_encoder.encoder.text_encoding_layer.8.attention.self.key.bias
 (768)" fillcolor=lightblue]
	140066495927536 -> 140067041592656
	140067041592656 [label=AccumulateGrad]
	140067041592512 -> 140067041593904
	140067041592512 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 11, 768)"]
	140067041600048 -> 140067041592512
	140067041595296 -> 140067041593904
	140067041595296 [label=TBackward0]
	140067041592416 -> 140067041595296
	140066495927456 [label="q_encoder.encoder.text_encoding_layer.8.attention.self.key.weight
 (768, 768)" fillcolor=lightblue]
	140066495927456 -> 140067041592416
	140067041592416 [label=AccumulateGrad]
	140067041597360 -> 140067041597168
	140067041597360 [label="UnsafeViewBackward0
-------------------------------
self_sym_sizes: (3, 12, 11, 64)"]
	140067041596928 -> 140067041597360
	140067041596928 [label=CloneBackward0]
	140067041596400 -> 140067041596928
	140067041596400 [label="ExpandBackward0
-------------------------------
self_sym_sizes: (3, 12, 11, 64)"]
	140067041596352 -> 140067041596400
	140067041596352 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041594912 -> 140067041596352
	140067041594912 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 11, 768)"]
	140067041593808 -> 140067041594912
	140067041593808 [label="ViewBackward0
-------------------------
self_sym_sizes: (33, 768)"]
	140067041593376 -> 140067041593808
	140067041593376 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (33, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041592848 -> 140067041593376
	140066495927696 [label="q_encoder.encoder.text_encoding_layer.8.attention.self.value.bias
 (768)" fillcolor=lightblue]
	140066495927696 -> 140067041592848
	140067041592848 [label=AccumulateGrad]
	140067041594048 -> 140067041593376
	140067041594048 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 11, 768)"]
	140067041600048 -> 140067041594048
	140067041596736 -> 140067041593376
	140067041596736 [label=TBackward0]
	140067041592224 -> 140067041596736
	140066495927616 [label="q_encoder.encoder.text_encoding_layer.8.attention.self.value.weight
 (768, 768)" fillcolor=lightblue]
	140066495927616 -> 140067041592224
	140067041592224 [label=AccumulateGrad]
	140067041599328 -> 140067041598368
	140067041599328 [label=TBackward0]
	140067041598128 -> 140067041599328
	140066495927776 [label="q_encoder.encoder.text_encoding_layer.8.attention.output.dense.weight
 (768, 768)" fillcolor=lightblue]
	140066495927776 -> 140067041598128
	140067041598128 [label=AccumulateGrad]
	140067041600048 -> 140067041602304
	140067041603312 -> 140067042035696
	140066495928016 [label="q_encoder.encoder.text_encoding_layer.8.attention.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	140066495928016 -> 140067041603312
	140067041603312 [label=AccumulateGrad]
	140067041601920 -> 140067042035696
	140066495927936 [label="q_encoder.encoder.text_encoding_layer.8.attention.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	140066495927936 -> 140067041601920
	140067041601920 [label=AccumulateGrad]
	140067041604560 -> 140067041604464
	140067041604560 [label=TBackward0]
	140067041599184 -> 140067041604560
	140066495928176 [label="q_encoder.encoder.text_encoding_layer.8.intermediate.dense.weight
 (3072, 768)" fillcolor=lightblue]
	140066495928176 -> 140067041599184
	140067041599184 [label=AccumulateGrad]
	140067042038672 -> 140067042165856
	140067042038672 [label=TBackward0]
	140067041521968 -> 140067042038672
	140066495928096 [label="q_encoder.encoder.text_encoding_layer.8.output.dense.weight
 (768, 3072)" fillcolor=lightblue]
	140066495928096 -> 140067041521968
	140067041521968 [label=AccumulateGrad]
	140067042035696 -> 140067042038144
	140067042036080 -> 140067042036416
	140066495928496 [label="q_encoder.encoder.text_encoding_layer.8.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	140066495928496 -> 140067042036080
	140067042036080 [label=AccumulateGrad]
	140067042041792 -> 140067042036416
	140066495928336 [label="q_encoder.encoder.text_encoding_layer.8.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	140066495928336 -> 140067042041792
	140067042041792 [label=AccumulateGrad]
	140067042041264 -> 140067042042368
	140067042041264 [label=TBackward0]
	140067042165376 -> 140067042041264
	140066794638944 [label="q_encoder.encoder.information_exchanging_layer.8.attention.self.query.weight
 (768, 768)" fillcolor=lightblue]
	140066794638944 -> 140067042165376
	140067042165376 [label=AccumulateGrad]
	140067042044288 -> 140067042043856
	140067042044288 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 64, 2)"]
	140067042043760 -> 140067042044288
	140067042043760 [label=CloneBackward0]
	140067042042656 -> 140067042043760
	140067042042656 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 64, 2)"]
	140067042042464 -> 140067042042656
	140067042042464 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551615
dim1: 18446744073709551614"]
	140067042040400 -> 140067042042464
	140067042040400 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067042038960 -> 140067042040400
	140067042038960 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140067042036368 -> 140067042038960
	140067042036368 [label="ViewBackward0
------------------------
self_sym_sizes: (6, 768)"]
	140067042037808 -> 140067042036368
	140067042037808 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (6, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041524896 -> 140067042037808
	140066794639664 [label="q_encoder.encoder.information_exchanging_layer.8.attention.self.key.bias
 (768)" fillcolor=lightblue]
	140066794639664 -> 140067041524896
	140067041524896 [label=AccumulateGrad]
	140067041524656 -> 140067042037808
	140067041524656 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140067042047744 -> 140067041524656
	140067042034400 -> 140067042037808
	140067042034400 [label=TBackward0]
	140067042044336 -> 140067042034400
	140066794639504 [label="q_encoder.encoder.information_exchanging_layer.8.attention.self.key.weight
 (768, 768)" fillcolor=lightblue]
	140066794639504 -> 140067042044336
	140067042044336 [label=AccumulateGrad]
	140067042045632 -> 140067042046016
	140067042045632 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 2, 64)"]
	140067042045392 -> 140067042045632
	140067042045392 [label=CloneBackward0]
	140067042044576 -> 140067042045392
	140067042044576 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 2, 64)"]
	140067042044240 -> 140067042044576
	140067042044240 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067042043280 -> 140067042044240
	140067042043280 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140067042041312 -> 140067042043280
	140067042041312 [label="ViewBackward0
------------------------
self_sym_sizes: (6, 768)"]
	140067042039776 -> 140067042041312
	140067042039776 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (6, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067042034784 -> 140067042039776
	140066794639824 [label="q_encoder.encoder.information_exchanging_layer.8.attention.self.value.bias
 (768)" fillcolor=lightblue]
	140066794639824 -> 140067042034784
	140067042034784 [label=AccumulateGrad]
	140067042041744 -> 140067042039776
	140067042041744 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140067042047744 -> 140067042041744
	140067042045536 -> 140067042039776
	140067042045536 [label=TBackward0]
	140067042036896 -> 140067042045536
	140066794639744 [label="q_encoder.encoder.information_exchanging_layer.8.attention.self.value.weight
 (768, 768)" fillcolor=lightblue]
	140066794639744 -> 140067042036896
	140067042036896 [label=AccumulateGrad]
	140067042046448 -> 140067042047024
	140067042046448 [label=TBackward0]
	140067042046640 -> 140067042046448
	140066794639904 [label="q_encoder.encoder.information_exchanging_layer.8.attention.output.dense.weight
 (768, 768)" fillcolor=lightblue]
	140066794639904 -> 140067042046640
	140067042046640 [label=AccumulateGrad]
	140067042047744 -> 140067042047792
	140067042048080 -> 140066688029968
	140066794640144 [label="q_encoder.encoder.information_exchanging_layer.8.attention.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	140066794640144 -> 140067042048080
	140067042048080 [label=AccumulateGrad]
	140067042047312 -> 140066688029968
	140066794640064 [label="q_encoder.encoder.information_exchanging_layer.8.attention.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	140066794640064 -> 140067042047312
	140067042047312 [label=AccumulateGrad]
	140067042047888 -> 140067042048656
	140067042047888 [label=TBackward0]
	140067042047408 -> 140067042047888
	140066794640304 [label="q_encoder.encoder.information_exchanging_layer.8.intermediate.dense.weight
 (3072, 768)" fillcolor=lightblue]
	140066794640304 -> 140067042047408
	140067042047408 [label=AccumulateGrad]
	140067042049040 -> 140066688029152
	140067042049040 [label=TBackward0]
	140067042049472 -> 140067042049040
	140066794639584 [label="q_encoder.encoder.information_exchanging_layer.8.output.dense.weight
 (768, 3072)" fillcolor=lightblue]
	140066794639584 -> 140067042049472
	140067042049472 [label=AccumulateGrad]
	140066688029968 -> 140066688027136
	140066688026800 -> 140066688031024
	140066794640624 [label="q_encoder.encoder.information_exchanging_layer.8.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	140066794640624 -> 140066688026800
	140066688026800 [label=AccumulateGrad]
	140066688025984 -> 140066688031024
	140066794640544 [label="q_encoder.encoder.information_exchanging_layer.8.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	140066794640544 -> 140066688025984
	140066688025984 [label=AccumulateGrad]
	140066688039328 -> 140067041678320
	140066688039328 [label=CloneBackward0]
	140066688036112 -> 140066688039328
	140066688036112 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 1, 768)
start         :                   0
step          :                   1"]
	140066688027760 -> 140066688036112
	140066688027760 [label="SelectBackward0
-------------------------------
dim           :               2
index         :               1
self_sym_sizes: (3, 1, 11, 768)"]
	140066688025072 -> 140066688027760
	140066688025072 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:     (3, 1, 11, 768)
start         :                   0
step          :                   1"]
	140066688028000 -> 140066688025072
	140066688028000 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:     (3, 1, 11, 768)
start         :                   0
step          :                   1"]
	140067042046304 -> 140066688028000
	140067042046304 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 11, 768)"]
	140067042046592 -> 140067042046304
	140067042046592 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140067042049184 -> 140067042046592
	140067042049184 [label="AddBackward0
------------
alpha: 1"]
	140067042047216 -> 140067042049184
	140067042047216 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067042048224 -> 140067042047216
	140067042048224 [label="ViewBackward0
-------------------------
self_sym_sizes: (33, 768)"]
	140067042041072 -> 140067042048224
	140067042041072 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :     (33, 3072)
mat1_sym_strides:      (3072, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (3072, 768)
mat2_sym_strides:      (1, 3072)"]
	140067042047504 -> 140067042041072
	140066495927136 [label="q_encoder.encoder.text_encoding_layer.9.output.dense.bias
 (768)" fillcolor=lightblue]
	140066495927136 -> 140067042047504
	140067042047504 [label=AccumulateGrad]
	140067042043952 -> 140067042041072
	140067042043952 [label="ViewBackward0
-----------------------------
self_sym_sizes: (3, 11, 3072)"]
	140067042042848 -> 140067042043952
	140067042042848 [label="GeluBackward0
---------------------------
approximate:           none
self       : [saved tensor]"]
	140067041599040 -> 140067042042848
	140067041599040 [label="ViewBackward0
--------------------------
self_sym_sizes: (33, 3072)"]
	140067041603216 -> 140067041599040
	140067041603216 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (33, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (768, 3072)
mat2_sym_strides:       (1, 768)"]
	140067041597456 -> 140067041603216
	140066495929536 [label="q_encoder.encoder.text_encoding_layer.9.intermediate.dense.bias
 (3072)" fillcolor=lightblue]
	140066495929536 -> 140067041597456
	140067041597456 [label=AccumulateGrad]
	140067041597744 -> 140067041603216
	140067041597744 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 11, 768)"]
	140067042045968 -> 140067041597744
	140067042045968 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140067041596496 -> 140067042045968
	140067041596496 [label="AddBackward0
------------
alpha: 1"]
	140067041594816 -> 140067041596496
	140067041594816 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041596784 -> 140067041594816
	140067041596784 [label="ViewBackward0
-------------------------
self_sym_sizes: (33, 768)"]
	140067041592176 -> 140067041596784
	140067041592176 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (33, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041601488 -> 140067041592176
	140066495929136 [label="q_encoder.encoder.text_encoding_layer.9.attention.output.dense.bias
 (768)" fillcolor=lightblue]
	140066495929136 -> 140067041601488
	140067041601488 [label=AccumulateGrad]
	140067041593328 -> 140067041592176
	140067041593328 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 11, 768)"]
	140067041591984 -> 140067041593328
	140067041591984 [label="ViewBackward0
-------------------------------
self_sym_sizes: (3, 11, 12, 64)"]
	140067041592128 -> 140067041591984
	140067041592128 [label=CloneBackward0]
	140067041592944 -> 140067041592128
	140067041592944 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041591648 -> 140067041592944
	140067041591648 [label="UnsafeViewBackward0
----------------------------
self_sym_sizes: (36, 11, 64)"]
	140067041591360 -> 140067041591648
	140067041591360 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140067041591888 -> 140067041591360
	140067041591888 [label="ViewBackward0
-------------------------------
self_sym_sizes: (3, 12, 11, 11)"]
	140067041640016 -> 140067041591888
	140067041640016 [label="ExpandBackward0
-------------------------------
self_sym_sizes: (3, 12, 11, 11)"]
	140067041639872 -> 140067041640016
	140067041639872 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041639728 -> 140067041639872
	140067041639728 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140067041639488 -> 140067041639728
	140067041639488 [label="AddBackward0
------------
alpha: 1"]
	140067041639440 -> 140067041639488
	140067041639440 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041639296 -> 140067041639440
	140067041639296 [label="UnsafeViewBackward0
----------------------------
self_sym_sizes: (36, 11, 11)"]
	140067041639152 -> 140067041639296
	140067041639152 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140067041639104 -> 140067041639152
	140067041639104 [label="UnsafeViewBackward0
-------------------------------
self_sym_sizes: (3, 12, 11, 64)"]
	140067041639008 -> 140067041639104
	140067041639008 [label=CloneBackward0]
	140067041638672 -> 140067041639008
	140067041638672 [label="ExpandBackward0
-------------------------------
self_sym_sizes: (3, 12, 11, 64)"]
	140067041638624 -> 140067041638672
	140067041638624 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041638384 -> 140067041638624
	140067041638384 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 11, 768)"]
	140067041638240 -> 140067041638384
	140067041638240 [label="ViewBackward0
-------------------------
self_sym_sizes: (33, 768)"]
	140067041638048 -> 140067041638240
	140067041638048 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (33, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041637760 -> 140067041638048
	140066495928656 [label="q_encoder.encoder.text_encoding_layer.9.attention.self.query.bias
 (768)" fillcolor=lightblue]
	140066495928656 -> 140067041637760
	140067041637760 [label=AccumulateGrad]
	140067041638000 -> 140067041638048
	140067041638000 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 11, 768)"]
	140067041594144 -> 140067041638000
	140067041594144 [label="ViewBackward0
-------------------------------
self_sym_sizes: (3, 1, 11, 768)"]
	140067041637424 -> 140067041594144
	140067041637424 [label="AsStridedBackward0
------------------------------------
size          :      (3, 1, 11, 768)
storage_offset:                    0
stride        : (8448, 8448, 768, 1)"]
	140067041637232 -> 140067041637424
	140067041637232 [label=CopySlices]
	140067042036416 -> 140067041637232
	140067041636992 -> 140067041637232
	140067041636992 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 1, 768)
start         :                   0
step          :                   1"]
	140067041637184 -> 140067041636992
	140067041637184 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:         (3, 2, 768)
start         :                   1
step          :                   1"]
	140067041636512 -> 140067041637184
	140067041636512 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:         (3, 2, 768)
start         :                   0
step          :                   1"]
	140066688031024 -> 140067041636512
	140067041638960 -> 140067041638048
	140067041638960 [label=TBackward0]
	140067041637664 -> 140067041638960
	140066495928576 [label="q_encoder.encoder.text_encoding_layer.9.attention.self.query.weight
 (768, 768)" fillcolor=lightblue]
	140066495928576 -> 140067041637664
	140067041637664 [label=AccumulateGrad]
	140067041639344 -> 140067041639152
	140067041639344 [label="UnsafeViewBackward0
-------------------------------
self_sym_sizes: (3, 12, 64, 11)"]
	140067041639248 -> 140067041639344
	140067041639248 [label=CloneBackward0]
	140067041638192 -> 140067041639248
	140067041638192 [label="ExpandBackward0
-------------------------------
self_sym_sizes: (3, 12, 64, 11)"]
	140067041638288 -> 140067041638192
	140067041638288 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551615
dim1: 18446744073709551614"]
	140067041637088 -> 140067041638288
	140067041637088 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041636368 -> 140067041637088
	140067041636368 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 11, 768)"]
	140067041637904 -> 140067041636368
	140067041637904 [label="ViewBackward0
-------------------------
self_sym_sizes: (33, 768)"]
	140067041637328 -> 140067041637904
	140067041637328 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (33, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041636272 -> 140067041637328
	140066495928816 [label="q_encoder.encoder.text_encoding_layer.9.attention.self.key.bias
 (768)" fillcolor=lightblue]
	140066495928816 -> 140067041636272
	140067041636272 [label=AccumulateGrad]
	140067041636176 -> 140067041637328
	140067041636176 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 11, 768)"]
	140067041594144 -> 140067041636176
	140067041638720 -> 140067041637328
	140067041638720 [label=TBackward0]
	140067041636032 -> 140067041638720
	140066495928736 [label="q_encoder.encoder.text_encoding_layer.9.attention.self.key.weight
 (768, 768)" fillcolor=lightblue]
	140066495928736 -> 140067041636032
	140067041636032 [label=AccumulateGrad]
	140067041640304 -> 140067041591360
	140067041640304 [label="UnsafeViewBackward0
-------------------------------
self_sym_sizes: (3, 12, 11, 64)"]
	140067041640160 -> 140067041640304
	140067041640160 [label=CloneBackward0]
	140067041639680 -> 140067041640160
	140067041639680 [label="ExpandBackward0
-------------------------------
self_sym_sizes: (3, 12, 11, 64)"]
	140067041639632 -> 140067041639680
	140067041639632 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041638576 -> 140067041639632
	140067041638576 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 11, 768)"]
	140067041637280 -> 140067041638576
	140067041637280 [label="ViewBackward0
-------------------------
self_sym_sizes: (33, 768)"]
	140067041636944 -> 140067041637280
	140067041636944 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (33, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041636416 -> 140067041636944
	140066495928976 [label="q_encoder.encoder.text_encoding_layer.9.attention.self.value.bias
 (768)" fillcolor=lightblue]
	140066495928976 -> 140067041636416
	140067041636416 [label=AccumulateGrad]
	140067041637520 -> 140067041636944
	140067041637520 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 11, 768)"]
	140067041594144 -> 140067041637520
	140067041639920 -> 140067041636944
	140067041639920 [label=TBackward0]
	140067041635984 -> 140067041639920
	140066495928896 [label="q_encoder.encoder.text_encoding_layer.9.attention.self.value.weight
 (768, 768)" fillcolor=lightblue]
	140066495928896 -> 140067041635984
	140067041635984 [label=AccumulateGrad]
	140067041593232 -> 140067041592176
	140067041593232 [label=TBackward0]
	140067041592032 -> 140067041593232
	140066495929056 [label="q_encoder.encoder.text_encoding_layer.9.attention.output.dense.weight
 (768, 768)" fillcolor=lightblue]
	140066495929056 -> 140067041592032
	140067041592032 [label=AccumulateGrad]
	140067041594144 -> 140067041596496
	140067041597936 -> 140067042045968
	140066495929296 [label="q_encoder.encoder.text_encoding_layer.9.attention.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	140066495929296 -> 140067041597936
	140067041597936 [label=AccumulateGrad]
	140067041596256 -> 140067042045968
	140066495929216 [label="q_encoder.encoder.text_encoding_layer.9.attention.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	140066495929216 -> 140067041596256
	140067041596256 [label=AccumulateGrad]
	140067041599136 -> 140067041603216
	140067041599136 [label=TBackward0]
	140067041593136 -> 140067041599136
	140066495929456 [label="q_encoder.encoder.text_encoding_layer.9.intermediate.dense.weight
 (3072, 768)" fillcolor=lightblue]
	140066495929456 -> 140067041593136
	140067041593136 [label=AccumulateGrad]
	140067042044720 -> 140067042041072
	140067042044720 [label=TBackward0]
	140067041603360 -> 140067042044720
	140066495929376 [label="q_encoder.encoder.text_encoding_layer.9.output.dense.weight
 (768, 3072)" fillcolor=lightblue]
	140066495929376 -> 140067041603360
	140067041603360 [label=AccumulateGrad]
	140067042045968 -> 140067042049184
	140067042046160 -> 140067042046592
	140066495929776 [label="q_encoder.encoder.text_encoding_layer.9.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	140066495929776 -> 140067042046160
	140067042046160 [label=AccumulateGrad]
	140067042047984 -> 140067042046592
	140066495929616 [label="q_encoder.encoder.text_encoding_layer.9.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	140066495929616 -> 140067042047984
	140067042047984 [label=AccumulateGrad]
	140066688038848 -> 140067041673376
	140066688038848 [label=TBackward0]
	140066688031120 -> 140066688038848
	140066794640224 [label="q_encoder.encoder.information_exchanging_layer.9.attention.self.query.weight
 (768, 768)" fillcolor=lightblue]
	140066794640224 -> 140066688031120
	140066688031120 [label=AccumulateGrad]
	140067041675008 -> 140067041674720
	140067041675008 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 64, 2)"]
	140067041674576 -> 140067041675008
	140067041674576 [label=CloneBackward0]
	140067041673520 -> 140067041674576
	140067041673520 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 64, 2)"]
	140067041675152 -> 140067041673520
	140067041675152 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551615
dim1: 18446744073709551614"]
	140066688029392 -> 140067041675152
	140066688029392 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140066688026656 -> 140066688029392
	140066688026656 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140066688031552 -> 140066688026656
	140066688031552 [label="ViewBackward0
------------------------
self_sym_sizes: (6, 768)"]
	140067042045008 -> 140066688031552
	140067042045008 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (6, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067042047072 -> 140067042045008
	140066794640944 [label="q_encoder.encoder.information_exchanging_layer.9.attention.self.key.bias
 (768)" fillcolor=lightblue]
	140066794640944 -> 140067042047072
	140067042047072 [label=AccumulateGrad]
	140067042043712 -> 140067042045008
	140067042043712 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140067041678320 -> 140067042043712
	140067042048800 -> 140067042045008
	140067042048800 [label=TBackward0]
	140067042045344 -> 140067042048800
	140066794640784 [label="q_encoder.encoder.information_exchanging_layer.9.attention.self.key.weight
 (768, 768)" fillcolor=lightblue]
	140066794640784 -> 140067042045344
	140067042045344 [label=AccumulateGrad]
	140067041676448 -> 140067041676640
	140067041676448 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 2, 64)"]
	140067041676256 -> 140067041676448
	140067041676256 [label=CloneBackward0]
	140067041675392 -> 140067041676256
	140067041675392 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 2, 64)"]
	140067041674960 -> 140067041675392
	140067041674960 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041674240 -> 140067041674960
	140067041674240 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140067041676304 -> 140067041674240
	140067041676304 [label="ViewBackward0
------------------------
self_sym_sizes: (6, 768)"]
	140066688029008 -> 140067041676304
	140066688029008 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (6, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140066688039184 -> 140066688029008
	140066794641104 [label="q_encoder.encoder.information_exchanging_layer.9.attention.self.value.bias
 (768)" fillcolor=lightblue]
	140066794641104 -> 140066688039184
	140066688039184 [label=AccumulateGrad]
	140067042049376 -> 140066688029008
	140067042049376 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140067041678320 -> 140067042049376
	140067042040208 -> 140066688029008
	140067042040208 [label=TBackward0]
	140067041597984 -> 140067042040208
	140066794641024 [label="q_encoder.encoder.information_exchanging_layer.9.attention.self.value.weight
 (768, 768)" fillcolor=lightblue]
	140066794641024 -> 140067041597984
	140067041597984 [label=AccumulateGrad]
	140067041677024 -> 140067041677552
	140067041677024 [label=TBackward0]
	140067041677216 -> 140067041677024
	140066794641184 [label="q_encoder.encoder.information_exchanging_layer.9.attention.output.dense.weight
 (768, 768)" fillcolor=lightblue]
	140066794641184 -> 140067041677216
	140067041677216 [label=AccumulateGrad]
	140067041678320 -> 140067041678368
	140067041678608 -> 140067041681248
	140066794641424 [label="q_encoder.encoder.information_exchanging_layer.9.attention.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	140066794641424 -> 140067041678608
	140067041678608 [label=AccumulateGrad]
	140067041677984 -> 140067041681248
	140066794641344 [label="q_encoder.encoder.information_exchanging_layer.9.attention.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	140066794641344 -> 140067041677984
	140067041677984 [label=AccumulateGrad]
	140067041678464 -> 140067041679232
	140067041678464 [label=TBackward0]
	140067041678128 -> 140067041678464
	140066794641584 [label="q_encoder.encoder.information_exchanging_layer.9.intermediate.dense.weight
 (3072, 768)" fillcolor=lightblue]
	140066794641584 -> 140067041678128
	140067041678128 [label=AccumulateGrad]
	140067041679472 -> 140067041680000
	140067041679472 [label=TBackward0]
	140067041679808 -> 140067041679472
	140066794640864 [label="q_encoder.encoder.information_exchanging_layer.9.output.dense.weight
 (768, 3072)" fillcolor=lightblue]
	140066794640864 -> 140067041679808
	140067041679808 [label=AccumulateGrad]
	140067041681248 -> 140067041680576
	140067041680480 -> 140067041681344
	140066794641904 [label="q_encoder.encoder.information_exchanging_layer.9.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	140066794641904 -> 140067041680480
	140067041680480 [label=AccumulateGrad]
	140067041680288 -> 140067041681344
	140066794641824 [label="q_encoder.encoder.information_exchanging_layer.9.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	140066794641824 -> 140067041680288
	140067041680288 [label=AccumulateGrad]
	140067041681872 -> 140067041687872
	140067041681872 [label=CloneBackward0]
	140067041681536 -> 140067041681872
	140067041681536 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 1, 768)
start         :                   0
step          :                   1"]
	140067041680768 -> 140067041681536
	140067041680768 [label="SelectBackward0
-------------------------------
dim           :               2
index         :               1
self_sym_sizes: (3, 1, 11, 768)"]
	140067041678176 -> 140067041680768
	140067041678176 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:     (3, 1, 11, 768)
start         :                   0
step          :                   1"]
	140067041680192 -> 140067041678176
	140067041680192 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:     (3, 1, 11, 768)
start         :                   0
step          :                   1"]
	140067041676928 -> 140067041680192
	140067041676928 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 11, 768)"]
	140067041677168 -> 140067041676928
	140067041677168 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140067041678848 -> 140067041677168
	140067041678848 [label="AddBackward0
------------
alpha: 1"]
	140067041677792 -> 140067041678848
	140067041677792 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140066688027184 -> 140067041677792
	140066688027184 [label="ViewBackward0
-------------------------
self_sym_sizes: (33, 768)"]
	140067041675584 -> 140066688027184
	140067041675584 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :     (33, 3072)
mat1_sym_strides:      (3072, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (3072, 768)
mat2_sym_strides:      (1, 3072)"]
	140067042046496 -> 140067041675584
	140066495928416 [label="q_encoder.encoder.text_encoding_layer.10.output.dense.bias
 (768)" fillcolor=lightblue]
	140066495928416 -> 140067042046496
	140067042046496 [label=AccumulateGrad]
	140067041674432 -> 140067041675584
	140067041674432 [label="ViewBackward0
-----------------------------
self_sym_sizes: (3, 11, 3072)"]
	140067041674816 -> 140067041674432
	140067041674816 [label="GeluBackward0
---------------------------
approximate:           none
self       : [saved tensor]"]
	140067041595728 -> 140067041674816
	140067041595728 [label="ViewBackward0
--------------------------
self_sym_sizes: (33, 3072)"]
	140067041593088 -> 140067041595728
	140067041593088 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (33, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (768, 3072)
mat2_sym_strides:       (1, 768)"]
	140067041591600 -> 140067041593088
	140066495930816 [label="q_encoder.encoder.text_encoding_layer.10.intermediate.dense.bias
 (3072)" fillcolor=lightblue]
	140066495930816 -> 140067041591600
	140067041591600 [label=AccumulateGrad]
	140067041591696 -> 140067041593088
	140067041591696 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 11, 768)"]
	140067041676592 -> 140067041591696
	140067041676592 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140067041597840 -> 140067041676592
	140067041597840 [label="AddBackward0
------------
alpha: 1"]
	140067041638528 -> 140067041597840
	140067041638528 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041640208 -> 140067041638528
	140067041640208 [label="ViewBackward0
-------------------------
self_sym_sizes: (33, 768)"]
	140067041635888 -> 140067041640208
	140067041635888 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (33, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041639968 -> 140067041635888
	140066495930416 [label="q_encoder.encoder.text_encoding_layer.10.attention.output.dense.bias
 (768)" fillcolor=lightblue]
	140066495930416 -> 140067041639968
	140067041639968 [label=AccumulateGrad]
	140067041636896 -> 140067041635888
	140067041636896 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 11, 768)"]
	140067041635552 -> 140067041636896
	140067041635552 [label="ViewBackward0
-------------------------------
self_sym_sizes: (3, 11, 12, 64)"]
	140067041635840 -> 140067041635552
	140067041635840 [label=CloneBackward0]
	140067041636608 -> 140067041635840
	140067041636608 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041635264 -> 140067041636608
	140067041635264 [label="UnsafeViewBackward0
----------------------------
self_sym_sizes: (36, 11, 64)"]
	140067041634976 -> 140067041635264
	140067041634976 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140067041634832 -> 140067041634976
	140067041634832 [label="ViewBackward0
-------------------------------
self_sym_sizes: (3, 12, 11, 11)"]
	140067041634688 -> 140067041634832
	140067041634688 [label="ExpandBackward0
-------------------------------
self_sym_sizes: (3, 12, 11, 11)"]
	140067041634400 -> 140067041634688
	140067041634400 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041634256 -> 140067041634400
	140067041634256 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140067041634016 -> 140067041634256
	140067041634016 [label="AddBackward0
------------
alpha: 1"]
	140067041633920 -> 140067041634016
	140067041633920 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041633728 -> 140067041633920
	140067041633728 [label="UnsafeViewBackward0
----------------------------
self_sym_sizes: (36, 11, 11)"]
	140067041633584 -> 140067041633728
	140067041633584 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140067041633440 -> 140067041633584
	140067041633440 [label="UnsafeViewBackward0
-------------------------------
self_sym_sizes: (3, 12, 11, 64)"]
	140067041633248 -> 140067041633440
	140067041633248 [label=CloneBackward0]
	140067041633008 -> 140067041633248
	140067041633008 [label="ExpandBackward0
-------------------------------
self_sym_sizes: (3, 12, 11, 64)"]
	140067041632960 -> 140067041633008
	140067041632960 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041632768 -> 140067041632960
	140067041632768 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 11, 768)"]
	140067041632528 -> 140067041632768
	140067041632528 [label="ViewBackward0
-------------------------
self_sym_sizes: (33, 768)"]
	140067041632336 -> 140067041632528
	140067041632336 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (33, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041632096 -> 140067041632336
	140066495929936 [label="q_encoder.encoder.text_encoding_layer.10.attention.self.query.bias
 (768)" fillcolor=lightblue]
	140066495929936 -> 140067041632096
	140067041632096 [label=AccumulateGrad]
	140067041632288 -> 140067041632336
	140067041632288 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 11, 768)"]
	140067041637568 -> 140067041632288
	140067041637568 [label="ViewBackward0
-------------------------------
self_sym_sizes: (3, 1, 11, 768)"]
	140067041631808 -> 140067041637568
	140067041631808 [label="AsStridedBackward0
------------------------------------
size          :      (3, 1, 11, 768)
storage_offset:                    0
stride        : (8448, 8448, 768, 1)"]
	140067041631616 -> 140067041631808
	140067041631616 [label=CopySlices]
	140067042046592 -> 140067041631616
	140067041631472 -> 140067041631616
	140067041631472 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 1, 768)
start         :                   0
step          :                   1"]
	140067041631568 -> 140067041631472
	140067041631568 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:         (3, 2, 768)
start         :                   1
step          :                   1"]
	140067041630848 -> 140067041631568
	140067041630848 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:         (3, 2, 768)
start         :                   0
step          :                   1"]
	140067041681344 -> 140067041630848
	140067041633152 -> 140067041632336
	140067041633152 [label=TBackward0]
	140067041632048 -> 140067041633152
	140066495929856 [label="q_encoder.encoder.text_encoding_layer.10.attention.self.query.weight
 (768, 768)" fillcolor=lightblue]
	140066495929856 -> 140067041632048
	140067041632048 [label=AccumulateGrad]
	140067041633824 -> 140067041633584
	140067041633824 [label="UnsafeViewBackward0
-------------------------------
self_sym_sizes: (3, 12, 64, 11)"]
	140067041633680 -> 140067041633824
	140067041633680 [label=CloneBackward0]
	140067041632432 -> 140067041633680
	140067041632432 [label="ExpandBackward0
-------------------------------
self_sym_sizes: (3, 12, 64, 11)"]
	140067041632720 -> 140067041632432
	140067041632720 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551615
dim1: 18446744073709551614"]
	140067041631520 -> 140067041632720
	140067041631520 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041630656 -> 140067041631520
	140067041630656 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 11, 768)"]
	140067041632192 -> 140067041630656
	140067041632192 [label="ViewBackward0
-------------------------
self_sym_sizes: (33, 768)"]
	140067041631712 -> 140067041632192
	140067041631712 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (33, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041630608 -> 140067041631712
	140066495930096 [label="q_encoder.encoder.text_encoding_layer.10.attention.self.key.bias
 (768)" fillcolor=lightblue]
	140066495930096 -> 140067041630608
	140067041630608 [label=AccumulateGrad]
	140067041630560 -> 140067041631712
	140067041630560 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 11, 768)"]
	140067041637568 -> 140067041630560
	140067041633056 -> 140067041631712
	140067041633056 [label=TBackward0]
	140067041630512 -> 140067041633056
	140066495930016 [label="q_encoder.encoder.text_encoding_layer.10.attention.self.key.weight
 (768, 768)" fillcolor=lightblue]
	140066495930016 -> 140067041630512
	140067041630512 [label=AccumulateGrad]
	140067041635024 -> 140067041634976
	140067041635024 [label="UnsafeViewBackward0
-------------------------------
self_sym_sizes: (3, 12, 11, 64)"]
	140067041634784 -> 140067041635024
	140067041634784 [label=CloneBackward0]
	140067041634208 -> 140067041634784
	140067041634208 [label="ExpandBackward0
-------------------------------
self_sym_sizes: (3, 12, 11, 64)"]
	140067041634112 -> 140067041634208
	140067041634112 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041632864 -> 140067041634112
	140067041632864 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 11, 768)"]
	140067041631664 -> 140067041632864
	140067041631664 [label="ViewBackward0
-------------------------
self_sym_sizes: (33, 768)"]
	140067041631424 -> 140067041631664
	140067041631424 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (33, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041630752 -> 140067041631424
	140066495930256 [label="q_encoder.encoder.text_encoding_layer.10.attention.self.value.bias
 (768)" fillcolor=lightblue]
	140066495930256 -> 140067041630752
	140067041630752 [label=AccumulateGrad]
	140067041631904 -> 140067041631424
	140067041631904 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 11, 768)"]
	140067041637568 -> 140067041631904
	140067041634496 -> 140067041631424
	140067041634496 [label=TBackward0]
	140067041630464 -> 140067041634496
	140066495930176 [label="q_encoder.encoder.text_encoding_layer.10.attention.self.value.weight
 (768, 768)" fillcolor=lightblue]
	140066495930176 -> 140067041630464
	140067041630464 [label=AccumulateGrad]
	140067041636848 -> 140067041635888
	140067041636848 [label=TBackward0]
	140067041635696 -> 140067041636848
	140066495930336 [label="q_encoder.encoder.text_encoding_layer.10.attention.output.dense.weight
 (768, 768)" fillcolor=lightblue]
	140066495930336 -> 140067041635696
	140067041635696 [label=AccumulateGrad]
	140067041637568 -> 140067041597840
	140067041639824 -> 140067041676592
	140066495930576 [label="q_encoder.encoder.text_encoding_layer.10.attention.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	140066495930576 -> 140067041639824
	140067041639824 [label=AccumulateGrad]
	140067041639200 -> 140067041676592
	140066495930496 [label="q_encoder.encoder.text_encoding_layer.10.attention.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	140066495930496 -> 140067041639200
	140067041639200 [label=AccumulateGrad]
	140067041593040 -> 140067041593088
	140067041593040 [label=TBackward0]
	140067041591840 -> 140067041593040
	140066495930736 [label="q_encoder.encoder.text_encoding_layer.10.intermediate.dense.weight
 (3072, 768)" fillcolor=lightblue]
	140066495930736 -> 140067041591840
	140067041591840 [label=AccumulateGrad]
	140067041679712 -> 140067041675584
	140067041679712 [label=TBackward0]
	140067041602736 -> 140067041679712
	140066495930656 [label="q_encoder.encoder.text_encoding_layer.10.output.dense.weight
 (768, 3072)" fillcolor=lightblue]
	140066495930656 -> 140067041602736
	140067041602736 [label=AccumulateGrad]
	140067041676592 -> 140067041678848
	140067041676784 -> 140067041677168
	140066495931056 [label="q_encoder.encoder.text_encoding_layer.10.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	140066495931056 -> 140067041676784
	140067041676784 [label=AccumulateGrad]
	140067041682064 -> 140067041677168
	140066495930896 [label="q_encoder.encoder.text_encoding_layer.10.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	140066495930896 -> 140067041682064
	140067041682064 [label=AccumulateGrad]
	140067041681728 -> 140067041682496
	140067041681728 [label=TBackward0]
	140067041681440 -> 140067041681728
	140066794641984 [label="q_encoder.encoder.information_exchanging_layer.10.attention.self.query.weight
 (768, 768)" fillcolor=lightblue]
	140066794641984 -> 140067041681440
	140067041681440 [label=AccumulateGrad]
	140067041684176 -> 140067041683936
	140067041684176 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 64, 2)"]
	140067041683744 -> 140067041684176
	140067041683744 [label=CloneBackward0]
	140067041682832 -> 140067041683744
	140067041682832 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 64, 2)"]
	140067041682592 -> 140067041682832
	140067041682592 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551615
dim1: 18446744073709551614"]
	140067041680048 -> 140067041682592
	140067041680048 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041677120 -> 140067041680048
	140067041677120 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140067041678512 -> 140067041677120
	140067041678512 [label="ViewBackward0
------------------------
self_sym_sizes: (6, 768)"]
	140067041675824 -> 140067041678512
	140067041675824 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (6, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041597072 -> 140067041675824
	140066794642224 [label="q_encoder.encoder.information_exchanging_layer.10.attention.self.key.bias
 (768)" fillcolor=lightblue]
	140066794642224 -> 140067041597072
	140067041597072 [label=AccumulateGrad]
	140067041591504 -> 140067041675824
	140067041591504 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140067041687872 -> 140067041591504
	140067041591744 -> 140067041675824
	140067041591744 [label=TBackward0]
	140067041679328 -> 140067041591744
	140066794642144 [label="q_encoder.encoder.information_exchanging_layer.10.attention.self.key.weight
 (768, 768)" fillcolor=lightblue]
	140066794642144 -> 140067041679328
	140067041679328 [label=AccumulateGrad]
	140067041685616 -> 140067041685952
	140067041685616 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 2, 64)"]
	140067041685280 -> 140067041685616
	140067041685280 [label=CloneBackward0]
	140067041684512 -> 140067041685280
	140067041684512 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 2, 64)"]
	140067041684128 -> 140067041684512
	140067041684128 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041683312 -> 140067041684128
	140067041683312 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140067041681824 -> 140067041683312
	140067041681824 [label="ViewBackward0
------------------------
self_sym_sizes: (6, 768)"]
	140067041677600 -> 140067041681824
	140067041677600 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (6, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041673856 -> 140067041677600
	140066794642384 [label="q_encoder.encoder.information_exchanging_layer.10.attention.self.value.bias
 (768)" fillcolor=lightblue]
	140066794642384 -> 140067041673856
	140067041673856 [label=AccumulateGrad]
	140067041681968 -> 140067041677600
	140067041681968 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140067041687872 -> 140067041681968
	140067041685472 -> 140067041677600
	140067041685472 [label=TBackward0]
	140067041680336 -> 140067041685472
	140066794642304 [label="q_encoder.encoder.information_exchanging_layer.10.attention.self.value.weight
 (768, 768)" fillcolor=lightblue]
	140066794642304 -> 140067041680336
	140067041680336 [label=AccumulateGrad]
	140067041686384 -> 140067041687008
	140067041686384 [label=TBackward0]
	140067041686576 -> 140067041686384
	140066794642464 [label="q_encoder.encoder.information_exchanging_layer.10.attention.output.dense.weight
 (768, 768)" fillcolor=lightblue]
	140066794642464 -> 140067041686576
	140067041686576 [label=AccumulateGrad]
	140067041687872 -> 140067041687920
	140067041688112 -> 140067041691248
	140066794642704 [label="q_encoder.encoder.information_exchanging_layer.10.attention.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	140066794642704 -> 140067041688112
	140067041688112 [label=AccumulateGrad]
	140067041687440 -> 140067041691248
	140066794642624 [label="q_encoder.encoder.information_exchanging_layer.10.attention.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	140066794642624 -> 140067041687440
	140067041687440 [label=AccumulateGrad]
	140067041688016 -> 140067041688688
	140067041688016 [label=TBackward0]
	140067041687488 -> 140067041688016
	140066794642864 [label="q_encoder.encoder.information_exchanging_layer.10.intermediate.dense.weight
 (3072, 768)" fillcolor=lightblue]
	140066794642864 -> 140067041687488
	140067041687488 [label=AccumulateGrad]
	140067041688976 -> 140067041691152
	140067041688976 [label=TBackward0]
	140067041689216 -> 140067041688976
	140066794635424 [label="q_encoder.encoder.information_exchanging_layer.10.output.dense.weight
 (768, 3072)" fillcolor=lightblue]
	140066794635424 -> 140067041689216
	140067041689216 [label=AccumulateGrad]
	140067041691248 -> 140067041690288
	140067041690096 -> 140067041691488
	140066794643184 [label="q_encoder.encoder.information_exchanging_layer.10.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	140066794643184 -> 140067041690096
	140067041690096 [label=AccumulateGrad]
	140067041689808 -> 140067041691488
	140066794643104 [label="q_encoder.encoder.information_exchanging_layer.10.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	140066794643104 -> 140067041689808
	140067041689808 [label=AccumulateGrad]
	140067041692112 -> 140067041699024
	140067041692112 [label=CloneBackward0]
	140067041691728 -> 140067041692112
	140067041691728 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 1, 768)
start         :                   0
step          :                   1"]
	140067041690528 -> 140067041691728
	140067041690528 [label="SelectBackward0
-------------------------------
dim           :               2
index         :               1
self_sym_sizes: (3, 1, 11, 768)"]
	140067041689664 -> 140067041690528
	140067041689664 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:     (3, 1, 11, 768)
start         :                   0
step          :                   1"]
	140067041692400 -> 140067041689664
	140067041692400 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:     (3, 1, 11, 768)
start         :                   0
step          :                   1"]
	140067041686288 -> 140067041692400
	140067041686288 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 11, 768)"]
	140067041686528 -> 140067041686288
	140067041686528 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140067041689072 -> 140067041686528
	140067041689072 [label="AddBackward0
------------
alpha: 1"]
	140067041687296 -> 140067041689072
	140067041687296 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041688352 -> 140067041687296
	140067041688352 [label="ViewBackward0
-------------------------
self_sym_sizes: (33, 768)"]
	140067041680672 -> 140067041688352
	140067041680672 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :     (33, 3072)
mat1_sym_strides:      (3072, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (3072, 768)
mat2_sym_strides:      (1, 3072)"]
	140067041687632 -> 140067041680672
	140066495929696 [label="q_encoder.encoder.text_encoding_layer.11.output.dense.bias
 (768)" fillcolor=lightblue]
	140066495929696 -> 140067041687632
	140067041687632 [label=AccumulateGrad]
	140067041683984 -> 140067041680672
	140067041683984 [label="ViewBackward0
-----------------------------
self_sym_sizes: (3, 11, 3072)"]
	140067041683024 -> 140067041683984
	140067041683024 [label="GeluBackward0
---------------------------
approximate:           none
self       : [saved tensor]"]
	140067041676160 -> 140067041683024
	140067041676160 [label="ViewBackward0
--------------------------
self_sym_sizes: (33, 3072)"]
	140067041636656 -> 140067041676160
	140067041636656 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (33, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (768, 3072)
mat2_sym_strides:       (1, 768)"]
	140067041635168 -> 140067041636656
	140066495932096 [label="q_encoder.encoder.text_encoding_layer.11.intermediate.dense.bias
 (3072)" fillcolor=lightblue]
	140066495932096 -> 140067041635168
	140067041635168 [label=AccumulateGrad]
	140067041635312 -> 140067041636656
	140067041635312 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 11, 768)"]
	140067041685856 -> 140067041635312
	140067041685856 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140067041634304 -> 140067041685856
	140067041634304 [label="AddBackward0
------------
alpha: 1"]
	140067041632816 -> 140067041634304
	140067041632816 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041634640 -> 140067041632816
	140067041634640 [label="ViewBackward0
-------------------------
self_sym_sizes: (33, 768)"]
	140067041630416 -> 140067041634640
	140067041630416 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (33, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041636752 -> 140067041630416
	140066495931696 [label="q_encoder.encoder.text_encoding_layer.11.attention.output.dense.bias
 (768)" fillcolor=lightblue]
	140066495931696 -> 140067041636752
	140067041636752 [label=AccumulateGrad]
	140067041631328 -> 140067041630416
	140067041631328 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 11, 768)"]
	140067041630176 -> 140067041631328
	140067041630176 [label="ViewBackward0
-------------------------------
self_sym_sizes: (3, 11, 12, 64)"]
	140067041630320 -> 140067041630176
	140067041630320 [label=CloneBackward0]
	140067041630944 -> 140067041630320
	140067041630944 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041629840 -> 140067041630944
	140067041629840 [label="UnsafeViewBackward0
----------------------------
self_sym_sizes: (36, 11, 64)"]
	140067041629504 -> 140067041629840
	140067041629504 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140067041629264 -> 140067041629504
	140067041629264 [label="ViewBackward0
-------------------------------
self_sym_sizes: (3, 12, 11, 11)"]
	140067041629120 -> 140067041629264
	140067041629120 [label="ExpandBackward0
-------------------------------
self_sym_sizes: (3, 12, 11, 11)"]
	140067041628928 -> 140067041629120
	140067041628928 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041628688 -> 140067041628928
	140067041628688 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140067041628160 -> 140067041628688
	140067041628160 [label="AddBackward0
------------
alpha: 1"]
	140067041628256 -> 140067041628160
	140067041628256 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041628112 -> 140067041628256
	140067041628112 [label="UnsafeViewBackward0
----------------------------
self_sym_sizes: (36, 11, 11)"]
	140067041627872 -> 140067041628112
	140067041627872 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140067041627824 -> 140067041627872
	140067041627824 [label="UnsafeViewBackward0
-------------------------------
self_sym_sizes: (3, 12, 11, 64)"]
	140067041627776 -> 140067041627824
	140067041627776 [label=CloneBackward0]
	140067041627536 -> 140067041627776
	140067041627536 [label="ExpandBackward0
-------------------------------
self_sym_sizes: (3, 12, 11, 64)"]
	140067041627392 -> 140067041627536
	140067041627392 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041627104 -> 140067041627392
	140067041627104 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 11, 768)"]
	140067041626960 -> 140067041627104
	140067041626960 [label="ViewBackward0
-------------------------
self_sym_sizes: (33, 768)"]
	140067041626768 -> 140067041626960
	140067041626768 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (33, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041626528 -> 140067041626768
	140066495931216 [label="q_encoder.encoder.text_encoding_layer.11.attention.self.query.bias
 (768)" fillcolor=lightblue]
	140066495931216 -> 140067041626528
	140067041626528 [label=AccumulateGrad]
	140067041626720 -> 140067041626768
	140067041626720 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 11, 768)"]
	140067041631952 -> 140067041626720
	140067041631952 [label="ViewBackward0
-------------------------------
self_sym_sizes: (3, 1, 11, 768)"]
	140067041626240 -> 140067041631952
	140067041626240 [label="AsStridedBackward0
------------------------------------
size          :      (3, 1, 11, 768)
storage_offset:                    0
stride        : (8448, 8448, 768, 1)"]
	140067041626000 -> 140067041626240
	140067041626000 [label=CopySlices]
	140067041677168 -> 140067041626000
	140067041625712 -> 140067041626000
	140067041625712 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 1, 768)
start         :                   0
step          :                   1"]
	140067041625856 -> 140067041625712
	140067041625856 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:         (3, 2, 768)
start         :                   1
step          :                   1"]
	140067041624992 -> 140067041625856
	140067041624992 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:         (3, 2, 768)
start         :                   0
step          :                   1"]
	140067041691488 -> 140067041624992
	140067041627680 -> 140067041626768
	140067041627680 [label=TBackward0]
	140067041626432 -> 140067041627680
	140066495931136 [label="q_encoder.encoder.text_encoding_layer.11.attention.self.query.weight
 (768, 768)" fillcolor=lightblue]
	140066495931136 -> 140067041626432
	140067041626432 [label=AccumulateGrad]
	140067041628208 -> 140067041627872
	140067041628208 [label="UnsafeViewBackward0
-------------------------------
self_sym_sizes: (3, 12, 64, 11)"]
	140067041628016 -> 140067041628208
	140067041628016 [label=CloneBackward0]
	140067041626864 -> 140067041628016
	140067041626864 [label="ExpandBackward0
-------------------------------
self_sym_sizes: (3, 12, 64, 11)"]
	140067041627056 -> 140067041626864
	140067041627056 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551615
dim1: 18446744073709551614"]
	140067041625760 -> 140067041627056
	140067041625760 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041624896 -> 140067041625760
	140067041624896 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 11, 768)"]
	140067041626624 -> 140067041624896
	140067041626624 [label="ViewBackward0
-------------------------
self_sym_sizes: (33, 768)"]
	140067041626144 -> 140067041626624
	140067041626144 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (33, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041624800 -> 140067041626144
	140066495931376 [label="q_encoder.encoder.text_encoding_layer.11.attention.self.key.bias
 (768)" fillcolor=lightblue]
	140066495931376 -> 140067041624800
	140067041624800 [label=AccumulateGrad]
	140067041624560 -> 140067041626144
	140067041624560 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 11, 768)"]
	140067041631952 -> 140067041624560
	140067041627584 -> 140067041626144
	140067041627584 [label=TBackward0]
	140067041624464 -> 140067041627584
	140066495931296 [label="q_encoder.encoder.text_encoding_layer.11.attention.self.key.weight
 (768, 768)" fillcolor=lightblue]
	140066495931296 -> 140067041624464
	140067041624464 [label=AccumulateGrad]
	140067041629552 -> 140067041629504
	140067041629552 [label="UnsafeViewBackward0
-------------------------------
self_sym_sizes: (3, 12, 11, 64)"]
	140067041629216 -> 140067041629552
	140067041629216 [label=CloneBackward0]
	140067041628592 -> 140067041629216
	140067041628592 [label="ExpandBackward0
-------------------------------
self_sym_sizes: (3, 12, 11, 64)"]
	140067041628496 -> 140067041628592
	140067041628496 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041627344 -> 140067041628496
	140067041627344 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 11, 768)"]
	140067041626048 -> 140067041627344
	140067041626048 [label="ViewBackward0
-------------------------
self_sym_sizes: (33, 768)"]
	140067041625616 -> 140067041626048
	140067041625616 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (33, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041624944 -> 140067041625616
	140066495931536 [label="q_encoder.encoder.text_encoding_layer.11.attention.self.value.bias
 (768)" fillcolor=lightblue]
	140066495931536 -> 140067041624944
	140067041624944 [label=AccumulateGrad]
	140067041626288 -> 140067041625616
	140067041626288 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 11, 768)"]
	140067041631952 -> 140067041626288
	140067041629024 -> 140067041625616
	140067041629024 [label=TBackward0]
	140067041624416 -> 140067041629024
	140066495931456 [label="q_encoder.encoder.text_encoding_layer.11.attention.self.value.weight
 (768, 768)" fillcolor=lightblue]
	140066495931456 -> 140067041624416
	140067041624416 [label=AccumulateGrad]
	140067041631232 -> 140067041630416
	140067041631232 [label=TBackward0]
	140067041630272 -> 140067041631232
	140066495931616 [label="q_encoder.encoder.text_encoding_layer.11.attention.output.dense.weight
 (768, 768)" fillcolor=lightblue]
	140066495931616 -> 140067041630272
	140067041630272 [label=AccumulateGrad]
	140067041631952 -> 140067041634304
	140067041635456 -> 140067041685856
	140066495931856 [label="q_encoder.encoder.text_encoding_layer.11.attention.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	140066495931856 -> 140067041635456
	140067041635456 [label=AccumulateGrad]
	140067041634064 -> 140067041685856
	140066495931776 [label="q_encoder.encoder.text_encoding_layer.11.attention.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	140066495931776 -> 140067041634064
	140067041634064 [label=AccumulateGrad]
	140067041636704 -> 140067041636656
	140067041636704 [label=TBackward0]
	140067041631136 -> 140067041636704
	140066495932016 [label="q_encoder.encoder.text_encoding_layer.11.intermediate.dense.weight
 (3072, 768)" fillcolor=lightblue]
	140066495932016 -> 140067041631136
	140067041631136 [label=AccumulateGrad]
	140067041684608 -> 140067041680672
	140067041684608 [label=TBackward0]
	140067041684272 -> 140067041684608
	140066495931936 [label="q_encoder.encoder.text_encoding_layer.11.output.dense.weight
 (768, 3072)" fillcolor=lightblue]
	140066495931936 -> 140067041684272
	140067041684272 [label=AccumulateGrad]
	140067041685856 -> 140067041689072
	140067041686192 -> 140067041686528
	140066495932336 [label="q_encoder.encoder.text_encoding_layer.11.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	140066495932336 -> 140067041686192
	140067041686192 [label=AccumulateGrad]
	140067041688064 -> 140067041686528
	140066495932176 [label="q_encoder.encoder.text_encoding_layer.11.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	140066495932176 -> 140067041688064
	140067041688064 [label=AccumulateGrad]
	140067041691968 -> 140067041692688
	140067041691968 [label=TBackward0]
	140067041691584 -> 140067041691968
	140066794642784 [label="q_encoder.encoder.information_exchanging_layer.11.attention.self.query.weight
 (768, 768)" fillcolor=lightblue]
	140066794642784 -> 140067041691584
	140067041691584 [label=AccumulateGrad]
	140067041694800 -> 140067041694464
	140067041694800 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 64, 2)"]
	140067041694176 -> 140067041694800
	140067041694176 [label=CloneBackward0]
	140067041693024 -> 140067041694176
	140067041693024 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 64, 2)"]
	140067041692736 -> 140067041693024
	140067041692736 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551615
dim1: 18446744073709551614"]
	140067041691200 -> 140067041692736
	140067041691200 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041689952 -> 140067041691200
	140067041689952 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140067041694896 -> 140067041689952
	140067041694896 [label="ViewBackward0
------------------------
self_sym_sizes: (6, 768)"]
	140067041684800 -> 140067041694896
	140067041684800 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (6, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041687152 -> 140067041684800
	140066794643504 [label="q_encoder.encoder.information_exchanging_layer.11.attention.self.key.bias
 (768)" fillcolor=lightblue]
	140066794643504 -> 140067041687152
	140067041687152 [label=AccumulateGrad]
	140067041683600 -> 140067041684800
	140067041683600 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140067041699024 -> 140067041683600
	140067041688784 -> 140067041684800
	140067041688784 [label=TBackward0]
	140067041685184 -> 140067041688784
	140066794643344 [label="q_encoder.encoder.information_exchanging_layer.11.attention.self.key.weight
 (768, 768)" fillcolor=lightblue]
	140066794643344 -> 140067041685184
	140067041685184 [label=AccumulateGrad]
	140067041696336 -> 140067041696576
	140067041696336 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 2, 64)"]
	140067041696000 -> 140067041696336
	140067041696000 [label=CloneBackward0]
	140067041695088 -> 140067041696000
	140067041695088 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 2, 64)"]
	140067041694656 -> 140067041695088
	140067041694656 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041693696 -> 140067041694656
	140067041693696 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140067041692064 -> 140067041693696
	140067041692064 [label="ViewBackward0
------------------------
self_sym_sizes: (6, 768)"]
	140067041692304 -> 140067041692064
	140067041692304 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (6, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041696048 -> 140067041692304
	140066794643664 [label="q_encoder.encoder.information_exchanging_layer.11.attention.self.value.bias
 (768)" fillcolor=lightblue]
	140066794643664 -> 140067041696048
	140067041696048 [label=AccumulateGrad]
	140067041689120 -> 140067041692304
	140067041689120 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140067041699024 -> 140067041689120
	140067041679664 -> 140067041692304
	140067041679664 [label=TBackward0]
	140067041635504 -> 140067041679664
	140066794643584 [label="q_encoder.encoder.information_exchanging_layer.11.attention.self.value.weight
 (768, 768)" fillcolor=lightblue]
	140066794643584 -> 140067041635504
	140067041635504 [label=AccumulateGrad]
	140067041697104 -> 140067041697680
	140067041697104 [label=TBackward0]
	140067041697392 -> 140067041697104
	140066794643744 [label="q_encoder.encoder.information_exchanging_layer.11.attention.output.dense.weight
 (768, 768)" fillcolor=lightblue]
	140066794643744 -> 140067041697392
	140067041697392 [label=AccumulateGrad]
	140067041699024 -> 140067041698832
	140067041698448 -> 140067041701904
	140066794643984 [label="q_encoder.encoder.information_exchanging_layer.11.attention.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	140066794643984 -> 140067041698448
	140067041698448 [label=AccumulateGrad]
	140067041697920 -> 140067041701904
	140066794643904 [label="q_encoder.encoder.information_exchanging_layer.11.attention.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	140066794643904 -> 140067041697920
	140067041697920 [label=AccumulateGrad]
	140067041699456 -> 140067041699264
	140067041699456 [label=TBackward0]
	140067041697968 -> 140067041699456
	140066794644144 [label="q_encoder.encoder.information_exchanging_layer.11.intermediate.dense.weight
 (3072, 768)" fillcolor=lightblue]
	140066794644144 -> 140067041697968
	140067041697968 [label=AccumulateGrad]
	140067041700128 -> 140067041699888
	140067041700128 [label=TBackward0]
	140067041699600 -> 140067041700128
	140066794643424 [label="q_encoder.encoder.information_exchanging_layer.11.output.dense.weight
 (768, 3072)" fillcolor=lightblue]
	140066794643424 -> 140067041699600
	140067041699600 [label=AccumulateGrad]
	140067041701904 -> 140067041700704
	140067041700224 -> 140067041700320
	140066794644464 [label="q_encoder.encoder.information_exchanging_layer.11.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	140066794644464 -> 140067041700224
	140067041700224 [label=AccumulateGrad]
	140067041703680 -> 140067041700320
	140066794644384 [label="q_encoder.encoder.information_exchanging_layer.11.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	140066794644384 -> 140067041703680
	140067041703680 [label=AccumulateGrad]
	140067041693168 -> 140067041701472
	140067041693168 [label="TransposeBackward0
------------------
dim0: 0
dim1: 1"]
	140067041701856 -> 140067041693168
	140067041701856 [label="SqueezeBackward1
---------------------------
dim           :           1
self_sym_sizes: (3, 1, 768)"]
	140067041700464 -> 140067041701856
	140067041700464 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 1, 768)
start         :                   0
step          :                   1"]
	140067041699120 -> 140067041700464
	140067041699120 [label="SliceBackward0
---------------------------
dim           :           1
end           :           1
self_sym_sizes: (3, 9, 768)
start         :           0
step          :           1"]
	140067041698592 -> 140067041699120
	140067041698592 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:         (3, 9, 768)
start         :                   0
step          :                   1"]
	140067041697008 -> 140067041698592
	140067041697008 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140067041697728 -> 140067041697008
	140067041697728 [label="AddBackward0
------------
alpha: 1"]
	140067041697824 -> 140067041697728
	140067041697824 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041699840 -> 140067041697824
	140067041699840 [label="ViewBackward0
-------------------------
self_sym_sizes: (27, 768)"]
	140067041695280 -> 140067041699840
	140067041695280 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :     (27, 3072)
mat1_sym_strides:      (3072, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (3072, 768)
mat2_sym_strides:      (1, 3072)"]
	140067041686480 -> 140067041695280
	140066770658448 [label="
 (768)" fillcolor=lightblue]
	140066770658448 -> 140067041686480
	140067041686480 [label=AccumulateGrad]
	140067041695952 -> 140067041695280
	140067041695952 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 9, 3072)"]
	140067041690336 -> 140067041695952
	140067041690336 [label="GeluBackward0
---------------------------
approximate:           none
self       : [saved tensor]"]
	140067041694080 -> 140067041690336
	140067041694080 [label="ViewBackward0
--------------------------
self_sym_sizes: (27, 3072)"]
	140067041633632 -> 140067041694080
	140067041633632 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (27, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (768, 3072)
mat2_sym_strides:       (1, 768)"]
	140067041634928 -> 140067041633632
	140066770658288 [label="
 (3072)" fillcolor=lightblue]
	140066770658288 -> 140067041634928
	140067041634928 [label=AccumulateGrad]
	140067041631040 -> 140067041633632
	140067041631040 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 9, 768)"]
	140067041699984 -> 140067041631040
	140067041699984 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140067041630080 -> 140067041699984
	140067041630080 [label="AddBackward0
------------
alpha: 1"]
	140067041627920 -> 140067041630080
	140067041627920 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041628784 -> 140067041627920
	140067041628784 [label="ViewBackward0
-------------------------
self_sym_sizes: (27, 768)"]
	140067041625520 -> 140067041628784
	140067041625520 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (27, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041635408 -> 140067041625520
	140066770657888 [label="
 (768)" fillcolor=lightblue]
	140066770657888 -> 140067041635408
	140067041635408 [label=AccumulateGrad]
	140067041625232 -> 140067041625520
	140067041625232 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 9, 768)"]
	140067041624320 -> 140067041625232
	140067041624320 [label="ViewBackward0
------------------------------
self_sym_sizes: (3, 9, 12, 64)"]
	140067041625328 -> 140067041624320
	140067041625328 [label=CloneBackward0]
	140067041625568 -> 140067041625328
	140067041625568 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041625088 -> 140067041625568
	140067041625088 [label="UnsafeViewBackward0
---------------------------
self_sym_sizes: (36, 9, 64)"]
	140067041935616 -> 140067041625088
	140067041935616 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140067041951648 -> 140067041935616
	140067041951648 [label="ViewBackward0
-----------------------------
self_sym_sizes: (3, 12, 9, 9)"]
	140067041951504 -> 140067041951648
	140067041951504 [label="ExpandBackward0
-----------------------------
self_sym_sizes: (3, 12, 9, 9)"]
	140067041951216 -> 140067041951504
	140067041951216 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041951072 -> 140067041951216
	140067041951072 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140067041950880 -> 140067041951072
	140067041950880 [label="AddBackward0
------------
alpha: 1"]
	140067041950784 -> 140067041950880
	140067041950784 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041950640 -> 140067041950784
	140067041950640 [label="UnsafeViewBackward0
--------------------------
self_sym_sizes: (36, 9, 9)"]
	140067041950352 -> 140067041950640
	140067041950352 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140067041950304 -> 140067041950352
	140067041950304 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 9, 64)"]
	140067041950256 -> 140067041950304
	140067041950256 [label=CloneBackward0]
	140067041950064 -> 140067041950256
	140067041950064 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 9, 64)"]
	140067041950016 -> 140067041950064
	140067041950016 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041949824 -> 140067041950016
	140067041949824 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 9, 768)"]
	140067041949680 -> 140067041949824
	140067041949680 [label="ViewBackward0
-------------------------
self_sym_sizes: (27, 768)"]
	140067041949536 -> 140067041949680
	140067041949536 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (27, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041949200 -> 140067041949536
	140066770657408 [label="
 (768)" fillcolor=lightblue]
	140066770657408 -> 140067041949200
	140067041949200 [label=AccumulateGrad]
	140067041949440 -> 140067041949536
	140067041949440 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 9, 768)"]
	140067041628400 -> 140067041949440
	140067041628400 [label="CatBackward0
------------
dim: 1"]
	140067041949056 -> 140067041628400
	140067041949056 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 1, 768)
start         :                   0
step          :                   1"]
	140067041949248 -> 140067041949056
	140067041949248 [label="SliceBackward0
---------------------------
dim           :           1
end           :           1
self_sym_sizes: (3, 9, 768)
start         :           0
step          :           1"]
	140067041948288 -> 140067041949248
	140067041948288 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:         (3, 9, 768)
start         :                   0
step          :                   1"]
	140067041948144 -> 140067041948288
	140067041948144 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140067041948384 -> 140067041948144
	140067041948384 [label="AddBackward0
------------
alpha: 1"]
	140067041948240 -> 140067041948384
	140067041948240 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041948672 -> 140067041948240
	140067041948672 [label="ViewBackward0
-------------------------
self_sym_sizes: (27, 768)"]
	140067041947760 -> 140067041948672
	140067041947760 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :     (27, 3072)
mat1_sym_strides:      (3072, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (3072, 768)
mat2_sym_strides:      (1, 3072)"]
	140067041948480 -> 140067041947760
	140066770657168 [label="
 (768)" fillcolor=lightblue]
	140066770657168 -> 140067041948480
	140067041948480 [label=AccumulateGrad]
	140067041947904 -> 140067041947760
	140067041947904 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 9, 3072)"]
	140067041947376 -> 140067041947904
	140067041947376 [label="GeluBackward0
---------------------------
approximate:           none
self       : [saved tensor]"]
	140067041947568 -> 140067041947376
	140067041947568 [label="ViewBackward0
--------------------------
self_sym_sizes: (27, 3072)"]
	140067041947184 -> 140067041947568
	140067041947184 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (27, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (768, 3072)
mat2_sym_strides:       (1, 768)"]
	140067041946944 -> 140067041947184
	140066770657008 [label="
 (3072)" fillcolor=lightblue]
	140066770657008 -> 140067041946944
	140067041946944 [label=AccumulateGrad]
	140067041946992 -> 140067041947184
	140067041946992 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 9, 768)"]
	140067041948720 -> 140067041946992
	140067041948720 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140067041946560 -> 140067041948720
	140067041946560 [label="AddBackward0
------------
alpha: 1"]
	140067041946272 -> 140067041946560
	140067041946272 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041946416 -> 140067041946272
	140067041946416 [label="ViewBackward0
-------------------------
self_sym_sizes: (27, 768)"]
	140067041945840 -> 140067041946416
	140067041945840 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (27, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041947040 -> 140067041945840
	140066770656688 [label="
 (768)" fillcolor=lightblue]
	140066770656688 -> 140067041947040
	140067041947040 [label=AccumulateGrad]
	140067041945984 -> 140067041945840
	140067041945984 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 9, 768)"]
	140067041945456 -> 140067041945984
	140067041945456 [label="ViewBackward0
------------------------------
self_sym_sizes: (3, 9, 12, 64)"]
	140067041945744 -> 140067041945456
	140067041945744 [label=CloneBackward0]
	140067041945792 -> 140067041945744
	140067041945792 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041945120 -> 140067041945792
	140067041945120 [label="UnsafeViewBackward0
---------------------------
self_sym_sizes: (36, 9, 64)"]
	140067041944976 -> 140067041945120
	140067041944976 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140067041944832 -> 140067041944976
	140067041944832 [label="ViewBackward0
-----------------------------
self_sym_sizes: (3, 12, 9, 9)"]
	140067041944688 -> 140067041944832
	140067041944688 [label="ExpandBackward0
-----------------------------
self_sym_sizes: (3, 12, 9, 9)"]
	140067041944400 -> 140067041944688
	140067041944400 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041944256 -> 140067041944400
	140067041944256 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140067041944016 -> 140067041944256
	140067041944016 [label="AddBackward0
------------
alpha: 1"]
	140067041943968 -> 140067041944016
	140067041943968 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041943728 -> 140067041943968
	140067041943728 [label="UnsafeViewBackward0
--------------------------
self_sym_sizes: (36, 9, 9)"]
	140067041943584 -> 140067041943728
	140067041943584 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140067041943536 -> 140067041943584
	140067041943536 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 9, 64)"]
	140067041943392 -> 140067041943536
	140067041943392 [label=CloneBackward0]
	140067041943152 -> 140067041943392
	140067041943152 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 9, 64)"]
	140067041943056 -> 140067041943152
	140067041943056 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041942816 -> 140067041943056
	140067041942816 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 9, 768)"]
	140067041942672 -> 140067041942816
	140067041942672 [label="ViewBackward0
-------------------------
self_sym_sizes: (27, 768)"]
	140067041942480 -> 140067041942672
	140067041942480 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (27, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041942336 -> 140067041942480
	140066770656208 [label="
 (768)" fillcolor=lightblue]
	140066770656208 -> 140067041942336
	140067041942336 [label=AccumulateGrad]
	140067041942432 -> 140067041942480
	140067041942432 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 9, 768)"]
	140067041946368 -> 140067041942432
	140067041946368 [label="CatBackward0
------------
dim: 1"]
	140067041941904 -> 140067041946368
	140067041941904 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 1, 768)
start         :                   0
step          :                   1"]
	140067041942384 -> 140067041941904
	140067041942384 [label="SliceBackward0
---------------------------
dim           :           1
end           :           1
self_sym_sizes: (3, 9, 768)
start         :           0
step          :           1"]
	140067041940992 -> 140067041942384
	140067041940992 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:         (3, 9, 768)
start         :                   0
step          :                   1"]
	140067041940800 -> 140067041940992
	140067041940800 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140067041941136 -> 140067041940800
	140067041941136 [label="AddBackward0
------------
alpha: 1"]
	140067041940896 -> 140067041941136
	140067041940896 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041941280 -> 140067041940896
	140067041941280 [label="ViewBackward0
-------------------------
self_sym_sizes: (27, 768)"]
	140067041940176 -> 140067041941280
	140067041940176 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :     (27, 3072)
mat1_sym_strides:      (3072, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (3072, 768)
mat2_sym_strides:      (1, 3072)"]
	140067041941328 -> 140067041940176
	140066770655888 [label="
 (768)" fillcolor=lightblue]
	140066770655888 -> 140067041941328
	140067041941328 [label=AccumulateGrad]
	140067041940368 -> 140067041940176
	140067041940368 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 9, 3072)"]
	140067041939792 -> 140067041940368
	140067041939792 [label="GeluBackward0
---------------------------
approximate:           none
self       : [saved tensor]"]
	140067041940032 -> 140067041939792
	140067041940032 [label="ViewBackward0
--------------------------
self_sym_sizes: (27, 3072)"]
	140067041939600 -> 140067041940032
	140067041939600 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (27, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (768, 3072)
mat2_sym_strides:       (1, 768)"]
	140067041939312 -> 140067041939600
	140066770655728 [label="
 (3072)" fillcolor=lightblue]
	140066770655728 -> 140067041939312
	140067041939312 [label=AccumulateGrad]
	140067041939360 -> 140067041939600
	140067041939360 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 9, 768)"]
	140067041941424 -> 140067041939360
	140067041941424 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140067041939024 -> 140067041941424
	140067041939024 [label="AddBackward0
------------
alpha: 1"]
	140067041938640 -> 140067041939024
	140067041938640 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041938784 -> 140067041938640
	140067041938784 [label="ViewBackward0
-------------------------
self_sym_sizes: (27, 768)"]
	140067041938112 -> 140067041938784
	140067041938112 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (27, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041939408 -> 140067041938112
	140066770655328 [label="
 (768)" fillcolor=lightblue]
	140066770655328 -> 140067041939408
	140067041939408 [label=AccumulateGrad]
	140067041938352 -> 140067041938112
	140067041938352 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 9, 768)"]
	140067041937776 -> 140067041938352
	140067041937776 [label="ViewBackward0
------------------------------
self_sym_sizes: (3, 9, 12, 64)"]
	140067041937920 -> 140067041937776
	140067041937920 [label=CloneBackward0]
	140067041937968 -> 140067041937920
	140067041937968 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041937344 -> 140067041937968
	140067041937344 [label="UnsafeViewBackward0
---------------------------
self_sym_sizes: (36, 9, 64)"]
	140067041937152 -> 140067041937344
	140067041937152 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140067041937008 -> 140067041937152
	140067041937008 [label="ViewBackward0
-----------------------------
self_sym_sizes: (3, 12, 9, 9)"]
	140067041936912 -> 140067041937008
	140067041936912 [label="ExpandBackward0
-----------------------------
self_sym_sizes: (3, 12, 9, 9)"]
	140067041936480 -> 140067041936912
	140067041936480 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041936336 -> 140067041936480
	140067041936336 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140067041936000 -> 140067041936336
	140067041936000 [label="AddBackward0
------------
alpha: 1"]
	140067041935952 -> 140067041936000
	140067041935952 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041935712 -> 140067041935952
	140067041935712 [label="UnsafeViewBackward0
--------------------------
self_sym_sizes: (36, 9, 9)"]
	140067041935424 -> 140067041935712
	140067041935424 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140067041942000 -> 140067041935424
	140067041942000 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 9, 64)"]
	140067041935568 -> 140067041942000
	140067041935568 [label=CloneBackward0]
	140067054817568 -> 140067041935568
	140067054817568 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 9, 64)"]
	140067054823568 -> 140067054817568
	140067054823568 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041721888 -> 140067054823568
	140067041721888 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 9, 768)"]
	140067041721696 -> 140067041721888
	140067041721696 [label="ViewBackward0
-------------------------
self_sym_sizes: (27, 768)"]
	140067041721552 -> 140067041721696
	140067041721552 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (27, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041721312 -> 140067041721552
	140066770654848 [label="
 (768)" fillcolor=lightblue]
	140066770654848 -> 140067041721312
	140067041721312 [label=AccumulateGrad]
	140067041721504 -> 140067041721552
	140067041721504 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 9, 768)"]
	140067041938832 -> 140067041721504
	140067041938832 [label="CatBackward0
------------
dim: 1"]
	140067041721024 -> 140067041938832
	140067041721024 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 1, 768)
start         :                   0
step          :                   1"]
	140067041721360 -> 140067041721024
	140067041721360 [label="SliceBackward0
---------------------------
dim           :           1
end           :           1
self_sym_sizes: (3, 9, 768)
start         :           0
step          :           1"]
	140067041720016 -> 140067041721360
	140067041720016 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:         (3, 9, 768)
start         :                   0
step          :                   1"]
	140067041719728 -> 140067041720016
	140067041719728 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140067041720160 -> 140067041719728
	140067041720160 [label="AddBackward0
------------
alpha: 1"]
	140067041719872 -> 140067041720160
	140067041719872 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041720448 -> 140067041719872
	140067041720448 [label="ViewBackward0
-------------------------
self_sym_sizes: (27, 768)"]
	140067041719200 -> 140067041720448
	140067041719200 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :     (27, 3072)
mat1_sym_strides:      (3072, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (3072, 768)
mat2_sym_strides:      (1, 3072)"]
	140067041720256 -> 140067041719200
	140066770654608 [label="
 (768)" fillcolor=lightblue]
	140066770654608 -> 140067041720256
	140067041720256 [label=AccumulateGrad]
	140067041719344 -> 140067041719200
	140067041719344 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 9, 3072)"]
	140067041718624 -> 140067041719344
	140067041718624 [label="GeluBackward0
---------------------------
approximate:           none
self       : [saved tensor]"]
	140067041718864 -> 140067041718624
	140067041718864 [label="ViewBackward0
--------------------------
self_sym_sizes: (27, 3072)"]
	140067041718480 -> 140067041718864
	140067041718480 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (27, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (768, 3072)
mat2_sym_strides:       (1, 768)"]
	140067041718192 -> 140067041718480
	140066770654448 [label="
 (3072)" fillcolor=lightblue]
	140066770654448 -> 140067041718192
	140067041718192 [label=AccumulateGrad]
	140067041718240 -> 140067041718480
	140067041718240 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 9, 768)"]
	140067041720544 -> 140067041718240
	140067041720544 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140067041717808 -> 140067041720544
	140067041717808 [label="AddBackward0
------------
alpha: 1"]
	140067041717136 -> 140067041717808
	140067041717136 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041717424 -> 140067041717136
	140067041717424 [label="ViewBackward0
-------------------------
self_sym_sizes: (27, 768)"]
	140067041716560 -> 140067041717424
	140067041716560 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (27, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041718288 -> 140067041716560
	140066770654048 [label="
 (768)" fillcolor=lightblue]
	140066770654048 -> 140067041718288
	140067041718288 [label=AccumulateGrad]
	140067041716704 -> 140067041716560
	140067041716704 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 9, 768)"]
	140067041716272 -> 140067041716704
	140067041716272 [label="ViewBackward0
------------------------------
self_sym_sizes: (3, 9, 12, 64)"]
	140067041716368 -> 140067041716272
	140067041716368 [label=CloneBackward0]
	140067041716464 -> 140067041716368
	140067041716464 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041715888 -> 140067041716464
	140067041715888 [label="UnsafeViewBackward0
---------------------------
self_sym_sizes: (36, 9, 64)"]
	140067041715504 -> 140067041715888
	140067041715504 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140067041715312 -> 140067041715504
	140067041715312 [label="ViewBackward0
-----------------------------
self_sym_sizes: (3, 12, 9, 9)"]
	140067041715072 -> 140067041715312
	140067041715072 [label="ExpandBackward0
-----------------------------
self_sym_sizes: (3, 12, 9, 9)"]
	140067041714976 -> 140067041715072
	140067041714976 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041714736 -> 140067041714976
	140067041714736 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140067041714400 -> 140067041714736
	140067041714400 [label="AddBackward0
------------
alpha: 1"]
	140067041714304 -> 140067041714400
	140067041714304 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041714064 -> 140067041714304
	140067041714064 [label="UnsafeViewBackward0
--------------------------
self_sym_sizes: (36, 9, 9)"]
	140067041713728 -> 140067041714064
	140067041713728 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140067041713632 -> 140067041713728
	140067041713632 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 9, 64)"]
	140067041713536 -> 140067041713632
	140067041713536 [label=CloneBackward0]
	140067041713344 -> 140067041713536
	140067041713344 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 9, 64)"]
	140067041713200 -> 140067041713344
	140067041713200 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041712960 -> 140067041713200
	140067041712960 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 9, 768)"]
	140067041712816 -> 140067041712960
	140067041712816 [label="ViewBackward0
-------------------------
self_sym_sizes: (27, 768)"]
	140067041712624 -> 140067041712816
	140067041712624 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (27, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041712288 -> 140067041712624
	140066770653568 [label="
 (768)" fillcolor=lightblue]
	140066770653568 -> 140067041712288
	140067041712288 [label=AccumulateGrad]
	140067041712576 -> 140067041712624
	140067041712576 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 9, 768)"]
	140067041717376 -> 140067041712576
	140067041717376 [label="CatBackward0
------------
dim: 1"]
	140067041712096 -> 140067041717376
	140067041712096 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 1, 768)
start         :                   0
step          :                   1"]
	140067041712384 -> 140067041712096
	140067041712384 [label="SliceBackward0
---------------------------
dim           :           1
end           :           1
self_sym_sizes: (3, 9, 768)
start         :           0
step          :           1"]
	140067041711328 -> 140067041712384
	140067041711328 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:         (3, 9, 768)
start         :                   0
step          :                   1"]
	140067041710896 -> 140067041711328
	140067041710896 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140067041711424 -> 140067041710896
	140067041711424 [label="AddBackward0
------------
alpha: 1"]
	140067041711232 -> 140067041711424
	140067041711232 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041711712 -> 140067041711232
	140067041711712 [label="ViewBackward0
-------------------------
self_sym_sizes: (27, 768)"]
	140067041710464 -> 140067041711712
	140067041710464 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :     (27, 3072)
mat1_sym_strides:      (3072, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (3072, 768)
mat2_sym_strides:      (1, 3072)"]
	140067041711520 -> 140067041710464
	140066770653328 [label="
 (768)" fillcolor=lightblue]
	140066770653328 -> 140067041711520
	140067041711520 [label=AccumulateGrad]
	140067041710608 -> 140067041710464
	140067041710608 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 9, 3072)"]
	140067041709984 -> 140067041710608
	140067041709984 [label="GeluBackward0
---------------------------
approximate:           none
self       : [saved tensor]"]
	140067041710272 -> 140067041709984
	140067041710272 [label="ViewBackward0
--------------------------
self_sym_sizes: (27, 3072)"]
	140067041709840 -> 140067041710272
	140067041709840 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (27, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (768, 3072)
mat2_sym_strides:       (1, 768)"]
	140067041709504 -> 140067041709840
	140066770653168 [label="
 (3072)" fillcolor=lightblue]
	140066770653168 -> 140067041709504
	140067041709504 [label=AccumulateGrad]
	140067041709600 -> 140067041709840
	140067041709600 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 9, 768)"]
	140067041711760 -> 140067041709600
	140067041711760 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140067041709216 -> 140067041711760
	140067041709216 [label="AddBackward0
------------
alpha: 1"]
	140067041708880 -> 140067041709216
	140067041708880 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041709024 -> 140067041708880
	140067041709024 [label="ViewBackward0
-------------------------
self_sym_sizes: (27, 768)"]
	140067041708448 -> 140067041709024
	140067041708448 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (27, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041709648 -> 140067041708448
	140066770652768 [label="
 (768)" fillcolor=lightblue]
	140066770652768 -> 140067041709648
	140067041709648 [label=AccumulateGrad]
	140067041708544 -> 140067041708448
	140067041708544 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 9, 768)"]
	140067041707968 -> 140067041708544
	140067041707968 [label="ViewBackward0
------------------------------
self_sym_sizes: (3, 9, 12, 64)"]
	140067041708256 -> 140067041707968
	140067041708256 [label=CloneBackward0]
	140067041708352 -> 140067041708256
	140067041708352 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041707680 -> 140067041708352
	140067041707680 [label="UnsafeViewBackward0
---------------------------
self_sym_sizes: (36, 9, 64)"]
	140067041707488 -> 140067041707680
	140067041707488 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140067041707296 -> 140067041707488
	140067041707296 [label="ViewBackward0
-----------------------------
self_sym_sizes: (3, 12, 9, 9)"]
	140067041707104 -> 140067041707296
	140067041707104 [label="ExpandBackward0
-----------------------------
self_sym_sizes: (3, 12, 9, 9)"]
	140067041706960 -> 140067041707104
	140067041706960 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041706864 -> 140067041706960
	140067041706864 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140067041706528 -> 140067041706864
	140067041706528 [label="AddBackward0
------------
alpha: 1"]
	140067041706480 -> 140067041706528
	140067041706480 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041706240 -> 140067041706480
	140067041706240 [label="UnsafeViewBackward0
--------------------------
self_sym_sizes: (36, 9, 9)"]
	140067041706048 -> 140067041706240
	140067041706048 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140067041707056 -> 140067041706048
	140067041707056 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 9, 64)"]
	140067041591056 -> 140067041707056
	140067041591056 [label=CloneBackward0]
	140067041590768 -> 140067041591056
	140067041590768 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 9, 64)"]
	140067041590720 -> 140067041590768
	140067041590720 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041590528 -> 140067041590720
	140067041590528 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 9, 768)"]
	140067041590288 -> 140067041590528
	140067041590288 [label="ViewBackward0
-------------------------
self_sym_sizes: (27, 768)"]
	140067041590144 -> 140067041590288
	140067041590144 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (27, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041589952 -> 140067041590144
	140066770652288 [label="
 (768)" fillcolor=lightblue]
	140066770652288 -> 140067041589952
	140067041589952 [label=AccumulateGrad]
	140067041590048 -> 140067041590144
	140067041590048 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 9, 768)"]
	140067041708928 -> 140067041590048
	140067041708928 [label="CatBackward0
------------
dim: 1"]
	140067041589712 -> 140067041708928
	140067041589712 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 1, 768)
start         :                   0
step          :                   1"]
	140067041590000 -> 140067041589712
	140067041590000 [label="SliceBackward0
---------------------------
dim           :           1
end           :           1
self_sym_sizes: (3, 9, 768)
start         :           0
step          :           1"]
	140067041588704 -> 140067041590000
	140067041588704 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:         (3, 9, 768)
start         :                   0
step          :                   1"]
	140067041588512 -> 140067041588704
	140067041588512 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140067041588896 -> 140067041588512
	140067041588896 [label="AddBackward0
------------
alpha: 1"]
	140067041588608 -> 140067041588896
	140067041588608 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041589184 -> 140067041588608
	140067041589184 [label="ViewBackward0
-------------------------
self_sym_sizes: (27, 768)"]
	140067041588032 -> 140067041589184
	140067041588032 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :     (27, 3072)
mat1_sym_strides:      (3072, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (3072, 768)
mat2_sym_strides:      (1, 3072)"]
	140067041588992 -> 140067041588032
	140066770652048 [label="
 (768)" fillcolor=lightblue]
	140066770652048 -> 140067041588992
	140067041588992 [label=AccumulateGrad]
	140067041588128 -> 140067041588032
	140067041588128 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 9, 3072)"]
	140067041587504 -> 140067041588128
	140067041587504 [label="GeluBackward0
---------------------------
approximate:           none
self       : [saved tensor]"]
	140067041587744 -> 140067041587504
	140067041587744 [label="ViewBackward0
--------------------------
self_sym_sizes: (27, 3072)"]
	140067041587264 -> 140067041587744
	140067041587264 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (27, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (768, 3072)
mat2_sym_strides:       (1, 768)"]
	140067041586736 -> 140067041587264
	140066770651888 [label="
 (3072)" fillcolor=lightblue]
	140066770651888 -> 140067041586736
	140067041586736 [label=AccumulateGrad]
	140067041586832 -> 140067041587264
	140067041586832 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 9, 768)"]
	140067041589280 -> 140067041586832
	140067041589280 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140067041586256 -> 140067041589280
	140067041586256 [label="AddBackward0
------------
alpha: 1"]
	140067041585824 -> 140067041586256
	140067041585824 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041586016 -> 140067041585824
	140067041586016 [label="ViewBackward0
-------------------------
self_sym_sizes: (27, 768)"]
	140067041585488 -> 140067041586016
	140067041585488 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (27, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041587168 -> 140067041585488
	140066770651488 [label="
 (768)" fillcolor=lightblue]
	140066770651488 -> 140067041587168
	140067041587168 [label=AccumulateGrad]
	140067041585584 -> 140067041585488
	140067041585584 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 9, 768)"]
	140067041584960 -> 140067041585584
	140067041584960 [label="ViewBackward0
------------------------------
self_sym_sizes: (3, 9, 12, 64)"]
	140067041585248 -> 140067041584960
	140067041585248 [label=CloneBackward0]
	140067041585344 -> 140067041585248
	140067041585344 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041584624 -> 140067041585344
	140067041584624 [label="UnsafeViewBackward0
---------------------------
self_sym_sizes: (36, 9, 64)"]
	140067041584480 -> 140067041584624
	140067041584480 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140067041584384 -> 140067041584480
	140067041584384 [label="ViewBackward0
-----------------------------
self_sym_sizes: (3, 12, 9, 9)"]
	140067041584288 -> 140067041584384
	140067041584288 [label="ExpandBackward0
-----------------------------
self_sym_sizes: (3, 12, 9, 9)"]
	140067041584048 -> 140067041584288
	140067041584048 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041583904 -> 140067041584048
	140067041583904 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140067041583664 -> 140067041583904
	140067041583664 [label="AddBackward0
------------
alpha: 1"]
	140067041583568 -> 140067041583664
	140067041583568 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041583424 -> 140067041583568
	140067041583424 [label="UnsafeViewBackward0
--------------------------
self_sym_sizes: (36, 9, 9)"]
	140067041583232 -> 140067041583424
	140067041583232 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140067041583040 -> 140067041583232
	140067041583040 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 9, 64)"]
	140067041582992 -> 140067041583040
	140067041582992 [label=CloneBackward0]
	140067041582704 -> 140067041582992
	140067041582704 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 9, 64)"]
	140067041582608 -> 140067041582704
	140067041582608 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041582416 -> 140067041582608
	140067041582416 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 9, 768)"]
	140067041582320 -> 140067041582416
	140067041582320 [label="ViewBackward0
-------------------------
self_sym_sizes: (27, 768)"]
	140067041582128 -> 140067041582320
	140067041582128 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (27, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041581936 -> 140067041582128
	140066770651008 [label="
 (768)" fillcolor=lightblue]
	140066770651008 -> 140067041581936
	140067041581936 [label=AccumulateGrad]
	140067041582080 -> 140067041582128
	140067041582080 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 9, 768)"]
	140067041585968 -> 140067041582080
	140067041585968 [label="CatBackward0
------------
dim: 1"]
	140067041581696 -> 140067041585968
	140067041581696 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 1, 768)
start         :                   0
step          :                   1"]
	140067041581984 -> 140067041581696
	140067041581984 [label="SliceBackward0
---------------------------
dim           :           1
end           :           1
self_sym_sizes: (3, 9, 768)
start         :           0
step          :           1"]
	140067041580592 -> 140067041581984
	140067041580592 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:         (3, 9, 768)
start         :                   0
step          :                   1"]
	140067041580400 -> 140067041580592
	140067041580400 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140067041580736 -> 140067041580400
	140067041580736 [label="AddBackward0
------------
alpha: 1"]
	140067041580544 -> 140067041580736
	140067041580544 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041581168 -> 140067041580544
	140067041581168 [label="ViewBackward0
-------------------------
self_sym_sizes: (27, 768)"]
	140067041580064 -> 140067041581168
	140067041580064 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :     (27, 3072)
mat1_sym_strides:      (3072, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (3072, 768)
mat2_sym_strides:      (1, 3072)"]
	140067041580976 -> 140067041580064
	140066770650768 [label="
 (768)" fillcolor=lightblue]
	140066770650768 -> 140067041580976
	140067041580976 [label=AccumulateGrad]
	140067041580160 -> 140067041580064
	140067041580160 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 9, 3072)"]
	140067041579680 -> 140067041580160
	140067041579680 [label="GeluBackward0
---------------------------
approximate:           none
self       : [saved tensor]"]
	140067041579920 -> 140067041579680
	140067041579920 [label="ViewBackward0
--------------------------
self_sym_sizes: (27, 3072)"]
	140067041579584 -> 140067041579920
	140067041579584 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (27, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (768, 3072)
mat2_sym_strides:       (1, 768)"]
	140067041579344 -> 140067041579584
	140066770650608 [label="
 (3072)" fillcolor=lightblue]
	140066770650608 -> 140067041579344
	140067041579344 [label=AccumulateGrad]
	140067041579440 -> 140067041579584
	140067041579440 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 9, 768)"]
	140067041581264 -> 140067041579440
	140067041581264 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140067041579008 -> 140067041581264
	140067041579008 [label="AddBackward0
------------
alpha: 1"]
	140067041578528 -> 140067041579008
	140067041578528 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041578720 -> 140067041578528
	140067041578720 [label="ViewBackward0
-------------------------
self_sym_sizes: (27, 768)"]
	140067041578096 -> 140067041578720
	140067041578096 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (27, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041579488 -> 140067041578096
	140066770650208 [label="
 (768)" fillcolor=lightblue]
	140066770650208 -> 140067041579488
	140067041579488 [label=AccumulateGrad]
	140067041578240 -> 140067041578096
	140067041578240 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 9, 768)"]
	140067041577616 -> 140067041578240
	140067041577616 [label="ViewBackward0
------------------------------
self_sym_sizes: (3, 9, 12, 64)"]
	140067041577904 -> 140067041577616
	140067041577904 [label=CloneBackward0]
	140067041578000 -> 140067041577904
	140067041578000 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041577424 -> 140067041578000
	140067041577424 [label="UnsafeViewBackward0
---------------------------
self_sym_sizes: (36, 9, 64)"]
	140067041577232 -> 140067041577424
	140067041577232 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140067041577088 -> 140067041577232
	140067041577088 [label="ViewBackward0
-----------------------------
self_sym_sizes: (3, 12, 9, 9)"]
	140067041576992 -> 140067041577088
	140067041576992 [label="ExpandBackward0
-----------------------------
self_sym_sizes: (3, 12, 9, 9)"]
	140067041576752 -> 140067041576992
	140067041576752 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041576608 -> 140067041576752
	140067041576608 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140067041576272 -> 140067041576608
	140067041576272 [label="AddBackward0
------------
alpha: 1"]
	140067041576224 -> 140067041576272
	140067041576224 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041576128 -> 140067041576224
	140067041576128 [label="UnsafeViewBackward0
--------------------------
self_sym_sizes: (36, 9, 9)"]
	140067041575840 -> 140067041576128
	140067041575840 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140067041575696 -> 140067041575840
	140067041575696 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 9, 64)"]
	140067041575648 -> 140067041575696
	140067041575648 [label=CloneBackward0]
	140067041575456 -> 140067041575648
	140067041575456 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 9, 64)"]
	140067041575408 -> 140067041575456
	140067041575408 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041575168 -> 140067041575408
	140067041575168 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 9, 768)"]
	140069262292064 -> 140067041575168
	140069262292064 [label="ViewBackward0
-------------------------
self_sym_sizes: (27, 768)"]
	140067041575552 -> 140069262292064
	140067041575552 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (27, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041575024 -> 140067041575552
	140066770649728 [label="
 (768)" fillcolor=lightblue]
	140066770649728 -> 140067041575024
	140067041575024 [label=AccumulateGrad]
	140067065372944 -> 140067041575552
	140067065372944 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 9, 768)"]
	140067041578624 -> 140067065372944
	140067041578624 [label="CatBackward0
------------
dim: 1"]
	140067065375488 -> 140067041578624
	140067065375488 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 1, 768)
start         :                   0
step          :                   1"]
	140067065380672 -> 140067065375488
	140067065380672 [label="SliceBackward0
---------------------------
dim           :           1
end           :           1
self_sym_sizes: (3, 9, 768)
start         :           0
step          :           1"]
	140067065374288 -> 140067065380672
	140067065374288 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:         (3, 9, 768)
start         :                   0
step          :                   1"]
	140067065364976 -> 140067065374288
	140067065364976 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140067065376448 -> 140067065364976
	140067065376448 [label="AddBackward0
------------
alpha: 1"]
	140067065377312 -> 140067065376448
	140067065377312 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067065373424 -> 140067065377312
	140067065373424 [label="ViewBackward0
-------------------------
self_sym_sizes: (27, 768)"]
	140067065377648 -> 140067065373424
	140067065377648 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :     (27, 3072)
mat1_sym_strides:      (3072, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (3072, 768)
mat2_sym_strides:      (1, 3072)"]
	140067065371744 -> 140067065377648
	140066770649488 [label="
 (768)" fillcolor=lightblue]
	140066770649488 -> 140067065371744
	140067065371744 [label=AccumulateGrad]
	140067065378224 -> 140067065377648
	140067065378224 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 9, 3072)"]
	140067065375776 -> 140067065378224
	140067065375776 [label="GeluBackward0
---------------------------
approximate:           none
self       : [saved tensor]"]
	140067065377600 -> 140067065375776
	140067065377600 [label="ViewBackward0
--------------------------
self_sym_sizes: (27, 3072)"]
	140067065377792 -> 140067065377600
	140067065377792 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (27, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (768, 3072)
mat2_sym_strides:       (1, 768)"]
	140067065374624 -> 140067065377792
	140066770649328 [label="
 (3072)" fillcolor=lightblue]
	140066770649328 -> 140067065374624
	140067065374624 [label=AccumulateGrad]
	140067065374768 -> 140067065377792
	140067065374768 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 9, 768)"]
	140067065376832 -> 140067065374768
	140067065376832 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140067065375056 -> 140067065376832
	140067065375056 [label="AddBackward0
------------
alpha: 1"]
	140067065373328 -> 140067065375056
	140067065373328 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067065376352 -> 140067065373328
	140067065376352 [label="ViewBackward0
-------------------------
self_sym_sizes: (27, 768)"]
	140067065378464 -> 140067065376352
	140067065378464 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (27, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067065371456 -> 140067065378464
	140066806152992 [label="
 (768)" fillcolor=lightblue]
	140066806152992 -> 140067065371456
	140067065371456 [label=AccumulateGrad]
	140067065379952 -> 140067065378464
	140067065379952 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 9, 768)"]
	140067065372752 -> 140067065379952
	140067065372752 [label="ViewBackward0
------------------------------
self_sym_sizes: (3, 9, 12, 64)"]
	140067065365648 -> 140067065372752
	140067065365648 [label=CloneBackward0]
	140067065375344 -> 140067065365648
	140067065375344 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067065365456 -> 140067065375344
	140067065365456 [label="UnsafeViewBackward0
---------------------------
self_sym_sizes: (36, 9, 64)"]
	140067065371120 -> 140067065365456
	140067065371120 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140067065372896 -> 140067065371120
	140067065372896 [label="ViewBackward0
-----------------------------
self_sym_sizes: (3, 12, 9, 9)"]
	140067065372608 -> 140067065372896
	140067065372608 [label="ExpandBackward0
-----------------------------
self_sym_sizes: (3, 12, 9, 9)"]
	140067065376544 -> 140067065372608
	140067065376544 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067065365936 -> 140067065376544
	140067065365936 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140067065379472 -> 140067065365936
	140067065379472 [label="AddBackward0
------------
alpha: 1"]
	140067065380480 -> 140067065379472
	140067065380480 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067065380048 -> 140067065380480
	140067065380048 [label="UnsafeViewBackward0
--------------------------
self_sym_sizes: (36, 9, 9)"]
	140067065372032 -> 140067065380048
	140067065372032 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140067065380528 -> 140067065372032
	140067065380528 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 9, 64)"]
	140067065371696 -> 140067065380528
	140067065371696 [label=CloneBackward0]
	140067065216208 -> 140067065371696
	140067065216208 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 9, 64)"]
	140067065210448 -> 140067065216208
	140067065210448 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067065213568 -> 140067065210448
	140067065213568 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 9, 768)"]
	140067065209632 -> 140067065213568
	140067065209632 [label="ViewBackward0
-------------------------
self_sym_sizes: (27, 768)"]
	140067065202240 -> 140067065209632
	140067065202240 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (27, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067065203776 -> 140067065202240
	140066806152432 [label="
 (768)" fillcolor=lightblue]
	140066806152432 -> 140067065203776
	140067065203776 [label=AccumulateGrad]
	140067065204832 -> 140067065202240
	140067065204832 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 9, 768)"]
	140067065376400 -> 140067065204832
	140067065376400 [label="CatBackward0
------------
dim: 1"]
	140067065210352 -> 140067065376400
	140067065210352 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 1, 768)
start         :                   0
step          :                   1"]
	140067065210736 -> 140067065210352
	140067065210736 [label="SliceBackward0
---------------------------
dim           :           1
end           :           1
self_sym_sizes: (3, 9, 768)
start         :           0
step          :           1"]
	140067065209776 -> 140067065210736
	140067065209776 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:         (3, 9, 768)
start         :                   0
step          :                   1"]
	140067065209344 -> 140067065209776
	140067065209344 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140067065216352 -> 140067065209344
	140067065216352 [label="AddBackward0
------------
alpha: 1"]
	140067065208480 -> 140067065216352
	140067065208480 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067065210016 -> 140067065208480
	140067065210016 [label="ViewBackward0
-------------------------
self_sym_sizes: (27, 768)"]
	140067065202912 -> 140067065210016
	140067065202912 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :     (27, 3072)
mat1_sym_strides:      (3072, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (3072, 768)
mat2_sym_strides:      (1, 3072)"]
	140067065215920 -> 140067065202912
	140066806152192 [label="
 (768)" fillcolor=lightblue]
	140066806152192 -> 140067065215920
	140067065215920 [label=AccumulateGrad]
	140067065201808 -> 140067065202912
	140067065201808 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 9, 3072)"]
	140067065201856 -> 140067065201808
	140067065201856 [label="GeluBackward0
---------------------------
approximate:           none
self       : [saved tensor]"]
	140067065210976 -> 140067065201856
	140067065210976 [label="ViewBackward0
--------------------------
self_sym_sizes: (27, 3072)"]
	140067065210304 -> 140067065210976
	140067065210304 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (27, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (768, 3072)
mat2_sym_strides:       (1, 768)"]
	140067065207232 -> 140067065210304
	140066806152112 [label="
 (3072)" fillcolor=lightblue]
	140066806152112 -> 140067065207232
	140067065207232 [label=AccumulateGrad]
	140067065213760 -> 140067065210304
	140067065213760 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 9, 768)"]
	140067065202432 -> 140067065213760
	140067065202432 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140067065213616 -> 140067065202432
	140067065213616 [label="AddBackward0
------------
alpha: 1"]
	140067065210496 -> 140067065213616
	140067065210496 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067065208864 -> 140067065210496
	140067065208864 [label="ViewBackward0
-------------------------
self_sym_sizes: (27, 768)"]
	140067065201520 -> 140067065208864
	140067065201520 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (27, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067065201232 -> 140067065201520
	140066806151712 [label="
 (768)" fillcolor=lightblue]
	140066806151712 -> 140067065201232
	140067065201232 [label=AccumulateGrad]
	140067065215488 -> 140067065201520
	140067065215488 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 9, 768)"]
	140067065209152 -> 140067065215488
	140067065209152 [label="ViewBackward0
------------------------------
self_sym_sizes: (3, 9, 12, 64)"]
	140067065216064 -> 140067065209152
	140067065216064 [label=CloneBackward0]
	140067065216976 -> 140067065216064
	140067065216976 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067065203680 -> 140067065216976
	140067065203680 [label="UnsafeViewBackward0
---------------------------
self_sym_sizes: (36, 9, 64)"]
	140067065216688 -> 140067065203680
	140067065216688 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140067065210880 -> 140067065216688
	140067065210880 [label="ViewBackward0
-----------------------------
self_sym_sizes: (3, 12, 9, 9)"]
	140067065216784 -> 140067065210880
	140067065216784 [label="ExpandBackward0
-----------------------------
self_sym_sizes: (3, 12, 9, 9)"]
	140067065209248 -> 140067065216784
	140067065209248 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067065216736 -> 140067065209248
	140067065216736 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140067065202000 -> 140067065216736
	140067065202000 [label="AddBackward0
------------
alpha: 1"]
	140067065211072 -> 140067065202000
	140067065211072 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067065202672 -> 140067065211072
	140067065202672 [label="UnsafeViewBackward0
--------------------------
self_sym_sizes: (36, 9, 9)"]
	140067065202480 -> 140067065202672
	140067065202480 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140067066360496 -> 140067065202480
	140067066360496 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 9, 64)"]
	140067065215536 -> 140067066360496
	140067065215536 [label=CloneBackward0]
	140067065216400 -> 140067065215536
	140067065216400 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 9, 64)"]
	140067053365392 -> 140067065216400
	140067053365392 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067053364480 -> 140067053365392
	140067053364480 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 9, 768)"]
	140067053361360 -> 140067053364480
	140067053361360 [label="ViewBackward0
-------------------------
self_sym_sizes: (27, 768)"]
	140067053370816 -> 140067053361360
	140067053370816 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (27, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067951673456 -> 140067053370816
	140066806151152 [label="
 (768)" fillcolor=lightblue]
	140066806151152 -> 140067951673456
	140067951673456 [label=AccumulateGrad]
	140067053367168 -> 140067053370816
	140067053367168 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 9, 768)"]
	140067065215440 -> 140067053367168
	140067065215440 [label="CatBackward0
------------
dim: 1"]
	140067053369664 -> 140067065215440
	140067053369664 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 1, 768)
start         :                   0
step          :                   1"]
	140067065264448 -> 140067053369664
	140067065264448 [label="SliceBackward0
---------------------------
dim           :           1
end           :           1
self_sym_sizes: (3, 9, 768)
start         :           0
step          :           1"]
	140067065256576 -> 140067065264448
	140067065256576 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:         (3, 9, 768)
start         :                   0
step          :                   1"]
	140067065257056 -> 140067065256576
	140067065257056 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140067065261136 -> 140067065257056
	140067065261136 [label="AddBackward0
------------
alpha: 1"]
	140067065258688 -> 140067065261136
	140067065258688 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067065263680 -> 140067065258688
	140067065263680 [label="ViewBackward0
-------------------------
self_sym_sizes: (27, 768)"]
	140067065260656 -> 140067065263680
	140067065260656 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :     (27, 3072)
mat1_sym_strides:      (3072, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (3072, 768)
mat2_sym_strides:      (1, 3072)"]
	140067065254368 -> 140067065260656
	140066806150912 [label="
 (768)" fillcolor=lightblue]
	140066806150912 -> 140067065254368
	140067065254368 [label=AccumulateGrad]
	140067065260848 -> 140067065260656
	140067065260848 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 9, 3072)"]
	140067065259984 -> 140067065260848
	140067065259984 [label="GeluBackward0
---------------------------
approximate:           none
self       : [saved tensor]"]
	140067065260704 -> 140067065259984
	140067065260704 [label="ViewBackward0
--------------------------
self_sym_sizes: (27, 3072)"]
	140067065261328 -> 140067065260704
	140067065261328 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (27, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (768, 3072)
mat2_sym_strides:       (1, 768)"]
	140067065258544 -> 140067065261328
	140066806150832 [label="
 (3072)" fillcolor=lightblue]
	140066806150832 -> 140067065258544
	140067065258544 [label=AccumulateGrad]
	140067065261472 -> 140067065261328
	140067065261472 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 9, 768)"]
	140067065252064 -> 140067065261472
	140067065252064 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140067065260560 -> 140067065252064
	140067065260560 [label="AddBackward0
------------
alpha: 1"]
	140067065265744 -> 140067065260560
	140067065265744 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067065261904 -> 140067065265744
	140067065261904 [label="ViewBackward0
-------------------------
self_sym_sizes: (27, 768)"]
	140067065265984 -> 140067065261904
	140067065265984 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (27, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067065260944 -> 140067065265984
	140066806150432 [label="
 (768)" fillcolor=lightblue]
	140066806150432 -> 140067065260944
	140067065260944 [label=AccumulateGrad]
	140067065265552 -> 140067065265984
	140067065265552 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 9, 768)"]
	140067065252544 -> 140067065265552
	140067065252544 [label="ViewBackward0
------------------------------
self_sym_sizes: (3, 9, 12, 64)"]
	140067065256432 -> 140067065252544
	140067065256432 [label=CloneBackward0]
	140067065266128 -> 140067065256432
	140067065266128 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067065251536 -> 140067065266128
	140067065251536 [label="UnsafeViewBackward0
---------------------------
self_sym_sizes: (36, 9, 64)"]
	140067065254896 -> 140067065251536
	140067065254896 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140067065255952 -> 140067065254896
	140067065255952 [label="ViewBackward0
-----------------------------
self_sym_sizes: (3, 12, 9, 9)"]
	140067065255856 -> 140067065255952
	140067065255856 [label="ExpandBackward0
-----------------------------
self_sym_sizes: (3, 12, 9, 9)"]
	140067065251056 -> 140067065255856
	140067065251056 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067065254320 -> 140067065251056
	140067065254320 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140067065251248 -> 140067065254320
	140067065251248 [label="AddBackward0
------------
alpha: 1"]
	140067065258448 -> 140067065251248
	140067065258448 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067065251488 -> 140067065258448
	140067065251488 [label="UnsafeViewBackward0
--------------------------
self_sym_sizes: (36, 9, 9)"]
	140067065258064 -> 140067065251488
	140067065258064 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140067065259696 -> 140067065258064
	140067065259696 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 9, 64)"]
	140067065256336 -> 140067065259696
	140067065256336 [label=CloneBackward0]
	140067065258496 -> 140067065256336
	140067065258496 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 9, 64)"]
	140067971886496 -> 140067065258496
	140067971886496 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067065258592 -> 140067971886496
	140067065258592 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 9, 768)"]
	140067065258304 -> 140067065258592
	140067065258304 [label="ViewBackward0
-------------------------
self_sym_sizes: (27, 768)"]
	140067007032448 -> 140067065258304
	140067007032448 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (27, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067007029088 -> 140067007032448
	140066806149872 [label="
 (768)" fillcolor=lightblue]
	140066806149872 -> 140067007029088
	140067007029088 [label=AccumulateGrad]
	140067007028272 -> 140067007032448
	140067007028272 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 9, 768)"]
	140067065261424 -> 140067007028272
	140067065261424 [label="CatBackward0
------------
dim: 1"]
	140067007027168 -> 140067065261424
	140067007027168 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 1, 768)
start         :                   0
step          :                   1"]
	140067007027600 -> 140067007027168
	140067007027600 [label="SliceBackward0
---------------------------
dim           :           1
end           :           1
self_sym_sizes: (3, 9, 768)
start         :           0
step          :           1"]
	140067007028656 -> 140067007027600
	140067007028656 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:         (3, 9, 768)
start         :                   0
step          :                   1"]
	140067007026304 -> 140067007028656
	140067007026304 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140067007026784 -> 140067007026304
	140067007026784 [label="AddBackward0
------------
alpha: 1"]
	140067007026736 -> 140067007026784
	140067007026736 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067007031440 -> 140067007026736
	140067007031440 [label="ViewBackward0
-------------------------
self_sym_sizes: (27, 768)"]
	140067007026016 -> 140067007031440
	140067007026016 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :     (27, 3072)
mat1_sym_strides:      (3072, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (3072, 768)
mat2_sym_strides:      (1, 3072)"]
	140067007034128 -> 140067007026016
	140066806149632 [label="
 (768)" fillcolor=lightblue]
	140066806149632 -> 140067007034128
	140067007034128 [label=AccumulateGrad]
	140067007028368 -> 140067007026016
	140067007028368 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 9, 3072)"]
	140067007034752 -> 140067007028368
	140067007034752 [label="GeluBackward0
---------------------------
approximate:           none
self       : [saved tensor]"]
	140067007035952 -> 140067007034752
	140067007035952 [label="ViewBackward0
--------------------------
self_sym_sizes: (27, 3072)"]
	140067007036000 -> 140067007035952
	140067007036000 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (27, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (768, 3072)
mat2_sym_strides:       (1, 768)"]
	140067007035472 -> 140067007036000
	140066806149552 [label="
 (3072)" fillcolor=lightblue]
	140066806149552 -> 140067007035472
	140067007035472 [label=AccumulateGrad]
	140067007032688 -> 140067007036000
	140067007032688 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 9, 768)"]
	140067007031920 -> 140067007032688
	140067007031920 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140067007035088 -> 140067007031920
	140067007035088 [label="AddBackward0
------------
alpha: 1"]
	140067007034368 -> 140067007035088
	140067007034368 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067007036720 -> 140067007034368
	140067007036720 [label="ViewBackward0
-------------------------
self_sym_sizes: (27, 768)"]
	140067007032592 -> 140067007036720
	140067007032592 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (27, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067007034608 -> 140067007032592
	140066806149152 [label="
 (768)" fillcolor=lightblue]
	140066806149152 -> 140067007034608
	140067007034608 [label=AccumulateGrad]
	140067007033984 -> 140067007032592
	140067007033984 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 9, 768)"]
	140067007035376 -> 140067007033984
	140067007035376 [label="ViewBackward0
------------------------------
self_sym_sizes: (3, 9, 12, 64)"]
	140067007032496 -> 140067007035376
	140067007032496 [label=CloneBackward0]
	140067007030912 -> 140067007032496
	140067007030912 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067007032544 -> 140067007030912
	140067007032544 [label="UnsafeViewBackward0
---------------------------
self_sym_sizes: (36, 9, 64)"]
	140067007036240 -> 140067007032544
	140067007036240 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140067007036528 -> 140067007036240
	140067007036528 [label="ViewBackward0
-----------------------------
self_sym_sizes: (3, 12, 9, 9)"]
	140067007036816 -> 140067007036528
	140067007036816 [label="ExpandBackward0
-----------------------------
self_sym_sizes: (3, 12, 9, 9)"]
	140067007036576 -> 140067007036816
	140067007036576 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067007029616 -> 140067007036576
	140067007029616 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140067007033840 -> 140067007029616
	140067007033840 [label="AddBackward0
------------
alpha: 1"]
	140067007032736 -> 140067007033840
	140067007032736 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067007033696 -> 140067007032736
	140067007033696 [label="UnsafeViewBackward0
--------------------------
self_sym_sizes: (36, 9, 9)"]
	140067007033504 -> 140067007033696
	140067007033504 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140067007034464 -> 140067007033504
	140067007034464 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 9, 64)"]
	140067007034272 -> 140067007034464
	140067007034272 [label=CloneBackward0]
	140067007033408 -> 140067007034272
	140067007033408 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 9, 64)"]
	140067007027984 -> 140067007033408
	140067007027984 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067007033648 -> 140067007027984
	140067007033648 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 9, 768)"]
	140067007025152 -> 140067007033648
	140067007025152 [label="ViewBackward0
-------------------------
self_sym_sizes: (27, 768)"]
	140067007030144 -> 140067007025152
	140067007030144 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (27, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067007026832 -> 140067007030144
	140066806148592 [label="
 (768)" fillcolor=lightblue]
	140066806148592 -> 140067007026832
	140067007026832 [label=AccumulateGrad]
	140067007033456 -> 140067007030144
	140067007033456 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 9, 768)"]
	140067007033792 -> 140067007033456
	140067007033792 [label="CatBackward0
------------
dim: 1"]
	140067007029760 -> 140067007033792
	140067007029760 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 1, 768)
start         :                   0
step          :                   1"]
	140067007030000 -> 140067007029760
	140067007030000 [label="SliceBackward0
---------------------------
dim           :           1
end           :           1
self_sym_sizes: (3, 9, 768)
start         :           0
step          :           1"]
	140067007031104 -> 140067007030000
	140067007031104 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:         (3, 9, 768)
start         :                   0
step          :                   1"]
	140067007030192 -> 140067007031104
	140067007030192 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140067007030816 -> 140067007030192
	140067007030816 [label="AddBackward0
------------
alpha: 1"]
	140067007031008 -> 140067007030816
	140067007031008 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067007030720 -> 140067007031008
	140067007030720 [label="ViewBackward0
-------------------------
self_sym_sizes: (27, 768)"]
	140067007030288 -> 140067007030720
	140067007030288 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :     (27, 3072)
mat1_sym_strides:      (3072, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (3072, 768)
mat2_sym_strides:      (1, 3072)"]
	140067007030960 -> 140067007030288
	140066806148352 [label="
 (768)" fillcolor=lightblue]
	140066806148352 -> 140067007030960
	140067007030960 [label=AccumulateGrad]
	140067007030336 -> 140067007030288
	140067007030336 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 9, 3072)"]
	140067007030048 -> 140067007030336
	140067007030048 [label="GeluBackward0
---------------------------
approximate:           none
self       : [saved tensor]"]
	140067007031968 -> 140067007030048
	140067007031968 [label="ViewBackward0
--------------------------
self_sym_sizes: (27, 3072)"]
	140067007025392 -> 140067007031968
	140067007025392 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (27, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (768, 3072)
mat2_sym_strides:       (1, 768)"]
	140067007031872 -> 140067007025392
	140066806148272 [label="
 (3072)" fillcolor=lightblue]
	140066806148272 -> 140067007031872
	140067007031872 [label=AccumulateGrad]
	140067007032352 -> 140067007025392
	140067007032352 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 9, 768)"]
	140067007031344 -> 140067007032352
	140067007031344 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140067007024240 -> 140067007031344
	140067007024240 [label="AddBackward0
------------
alpha: 1"]
	140067007024288 -> 140067007024240
	140067007024288 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067007024384 -> 140067007024288
	140067007024384 [label="ViewBackward0
-------------------------
self_sym_sizes: (27, 768)"]
	140067007024624 -> 140067007024384
	140067007024624 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (27, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067007023856 -> 140067007024624
	140066806147872 [label="
 (768)" fillcolor=lightblue]
	140066806147872 -> 140067007023856
	140067007023856 [label=AccumulateGrad]
	140067007024720 -> 140067007024624
	140067007024720 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 9, 768)"]
	140067007024768 -> 140067007024720
	140067007024768 [label="ViewBackward0
------------------------------
self_sym_sizes: (3, 9, 12, 64)"]
	140067007024912 -> 140067007024768
	140067007024912 [label=CloneBackward0]
	140067007024576 -> 140067007024912
	140067007024576 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067007024816 -> 140067007024576
	140067007024816 [label="UnsafeViewBackward0
---------------------------
self_sym_sizes: (36, 9, 64)"]
	140067949274512 -> 140067007024816
	140067949274512 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140067949271152 -> 140067949274512
	140067949271152 [label="ViewBackward0
-----------------------------
self_sym_sizes: (3, 12, 9, 9)"]
	140067054890256 -> 140067949271152
	140067054890256 [label="ExpandBackward0
-----------------------------
self_sym_sizes: (3, 12, 9, 9)"]
	140067054796560 -> 140067054890256
	140067054796560 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067054795360 -> 140067054796560
	140067054795360 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140067054793632 -> 140067054795360
	140067054793632 [label="AddBackward0
------------
alpha: 1"]
	140067054793248 -> 140067054793632
	140067054793248 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067054791136 -> 140067054793248
	140067054791136 [label="UnsafeViewBackward0
--------------------------
self_sym_sizes: (36, 9, 9)"]
	140067054790800 -> 140067054791136
	140067054790800 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140067054790608 -> 140067054790800
	140067054790608 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 9, 64)"]
	140067054790464 -> 140067054790608
	140067054790464 [label=CloneBackward0]
	140067054790080 -> 140067054790464
	140067054790080 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 9, 64)"]
	140067054789936 -> 140067054790080
	140067054789936 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067054788496 -> 140067054789936
	140067054788496 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 9, 768)"]
	140067054788256 -> 140067054788496
	140067054788256 [label="ViewBackward0
-------------------------
self_sym_sizes: (27, 768)"]
	140067054787968 -> 140067054788256
	140067054787968 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (27, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067054787440 -> 140067054787968
	140066806147392 [label="
 (768)" fillcolor=lightblue]
	140066806147392 -> 140067054787440
	140067054787440 [label=AccumulateGrad]
	140067054787584 -> 140067054787968
	140067054787584 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 9, 768)"]
	140067007024336 -> 140067054787584
	140067007024336 [label="CatBackward0
------------
dim: 1"]
	140067054787104 -> 140067007024336
	140067054787104 [label="RepeatBackward0
---------------------------
repeats       :   (3, 1, 1)
self_sym_sizes: (1, 1, 768)"]
	140067054787488 -> 140067054787104
	140067054787488 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (1, 1, 768)
start         :                   0
step          :                   1"]
	140067054786384 -> 140067054787488
	140067054786384 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:         (1, 1, 768)
start         :                   0
step          :                   1"]
	140067054784800 -> 140067054786384
	140067054784800 [label="UnsqueezeBackward0
------------------
dim: 0"]
	140067054784656 -> 140067054784800
	140066489225360 [label="
 (1, 768)" fillcolor=lightblue]
	140066489225360 -> 140067054784656
	140067054784656 [label=AccumulateGrad]
	140067054787344 -> 140067007024336
	140067054787344 [label=CloneBackward0]
	140067054786336 -> 140067054787344
	140067054786336 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 8, 768)
start         :                   0
step          :                   1"]
	140067054787248 -> 140067054786336
	140067054787248 [label="SelectBackward0
--------------------------------
dim           :                2
index         :                1
self_sym_sizes: (3, 8, 511, 768)"]
	140067054784032 -> 140067054787248
	140067054784032 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:    (3, 8, 511, 768)
start         :                   0
step          :                   1"]
	140067054786672 -> 140067054784032
	140067054786672 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:    (3, 8, 511, 768)
start         :                   0
step          :                   1"]
	140067054782880 -> 140067054786672
	140067054782880 [label="ViewBackward0
------------------------------
self_sym_sizes: (24, 511, 768)"]
	140067054782640 -> 140067054782880
	140067054782640 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140067054783840 -> 140067054782640
	140067054783840 [label="AddBackward0
------------
alpha: 1"]
	140067054783552 -> 140067054783840
	140067054783552 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067054786288 -> 140067054783552
	140067054786288 [label="ViewBackward0
----------------------------
self_sym_sizes: (12264, 768)"]
	140067054781824 -> 140067054786288
	140067054781824 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :  (12264, 3072)
mat1_sym_strides:      (3072, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (3072, 768)
mat2_sym_strides:      (1, 3072)"]
	140067054781632 -> 140067054781824
	140066537519808 [label="
 (768)" fillcolor=lightblue]
	140066537519808 -> 140067054781632
	140067054781632 [label=AccumulateGrad]
	140067054782256 -> 140067054781824
	140067054782256 [label="ViewBackward0
-------------------------------
self_sym_sizes: (24, 511, 3072)"]
	140067054780960 -> 140067054782256
	140067054780960 [label="GeluBackward0
---------------------------
approximate:           none
self       : [saved tensor]"]
	140067054781584 -> 140067054780960
	140067054781584 [label="ViewBackward0
-----------------------------
self_sym_sizes: (12264, 3072)"]
	140067054785952 -> 140067054781584
	140067054785952 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :   (12264, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (768, 3072)
mat2_sym_strides:       (1, 768)"]
	140067054787200 -> 140067054785952
	140066537520368 [label="
 (3072)" fillcolor=lightblue]
	140066537520368 -> 140067054787200
	140067054787200 [label=AccumulateGrad]
	140067054782976 -> 140067054785952
	140067054782976 [label="ViewBackward0
------------------------------
self_sym_sizes: (24, 511, 768)"]
	140067054782592 -> 140067054782976
	140067054782592 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140067054787008 -> 140067054782592
	140067054787008 [label="AddBackward0
------------
alpha: 1"]
	140067054794640 -> 140067054787008
	140067054794640 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067054787824 -> 140067054794640
	140067054787824 [label="ViewBackward0
----------------------------
self_sym_sizes: (12264, 768)"]
	140067949452240 -> 140067054787824
	140067949452240 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :   (12264, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067949455312 -> 140067949452240
	140066537519648 [label="
 (768)" fillcolor=lightblue]
	140066537519648 -> 140067949455312
	140067949455312 [label=AccumulateGrad]
	140067949448208 -> 140067949452240
	140067949448208 [label="ViewBackward0
------------------------------
self_sym_sizes: (24, 511, 768)"]
	140067003957600 -> 140067949448208
	140067003957600 [label="ViewBackward0
---------------------------------
self_sym_sizes: (24, 511, 12, 64)"]
	140067003957408 -> 140067003957600
	140067003957408 [label=CloneBackward0]
	140067065000288 -> 140067003957408
	140067065000288 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067064998128 -> 140067065000288
	140067064998128 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (288, 511, 64)"]
	140067064997408 -> 140067064998128
	140067064997408 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140067064997888 -> 140067064997408
	140067064997888 [label="ViewBackward0
----------------------------------
self_sym_sizes: (24, 12, 511, 511)"]
	140067064992944 -> 140067064997888
	140067064992944 [label="ExpandBackward0
----------------------------------
self_sym_sizes: (24, 12, 511, 511)"]
	140067065001152 -> 140067064992944
	140067065001152 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067065001008 -> 140067065001152
	140067065001008 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140067065002640 -> 140067065001008
	140067065002640 [label="AddBackward0
------------
alpha: 1"]
	140067053501888 -> 140067065002640
	140067053501888 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067053496704 -> 140067053501888
	140067053496704 [label="UnsafeViewBackward0
-------------------------------
self_sym_sizes: (288, 511, 511)"]
	140067053501696 -> 140067053496704
	140067053501696 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140067053499392 -> 140067053501696
	140067053499392 [label="UnsafeViewBackward0
---------------------------------
self_sym_sizes: (24, 12, 511, 64)"]
	140067053498096 -> 140067053499392
	140067053498096 [label=CloneBackward0]
	140067053486864 -> 140067053498096
	140067053486864 [label="ExpandBackward0
---------------------------------
self_sym_sizes: (24, 12, 511, 64)"]
	140067053486144 -> 140067053486864
	140067053486144 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067053487584 -> 140067053486144
	140067053487584 [label="ViewBackward0
------------------------------
self_sym_sizes: (24, 511, 768)"]
	140067053487824 -> 140067053487584
	140067053487824 [label="ViewBackward0
----------------------------
self_sym_sizes: (12264, 768)"]
	140067053486624 -> 140067053487824
	140067053486624 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :   (12264, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067053487008 -> 140067053486624
	140066537519568 [label="
 (768)" fillcolor=lightblue]
	140066537519568 -> 140067053487008
	140067053487008 [label=AccumulateGrad]
	140067053487488 -> 140067053486624
	140067053487488 [label="ViewBackward0
------------------------------
self_sym_sizes: (24, 511, 768)"]
	140067054791376 -> 140067053487488
	140067054791376 [label="CatBackward0
------------
dim: 1"]
	140067053497232 -> 140067054791376
	140067053497232 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067053497184 -> 140067053497232
	140067053497184 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140067053497616 -> 140067053497184
	140067053497616 [label="AddBackward0
------------
alpha: 1"]
	140067053495696 -> 140067053497616
	140067053495696 [label="AddBackward0
------------
alpha: 1"]
	140067053486192 -> 140067053495696
	140067053486192 [label="EmbeddingBackward0
------------------------------------
indices             : [saved tensor]
padding_idx         :              0
scale_grad_by_freq  :          False
sparse              :          False
weight_sym_argsize_0:          30522"]
	140067053487728 -> 140067053486192
	140066537520768 [label="
 (30522, 768)" fillcolor=lightblue]
	140066537520768 -> 140067053487728
	140067053487728 [label=AccumulateGrad]
	140067053486576 -> 140067053495696
	140067053486576 [label="EmbeddingBackward0
------------------------------------------
indices             :       [saved tensor]
padding_idx         : 18446744073709551615
scale_grad_by_freq  :                False
sparse              :                False
weight_sym_argsize_0:                    2"]
	140067053487776 -> 140067053486576
	140066537520128 [label="
 (2, 768)" fillcolor=lightblue]
	140066537520128 -> 140067053487776
	140067053487776 [label=AccumulateGrad]
	140067053486288 -> 140067053497616
	140067053486288 [label="EmbeddingBackward0
------------------------------------------
indices             :       [saved tensor]
padding_idx         : 18446744073709551615
scale_grad_by_freq  :                False
sparse              :                False
weight_sym_argsize_0:                  512"]
	140067053492672 -> 140067053486288
	140066537520608 [label="
 (512, 768)" fillcolor=lightblue]
	140066537520608 -> 140067053492672
	140067053492672 [label=AccumulateGrad]
	140067053487632 -> 140067053497184
	140066537520848 [label="
 (768)" fillcolor=lightblue]
	140066537520848 -> 140067053487632
	140067053487632 [label=AccumulateGrad]
	140067053488112 -> 140067053497184
	140066489449152 [label="
 (768)" fillcolor=lightblue]
	140066489449152 -> 140067053488112
	140067053488112 [label=AccumulateGrad]
	140067053498000 -> 140067053486624
	140067053498000 [label=TBackward0]
	140067053489168 -> 140067053498000
	140066537520048 [label="
 (768, 768)" fillcolor=lightblue]
	140066537520048 -> 140067053489168
	140067053489168 [label=AccumulateGrad]
	140067053496656 -> 140067053501696
	140067053496656 [label="UnsafeViewBackward0
---------------------------------
self_sym_sizes: (24, 12, 64, 511)"]
	140067053502032 -> 140067053496656
	140067053502032 [label=CloneBackward0]
	140067053487104 -> 140067053502032
	140067053487104 [label="ExpandBackward0
---------------------------------
self_sym_sizes: (24, 12, 64, 511)"]
	140067053487536 -> 140067053487104
	140067053487536 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551615
dim1: 18446744073709551614"]
	140067053490080 -> 140067053487536
	140067053490080 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067053498384 -> 140067053490080
	140067053498384 [label="ViewBackward0
------------------------------
self_sym_sizes: (24, 511, 768)"]
	140067053494736 -> 140067053498384
	140067053494736 [label="ViewBackward0
----------------------------
self_sym_sizes: (12264, 768)"]
	140067053494784 -> 140067053494736
	140067053494784 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :   (12264, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067053486816 -> 140067053494784
	140066537519968 [label="
 (768)" fillcolor=lightblue]
	140066537519968 -> 140067053486816
	140067053486816 [label=AccumulateGrad]
	140067053486768 -> 140067053494784
	140067053486768 [label="ViewBackward0
------------------------------
self_sym_sizes: (24, 511, 768)"]
	140067054791376 -> 140067053486768
	140067053487392 -> 140067053494784
	140067053487392 [label=TBackward0]
	140067053497712 -> 140067053487392
	140066489223840 [label="
 (768, 768)" fillcolor=lightblue]
	140066489223840 -> 140067053497712
	140067053497712 [label=AccumulateGrad]
	140067065000816 -> 140067064997408
	140067065000816 [label="UnsafeViewBackward0
---------------------------------
self_sym_sizes: (24, 12, 511, 64)"]
	140067065002784 -> 140067065000816
	140067065002784 [label=CloneBackward0]
	140067064988576 -> 140067065002784
	140067064988576 [label="ExpandBackward0
---------------------------------
self_sym_sizes: (24, 12, 511, 64)"]
	140067053486384 -> 140067064988576
	140067053486384 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067053488304 -> 140067053486384
	140067053488304 [label="ViewBackward0
------------------------------
self_sym_sizes: (24, 511, 768)"]
	140067053498864 -> 140067053488304
	140067053498864 [label="ViewBackward0
----------------------------
self_sym_sizes: (12264, 768)"]
	140067053489600 -> 140067053498864
	140067053489600 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :   (12264, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067053497136 -> 140067053489600
	140066489237360 [label="
 (768)" fillcolor=lightblue]
	140066489237360 -> 140067053497136
	140067053497136 [label=AccumulateGrad]
	140067053487920 -> 140067053489600
	140067053487920 [label="ViewBackward0
------------------------------
self_sym_sizes: (24, 511, 768)"]
	140067054791376 -> 140067053487920
	140067053487056 -> 140067053489600
	140067053487056 [label=TBackward0]
	140067053502272 -> 140067053487056
	140066489237280 [label="
 (768, 768)" fillcolor=lightblue]
	140066489237280 -> 140067053502272
	140067053502272 [label=AccumulateGrad]
	140067949451952 -> 140067949452240
	140067949451952 [label=TBackward0]
	140067003957504 -> 140067949451952
	140066489075984 [label="
 (768, 768)" fillcolor=lightblue]
	140066489075984 -> 140067003957504
	140067003957504 [label=AccumulateGrad]
	140067054791376 -> 140067054787008
	140067054781680 -> 140067054782592
	140066489084304 [label="
 (768)" fillcolor=lightblue]
	140066489084304 -> 140067054781680
	140067054781680 [label=AccumulateGrad]
	140067054794832 -> 140067054782592
	140066489082064 [label="
 (768)" fillcolor=lightblue]
	140066489082064 -> 140067054794832
	140067054794832 [label=AccumulateGrad]
	140067054781056 -> 140067054785952
	140067054781056 [label=TBackward0]
	140067054795168 -> 140067054781056
	140066537521088 [label="
 (3072, 768)" fillcolor=lightblue]
	140066537521088 -> 140067054795168
	140067054795168 [label=AccumulateGrad]
	140067054782352 -> 140067054781824
	140067054782352 [label=TBackward0]
	140067054781296 -> 140067054782352
	140066537520928 [label="
 (768, 3072)" fillcolor=lightblue]
	140066537520928 -> 140067054781296
	140067054781296 [label=AccumulateGrad]
	140067054782592 -> 140067054783840
	140067054782688 -> 140067054782640
	140066537519888 [label="
 (768)" fillcolor=lightblue]
	140066537519888 -> 140067054782688
	140067054782688 [label=AccumulateGrad]
	140067054786768 -> 140067054782640
	140066537521008 [label="
 (768)" fillcolor=lightblue]
	140066537521008 -> 140067054786768
	140067054786768 [label=AccumulateGrad]
	140067054790224 -> 140067054787968
	140067054790224 [label=TBackward0]
	140067054786960 -> 140067054790224
	140066806147312 [label="
 (768, 768)" fillcolor=lightblue]
	140066806147312 -> 140067054786960
	140067054786960 [label=AccumulateGrad]
	140067054791232 -> 140067054790800
	140067054791232 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 64, 9)"]
	140067054790992 -> 140067054791232
	140067054790992 [label=CloneBackward0]
	140067054788112 -> 140067054790992
	140067054788112 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 64, 9)"]
	140067054788400 -> 140067054788112
	140067054788400 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551615
dim1: 18446744073709551614"]
	140067054785808 -> 140067054788400
	140067054785808 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067054783024 -> 140067054785808
	140067054783024 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 9, 768)"]
	140067054786048 -> 140067054783024
	140067054786048 [label="ViewBackward0
-------------------------
self_sym_sizes: (27, 768)"]
	140067054782448 -> 140067054786048
	140067054782448 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (27, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067054781872 -> 140067054782448
	140066806147552 [label="
 (768)" fillcolor=lightblue]
	140066806147552 -> 140067054781872
	140067054781872 [label=AccumulateGrad]
	140067054782544 -> 140067054782448
	140067054782544 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 9, 768)"]
	140067007024336 -> 140067054782544
	140067054790176 -> 140067054782448
	140067054790176 [label=TBackward0]
	140067054795216 -> 140067054790176
	140066806147472 [label="
 (768, 768)" fillcolor=lightblue]
	140066806147472 -> 140067054795216
	140067054795216 [label=AccumulateGrad]
	140067054884496 -> 140067949274512
	140067054884496 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 9, 64)"]
	140067054881856 -> 140067054884496
	140067054881856 [label=CloneBackward0]
	140067054794880 -> 140067054881856
	140067054794880 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 9, 64)"]
	140067054794448 -> 140067054794880
	140067054794448 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067054789792 -> 140067054794448
	140067054789792 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 9, 768)"]
	140067054786816 -> 140067054789792
	140067054786816 [label="ViewBackward0
-------------------------
self_sym_sizes: (27, 768)"]
	140067054785376 -> 140067054786816
	140067054785376 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (27, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067949458288 -> 140067054785376
	140066806147712 [label="
 (768)" fillcolor=lightblue]
	140066806147712 -> 140067949458288
	140067949458288 [label=AccumulateGrad]
	140067054787152 -> 140067054785376
	140067054787152 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 9, 768)"]
	140067007024336 -> 140067054787152
	140067054795744 -> 140067054785376
	140067054795744 [label=TBackward0]
	140067054784752 -> 140067054795744
	140066806147632 [label="
 (768, 768)" fillcolor=lightblue]
	140066806147632 -> 140067054784752
	140067054784752 [label=AccumulateGrad]
	140067007024432 -> 140067007024624
	140067007024432 [label=TBackward0]
	140067007024864 -> 140067007024432
	140066806147792 [label="
 (768, 768)" fillcolor=lightblue]
	140066806147792 -> 140067007024864
	140067007024864 [label=AccumulateGrad]
	140067007024336 -> 140067007024240
	140067007024096 -> 140067007031344
	140066806148032 [label="
 (768)" fillcolor=lightblue]
	140066806148032 -> 140067007024096
	140067007024096 [label=AccumulateGrad]
	140067007024192 -> 140067007031344
	140066806147952 [label="
 (768)" fillcolor=lightblue]
	140066806147952 -> 140067007024192
	140067007024192 [label=AccumulateGrad]
	140067007031488 -> 140067007025392
	140067007031488 [label=TBackward0]
	140067007024480 -> 140067007031488
	140066806148192 [label="
 (3072, 768)" fillcolor=lightblue]
	140066806148192 -> 140067007024480
	140067007024480 [label=AccumulateGrad]
	140067007031056 -> 140067007030288
	140067007031056 [label=TBackward0]
	140067007032016 -> 140067007031056
	140066806147152 [label="
 (768, 3072)" fillcolor=lightblue]
	140066806147152 -> 140067007032016
	140067007032016 [label=AccumulateGrad]
	140067007031344 -> 140067007030816
	140067007027312 -> 140067007030192
	140066806148512 [label="
 (768)" fillcolor=lightblue]
	140066806148512 -> 140067007027312
	140067007027312 [label=AccumulateGrad]
	140067007030576 -> 140067007030192
	140066806148432 [label="
 (768)" fillcolor=lightblue]
	140066806148432 -> 140067007030576
	140067007030576 [label=AccumulateGrad]
	140067007030384 -> 140067007033792
	140067007030384 [label=CloneBackward0]
	140067007031152 -> 140067007030384
	140067007031152 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 8, 768)
start         :                   0
step          :                   1"]
	140067007026256 -> 140067007031152
	140067007026256 [label="SelectBackward0
--------------------------------
dim           :                2
index         :                1
self_sym_sizes: (3, 8, 511, 768)"]
	140067007024048 -> 140067007026256
	140067007024048 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:    (3, 8, 511, 768)
start         :                   0
step          :                   1"]
	140067007029904 -> 140067007024048
	140067007029904 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:    (3, 8, 511, 768)
start         :                   0
step          :                   1"]
	140067949271248 -> 140067007029904
	140067949271248 [label="ViewBackward0
------------------------------
self_sym_sizes: (24, 511, 768)"]
	140067007030240 -> 140067949271248
	140067007030240 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140067054890832 -> 140067007030240
	140067054890832 [label="AddBackward0
------------
alpha: 1"]
	140067007026592 -> 140067054890832
	140067007026592 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067007024000 -> 140067007026592
	140067007024000 [label="ViewBackward0
----------------------------
self_sym_sizes: (12264, 768)"]
	140067054785328 -> 140067007024000
	140067054785328 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :  (12264, 3072)
mat1_sym_strides:      (3072, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (3072, 768)
mat2_sym_strides:      (1, 3072)"]
	140067963829744 -> 140067054785328
	140066488939968 [label="
 (768)" fillcolor=lightblue]
	140066488939968 -> 140067963829744
	140067963829744 [label=AccumulateGrad]
	140067054790848 -> 140067054785328
	140067054790848 [label="ViewBackward0
-------------------------------
self_sym_sizes: (24, 511, 3072)"]
	140067054780768 -> 140067054790848
	140067054780768 [label="GeluBackward0
---------------------------
approximate:           none
self       : [saved tensor]"]
	140067054784272 -> 140067054780768
	140067054784272 [label="ViewBackward0
-----------------------------
self_sym_sizes: (12264, 3072)"]
	140067054786240 -> 140067054784272
	140067054786240 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :   (12264, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (768, 3072)
mat2_sym_strides:       (1, 768)"]
	140067054782928 -> 140067054786240
	140066488939888 [label="
 (3072)" fillcolor=lightblue]
	140066488939888 -> 140067054782928
	140067054782928 [label=AccumulateGrad]
	140067054796704 -> 140067054786240
	140067054796704 [label="ViewBackward0
------------------------------
self_sym_sizes: (24, 511, 768)"]
	140067007030528 -> 140067054796704
	140067007030528 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140067064999136 -> 140067007030528
	140067064999136 [label="AddBackward0
------------
alpha: 1"]
	140067053497952 -> 140067064999136
	140067053497952 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067053501744 -> 140067053497952
	140067053501744 [label="ViewBackward0
----------------------------
self_sym_sizes: (12264, 768)"]
	140067053498720 -> 140067053501744
	140067053498720 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :   (12264, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067053501792 -> 140067053498720
	140066488939568 [label="
 (768)" fillcolor=lightblue]
	140066488939568 -> 140067053501792
	140067053501792 [label=AccumulateGrad]
	140067053498912 -> 140067053498720
	140067053498912 [label="ViewBackward0
------------------------------
self_sym_sizes: (24, 511, 768)"]
	140067053502176 -> 140067053498912
	140067053502176 [label="ViewBackward0
---------------------------------
self_sym_sizes: (24, 511, 12, 64)"]
	140067053498240 -> 140067053502176
	140067053498240 [label=CloneBackward0]
	140067053488880 -> 140067053498240
	140067053488880 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067053499200 -> 140067053488880
	140067053499200 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (288, 511, 64)"]
	140067053502224 -> 140067053499200
	140067053502224 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140067053497856 -> 140067053502224
	140067053497856 [label="ViewBackward0
----------------------------------
self_sym_sizes: (24, 12, 511, 511)"]
	140067053486336 -> 140067053497856
	140067053486336 [label="ExpandBackward0
----------------------------------
self_sym_sizes: (24, 12, 511, 511)"]
	140067053496896 -> 140067053486336
	140067053496896 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067053494976 -> 140067053496896
	140067053494976 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140067053502368 -> 140067053494976
	140067053502368 [label="AddBackward0
------------
alpha: 1"]
	140067053494880 -> 140067053502368
	140067053494880 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067053494832 -> 140067053494880
	140067053494832 [label="UnsafeViewBackward0
-------------------------------
self_sym_sizes: (288, 511, 511)"]
	140067053495120 -> 140067053494832
	140067053495120 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140067053495408 -> 140067053495120
	140067053495408 [label="UnsafeViewBackward0
---------------------------------
self_sym_sizes: (24, 12, 511, 64)"]
	140067053495648 -> 140067053495408
	140067053495648 [label=CloneBackward0]
	140067053495552 -> 140067053495648
	140067053495552 [label="ExpandBackward0
---------------------------------
self_sym_sizes: (24, 12, 511, 64)"]
	140067053495360 -> 140067053495552
	140067053495360 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067053495840 -> 140067053495360
	140067053495840 [label="ViewBackward0
------------------------------
self_sym_sizes: (24, 511, 768)"]
	140067053495504 -> 140067053495840
	140067053495504 [label="ViewBackward0
----------------------------
self_sym_sizes: (12264, 768)"]
	140067053495888 -> 140067053495504
	140067053495888 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :   (12264, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067053495216 -> 140067053495888
	140066537519728 [label="
 (768)" fillcolor=lightblue]
	140066537519728 -> 140067053495216
	140067053495216 [label=AccumulateGrad]
	140067053496032 -> 140067053495888
	140067053496032 [label="ViewBackward0
------------------------------
self_sym_sizes: (24, 511, 768)"]
	140067053488016 -> 140067053496032
	140067053488016 [label="ViewBackward0
--------------------------------
self_sym_sizes: (3, 8, 511, 768)"]
	140067053495792 -> 140067053488016
	140067053495792 [label="AsStridedBackward0
-----------------------------------------
size          :          (3, 8, 511, 768)
storage_offset:                         0
stride        : (3139584, 392448, 768, 1)"]
	140067053496320 -> 140067053495792
	140067053496320 [label=CopySlices]
	140067054782640 -> 140067053496320
	140067053496416 -> 140067053496320
	140067053496416 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 8, 768)
start         :                   0
step          :                   1"]
	140067053496368 -> 140067053496416
	140067053496368 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:         (3, 9, 768)
start         :                   1
step          :                   1"]
	140067053498144 -> 140067053496368
	140067053498144 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:         (3, 9, 768)
start         :                   0
step          :                   1"]
	140067007030192 -> 140067053498144
	140067053495600 -> 140067053495888
	140067053495600 [label=TBackward0]
	140067053495936 -> 140067053495600
	140066537520288 [label="
 (768, 768)" fillcolor=lightblue]
	140066537520288 -> 140067053495936
	140067053495936 [label=AccumulateGrad]
	140067053494928 -> 140067053495120
	140067053494928 [label="UnsafeViewBackward0
---------------------------------
self_sym_sizes: (24, 12, 64, 511)"]
	140067053494544 -> 140067053494928
	140067053494544 [label=CloneBackward0]
	140067053496512 -> 140067053494544
	140067053496512 [label="ExpandBackward0
---------------------------------
self_sym_sizes: (24, 12, 64, 511)"]
	140067053495264 -> 140067053496512
	140067053495264 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551615
dim1: 18446744073709551614"]
	140067053496752 -> 140067053495264
	140067053496752 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067053499104 -> 140067053496752
	140067053499104 [label="ViewBackward0
------------------------------
self_sym_sizes: (24, 511, 768)"]
	140067053496080 -> 140067053499104
	140067053496080 [label="ViewBackward0
----------------------------
self_sym_sizes: (12264, 768)"]
	140067053495168 -> 140067053496080
	140067053495168 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :   (12264, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067053496560 -> 140067053495168
	140066489721120 [label="
 (768)" fillcolor=lightblue]
	140066489721120 -> 140067053496560
	140067053496560 [label=AccumulateGrad]
	140067053497904 -> 140067053495168
	140067053497904 [label="ViewBackward0
------------------------------
self_sym_sizes: (24, 511, 768)"]
	140067053488016 -> 140067053497904
	140067053494208 -> 140067053495168
	140067053494208 [label=TBackward0]
	140067053501984 -> 140067053494208
	140066537520208 [label="
 (768, 768)" fillcolor=lightblue]
	140066537520208 -> 140067053501984
	140067053501984 [label=AccumulateGrad]
	140067053502320 -> 140067053502224
	140067053502320 [label="UnsafeViewBackward0
---------------------------------
self_sym_sizes: (24, 12, 511, 64)"]
	140067053487296 -> 140067053502320
	140067053487296 [label=CloneBackward0]
	140067053494448 -> 140067053487296
	140067053494448 [label="ExpandBackward0
---------------------------------
self_sym_sizes: (24, 12, 511, 64)"]
	140067053494256 -> 140067053494448
	140067053494256 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067053495984 -> 140067053494256
	140067053495984 [label="ViewBackward0
------------------------------
self_sym_sizes: (24, 511, 768)"]
	140067053496272 -> 140067053495984
	140067053496272 [label="ViewBackward0
----------------------------
self_sym_sizes: (12264, 768)"]
	140067053496944 -> 140067053496272
	140067053496944 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :   (12264, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067053496848 -> 140067053496944
	140066489720160 [label="
 (768)" fillcolor=lightblue]
	140066489720160 -> 140067053496848
	140067053496848 [label=AccumulateGrad]
	140067053496224 -> 140067053496944
	140067053496224 [label="ViewBackward0
------------------------------
self_sym_sizes: (24, 511, 768)"]
	140067053488016 -> 140067053496224
	140067053498816 -> 140067053496944
	140067053498816 [label=TBackward0]
	140067053496992 -> 140067053498816
	140066489716800 [label="
 (768, 768)" fillcolor=lightblue]
	140066489716800 -> 140067053496992
	140067053496992 [label=AccumulateGrad]
	140067053486432 -> 140067053498720
	140067053486432 [label=TBackward0]
	140067053498480 -> 140067053486432
	140066488939488 [label="
 (768, 768)" fillcolor=lightblue]
	140066488939488 -> 140067053498480
	140067053498480 [label=AccumulateGrad]
	140067053488016 -> 140067064999136
	140067065002544 -> 140067007030528
	140066488939728 [label="
 (768)" fillcolor=lightblue]
	140066488939728 -> 140067065002544
	140067065002544 [label=AccumulateGrad]
	140067065003408 -> 140067007030528
	140066488939648 [label="
 (768)" fillcolor=lightblue]
	140066488939648 -> 140067065003408
	140067065003408 [label=AccumulateGrad]
	140067065001632 -> 140067054786240
	140067065001632 [label=TBackward0]
	140067065002736 -> 140067065001632
	140066537520688 [label="
 (3072, 768)" fillcolor=lightblue]
	140066537520688 -> 140067065002736
	140067065002736 [label=AccumulateGrad]
	140067054789600 -> 140067054785328
	140067054789600 [label=TBackward0]
	140067054780720 -> 140067054789600
	140066488939808 [label="
 (768, 3072)" fillcolor=lightblue]
	140066488939808 -> 140067054780720
	140067054780720 [label=AccumulateGrad]
	140067007030528 -> 140067054890832
	140067007024144 -> 140067007030240
	140066488940128 [label="
 (768)" fillcolor=lightblue]
	140066488940128 -> 140067007024144
	140067007024144 [label=AccumulateGrad]
	140067007029424 -> 140067007030240
	140066488940048 [label="
 (768)" fillcolor=lightblue]
	140066488940048 -> 140067007029424
	140067007029424 [label=AccumulateGrad]
	140067007034080 -> 140067007030144
	140067007034080 [label=TBackward0]
	140067007030480 -> 140067007034080
	140066806148112 [label="
 (768, 768)" fillcolor=lightblue]
	140066806148112 -> 140067007030480
	140067007030480 [label=AccumulateGrad]
	140067007032208 -> 140067007033504
	140067007032208 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 64, 9)"]
	140067007033936 -> 140067007032208
	140067007033936 [label=CloneBackward0]
	140067007025248 -> 140067007033936
	140067007025248 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 64, 9)"]
	140067007033360 -> 140067007025248
	140067007033360 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551615
dim1: 18446744073709551614"]
	140067007027456 -> 140067007033360
	140067007027456 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067007025008 -> 140067007027456
	140067007025008 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 9, 768)"]
	140067007025056 -> 140067007025008
	140067007025056 [label="ViewBackward0
-------------------------
self_sym_sizes: (27, 768)"]
	140067007024960 -> 140067007025056
	140067007024960 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (27, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067007029568 -> 140067007024960
	140066806148832 [label="
 (768)" fillcolor=lightblue]
	140066806148832 -> 140067007029568
	140067007029568 [label=AccumulateGrad]
	140067007024672 -> 140067007024960
	140067007024672 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 9, 768)"]
	140067007033792 -> 140067007024672
	140067007033120 -> 140067007024960
	140067007033120 [label=TBackward0]
	140067065000432 -> 140067007033120
	140066806148672 [label="
 (768, 768)" fillcolor=lightblue]
	140066806148672 -> 140067065000432
	140067065000432 [label=AccumulateGrad]
	140067007036192 -> 140067007036240
	140067007036192 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 9, 64)"]
	140067007036864 -> 140067007036192
	140067007036864 [label=CloneBackward0]
	140067007032928 -> 140067007036864
	140067007032928 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 9, 64)"]
	140067007027648 -> 140067007032928
	140067007027648 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067007026640 -> 140067007027648
	140067007026640 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 9, 768)"]
	140067007030768 -> 140067007026640
	140067007030768 [label="ViewBackward0
-------------------------
self_sym_sizes: (27, 768)"]
	140067065003120 -> 140067007030768
	140067065003120 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (27, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067054794352 -> 140067065003120
	140066806148992 [label="
 (768)" fillcolor=lightblue]
	140066806148992 -> 140067054794352
	140067054794352 [label=AccumulateGrad]
	140067054795840 -> 140067065003120
	140067054795840 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 9, 768)"]
	140067007033792 -> 140067054795840
	140067054788736 -> 140067065003120
	140067054788736 [label=TBackward0]
	140067007030432 -> 140067054788736
	140066806148912 [label="
 (768, 768)" fillcolor=lightblue]
	140066806148912 -> 140067007030432
	140067007030432 [label=AccumulateGrad]
	140067007033888 -> 140067007032592
	140067007033888 [label=TBackward0]
	140067054793824 -> 140067007033888
	140066806149072 [label="
 (768, 768)" fillcolor=lightblue]
	140066806149072 -> 140067054793824
	140067054793824 [label=AccumulateGrad]
	140067007033792 -> 140067007035088
	140067007034992 -> 140067007031920
	140066806149312 [label="
 (768)" fillcolor=lightblue]
	140066806149312 -> 140067007034992
	140067007034992 [label=AccumulateGrad]
	140067007034800 -> 140067007031920
	140066806149232 [label="
 (768)" fillcolor=lightblue]
	140066806149232 -> 140067007034800
	140067007034800 [label=AccumulateGrad]
	140067007027552 -> 140067007036000
	140067007027552 [label=TBackward0]
	140067007032784 -> 140067007027552
	140066806149472 [label="
 (3072, 768)" fillcolor=lightblue]
	140066806149472 -> 140067007032784
	140067007032784 [label=AccumulateGrad]
	140067007028080 -> 140067007026016
	140067007028080 [label=TBackward0]
	140067007034176 -> 140067007028080
	140066806148752 [label="
 (768, 3072)" fillcolor=lightblue]
	140066806148752 -> 140067007034176
	140067007034176 [label=AccumulateGrad]
	140067007031920 -> 140067007026784
	140067007026688 -> 140067007026304
	140066806149792 [label="
 (768)" fillcolor=lightblue]
	140066806149792 -> 140067007026688
	140067007026688 [label=AccumulateGrad]
	140067007025920 -> 140067007026304
	140066806149712 [label="
 (768)" fillcolor=lightblue]
	140066806149712 -> 140067007025920
	140067007025920 [label=AccumulateGrad]
	140067007027840 -> 140067065261424
	140067007027840 [label=CloneBackward0]
	140067007028128 -> 140067007027840
	140067007028128 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 8, 768)
start         :                   0
step          :                   1"]
	140067007036480 -> 140067007028128
	140067007036480 [label="SelectBackward0
--------------------------------
dim           :                2
index         :                1
self_sym_sizes: (3, 8, 511, 768)"]
	140067007032832 -> 140067007036480
	140067007032832 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:    (3, 8, 511, 768)
start         :                   0
step          :                   1"]
	140067007029328 -> 140067007032832
	140067007029328 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:    (3, 8, 511, 768)
start         :                   0
step          :                   1"]
	140067007034224 -> 140067007029328
	140067007034224 [label="ViewBackward0
------------------------------
self_sym_sizes: (24, 511, 768)"]
	140067007028416 -> 140067007034224
	140067007028416 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140067007035616 -> 140067007028416
	140067007035616 [label="AddBackward0
------------
alpha: 1"]
	140067007035136 -> 140067007035616
	140067007035136 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067007032400 -> 140067007035136
	140067007032400 [label="ViewBackward0
----------------------------
self_sym_sizes: (12264, 768)"]
	140067007033216 -> 140067007032400
	140067007033216 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :  (12264, 3072)
mat1_sym_strides:      (3072, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (3072, 768)
mat2_sym_strides:      (1, 3072)"]
	140067007030864 -> 140067007033216
	140066488941248 [label="
 (768)" fillcolor=lightblue]
	140066488941248 -> 140067007030864
	140067007030864 [label=AccumulateGrad]
	140067007033264 -> 140067007033216
	140067007033264 [label="ViewBackward0
-------------------------------
self_sym_sizes: (24, 511, 3072)"]
	140067007024528 -> 140067007033264
	140067007024528 [label="GeluBackward0
---------------------------
approximate:           none
self       : [saved tensor]"]
	140067007037344 -> 140067007024528
	140067007037344 [label="ViewBackward0
-----------------------------
self_sym_sizes: (12264, 3072)"]
	140067053499008 -> 140067007037344
	140067053499008 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :   (12264, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (768, 3072)
mat2_sym_strides:       (1, 768)"]
	140067053497760 -> 140067053499008
	140066488941168 [label="
 (3072)" fillcolor=lightblue]
	140066488941168 -> 140067053497760
	140067053497760 [label=AccumulateGrad]
	140067053499056 -> 140067053499008
	140067053499056 [label="ViewBackward0
------------------------------
self_sym_sizes: (24, 511, 768)"]
	140067007033168 -> 140067053499056
	140067007033168 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140067053494688 -> 140067007033168
	140067053494688 [label="AddBackward0
------------
alpha: 1"]
	140067053495456 -> 140067053494688
	140067053495456 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067053495312 -> 140067053495456
	140067053495312 [label="ViewBackward0
----------------------------
self_sym_sizes: (12264, 768)"]
	140067053497088 -> 140067053495312
	140067053497088 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :   (12264, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067053496128 -> 140067053497088
	140066488940768 [label="
 (768)" fillcolor=lightblue]
	140066488940768 -> 140067053496128
	140067053496128 [label=AccumulateGrad]
	140067053497040 -> 140067053497088
	140067053497040 [label="ViewBackward0
------------------------------
self_sym_sizes: (24, 511, 768)"]
	140067053488832 -> 140067053497040
	140067053488832 [label="ViewBackward0
---------------------------------
self_sym_sizes: (24, 511, 12, 64)"]
	140067053498048 -> 140067053488832
	140067053498048 [label=CloneBackward0]
	140067053497808 -> 140067053498048
	140067053497808 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067053488208 -> 140067053497808
	140067053488208 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (288, 511, 64)"]
	140067053488400 -> 140067053488208
	140067053488400 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140067053489984 -> 140067053488400
	140067053489984 [label="ViewBackward0
----------------------------------
self_sym_sizes: (24, 12, 511, 511)"]
	140067053488784 -> 140067053489984
	140067053488784 [label="ExpandBackward0
----------------------------------
self_sym_sizes: (24, 12, 511, 511)"]
	140067053489504 -> 140067053488784
	140067053489504 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067053489264 -> 140067053489504
	140067053489264 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140067053488928 -> 140067053489264
	140067053488928 [label="AddBackward0
------------
alpha: 1"]
	140067053489456 -> 140067053488928
	140067053489456 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067053489648 -> 140067053489456
	140067053489648 [label="UnsafeViewBackward0
-------------------------------
self_sym_sizes: (288, 511, 511)"]
	140067053491040 -> 140067053489648
	140067053491040 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140067053491088 -> 140067053491040
	140067053491088 [label="UnsafeViewBackward0
---------------------------------
self_sym_sizes: (24, 12, 511, 64)"]
	140067053490848 -> 140067053491088
	140067053490848 [label=CloneBackward0]
	140067053490992 -> 140067053490848
	140067053490992 [label="ExpandBackward0
---------------------------------
self_sym_sizes: (24, 12, 511, 64)"]
	140067053491328 -> 140067053490992
	140067053491328 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067053490944 -> 140067053491328
	140067053490944 [label="ViewBackward0
------------------------------
self_sym_sizes: (24, 511, 768)"]
	140067053491568 -> 140067053490944
	140067053491568 [label="ViewBackward0
----------------------------
self_sym_sizes: (12264, 768)"]
	140067053491472 -> 140067053491568
	140067053491472 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :   (12264, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067053491856 -> 140067053491472
	140066488940288 [label="
 (768)" fillcolor=lightblue]
	140066488940288 -> 140067053491856
	140067053491856 [label=AccumulateGrad]
	140067053491808 -> 140067053491472
	140067053491808 [label="ViewBackward0
------------------------------
self_sym_sizes: (24, 511, 768)"]
	140067053497664 -> 140067053491808
	140067053497664 [label="ViewBackward0
--------------------------------
self_sym_sizes: (3, 8, 511, 768)"]
	140067053491184 -> 140067053497664
	140067053491184 [label="AsStridedBackward0
-----------------------------------------
size          :          (3, 8, 511, 768)
storage_offset:                         0
stride        : (3139584, 392448, 768, 1)"]
	140067053491760 -> 140067053491184
	140067053491760 [label=CopySlices]
	140067007030240 -> 140067053491760
	140067053492528 -> 140067053491760
	140067053492528 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 8, 768)
start         :                   0
step          :                   1"]
	140067053492384 -> 140067053492528
	140067053492384 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:         (3, 9, 768)
start         :                   1
step          :                   1"]
	140067053492576 -> 140067053492384
	140067053492576 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:         (3, 9, 768)
start         :                   0
step          :                   1"]
	140067007026304 -> 140067053492576
	140067053490128 -> 140067053491472
	140067053490128 [label=TBackward0]
	140067053491952 -> 140067053490128
	140066488940208 [label="
 (768, 768)" fillcolor=lightblue]
	140066488940208 -> 140067053491952
	140067053491952 [label=AccumulateGrad]
	140067053490896 -> 140067053491040
	140067053490896 [label="UnsafeViewBackward0
---------------------------------
self_sym_sizes: (24, 12, 64, 511)"]
	140067053489888 -> 140067053490896
	140067053489888 [label=CloneBackward0]
	140067053491616 -> 140067053489888
	140067053491616 [label="ExpandBackward0
---------------------------------
self_sym_sizes: (24, 12, 64, 511)"]
	140067053491520 -> 140067053491616
	140067053491520 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551615
dim1: 18446744073709551614"]
	140067053492096 -> 140067053491520
	140067053492096 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067053492960 -> 140067053492096
	140067053492960 [label="ViewBackward0
------------------------------
self_sym_sizes: (24, 511, 768)"]
	140067053491232 -> 140067053492960
	140067053491232 [label="ViewBackward0
----------------------------
self_sym_sizes: (12264, 768)"]
	140067053491136 -> 140067053491232
	140067053491136 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :   (12264, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067053492816 -> 140067053491136
	140066488940448 [label="
 (768)" fillcolor=lightblue]
	140066488940448 -> 140067053492816
	140067053492816 [label=AccumulateGrad]
	140067053492192 -> 140067053491136
	140067053492192 [label="ViewBackward0
------------------------------
self_sym_sizes: (24, 511, 768)"]
	140067053497664 -> 140067053492192
	140067053497424 -> 140067053491136
	140067053497424 [label=TBackward0]
	140067053492144 -> 140067053497424
	140066488940368 [label="
 (768, 768)" fillcolor=lightblue]
	140066488940368 -> 140067053492144
	140067053492144 [label=AccumulateGrad]
	140067053488448 -> 140067053488400
	140067053488448 [label="UnsafeViewBackward0
---------------------------------
self_sym_sizes: (24, 12, 511, 64)"]
	140067053489936 -> 140067053488448
	140067053489936 [label=CloneBackward0]
	140067053489360 -> 140067053489936
	140067053489360 [label="ExpandBackward0
---------------------------------
self_sym_sizes: (24, 12, 511, 64)"]
	140067053489312 -> 140067053489360
	140067053489312 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067053491376 -> 140067053489312
	140067053491376 [label="ViewBackward0
------------------------------
self_sym_sizes: (24, 511, 768)"]
	140067053492336 -> 140067053491376
	140067053492336 [label="ViewBackward0
----------------------------
self_sym_sizes: (12264, 768)"]
	140067053497472 -> 140067053492336
	140067053497472 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :   (12264, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067053492864 -> 140067053497472
	140066488940608 [label="
 (768)" fillcolor=lightblue]
	140066488940608 -> 140067053492864
	140067053492864 [label=AccumulateGrad]
	140067053491904 -> 140067053497472
	140067053491904 [label="ViewBackward0
------------------------------
self_sym_sizes: (24, 511, 768)"]
	140067053497664 -> 140067053491904
	140067053502416 -> 140067053497472
	140067053502416 [label=TBackward0]
	140067053492624 -> 140067053502416
	140066488940528 [label="
 (768, 768)" fillcolor=lightblue]
	140066488940528 -> 140067053492624
	140067053492624 [label=AccumulateGrad]
	140067053496608 -> 140067053497088
	140067053496608 [label=TBackward0]
	140067053498576 -> 140067053496608
	140066488940688 [label="
 (768, 768)" fillcolor=lightblue]
	140066488940688 -> 140067053498576
	140067053498576 [label=AccumulateGrad]
	140067053497664 -> 140067053494688
	140067053499536 -> 140067007033168
	140066488940928 [label="
 (768)" fillcolor=lightblue]
	140066488940928 -> 140067053499536
	140067053499536 [label=AccumulateGrad]
	140067053495024 -> 140067007033168
	140066488940848 [label="
 (768)" fillcolor=lightblue]
	140066488940848 -> 140067053495024
	140067053495024 [label=AccumulateGrad]
	140067053502080 -> 140067053499008
	140067053502080 [label=TBackward0]
	140067053496464 -> 140067053502080
	140066488941088 [label="
 (3072, 768)" fillcolor=lightblue]
	140066488941088 -> 140067053496464
	140067053496464 [label=AccumulateGrad]
	140067007032640 -> 140067007033216
	140067007032640 [label=TBackward0]
	140067007031296 -> 140067007032640
	140066488941008 [label="
 (768, 3072)" fillcolor=lightblue]
	140066488941008 -> 140067007031296
	140067007031296 [label=AccumulateGrad]
	140067007033168 -> 140067007035616
	140067007030096 -> 140067007028416
	140066488941408 [label="
 (768)" fillcolor=lightblue]
	140066488941408 -> 140067007030096
	140067007030096 [label=AccumulateGrad]
	140067007025776 -> 140067007028416
	140066488941328 [label="
 (768)" fillcolor=lightblue]
	140066488941328 -> 140067007025776
	140067007025776 [label=AccumulateGrad]
	140067007025632 -> 140067007032448
	140067007025632 [label=TBackward0]
	140067007036384 -> 140067007025632
	140066806149392 [label="
 (768, 768)" fillcolor=lightblue]
	140066806149392 -> 140067007036384
	140067007036384 [label=AccumulateGrad]
	140067065256240 -> 140067065258064
	140067065256240 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 64, 9)"]
	140067065257776 -> 140067065256240
	140067065257776 [label=CloneBackward0]
	140067065258160 -> 140067065257776
	140067065258160 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 64, 9)"]
	140067007026496 -> 140067065258160
	140067007026496 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551615
dim1: 18446744073709551614"]
	140067007027888 -> 140067007026496
	140067007027888 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067007035856 -> 140067007027888
	140067007035856 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 9, 768)"]
	140067007035040 -> 140067007035856
	140067007035040 [label="ViewBackward0
-------------------------
self_sym_sizes: (27, 768)"]
	140067007032160 -> 140067007035040
	140067007032160 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (27, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067007033600 -> 140067007032160
	140066806150112 [label="
 (768)" fillcolor=lightblue]
	140066806150112 -> 140067007033600
	140067007033600 [label=AccumulateGrad]
	140067007036768 -> 140067007032160
	140067007036768 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 9, 768)"]
	140067065261424 -> 140067007036768
	140067007037104 -> 140067007032160
	140067007037104 [label=TBackward0]
	140067007031536 -> 140067007037104
	140066806149952 [label="
 (768, 768)" fillcolor=lightblue]
	140066806149952 -> 140067007031536
	140067007031536 [label=AccumulateGrad]
	140067065258784 -> 140067065254896
	140067065258784 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 9, 64)"]
	140067065254848 -> 140067065258784
	140067065254848 [label=CloneBackward0]
	140067065259744 -> 140067065254848
	140067065259744 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 9, 64)"]
	140067065258832 -> 140067065259744
	140067065258832 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067065253936 -> 140067065258832
	140067065253936 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 9, 768)"]
	140067065255616 -> 140067065253936
	140067065255616 [label="ViewBackward0
-------------------------
self_sym_sizes: (27, 768)"]
	140067007034416 -> 140067065255616
	140067007034416 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (27, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067007034848 -> 140067007034416
	140066806150272 [label="
 (768)" fillcolor=lightblue]
	140066806150272 -> 140067007034848
	140067007034848 [label=AccumulateGrad]
	140067007035520 -> 140067007034416
	140067007035520 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 9, 768)"]
	140067065261424 -> 140067007035520
	140067007035808 -> 140067007034416
	140067007035808 [label=TBackward0]
	140067007034704 -> 140067007035808
	140066806150192 [label="
 (768, 768)" fillcolor=lightblue]
	140066806150192 -> 140067007034704
	140067007034704 [label=AccumulateGrad]
	140067065265696 -> 140067065265984
	140067065265696 [label=TBackward0]
	140067065254080 -> 140067065265696
	140066806150352 [label="
 (768, 768)" fillcolor=lightblue]
	140066806150352 -> 140067065254080
	140067065254080 [label=AccumulateGrad]
	140067065261424 -> 140067065260560
	140067065254128 -> 140067065252064
	140066806150592 [label="
 (768)" fillcolor=lightblue]
	140066806150592 -> 140067065254128
	140067065254128 [label=AccumulateGrad]
	140067065258640 -> 140067065252064
	140066806150512 [label="
 (768)" fillcolor=lightblue]
	140066806150512 -> 140067065258640
	140067065258640 [label=AccumulateGrad]
	140067065259792 -> 140067065261328
	140067065259792 [label=TBackward0]
	140067065262048 -> 140067065259792
	140066806150752 [label="
 (3072, 768)" fillcolor=lightblue]
	140066806150752 -> 140067065262048
	140067065262048 [label=AccumulateGrad]
	140067065260032 -> 140067065260656
	140067065260032 [label=TBackward0]
	140067065260272 -> 140067065260032
	140066806150032 [label="
 (768, 3072)" fillcolor=lightblue]
	140066806150032 -> 140067065260272
	140067065260272 [label=AccumulateGrad]
	140067065252064 -> 140067065261136
	140067065260368 -> 140067065257056
	140066806151072 [label="
 (768)" fillcolor=lightblue]
	140066806151072 -> 140067065260368
	140067065260368 [label=AccumulateGrad]
	140067065251680 -> 140067065257056
	140066806150992 [label="
 (768)" fillcolor=lightblue]
	140066806150992 -> 140067065251680
	140067065251680 [label=AccumulateGrad]
	140067053365776 -> 140067065215440
	140067053365776 [label=CloneBackward0]
	140067065260320 -> 140067053365776
	140067065260320 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 8, 768)
start         :                   0
step          :                   1"]
	140067065261568 -> 140067065260320
	140067065261568 [label="SelectBackward0
--------------------------------
dim           :                2
index         :                1
self_sym_sizes: (3, 8, 511, 768)"]
	140067065258736 -> 140067065261568
	140067065258736 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:    (3, 8, 511, 768)
start         :                   0
step          :                   1"]
	140067065260992 -> 140067065258736
	140067065260992 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:    (3, 8, 511, 768)
start         :                   0
step          :                   1"]
	140067065251776 -> 140067065260992
	140067065251776 [label="ViewBackward0
------------------------------
self_sym_sizes: (24, 511, 768)"]
	140067065258976 -> 140067065251776
	140067065258976 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140067065261184 -> 140067065258976
	140067065261184 [label="AddBackward0
------------
alpha: 1"]
	140067065252016 -> 140067065261184
	140067065252016 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067065264736 -> 140067065252016
	140067065264736 [label="ViewBackward0
----------------------------
self_sym_sizes: (12264, 768)"]
	140067065265408 -> 140067065264736
	140067065265408 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :  (12264, 3072)
mat1_sym_strides:      (3072, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (3072, 768)
mat2_sym_strides:      (1, 3072)"]
	140067065260752 -> 140067065265408
	140066488942528 [label="
 (768)" fillcolor=lightblue]
	140066488942528 -> 140067065260752
	140067065260752 [label=AccumulateGrad]
	140067065260416 -> 140067065265408
	140067065260416 [label="ViewBackward0
-------------------------------
self_sym_sizes: (24, 511, 3072)"]
	140067007023616 -> 140067065260416
	140067007023616 [label="GeluBackward0
---------------------------
approximate:           none
self       : [saved tensor]"]
	140067053496800 -> 140067007023616
	140067053496800 [label="ViewBackward0
-----------------------------
self_sym_sizes: (12264, 3072)"]
	140067053496176 -> 140067053496800
	140067053496176 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :   (12264, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (768, 3072)
mat2_sym_strides:       (1, 768)"]
	140067053499248 -> 140067053496176
	140066488942448 [label="
 (3072)" fillcolor=lightblue]
	140066488942448 -> 140067053499248
	140067053499248 [label=AccumulateGrad]
	140067053487968 -> 140067053496176
	140067053487968 [label="ViewBackward0
------------------------------
self_sym_sizes: (24, 511, 768)"]
	140067065256144 -> 140067053487968
	140067065256144 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140067053489408 -> 140067065256144
	140067053489408 [label="AddBackward0
------------
alpha: 1"]
	140067053491424 -> 140067053489408
	140067053491424 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067053491280 -> 140067053491424
	140067053491280 [label="ViewBackward0
----------------------------
self_sym_sizes: (12264, 768)"]
	140067053493008 -> 140067053491280
	140067053493008 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :   (12264, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067053497520 -> 140067053493008
	140066488942048 [label="
 (768)" fillcolor=lightblue]
	140066488942048 -> 140067053497520
	140067053497520 [label=AccumulateGrad]
	140067053492432 -> 140067053493008
	140067053492432 [label="ViewBackward0
------------------------------
self_sym_sizes: (24, 511, 768)"]
	140067053492912 -> 140067053492432
	140067053492912 [label="ViewBackward0
---------------------------------
self_sym_sizes: (24, 511, 12, 64)"]
	140067053493344 -> 140067053492912
	140067053493344 [label=CloneBackward0]
	140067053488496 -> 140067053493344
	140067053488496 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067053493200 -> 140067053488496
	140067053493200 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (288, 511, 64)"]
	140067053493488 -> 140067053493200
	140067053493488 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140067053493920 -> 140067053493488
	140067053493920 [label="ViewBackward0
----------------------------------
self_sym_sizes: (24, 12, 511, 511)"]
	140067053493872 -> 140067053493920
	140067053493872 [label="ExpandBackward0
----------------------------------
self_sym_sizes: (24, 12, 511, 511)"]
	140067053493104 -> 140067053493872
	140067053493104 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067053494064 -> 140067053493104
	140067053494064 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140067053497568 -> 140067053494064
	140067053497568 [label="AddBackward0
------------
alpha: 1"]
	140067053494352 -> 140067053497568
	140067053494352 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067053487680 -> 140067053494352
	140067053487680 [label="UnsafeViewBackward0
-------------------------------
self_sym_sizes: (288, 511, 511)"]
	140067053488592 -> 140067053487680
	140067053488592 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140067053489792 -> 140067053488592
	140067053489792 [label="UnsafeViewBackward0
---------------------------------
self_sym_sizes: (24, 12, 511, 64)"]
	140067053487872 -> 140067053489792
	140067053487872 [label=CloneBackward0]
	140067053488352 -> 140067053487872
	140067053488352 [label="ExpandBackward0
---------------------------------
self_sym_sizes: (24, 12, 511, 64)"]
	140067053486912 -> 140067053488352
	140067053486912 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067053452976 -> 140067053486912
	140067053452976 [label="ViewBackward0
------------------------------
self_sym_sizes: (24, 511, 768)"]
	140067053451728 -> 140067053452976
	140067053451728 [label="ViewBackward0
----------------------------
self_sym_sizes: (12264, 768)"]
	140067053451200 -> 140067053451728
	140067053451200 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :   (12264, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067053442656 -> 140067053451200
	140066488941568 [label="
 (768)" fillcolor=lightblue]
	140066488941568 -> 140067053442656
	140067053442656 [label=AccumulateGrad]
	140067053450144 -> 140067053451200
	140067053450144 [label="ViewBackward0
------------------------------
self_sym_sizes: (24, 511, 768)"]
	140067053492048 -> 140067053450144
	140067053492048 [label="ViewBackward0
--------------------------------
self_sym_sizes: (3, 8, 511, 768)"]
	140067053448512 -> 140067053492048
	140067053448512 [label="AsStridedBackward0
-----------------------------------------
size          :          (3, 8, 511, 768)
storage_offset:                         0
stride        : (3139584, 392448, 768, 1)"]
	140067053450096 -> 140067053448512
	140067053450096 [label=CopySlices]
	140067007028416 -> 140067053450096
	140067053452256 -> 140067053450096
	140067053452256 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 8, 768)
start         :                   0
step          :                   1"]
	140067053445488 -> 140067053452256
	140067053445488 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:         (3, 9, 768)
start         :                   1
step          :                   1"]
	140067053448080 -> 140067053445488
	140067053448080 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:         (3, 9, 768)
start         :                   0
step          :                   1"]
	140067065257056 -> 140067053448080
	140067053447552 -> 140067053451200
	140067053447552 [label=TBackward0]
	140067053452112 -> 140067053447552
	140066488941488 [label="
 (768, 768)" fillcolor=lightblue]
	140066488941488 -> 140067053452112
	140067053452112 [label=AccumulateGrad]
	140067053486720 -> 140067053488592
	140067053486720 [label="UnsafeViewBackward0
---------------------------------
self_sym_sizes: (24, 12, 64, 511)"]
	140067053494640 -> 140067053486720
	140067053494640 [label=CloneBackward0]
	140067053486528 -> 140067053494640
	140067053486528 [label="ExpandBackward0
---------------------------------
self_sym_sizes: (24, 12, 64, 511)"]
	140067053452016 -> 140067053486528
	140067053452016 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551615
dim1: 18446744073709551614"]
	140067053448032 -> 140067053452016
	140067053448032 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067053452160 -> 140067053448032
	140067053452160 [label="ViewBackward0
------------------------------
self_sym_sizes: (24, 511, 768)"]
	140067053447792 -> 140067053452160
	140067053447792 [label="ViewBackward0
----------------------------
self_sym_sizes: (12264, 768)"]
	140067053450624 -> 140067053447792
	140067053450624 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :   (12264, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067053450672 -> 140067053450624
	140066488941728 [label="
 (768)" fillcolor=lightblue]
	140066488941728 -> 140067053450672
	140067053450672 [label=AccumulateGrad]
	140067053437856 -> 140067053450624
	140067053437856 [label="ViewBackward0
------------------------------
self_sym_sizes: (24, 511, 768)"]
	140067053492048 -> 140067053437856
	140067053451296 -> 140067053450624
	140067053451296 [label=TBackward0]
	140067053445584 -> 140067053451296
	140066488941648 [label="
 (768, 768)" fillcolor=lightblue]
	140066488941648 -> 140067053445584
	140067053445584 [label=AccumulateGrad]
	140067053493632 -> 140067053493488
	140067053493632 [label="UnsafeViewBackward0
---------------------------------
self_sym_sizes: (24, 12, 511, 64)"]
	140067053493968 -> 140067053493632
	140067053493968 [label=CloneBackward0]
	140067053494112 -> 140067053493968
	140067053494112 [label="ExpandBackward0
---------------------------------
self_sym_sizes: (24, 12, 511, 64)"]
	140067053494496 -> 140067053494112
	140067053494496 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067053487152 -> 140067053494496
	140067053487152 [label="ViewBackward0
------------------------------
self_sym_sizes: (24, 511, 768)"]
	140067053494016 -> 140067053487152
	140067053494016 [label="ViewBackward0
----------------------------
self_sym_sizes: (12264, 768)"]
	140067053449760 -> 140067053494016
	140067053449760 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :   (12264, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067053452784 -> 140067053449760
	140066488941888 [label="
 (768)" fillcolor=lightblue]
	140066488941888 -> 140067053452784
	140067053452784 [label=AccumulateGrad]
	140067053451824 -> 140067053449760
	140067053451824 [label="ViewBackward0
------------------------------
self_sym_sizes: (24, 511, 768)"]
	140067053492048 -> 140067053451824
	140067053449712 -> 140067053449760
	140067053449712 [label=TBackward0]
	140067053449040 -> 140067053449712
	140066488941808 [label="
 (768, 768)" fillcolor=lightblue]
	140066488941808 -> 140067053449040
	140067053449040 [label=AccumulateGrad]
	140067053492288 -> 140067053493008
	140067053492288 [label=TBackward0]
	140067053493440 -> 140067053492288
	140066488941968 [label="
 (768, 768)" fillcolor=lightblue]
	140066488941968 -> 140067053493440
	140067053493440 [label=AccumulateGrad]
	140067053492048 -> 140067053489408
	140067053489120 -> 140067065256144
	140066488942208 [label="
 (768)" fillcolor=lightblue]
	140066488942208 -> 140067053489120
	140067053489120 [label=AccumulateGrad]
	140067053488160 -> 140067065256144
	140066488942128 [label="
 (768)" fillcolor=lightblue]
	140066488942128 -> 140067053488160
	140067053488160 [label=AccumulateGrad]
	140067053498624 -> 140067053496176
	140067053498624 [label=TBackward0]
	140067053492480 -> 140067053498624
	140066488942368 [label="
 (3072, 768)" fillcolor=lightblue]
	140066488942368 -> 140067053492480
	140067053492480 [label=AccumulateGrad]
	140067065258400 -> 140067065265408
	140067065258400 [label=TBackward0]
	140067053494592 -> 140067065258400
	140066488942288 [label="
 (768, 3072)" fillcolor=lightblue]
	140066488942288 -> 140067053494592
	140067053494592 [label=AccumulateGrad]
	140067065256144 -> 140067065261184
	140067065260512 -> 140067065258976
	140066806141232 [label="
 (768)" fillcolor=lightblue]
	140066806141232 -> 140067065260512
	140067065260512 [label=AccumulateGrad]
	140067065262432 -> 140067065258976
	140066537520528 [label="
 (768)" fillcolor=lightblue]
	140066537520528 -> 140067065262432
	140067065262432 [label=AccumulateGrad]
	140067053369424 -> 140067053370816
	140067053369424 [label=TBackward0]
	140067053357136 -> 140067053369424
	140066806150672 [label="
 (768, 768)" fillcolor=lightblue]
	140066806150672 -> 140067053357136
	140067053357136 [label=AccumulateGrad]
	140067066362944 -> 140067065202480
	140067066362944 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 64, 9)"]
	140067065209008 -> 140067066362944
	140067065209008 [label=CloneBackward0]
	140067053366304 -> 140067065209008
	140067053366304 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 64, 9)"]
	140067053366208 -> 140067053366304
	140067053366208 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551615
dim1: 18446744073709551614"]
	140067053359008 -> 140067053366208
	140067053359008 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067065260800 -> 140067053359008
	140067065260800 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 9, 768)"]
	140067065261088 -> 140067065260800
	140067065261088 [label="ViewBackward0
-------------------------
self_sym_sizes: (27, 768)"]
	140067065254560 -> 140067065261088
	140067065254560 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (27, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067007025824 -> 140067065254560
	140066806151392 [label="
 (768)" fillcolor=lightblue]
	140066806151392 -> 140067007025824
	140067007025824 [label=AccumulateGrad]
	140067065250384 -> 140067065254560
	140067065250384 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 9, 768)"]
	140067065215440 -> 140067065250384
	140067065256528 -> 140067065254560
	140067065256528 [label=TBackward0]
	140067065261280 -> 140067065256528
	140066806151232 [label="
 (768, 768)" fillcolor=lightblue]
	140066806151232 -> 140067065261280
	140067065261280 [label=AccumulateGrad]
	140067065215584 -> 140067065216688
	140067065215584 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 9, 64)"]
	140067066358192 -> 140067065215584
	140067066358192 [label=CloneBackward0]
	140067065207472 -> 140067066358192
	140067065207472 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 9, 64)"]
	140067065215872 -> 140067065207472
	140067065215872 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067065216160 -> 140067065215872
	140067065216160 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 9, 768)"]
	140067053358192 -> 140067065216160
	140067053358192 [label="ViewBackward0
-------------------------
self_sym_sizes: (27, 768)"]
	140067065202384 -> 140067053358192
	140067065202384 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (27, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067065260608 -> 140067065202384
	140066806151552 [label="
 (768)" fillcolor=lightblue]
	140066806151552 -> 140067065260608
	140067065260608 [label=AccumulateGrad]
	140067065260080 -> 140067065202384
	140067065260080 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 9, 768)"]
	140067065215440 -> 140067065260080
	140067065264352 -> 140067065202384
	140067065264352 [label=TBackward0]
	140067053488640 -> 140067065264352
	140066806151472 [label="
 (768, 768)" fillcolor=lightblue]
	140066806151472 -> 140067053488640
	140067053488640 [label=AccumulateGrad]
	140067065201088 -> 140067065201520
	140067065201088 [label=TBackward0]
	140067065216928 -> 140067065201088
	140066806151632 [label="
 (768, 768)" fillcolor=lightblue]
	140066806151632 -> 140067065216928
	140067065216928 [label=AccumulateGrad]
	140067065215440 -> 140067065213616
	140067065213664 -> 140067065202432
	140066806151872 [label="
 (768)" fillcolor=lightblue]
	140066806151872 -> 140067065213664
	140067065213664 [label=AccumulateGrad]
	140067065204112 -> 140067065202432
	140066806151792 [label="
 (768)" fillcolor=lightblue]
	140066806151792 -> 140067065204112
	140067065204112 [label=AccumulateGrad]
	140067065202864 -> 140067065210304
	140067065202864 [label=TBackward0]
	140067065200944 -> 140067065202864
	140066806152032 [label="
 (3072, 768)" fillcolor=lightblue]
	140066806152032 -> 140067065200944
	140067065200944 [label=AccumulateGrad]
	140067065208816 -> 140067065202912
	140067065208816 [label=TBackward0]
	140067065203296 -> 140067065208816
	140066806151312 [label="
 (768, 3072)" fillcolor=lightblue]
	140066806151312 -> 140067065203296
	140067065203296 [label=AccumulateGrad]
	140067065202432 -> 140067065216352
	140067065208000 -> 140067065209344
	140066806152352 [label="
 (768)" fillcolor=lightblue]
	140066806152352 -> 140067065208000
	140067065208000 [label=AccumulateGrad]
	140067065216304 -> 140067065209344
	140066806152272 [label="
 (768)" fillcolor=lightblue]
	140066806152272 -> 140067065216304
	140067065216304 [label=AccumulateGrad]
	140067065209392 -> 140067065376400
	140067065209392 [label=CloneBackward0]
	140067065210160 -> 140067065209392
	140067065210160 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 8, 768)
start         :                   0
step          :                   1"]
	140067065201904 -> 140067065210160
	140067065201904 [label="SelectBackward0
--------------------------------
dim           :                2
index         :                1
self_sym_sizes: (3, 8, 511, 768)"]
	140067065211840 -> 140067065201904
	140067065211840 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:    (3, 8, 511, 768)
start         :                   0
step          :                   1"]
	140067065216544 -> 140067065211840
	140067065216544 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:    (3, 8, 511, 768)
start         :                   0
step          :                   1"]
	140067065204592 -> 140067065216544
	140067065204592 [label="ViewBackward0
------------------------------
self_sym_sizes: (24, 511, 768)"]
	140067065202144 -> 140067065204592
	140067065202144 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140067065211456 -> 140067065202144
	140067065211456 [label="AddBackward0
------------
alpha: 1"]
	140067065216832 -> 140067065211456
	140067065216832 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067053370384 -> 140067065216832
	140067053370384 [label="ViewBackward0
----------------------------
self_sym_sizes: (12264, 768)"]
	140067065215968 -> 140067053370384
	140067065215968 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :  (12264, 3072)
mat1_sym_strides:      (3072, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (3072, 768)
mat2_sym_strides:      (1, 3072)"]
	140067065203920 -> 140067065215968
	140066806141312 [label="
 (768)" fillcolor=lightblue]
	140066806141312 -> 140067065203920
	140067065203920 [label=AccumulateGrad]
	140067065216880 -> 140067065215968
	140067065216880 [label="ViewBackward0
-------------------------------
self_sym_sizes: (24, 511, 3072)"]
	140067065261712 -> 140067065216880
	140067065261712 [label="GeluBackward0
---------------------------
approximate:           none
self       : [saved tensor]"]
	140067053489840 -> 140067065261712
	140067053489840 [label="ViewBackward0
-----------------------------
self_sym_sizes: (12264, 3072)"]
	140067053492240 -> 140067053489840
	140067053492240 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :   (12264, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (768, 3072)
mat2_sym_strides:       (1, 768)"]
	140067053489072 -> 140067053492240
	140066806140112 [label="
 (3072)" fillcolor=lightblue]
	140066806140112 -> 140067053489072
	140067053489072 [label=AccumulateGrad]
	140067053493296 -> 140067053492240
	140067053493296 [label="ViewBackward0
------------------------------
self_sym_sizes: (24, 511, 768)"]
	140067065215392 -> 140067053493296
	140067065215392 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140067053494400 -> 140067065215392
	140067053494400 [label="AddBackward0
------------
alpha: 1"]
	140067053495072 -> 140067053494400
	140067053495072 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067053444336 -> 140067053495072
	140067053444336 [label="ViewBackward0
----------------------------
self_sym_sizes: (12264, 768)"]
	140067053450768 -> 140067053444336
	140067053450768 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :   (12264, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067053438144 -> 140067053450768
	140066806140512 [label="
 (768)" fillcolor=lightblue]
	140066806140512 -> 140067053438144
	140067053438144 [label=AccumulateGrad]
	140067053452640 -> 140067053450768
	140067053452640 [label="ViewBackward0
------------------------------
self_sym_sizes: (24, 511, 768)"]
	140067053449184 -> 140067053452640
	140067053449184 [label="ViewBackward0
---------------------------------
self_sym_sizes: (24, 511, 12, 64)"]
	140067053451056 -> 140067053449184
	140067053451056 [label=CloneBackward0]
	140067053450912 -> 140067053451056
	140067053450912 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067053452688 -> 140067053450912
	140067053452688 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (288, 511, 64)"]
	140067053449424 -> 140067053452688
	140067053449424 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140067053452928 -> 140067053449424
	140067053452928 [label="ViewBackward0
----------------------------------
self_sym_sizes: (24, 12, 511, 511)"]
	140067053452544 -> 140067053452928
	140067053452544 [label="ExpandBackward0
----------------------------------
self_sym_sizes: (24, 12, 511, 511)"]
	140067053450576 -> 140067053452544
	140067053450576 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067053447504 -> 140067053450576
	140067053447504 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140067053439200 -> 140067053447504
	140067053439200 [label="AddBackward0
------------
alpha: 1"]
	140067053450000 -> 140067053439200
	140067053450000 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067053450480 -> 140067053450000
	140067053450480 [label="UnsafeViewBackward0
-------------------------------
self_sym_sizes: (288, 511, 511)"]
	140067053449616 -> 140067053450480
	140067053449616 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140067053449328 -> 140067053449616
	140067053449328 [label="UnsafeViewBackward0
---------------------------------
self_sym_sizes: (24, 12, 511, 64)"]
	140067053450528 -> 140067053449328
	140067053450528 [label=CloneBackward0]
	140067053447216 -> 140067053450528
	140067053447216 [label="ExpandBackward0
---------------------------------
self_sym_sizes: (24, 12, 511, 64)"]
	140067053451872 -> 140067053447216
	140067053451872 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067053451440 -> 140067053451872
	140067053451440 [label="ViewBackward0
------------------------------
self_sym_sizes: (24, 511, 768)"]
	140067053438624 -> 140067053451440
	140067053438624 [label="ViewBackward0
----------------------------
self_sym_sizes: (12264, 768)"]
	140067053448992 -> 140067053438624
	140067053448992 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :   (12264, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067053451392 -> 140067053448992
	140066806141072 [label="
 (768)" fillcolor=lightblue]
	140066806141072 -> 140067053451392
	140067053451392 [label=AccumulateGrad]
	140067053448896 -> 140067053448992
	140067053448896 [label="ViewBackward0
------------------------------
self_sym_sizes: (24, 511, 768)"]
	140067053493152 -> 140067053448896
	140067053493152 [label="ViewBackward0
--------------------------------
self_sym_sizes: (3, 8, 511, 768)"]
	140067053449376 -> 140067053493152
	140067053449376 [label="AsStridedBackward0
-----------------------------------------
size          :          (3, 8, 511, 768)
storage_offset:                         0
stride        : (3139584, 392448, 768, 1)"]
	140067053448848 -> 140067053449376
	140067053448848 [label=CopySlices]
	140067065258976 -> 140067053448848
	140067053441792 -> 140067053448848
	140067053441792 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 8, 768)
start         :                   0
step          :                   1"]
	140067053452400 -> 140067053441792
	140067053452400 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:         (3, 9, 768)
start         :                   1
step          :                   1"]
	140067053452832 -> 140067053452400
	140067053452832 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:         (3, 9, 768)
start         :                   0
step          :                   1"]
	140067065209344 -> 140067053452832
	140067053439776 -> 140067053448992
	140067053439776 [label=TBackward0]
	140067053447744 -> 140067053439776
	140066806141152 [label="
 (768, 768)" fillcolor=lightblue]
	140066806141152 -> 140067053447744
	140067053447744 [label=AccumulateGrad]
	140067053448656 -> 140067053449616
	140067053448656 [label="UnsafeViewBackward0
---------------------------------
self_sym_sizes: (24, 12, 64, 511)"]
	140067053443856 -> 140067053448656
	140067053443856 [label=CloneBackward0]
	140067053448416 -> 140067053443856
	140067053448416 [label="ExpandBackward0
---------------------------------
self_sym_sizes: (24, 12, 64, 511)"]
	140067053448944 -> 140067053448416
	140067053448944 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551615
dim1: 18446744073709551614"]
	140067053447600 -> 140067053448944
	140067053447600 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067053451008 -> 140067053447600
	140067053451008 [label="ViewBackward0
------------------------------
self_sym_sizes: (24, 511, 768)"]
	140067053453264 -> 140067053451008
	140067053453264 [label="ViewBackward0
----------------------------
self_sym_sizes: (12264, 768)"]
	140067053449232 -> 140067053453264
	140067053449232 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :   (12264, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067053451248 -> 140067053449232
	140066806139952 [label="
 (768)" fillcolor=lightblue]
	140066806139952 -> 140067053451248
	140067053451248 [label=AccumulateGrad]
	140067053451920 -> 140067053449232
	140067053451920 [label="ViewBackward0
------------------------------
self_sym_sizes: (24, 511, 768)"]
	140067053493152 -> 140067053451920
	140067053449568 -> 140067053449232
	140067053449568 [label=TBackward0]
	140067053452496 -> 140067053449568
	140066806140832 [label="
 (768, 768)" fillcolor=lightblue]
	140066806140832 -> 140067053452496
	140067053452496 [label=AccumulateGrad]
	140067053448224 -> 140067053449424
	140067053448224 [label="UnsafeViewBackward0
---------------------------------
self_sym_sizes: (24, 12, 511, 64)"]
	140067053447936 -> 140067053448224
	140067053447936 [label=CloneBackward0]
	140067053442176 -> 140067053447936
	140067053442176 [label="ExpandBackward0
---------------------------------
self_sym_sizes: (24, 12, 511, 64)"]
	140067053451968 -> 140067053442176
	140067053451968 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067053449664 -> 140067053451968
	140067053449664 [label="ViewBackward0
------------------------------
self_sym_sizes: (24, 511, 768)"]
	140067053440832 -> 140067053449664
	140067053440832 [label="ViewBackward0
----------------------------
self_sym_sizes: (12264, 768)"]
	140067053453072 -> 140067053440832
	140067053453072 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :   (12264, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067053448272 -> 140067053453072
	140066806140672 [label="
 (768)" fillcolor=lightblue]
	140066806140672 -> 140067053448272
	140067053448272 [label=AccumulateGrad]
	140067053449280 -> 140067053453072
	140067053449280 [label="ViewBackward0
------------------------------
self_sym_sizes: (24, 511, 768)"]
	140067053493152 -> 140067053449280
	140067053449472 -> 140067053453072
	140067053449472 [label=TBackward0]
	140067053453216 -> 140067053449472
	140066806140752 [label="
 (768, 768)" fillcolor=lightblue]
	140066806140752 -> 140067053453216
	140067053453216 [label=AccumulateGrad]
	140067053451488 -> 140067053450768
	140067053451488 [label=TBackward0]
	140067053448560 -> 140067053451488
	140066806140592 [label="
 (768, 768)" fillcolor=lightblue]
	140066806140592 -> 140067053448560
	140067053448560 [label=AccumulateGrad]
	140067053493152 -> 140067053494400
	140067053493680 -> 140067065215392
	140066806140352 [label="
 (768)" fillcolor=lightblue]
	140066806140352 -> 140067053493680
	140067053493680 [label=AccumulateGrad]
	140067053494304 -> 140067065215392
	140066806140432 [label="
 (768)" fillcolor=lightblue]
	140066806140432 -> 140067053494304
	140067053494304 [label=AccumulateGrad]
	140067053488544 -> 140067053492240
	140067053488544 [label=TBackward0]
	140067053493056 -> 140067053488544
	140066806140192 [label="
 (3072, 768)" fillcolor=lightblue]
	140066806140192 -> 140067053493056
	140067053493056 [label=AccumulateGrad]
	140067065207520 -> 140067065215968
	140067065207520 [label=TBackward0]
	140067053487200 -> 140067065207520
	140066806140272 [label="
 (768, 3072)" fillcolor=lightblue]
	140066806140272 -> 140067053487200
	140067053487200 [label=AccumulateGrad]
	140067065215392 -> 140067065211456
	140067065207664 -> 140067065202144
	140066806139792 [label="
 (768)" fillcolor=lightblue]
	140066806139792 -> 140067065207664
	140067065207664 [label=AccumulateGrad]
	140067065201760 -> 140067065202144
	140066806140032 [label="
 (768)" fillcolor=lightblue]
	140066806140032 -> 140067065201760
	140067065201760 [label=AccumulateGrad]
	140067065201952 -> 140067065202240
	140067065201952 [label=TBackward0]
	140067065215728 -> 140067065201952
	140066806151952 [label="
 (768, 768)" fillcolor=lightblue]
	140066806151952 -> 140067065215728
	140067065215728 [label=AccumulateGrad]
	140067065366368 -> 140067065372032
	140067065366368 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 64, 9)"]
	140067065216592 -> 140067065366368
	140067065216592 [label=CloneBackward0]
	140067065204928 -> 140067065216592
	140067065204928 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 64, 9)"]
	140067065209824 -> 140067065204928
	140067065209824 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551615
dim1: 18446744073709551614"]
	140067065209680 -> 140067065209824
	140067065209680 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067065206320 -> 140067065209680
	140067065206320 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 9, 768)"]
	140067065204736 -> 140067065206320
	140067065204736 [label="ViewBackward0
-------------------------
self_sym_sizes: (27, 768)"]
	140067065201184 -> 140067065204736
	140067065201184 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (27, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067053492768 -> 140067065201184
	140066806152672 [label="
 (768)" fillcolor=lightblue]
	140066806152672 -> 140067053492768
	140067053492768 [label=AccumulateGrad]
	140067053493536 -> 140067065201184
	140067053493536 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 9, 768)"]
	140067065376400 -> 140067053493536
	140067053494160 -> 140067065201184
	140067053494160 [label=TBackward0]
	140067065261232 -> 140067053494160
	140066806152512 [label="
 (768, 768)" fillcolor=lightblue]
	140066806152512 -> 140067065261232
	140067065261232 [label=AccumulateGrad]
	140067065366272 -> 140067065371120
	140067065366272 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 9, 64)"]
	140067065367808 -> 140067065366272
	140067065367808 [label=CloneBackward0]
	140067065380240 -> 140067065367808
	140067065380240 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 9, 64)"]
	140067053493248 -> 140067065380240
	140067053493248 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067065374576 -> 140067053493248
	140067065374576 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 9, 768)"]
	140067065378704 -> 140067065374576
	140067065378704 [label="ViewBackward0
-------------------------
self_sym_sizes: (27, 768)"]
	140067065208384 -> 140067065378704
	140067065208384 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (27, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067065210064 -> 140067065208384
	140066806152832 [label="
 (768)" fillcolor=lightblue]
	140066806152832 -> 140067065210064
	140067065210064 [label=AccumulateGrad]
	140067065206224 -> 140067065208384
	140067065206224 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 9, 768)"]
	140067065376400 -> 140067065206224
	140067065216496 -> 140067065208384
	140067065216496 [label=TBackward0]
	140067065208912 -> 140067065216496
	140066806152752 [label="
 (768, 768)" fillcolor=lightblue]
	140066806152752 -> 140067065208912
	140067065208912 [label=AccumulateGrad]
	140067065372560 -> 140067065378464
	140067065372560 [label=TBackward0]
	140067065378848 -> 140067065372560
	140066806152912 [label="
 (768, 768)" fillcolor=lightblue]
	140066806152912 -> 140067065378848
	140067065378848 [label=AccumulateGrad]
	140067065376400 -> 140067065375056
	140067065373232 -> 140067065376832
	140066806153152 [label="
 (768)" fillcolor=lightblue]
	140066806153152 -> 140067065373232
	140067065373232 [label=AccumulateGrad]
	140067065372992 -> 140067065376832
	140066806153072 [label="
 (768)" fillcolor=lightblue]
	140066806153072 -> 140067065372992
	140067065372992 [label=AccumulateGrad]
	140067065380576 -> 140067065377792
	140067065380576 [label=TBackward0]
	140067065374672 -> 140067065380576
	140066770649248 [label="
 (3072, 768)" fillcolor=lightblue]
	140066770649248 -> 140067065374672
	140067065374672 [label=AccumulateGrad]
	140067065373856 -> 140067065377648
	140067065373856 [label=TBackward0]
	140067065364832 -> 140067065373856
	140066770649408 [label="
 (768, 3072)" fillcolor=lightblue]
	140066770649408 -> 140067065364832
	140067065364832 [label=AccumulateGrad]
	140067065376832 -> 140067065376448
	140067065378080 -> 140067065364976
	140066770649648 [label="
 (768)" fillcolor=lightblue]
	140066770649648 -> 140067065378080
	140067065378080 [label=AccumulateGrad]
	140067065375968 -> 140067065364976
	140066770649568 [label="
 (768)" fillcolor=lightblue]
	140066770649568 -> 140067065375968
	140067065375968 [label=AccumulateGrad]
	140067065375200 -> 140067041578624
	140067065375200 [label=CloneBackward0]
	140067065365264 -> 140067065375200
	140067065365264 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 8, 768)
start         :                   0
step          :                   1"]
	140067065374816 -> 140067065365264
	140067065374816 [label="SelectBackward0
--------------------------------
dim           :                2
index         :                1
self_sym_sizes: (3, 8, 511, 768)"]
	140067065380768 -> 140067065374816
	140067065380768 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:    (3, 8, 511, 768)
start         :                   0
step          :                   1"]
	140067065365024 -> 140067065380768
	140067065365024 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:    (3, 8, 511, 768)
start         :                   0
step          :                   1"]
	140067065365216 -> 140067065365024
	140067065365216 [label="ViewBackward0
------------------------------
self_sym_sizes: (24, 511, 768)"]
	140067065376304 -> 140067065365216
	140067065376304 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140067065366320 -> 140067065376304
	140067065366320 [label="AddBackward0
------------
alpha: 1"]
	140067065365792 -> 140067065366320
	140067065365792 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067065376208 -> 140067065365792
	140067065376208 [label="ViewBackward0
----------------------------
self_sym_sizes: (12264, 768)"]
	140067065372368 -> 140067065376208
	140067065372368 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :  (12264, 3072)
mat1_sym_strides:      (3072, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (3072, 768)
mat2_sym_strides:      (1, 3072)"]
	140067065378992 -> 140067065372368
	140066806139872 [label="
 (768)" fillcolor=lightblue]
	140066806139872 -> 140067065378992
	140067065378992 [label=AccumulateGrad]
	140067065379376 -> 140067065372368
	140067065379376 [label="ViewBackward0
-------------------------------
self_sym_sizes: (24, 511, 3072)"]
	140067065210688 -> 140067065379376
	140067065210688 [label="GeluBackward0
---------------------------
approximate:           none
self       : [saved tensor]"]
	140067065207088 -> 140067065210688
	140067065207088 [label="ViewBackward0
-----------------------------
self_sym_sizes: (12264, 3072)"]
	140067053441840 -> 140067065207088
	140067053441840 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :   (12264, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (768, 3072)
mat2_sym_strides:       (1, 768)"]
	140067053451584 -> 140067053441840
	140066806138672 [label="
 (3072)" fillcolor=lightblue]
	140066806138672 -> 140067053451584
	140067053451584 [label=AccumulateGrad]
	140067053450432 -> 140067053441840
	140067053450432 [label="ViewBackward0
------------------------------
self_sym_sizes: (24, 511, 768)"]
	140067065365360 -> 140067053450432
	140067065365360 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140067053448464 -> 140067065365360
	140067053448464 [label="AddBackward0
------------
alpha: 1"]
	140067053449088 -> 140067053448464
	140067053449088 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067053450336 -> 140067053449088
	140067053450336 [label="ViewBackward0
----------------------------
self_sym_sizes: (12264, 768)"]
	140067053450192 -> 140067053450336
	140067053450192 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :   (12264, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067053449136 -> 140067053450192
	140066806139072 [label="
 (768)" fillcolor=lightblue]
	140066806139072 -> 140067053449136
	140067053449136 [label=AccumulateGrad]
	140067053447408 -> 140067053450192
	140067053447408 [label="ViewBackward0
------------------------------
self_sym_sizes: (24, 511, 768)"]
	140067053447984 -> 140067053447408
	140067053447984 [label="ViewBackward0
---------------------------------
self_sym_sizes: (24, 511, 12, 64)"]
	140067053443232 -> 140067053447984
	140067053443232 [label=CloneBackward0]
	140067053453024 -> 140067053443232
	140067053453024 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140066688180160 -> 140067053453024
	140066688180160 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (288, 511, 64)"]
	140066688179488 -> 140066688180160
	140066688179488 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140066688180352 -> 140066688179488
	140066688180352 [label="ViewBackward0
----------------------------------
self_sym_sizes: (24, 12, 511, 511)"]
	140066688176752 -> 140066688180352
	140066688176752 [label="ExpandBackward0
----------------------------------
self_sym_sizes: (24, 12, 511, 511)"]
	140066688182464 -> 140066688176752
	140066688182464 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140066688181120 -> 140066688182464
	140066688181120 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140066688180400 -> 140066688181120
	140066688180400 [label="AddBackward0
------------
alpha: 1"]
	140066688180304 -> 140066688180400
	140066688180304 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140066688179920 -> 140066688180304
	140066688179920 [label="UnsafeViewBackward0
-------------------------------
self_sym_sizes: (288, 511, 511)"]
	140066688179728 -> 140066688179920
	140066688179728 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140066688179632 -> 140066688179728
	140066688179632 [label="UnsafeViewBackward0
---------------------------------
self_sym_sizes: (24, 12, 511, 64)"]
	140066688179584 -> 140066688179632
	140066688179584 [label=CloneBackward0]
	140066688179344 -> 140066688179584
	140066688179344 [label="ExpandBackward0
---------------------------------
self_sym_sizes: (24, 12, 511, 64)"]
	140066688179296 -> 140066688179344
	140066688179296 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140066688178864 -> 140066688179296
	140066688178864 [label="ViewBackward0
------------------------------
self_sym_sizes: (24, 511, 768)"]
	140066688178672 -> 140066688178864
	140066688178672 [label="ViewBackward0
----------------------------
self_sym_sizes: (12264, 768)"]
	140066688178480 -> 140066688178672
	140066688178480 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :   (12264, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140066688178240 -> 140066688178480
	140066806139632 [label="
 (768)" fillcolor=lightblue]
	140066806139632 -> 140066688178240
	140066688178240 [label=AccumulateGrad]
	140066688178432 -> 140066688178480
	140066688178432 [label="ViewBackward0
------------------------------
self_sym_sizes: (24, 511, 768)"]
	140067053452064 -> 140066688178432
	140067053452064 [label="ViewBackward0
--------------------------------
self_sym_sizes: (3, 8, 511, 768)"]
	140066688177856 -> 140067053452064
	140066688177856 [label="AsStridedBackward0
-----------------------------------------
size          :          (3, 8, 511, 768)
storage_offset:                         0
stride        : (3139584, 392448, 768, 1)"]
	140066688177568 -> 140066688177856
	140066688177568 [label=CopySlices]
	140067065202144 -> 140066688177568
	140066688177280 -> 140066688177568
	140066688177280 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 8, 768)
start         :                   0
step          :                   1"]
	140066688177376 -> 140066688177280
	140066688177376 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:         (3, 9, 768)
start         :                   1
step          :                   1"]
	140066688176080 -> 140066688177376
	140066688176080 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:         (3, 9, 768)
start         :                   0
step          :                   1"]
	140067065364976 -> 140066688176080
	140066688179440 -> 140066688178480
	140066688179440 [label=TBackward0]
	140066688177952 -> 140066688179440
	140066806139712 [label="
 (768, 768)" fillcolor=lightblue]
	140066806139712 -> 140066688177952
	140066688177952 [label=AccumulateGrad]
	140066688180016 -> 140066688179728
	140066688180016 [label="UnsafeViewBackward0
---------------------------------
self_sym_sizes: (24, 12, 64, 511)"]
	140066688179872 -> 140066688180016
	140066688179872 [label=CloneBackward0]
	140066688178528 -> 140066688179872
	140066688178528 [label="ExpandBackward0
---------------------------------
self_sym_sizes: (24, 12, 64, 511)"]
	140066688178720 -> 140066688178528
	140066688178720 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551615
dim1: 18446744073709551614"]
	140066688177328 -> 140066688178720
	140066688177328 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140066688175792 -> 140066688177328
	140066688175792 [label="ViewBackward0
------------------------------
self_sym_sizes: (24, 511, 768)"]
	140066688178000 -> 140066688175792
	140066688178000 [label="ViewBackward0
----------------------------
self_sym_sizes: (12264, 768)"]
	140066688177664 -> 140066688178000
	140066688177664 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :   (12264, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140066688176512 -> 140066688177664
	140066806138512 [label="
 (768)" fillcolor=lightblue]
	140066806138512 -> 140066688176512
	140066688176512 [label=AccumulateGrad]
	140066688175552 -> 140066688177664
	140066688175552 [label="ViewBackward0
------------------------------
self_sym_sizes: (24, 511, 768)"]
	140067053452064 -> 140066688175552
	140066688179392 -> 140066688177664
	140066688179392 [label=TBackward0]
	140066688175456 -> 140066688179392
	140066806139392 [label="
 (768, 768)" fillcolor=lightblue]
	140066806139392 -> 140066688175456
	140066688175456 [label=AccumulateGrad]
	140066688180064 -> 140066688179488
	140066688180064 [label="UnsafeViewBackward0
---------------------------------
self_sym_sizes: (24, 12, 511, 64)"]
	140066688174352 -> 140066688180064
	140066688174352 [label=CloneBackward0]
	140066688180688 -> 140066688174352
	140066688180688 [label="ExpandBackward0
---------------------------------
self_sym_sizes: (24, 12, 511, 64)"]
	140066688180592 -> 140066688180688
	140066688180592 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140066688179104 -> 140066688180592
	140066688179104 [label="ViewBackward0
------------------------------
self_sym_sizes: (24, 511, 768)"]
	140066688177472 -> 140066688179104
	140066688177472 [label="ViewBackward0
----------------------------
self_sym_sizes: (12264, 768)"]
	140066688177184 -> 140066688177472
	140066688177184 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :   (12264, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140066688176128 -> 140066688177184
	140066806139232 [label="
 (768)" fillcolor=lightblue]
	140066806139232 -> 140066688176128
	140066688176128 [label=AccumulateGrad]
	140066688177760 -> 140066688177184
	140066688177760 [label="ViewBackward0
------------------------------
self_sym_sizes: (24, 511, 768)"]
	140067053452064 -> 140066688177760
	140066688182656 -> 140066688177184
	140066688182656 [label=TBackward0]
	140066688175840 -> 140066688182656
	140066806139312 [label="
 (768, 768)" fillcolor=lightblue]
	140066806139312 -> 140066688175840
	140066688175840 [label=AccumulateGrad]
	140067053451104 -> 140067053450192
	140067053451104 [label=TBackward0]
	140067053448128 -> 140067053451104
	140066806139152 [label="
 (768, 768)" fillcolor=lightblue]
	140066806139152 -> 140067053448128
	140067053448128 [label=AccumulateGrad]
	140067053452064 -> 140067053448464
	140067053452208 -> 140067065365360
	140066806138912 [label="
 (768)" fillcolor=lightblue]
	140066806138912 -> 140067053452208
	140067053452208 [label=AccumulateGrad]
	140067053449808 -> 140067065365360
	140066806138992 [label="
 (768)" fillcolor=lightblue]
	140066806138992 -> 140067053449808
	140067053449808 [label=AccumulateGrad]
	140067053451632 -> 140067053441840
	140067053451632 [label=TBackward0]
	140067053450240 -> 140067053451632
	140066806138752 [label="
 (3072, 768)" fillcolor=lightblue]
	140066806138752 -> 140067053450240
	140067053450240 [label=AccumulateGrad]
	140067065201664 -> 140067065372368
	140067065201664 [label=TBackward0]
	140067065211168 -> 140067065201664
	140066806138832 [label="
 (768, 3072)" fillcolor=lightblue]
	140066806138832 -> 140067065211168
	140067065211168 [label=AccumulateGrad]
	140067065365360 -> 140067065366320
	140067065367328 -> 140067065376304
	140066806138352 [label="
 (768)" fillcolor=lightblue]
	140066806138352 -> 140067065367328
	140067065367328 [label=AccumulateGrad]
	140067065374912 -> 140067065376304
	140066806138592 [label="
 (768)" fillcolor=lightblue]
	140066806138592 -> 140067065374912
	140067065374912 [label=AccumulateGrad]
	140067065380624 -> 140067041575552
	140067065380624 [label=TBackward0]
	140067065379232 -> 140067065380624
	140066770649168 [label="
 (768, 768)" fillcolor=lightblue]
	140066770649168 -> 140067065379232
	140067065379232 [label=AccumulateGrad]
	140067041576176 -> 140067041575840
	140067041576176 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 64, 9)"]
	140067041576032 -> 140067041576176
	140067041576032 [label=CloneBackward0]
	140067041574976 -> 140067041576032
	140067041574976 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 64, 9)"]
	140067041575504 -> 140067041574976
	140067041575504 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551615
dim1: 18446744073709551614"]
	140067065378560 -> 140067041575504
	140067065378560 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067065366752 -> 140067065378560
	140067065366752 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 9, 768)"]
	140067065375632 -> 140067065366752
	140067065375632 [label="ViewBackward0
-------------------------
self_sym_sizes: (27, 768)"]
	140067065365312 -> 140067065375632
	140067065365312 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (27, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067065203056 -> 140067065365312
	140066770649888 [label="
 (768)" fillcolor=lightblue]
	140066770649888 -> 140067065203056
	140067065203056 [label=AccumulateGrad]
	140067065208576 -> 140067065365312
	140067065208576 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 9, 768)"]
	140067041578624 -> 140067065208576
	140067065377456 -> 140067065365312
	140067065377456 [label=TBackward0]
	140067065375680 -> 140067065377456
	140066770649808 [label="
 (768, 768)" fillcolor=lightblue]
	140066770649808 -> 140067065375680
	140067065375680 [label=AccumulateGrad]
	140067041577280 -> 140067041577232
	140067041577280 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 9, 64)"]
	140067041577040 -> 140067041577280
	140067041577040 [label=CloneBackward0]
	140067041576464 -> 140067041577040
	140067041576464 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 9, 64)"]
	140067041576416 -> 140067041576464
	140067041576416 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041575312 -> 140067041576416
	140067041575312 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 9, 768)"]
	140067041575120 -> 140067041575312
	140067041575120 [label="ViewBackward0
-------------------------
self_sym_sizes: (27, 768)"]
	140067041576848 -> 140067041575120
	140067041576848 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (27, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067065365888 -> 140067041576848
	140066770650048 [label="
 (768)" fillcolor=lightblue]
	140066770650048 -> 140067065365888
	140067065365888 [label=AccumulateGrad]
	140067065366896 -> 140067041576848
	140067065366896 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 9, 768)"]
	140067041578624 -> 140067065366896
	140067065377696 -> 140067041576848
	140067065377696 [label=TBackward0]
	140067065375104 -> 140067065377696
	140066770649968 [label="
 (768, 768)" fillcolor=lightblue]
	140066770649968 -> 140067065375104
	140067065375104 [label=AccumulateGrad]
	140067041578288 -> 140067041578096
	140067041578288 [label=TBackward0]
	140067041577856 -> 140067041578288
	140066770650128 [label="
 (768, 768)" fillcolor=lightblue]
	140066770650128 -> 140067041577856
	140067041577856 [label=AccumulateGrad]
	140067041578624 -> 140067041579008
	140067041579296 -> 140067041581264
	140066770650368 [label="
 (768)" fillcolor=lightblue]
	140066770650368 -> 140067041579296
	140067041579296 [label=AccumulateGrad]
	140067041578912 -> 140067041581264
	140066770650288 [label="
 (768)" fillcolor=lightblue]
	140066770650288 -> 140067041578912
	140067041578912 [label=AccumulateGrad]
	140067041579776 -> 140067041579584
	140067041579776 [label=TBackward0]
	140067041578384 -> 140067041579776
	140066770650528 [label="
 (3072, 768)" fillcolor=lightblue]
	140066770650528 -> 140067041578384
	140067041578384 [label=AccumulateGrad]
	140067041580256 -> 140067041580064
	140067041580256 [label=TBackward0]
	140067041579824 -> 140067041580256
	140066770650688 [label="
 (768, 3072)" fillcolor=lightblue]
	140066770650688 -> 140067041579824
	140067041579824 [label=AccumulateGrad]
	140067041581264 -> 140067041580736
	140067041580352 -> 140067041580400
	140066770650928 [label="
 (768)" fillcolor=lightblue]
	140066770650928 -> 140067041580352
	140067041580352 [label=AccumulateGrad]
	140067041581312 -> 140067041580400
	140066770650848 [label="
 (768)" fillcolor=lightblue]
	140066770650848 -> 140067041581312
	140067041581312 [label=AccumulateGrad]
	140067041581840 -> 140067041585968
	140067041581840 [label=CloneBackward0]
	140067041580640 -> 140067041581840
	140067041580640 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 8, 768)
start         :                   0
step          :                   1"]
	140067041581744 -> 140067041580640
	140067041581744 [label="SelectBackward0
--------------------------------
dim           :                2
index         :                1
self_sym_sizes: (3, 8, 511, 768)"]
	140067041579248 -> 140067041581744
	140067041579248 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:    (3, 8, 511, 768)
start         :                   0
step          :                   1"]
	140067041580112 -> 140067041579248
	140067041580112 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:    (3, 8, 511, 768)
start         :                   0
step          :                   1"]
	140067041577472 -> 140067041580112
	140067041577472 [label="ViewBackward0
------------------------------
self_sym_sizes: (24, 511, 768)"]
	140067041577568 -> 140067041577472
	140067041577568 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140067041579104 -> 140067041577568
	140067041579104 [label="AddBackward0
------------
alpha: 1"]
	140067041578192 -> 140067041579104
	140067041578192 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041581120 -> 140067041578192
	140067041581120 [label="ViewBackward0
----------------------------
self_sym_sizes: (12264, 768)"]
	140067041575216 -> 140067041581120
	140067041575216 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :  (12264, 3072)
mat1_sym_strides:      (3072, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (3072, 768)
mat2_sym_strides:      (1, 3072)"]
	140067041579632 -> 140067041575216
	140066806138432 [label="
 (768)" fillcolor=lightblue]
	140066806138432 -> 140067041579632
	140067041579632 [label=AccumulateGrad]
	140067041575888 -> 140067041575216
	140067041575888 [label="ViewBackward0
-------------------------------
self_sym_sizes: (24, 511, 3072)"]
	140067065376976 -> 140067041575888
	140067065376976 [label="GeluBackward0
---------------------------
approximate:           none
self       : [saved tensor]"]
	140067053448368 -> 140067065376976
	140067053448368 [label="ViewBackward0
-----------------------------
self_sym_sizes: (12264, 3072)"]
	140067053450864 -> 140067053448368
	140067053450864 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :   (12264, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (768, 3072)
mat2_sym_strides:       (1, 768)"]
	140067053451536 -> 140067053450864
	140066806137232 [label="
 (3072)" fillcolor=lightblue]
	140066806137232 -> 140067053451536
	140067053451536 [label=AccumulateGrad]
	140067053452352 -> 140067053450864
	140067053452352 [label="ViewBackward0
------------------------------
self_sym_sizes: (24, 511, 768)"]
	140067041577136 -> 140067053452352
	140067041577136 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140066688181168 -> 140067041577136
	140066688181168 [label="AddBackward0
------------
alpha: 1"]
	140066688178960 -> 140066688181168
	140066688178960 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140066688179824 -> 140066688178960
	140066688179824 [label="ViewBackward0
----------------------------
self_sym_sizes: (12264, 768)"]
	140066688175696 -> 140066688179824
	140066688175696 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :   (12264, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140066688175408 -> 140066688175696
	140066806137632 [label="
 (768)" fillcolor=lightblue]
	140066806137632 -> 140066688175408
	140066688175408 [label=AccumulateGrad]
	140066688176608 -> 140066688175696
	140066688176608 [label="ViewBackward0
------------------------------
self_sym_sizes: (24, 511, 768)"]
	140066688174784 -> 140066688176608
	140066688174784 [label="ViewBackward0
---------------------------------
self_sym_sizes: (24, 511, 12, 64)"]
	140066688175312 -> 140066688174784
	140066688175312 [label=CloneBackward0]
	140066688177136 -> 140066688175312
	140066688177136 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140066688174448 -> 140066688177136
	140066688174448 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (288, 511, 64)"]
	140066688174064 -> 140066688174448
	140066688174064 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140066688173728 -> 140066688174064
	140066688173728 [label="ViewBackward0
----------------------------------
self_sym_sizes: (24, 12, 511, 511)"]
	140066688173632 -> 140066688173728
	140066688173632 [label="ExpandBackward0
----------------------------------
self_sym_sizes: (24, 12, 511, 511)"]
	140066688173488 -> 140066688173632
	140066688173488 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140066688173392 -> 140066688173488
	140066688173392 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140066688173056 -> 140066688173392
	140066688173056 [label="AddBackward0
------------
alpha: 1"]
	140066688173008 -> 140066688173056
	140066688173008 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140066688172864 -> 140066688173008
	140066688172864 [label="UnsafeViewBackward0
-------------------------------
self_sym_sizes: (288, 511, 511)"]
	140066688172528 -> 140066688172864
	140066688172528 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140066688172480 -> 140066688172528
	140066688172480 [label="UnsafeViewBackward0
---------------------------------
self_sym_sizes: (24, 12, 511, 64)"]
	140066688172432 -> 140066688172480
	140066688172432 [label=CloneBackward0]
	140066688172144 -> 140066688172432
	140066688172144 [label="ExpandBackward0
---------------------------------
self_sym_sizes: (24, 12, 511, 64)"]
	140066688172096 -> 140066688172144
	140066688172096 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140066688182992 -> 140066688172096
	140066688182992 [label="ViewBackward0
------------------------------
self_sym_sizes: (24, 511, 768)"]
	140066688182224 -> 140066688182992
	140066688182224 [label="ViewBackward0
----------------------------
self_sym_sizes: (12264, 768)"]
	140066688182704 -> 140066688182224
	140066688182704 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :   (12264, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140066688188032 -> 140066688182704
	140066806138192 [label="
 (768)" fillcolor=lightblue]
	140066806138192 -> 140066688188032
	140066688188032 [label=AccumulateGrad]
	140066688187936 -> 140066688182704
	140066688187936 [label="ViewBackward0
------------------------------
self_sym_sizes: (24, 511, 768)"]
	140066688177904 -> 140066688187936
	140066688177904 [label="ViewBackward0
--------------------------------
self_sym_sizes: (3, 8, 511, 768)"]
	140066688187840 -> 140066688177904
	140066688187840 [label="AsStridedBackward0
-----------------------------------------
size          :          (3, 8, 511, 768)
storage_offset:                         0
stride        : (3139584, 392448, 768, 1)"]
	140066688187600 -> 140066688187840
	140066688187600 [label=CopySlices]
	140067065376304 -> 140066688187600
	140066688187408 -> 140066688187600
	140066688187408 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 8, 768)
start         :                   0
step          :                   1"]
	140066688187696 -> 140066688187408
	140066688187696 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:         (3, 9, 768)
start         :                   1
step          :                   1"]
	140066688187120 -> 140066688187696
	140066688187120 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:         (3, 9, 768)
start         :                   0
step          :                   1"]
	140067041580400 -> 140066688187120
	140066688172384 -> 140066688182704
	140066688172384 [label=TBackward0]
	140066688188128 -> 140066688172384
	140066806138272 [label="
 (768, 768)" fillcolor=lightblue]
	140066806138272 -> 140066688188128
	140066688188128 [label=AccumulateGrad]
	140066688172912 -> 140066688172528
	140066688172912 [label="UnsafeViewBackward0
---------------------------------
self_sym_sizes: (24, 12, 64, 511)"]
	140066688172816 -> 140066688172912
	140066688172816 [label=CloneBackward0]
	140066688183808 -> 140066688172816
	140066688183808 [label="ExpandBackward0
---------------------------------
self_sym_sizes: (24, 12, 64, 511)"]
	140066688188176 -> 140066688183808
	140066688188176 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551615
dim1: 18446744073709551614"]
	140066688187744 -> 140066688188176
	140066688187744 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140066688187216 -> 140066688187744
	140066688187216 [label="ViewBackward0
------------------------------
self_sym_sizes: (24, 511, 768)"]
	140066688188080 -> 140066688187216
	140066688188080 [label="ViewBackward0
----------------------------
self_sym_sizes: (12264, 768)"]
	140066688187984 -> 140066688188080
	140066688187984 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :   (12264, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140066688187360 -> 140066688187984
	140066806137072 [label="
 (768)" fillcolor=lightblue]
	140066806137072 -> 140066688187360
	140066688187360 [label=AccumulateGrad]
	140066688186976 -> 140066688187984
	140066688186976 [label="ViewBackward0
------------------------------
self_sym_sizes: (24, 511, 768)"]
	140066688177904 -> 140066688186976
	140066688172288 -> 140066688187984
	140066688172288 [label=TBackward0]
	140066688187024 -> 140066688172288
	140066806137952 [label="
 (768, 768)" fillcolor=lightblue]
	140066806137952 -> 140066688187024
	140066688187024 [label=AccumulateGrad]
	140066688174208 -> 140066688174064
	140066688174208 [label="UnsafeViewBackward0
---------------------------------
self_sym_sizes: (24, 12, 511, 64)"]
	140066688173680 -> 140066688174208
	140066688173680 [label=CloneBackward0]
	140066688173296 -> 140066688173680
	140066688173296 [label="ExpandBackward0
---------------------------------
self_sym_sizes: (24, 12, 511, 64)"]
	140066688173200 -> 140066688173296
	140066688173200 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140066688184048 -> 140066688173200
	140066688184048 [label="ViewBackward0
------------------------------
self_sym_sizes: (24, 511, 768)"]
	140066688187648 -> 140066688184048
	140066688187648 [label="ViewBackward0
----------------------------
self_sym_sizes: (12264, 768)"]
	140066688187456 -> 140066688187648
	140066688187456 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :   (12264, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140066688187072 -> 140066688187456
	140066806137792 [label="
 (768)" fillcolor=lightblue]
	140066806137792 -> 140066688187072
	140066688187072 [label=AccumulateGrad]
	140066688187888 -> 140066688187456
	140066688187888 [label="ViewBackward0
------------------------------
self_sym_sizes: (24, 511, 768)"]
	140066688177904 -> 140066688187888
	140066688173536 -> 140066688187456
	140066688173536 [label=TBackward0]
	140066688187168 -> 140066688173536
	140066806137872 [label="
 (768, 768)" fillcolor=lightblue]
	140066806137872 -> 140066688187168
	140066688187168 [label=AccumulateGrad]
	140066688177088 -> 140066688175696
	140066688177088 [label=TBackward0]
	140066688175120 -> 140066688177088
	140066806137712 [label="
 (768, 768)" fillcolor=lightblue]
	140066806137712 -> 140066688175120
	140066688175120 [label=AccumulateGrad]
	140066688177904 -> 140066688181168
	140066688178912 -> 140067041577136
	140066806137472 [label="
 (768)" fillcolor=lightblue]
	140066806137472 -> 140066688178912
	140066688178912 [label=AccumulateGrad]
	140066688180496 -> 140067041577136
	140066806137552 [label="
 (768)" fillcolor=lightblue]
	140066806137552 -> 140066688180496
	140066688180496 [label=AccumulateGrad]
	140067053449856 -> 140067053450864
	140067053449856 [label=TBackward0]
	140066688177040 -> 140067053449856
	140066806137312 [label="
 (3072, 768)" fillcolor=lightblue]
	140066806137312 -> 140066688177040
	140066688177040 [label=AccumulateGrad]
	140067041576368 -> 140067041575216
	140067041576368 [label=TBackward0]
	140067053452304 -> 140067041576368
	140066806137392 [label="
 (768, 3072)" fillcolor=lightblue]
	140066806137392 -> 140067053452304
	140067053452304 [label=AccumulateGrad]
	140067041577136 -> 140067041579104
	140067041577376 -> 140067041577568
	140066806136912 [label="
 (768)" fillcolor=lightblue]
	140066806136912 -> 140067041577376
	140067041577376 [label=AccumulateGrad]
	140067041581360 -> 140067041577568
	140066806137152 [label="
 (768)" fillcolor=lightblue]
	140066806137152 -> 140067041581360
	140067041581360 [label=AccumulateGrad]
	140067041582896 -> 140067041582128
	140067041582896 [label=TBackward0]
	140067041581552 -> 140067041582896
	140066770650448 [label="
 (768, 768)" fillcolor=lightblue]
	140066770650448 -> 140067041581552
	140067041581552 [label=AccumulateGrad]
	140067041583520 -> 140067041583232
	140067041583520 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 64, 9)"]
	140067041583328 -> 140067041583520
	140067041583328 [label=CloneBackward0]
	140067041582224 -> 140067041583328
	140067041582224 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 64, 9)"]
	140067041582368 -> 140067041582224
	140067041582368 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551615
dim1: 18446744073709551614"]
	140067041580304 -> 140067041582368
	140067041580304 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041577520 -> 140067041580304
	140067041577520 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 9, 768)"]
	140067041578816 -> 140067041577520
	140067041578816 [label="ViewBackward0
-------------------------
self_sym_sizes: (27, 768)"]
	140067041576656 -> 140067041578816
	140067041576656 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (27, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067053444768 -> 140067041576656
	140066770651168 [label="
 (768)" fillcolor=lightblue]
	140066770651168 -> 140067053444768
	140067053444768 [label=AccumulateGrad]
	140067053452736 -> 140067041576656
	140067053452736 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 9, 768)"]
	140067041585968 -> 140067053452736
	140067065367040 -> 140067041576656
	140067065367040 [label=TBackward0]
	140067041576896 -> 140067065367040
	140066770651088 [label="
 (768, 768)" fillcolor=lightblue]
	140066770651088 -> 140067041576896
	140067041576896 [label=AccumulateGrad]
	140067041584528 -> 140067041584480
	140067041584528 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 9, 64)"]
	140067041584336 -> 140067041584528
	140067041584336 [label=CloneBackward0]
	140067041583856 -> 140067041584336
	140067041583856 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 9, 64)"]
	140067041583808 -> 140067041583856
	140067041583808 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041582560 -> 140067041583808
	140067041582560 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 9, 768)"]
	140067041581456 -> 140067041582560
	140067041581456 [label="ViewBackward0
-------------------------
self_sym_sizes: (27, 768)"]
	140067041578432 -> 140067041581456
	140067041578432 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (27, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041577712 -> 140067041578432
	140066770651328 [label="
 (768)" fillcolor=lightblue]
	140066770651328 -> 140067041577712
	140067041577712 [label=AccumulateGrad]
	140067041581648 -> 140067041578432
	140067041581648 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 9, 768)"]
	140067041585968 -> 140067041581648
	140067041584096 -> 140067041578432
	140067041584096 [label=TBackward0]
	140067041580016 -> 140067041584096
	140066770651248 [label="
 (768, 768)" fillcolor=lightblue]
	140066770651248 -> 140067041580016
	140067041580016 [label=AccumulateGrad]
	140067041585632 -> 140067041585488
	140067041585632 [label=TBackward0]
	140067041585152 -> 140067041585632
	140066770651408 [label="
 (768, 768)" fillcolor=lightblue]
	140066770651408 -> 140067041585152
	140067041585152 [label=AccumulateGrad]
	140067041585968 -> 140067041586256
	140067041586592 -> 140067041589280
	140066770651648 [label="
 (768)" fillcolor=lightblue]
	140066770651648 -> 140067041586592
	140067041586592 [label=AccumulateGrad]
	140067041586160 -> 140067041589280
	140066770651568 [label="
 (768)" fillcolor=lightblue]
	140066770651568 -> 140067041586160
	140067041586160 [label=AccumulateGrad]
	140067041587552 -> 140067041587264
	140067041587552 [label=TBackward0]
	140067041585680 -> 140067041587552
	140066770651808 [label="
 (3072, 768)" fillcolor=lightblue]
	140066770651808 -> 140067041585680
	140067041585680 [label=AccumulateGrad]
	140067041588224 -> 140067041588032
	140067041588224 [label=TBackward0]
	140067041587696 -> 140067041588224
	140066770651968 [label="
 (768, 3072)" fillcolor=lightblue]
	140066770651968 -> 140067041587696
	140067041587696 [label=AccumulateGrad]
	140067041589280 -> 140067041588896
	140067041588416 -> 140067041588512
	140066770652208 [label="
 (768)" fillcolor=lightblue]
	140066770652208 -> 140067041588416
	140067041588416 [label=AccumulateGrad]
	140067041589376 -> 140067041588512
	140066770652128 [label="
 (768)" fillcolor=lightblue]
	140066770652128 -> 140067041589376
	140067041589376 [label=AccumulateGrad]
	140067041589856 -> 140067041708928
	140067041589856 [label=CloneBackward0]
	140067041588752 -> 140067041589856
	140067041588752 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 8, 768)
start         :                   0
step          :                   1"]
	140067041589760 -> 140067041588752
	140067041589760 [label="SelectBackward0
--------------------------------
dim           :                2
index         :                1
self_sym_sizes: (3, 8, 511, 768)"]
	140067041586448 -> 140067041589760
	140067041586448 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:    (3, 8, 511, 768)
start         :                   0
step          :                   1"]
	140067041588080 -> 140067041586448
	140067041588080 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:    (3, 8, 511, 768)
start         :                   0
step          :                   1"]
	140067041584672 -> 140067041588080
	140067041584672 [label="ViewBackward0
------------------------------
self_sym_sizes: (24, 511, 768)"]
	140067041584864 -> 140067041584672
	140067041584864 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140067041586352 -> 140067041584864
	140067041586352 [label="AddBackward0
------------
alpha: 1"]
	140067041585536 -> 140067041586352
	140067041585536 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041589088 -> 140067041585536
	140067041589088 [label="ViewBackward0
----------------------------
self_sym_sizes: (12264, 768)"]
	140067041580496 -> 140067041589088
	140067041580496 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :  (12264, 3072)
mat1_sym_strides:      (3072, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (3072, 768)
mat2_sym_strides:      (1, 3072)"]
	140067041582464 -> 140067041580496
	140066806136992 [label="
 (768)" fillcolor=lightblue]
	140066806136992 -> 140067041582464
	140067041582464 [label=AccumulateGrad]
	140067041583280 -> 140067041580496
	140067041583280 [label="ViewBackward0
-------------------------------
self_sym_sizes: (24, 511, 3072)"]
	140067041582752 -> 140067041583280
	140067041582752 [label="GeluBackward0
---------------------------
approximate:           none
self       : [saved tensor]"]
	140066688173776 -> 140067041582752
	140066688173776 [label="ViewBackward0
-----------------------------
self_sym_sizes: (12264, 3072)"]
	140066688176560 -> 140066688173776
	140066688176560 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :   (12264, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (768, 3072)
mat2_sym_strides:       (1, 768)"]
	140066688185008 -> 140066688176560
	140066806141872 [label="
 (3072)" fillcolor=lightblue]
	140066806141872 -> 140066688185008
	140066688185008 [label=AccumulateGrad]
	140066688174592 -> 140066688176560
	140066688174592 [label="ViewBackward0
------------------------------
self_sym_sizes: (24, 511, 768)"]
	140067041584432 -> 140066688174592
	140067041584432 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140066688173440 -> 140067041584432
	140066688173440 [label="AddBackward0
------------
alpha: 1"]
	140066688181600 -> 140066688173440
	140066688181600 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140066688172672 -> 140066688181600
	140066688172672 [label="ViewBackward0
----------------------------
self_sym_sizes: (12264, 768)"]
	140066688186928 -> 140066688172672
	140066688186928 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :   (12264, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140066688186784 -> 140066688186928
	140066806141472 [label="
 (768)" fillcolor=lightblue]
	140066806141472 -> 140066688186784
	140066688186784 [label=AccumulateGrad]
	140066688187264 -> 140066688186928
	140066688187264 [label="ViewBackward0
------------------------------
self_sym_sizes: (24, 511, 768)"]
	140066688186688 -> 140066688187264
	140066688186688 [label="ViewBackward0
---------------------------------
self_sym_sizes: (24, 511, 12, 64)"]
	140066688186880 -> 140066688186688
	140066688186880 [label=CloneBackward0]
	140066688174256 -> 140066688186880
	140066688174256 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140066688186496 -> 140066688174256
	140066688186496 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (288, 511, 64)"]
	140066688186304 -> 140066688186496
	140066688186304 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140066688186112 -> 140066688186304
	140066688186112 [label="ViewBackward0
----------------------------------
self_sym_sizes: (24, 12, 511, 511)"]
	140066688186208 -> 140066688186112
	140066688186208 [label="ExpandBackward0
----------------------------------
self_sym_sizes: (24, 12, 511, 511)"]
	140066688186016 -> 140066688186208
	140066688186016 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140066688185776 -> 140066688186016
	140066688185776 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140066688185536 -> 140066688185776
	140066688185536 [label="AddBackward0
------------
alpha: 1"]
	140066688185584 -> 140066688185536
	140066688185584 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140066688185680 -> 140066688185584
	140066688185680 [label="UnsafeViewBackward0
-------------------------------
self_sym_sizes: (288, 511, 511)"]
	140066688185488 -> 140066688185680
	140066688185488 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140066688185248 -> 140066688185488
	140066688185248 [label="UnsafeViewBackward0
---------------------------------
self_sym_sizes: (24, 12, 511, 64)"]
	140066688185296 -> 140066688185248
	140066688185296 [label=CloneBackward0]
	140066688185152 -> 140066688185296
	140066688185152 [label="ExpandBackward0
---------------------------------
self_sym_sizes: (24, 12, 511, 64)"]
	140066688185200 -> 140066688185152
	140066688185200 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140066688184960 -> 140066688185200
	140066688184960 [label="ViewBackward0
------------------------------
self_sym_sizes: (24, 511, 768)"]
	140066688182512 -> 140066688184960
	140066688182512 [label="ViewBackward0
----------------------------
self_sym_sizes: (12264, 768)"]
	140066688184816 -> 140066688182512
	140066688184816 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :   (12264, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140066688184672 -> 140066688184816
	140066806138032 [label="
 (768)" fillcolor=lightblue]
	140066806138032 -> 140066688184672
	140066688184672 [label=AccumulateGrad]
	140066688184720 -> 140066688184816
	140066688184720 [label="ViewBackward0
------------------------------
self_sym_sizes: (24, 511, 768)"]
	140066688187792 -> 140066688184720
	140066688187792 [label="ViewBackward0
--------------------------------
self_sym_sizes: (3, 8, 511, 768)"]
	140066688184624 -> 140066688187792
	140066688184624 [label="AsStridedBackward0
-----------------------------------------
size          :          (3, 8, 511, 768)
storage_offset:                         0
stride        : (3139584, 392448, 768, 1)"]
	140066688184912 -> 140066688184624
	140066688184912 [label=CopySlices]
	140067041577568 -> 140066688184912
	140066688184528 -> 140066688184912
	140066688184528 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 8, 768)
start         :                   0
step          :                   1"]
	140066688181072 -> 140066688184528
	140066688181072 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:         (3, 9, 768)
start         :                   1
step          :                   1"]
	140066688183856 -> 140066688181072
	140066688183856 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:         (3, 9, 768)
start         :                   0
step          :                   1"]
	140067041588512 -> 140066688183856
	140066688185344 -> 140066688184816
	140066688185344 [label=TBackward0]
	140066688184576 -> 140066688185344
	140066806139472 [label="
 (768, 768)" fillcolor=lightblue]
	140066806139472 -> 140066688184576
	140066688184576 [label=AccumulateGrad]
	140066688185632 -> 140066688185488
	140066688185632 [label="UnsafeViewBackward0
---------------------------------
self_sym_sizes: (24, 12, 64, 511)"]
	140066688185392 -> 140066688185632
	140066688185392 [label=CloneBackward0]
	140066688181936 -> 140066688185392
	140066688181936 [label="ExpandBackward0
---------------------------------
self_sym_sizes: (24, 12, 64, 511)"]
	140066688182080 -> 140066688181936
	140066688182080 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551615
dim1: 18446744073709551614"]
	140066688184480 -> 140066688182080
	140066688184480 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140066688183904 -> 140066688184480
	140066688183904 [label="ViewBackward0
------------------------------
self_sym_sizes: (24, 511, 768)"]
	140066688183424 -> 140066688183904
	140066688183424 [label="ViewBackward0
----------------------------
self_sym_sizes: (12264, 768)"]
	140066688183328 -> 140066688183424
	140066688183328 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :   (12264, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140066688183952 -> 140066688183328
	140066806139552 [label="
 (768)" fillcolor=lightblue]
	140066806139552 -> 140066688183952
	140066688183952 [label=AccumulateGrad]
	140066688183664 -> 140066688183328
	140066688183664 [label="ViewBackward0
------------------------------
self_sym_sizes: (24, 511, 768)"]
	140066688187792 -> 140066688183664
	140066688185056 -> 140066688183328
	140066688185056 [label=TBackward0]
	140066688183520 -> 140066688185056
	140066806138112 [label="
 (768, 768)" fillcolor=lightblue]
	140066806138112 -> 140066688183520
	140066688183520 [label=AccumulateGrad]
	140066688186256 -> 140066688186304
	140066688186256 [label="UnsafeViewBackward0
---------------------------------
self_sym_sizes: (24, 12, 511, 64)"]
	140066688186160 -> 140066688186256
	140066688186160 [label=CloneBackward0]
	140066688185728 -> 140066688186160
	140066688185728 [label="ExpandBackward0
---------------------------------
self_sym_sizes: (24, 12, 511, 64)"]
	140066688185872 -> 140066688185728
	140066688185872 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140066688184768 -> 140066688185872
	140066688184768 [label="ViewBackward0
------------------------------
self_sym_sizes: (24, 511, 768)"]
	140066688184192 -> 140066688184768
	140066688184192 [label="ViewBackward0
----------------------------
self_sym_sizes: (12264, 768)"]
	140066688184288 -> 140066688184192
	140066688184288 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :   (12264, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140066688181312 -> 140066688184288
	140066806140992 [label="
 (768)" fillcolor=lightblue]
	140066806140992 -> 140066688181312
	140066688181312 [label=AccumulateGrad]
	140066688180832 -> 140066688184288
	140066688180832 [label="ViewBackward0
------------------------------
self_sym_sizes: (24, 511, 768)"]
	140066688187792 -> 140066688180832
	140066688185968 -> 140066688184288
	140066688185968 [label=TBackward0]
	140066688182800 -> 140066688185968
	140066806140912 [label="
 (768, 768)" fillcolor=lightblue]
	140066806140912 -> 140066688182800
	140066688182800 [label=AccumulateGrad]
	140066688187504 -> 140066688186928
	140066688187504 [label=TBackward0]
	140066688186592 -> 140066688187504
	140066806141392 [label="
 (768, 768)" fillcolor=lightblue]
	140066806141392 -> 140066688186592
	140066688186592 [label=AccumulateGrad]
	140066688187792 -> 140066688173440
	140066688174736 -> 140067041584432
	140066806141632 [label="
 (768)" fillcolor=lightblue]
	140066806141632 -> 140066688174736
	140066688174736 [label=AccumulateGrad]
	140066688173152 -> 140067041584432
	140066806141552 [label="
 (768)" fillcolor=lightblue]
	140066806141552 -> 140066688173152
	140066688173152 [label=AccumulateGrad]
	140066688175216 -> 140066688176560
	140066688175216 [label=TBackward0]
	140066688187552 -> 140066688175216
	140066806141792 [label="
 (3072, 768)" fillcolor=lightblue]
	140066806141792 -> 140066688187552
	140066688187552 [label=AccumulateGrad]
	140067041583760 -> 140067041580496
	140067041583760 [label=TBackward0]
	140067041587360 -> 140067041583760
	140066806141712 [label="
 (768, 3072)" fillcolor=lightblue]
	140066806141712 -> 140067041587360
	140067041587360 [label=AccumulateGrad]
	140067041584432 -> 140067041586352
	140067041584576 -> 140067041584864
	140066806142112 [label="
 (768)" fillcolor=lightblue]
	140066806142112 -> 140067041584576
	140067041584576 [label=AccumulateGrad]
	140067041589424 -> 140067041584864
	140066806141952 [label="
 (768)" fillcolor=lightblue]
	140066806141952 -> 140067041589424
	140067041589424 [label=AccumulateGrad]
	140067041590912 -> 140067041590144
	140067041590912 [label=TBackward0]
	140067041589616 -> 140067041590912
	140066770651728 [label="
 (768, 768)" fillcolor=lightblue]
	140066770651728 -> 140067041589616
	140067041589616 [label=AccumulateGrad]
	140067041706144 -> 140067041706048
	140067041706144 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 64, 9)"]
	140067041591152 -> 140067041706144
	140067041591152 [label=CloneBackward0]
	140067041590240 -> 140067041591152
	140067041590240 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 64, 9)"]
	140067041590384 -> 140067041590240
	140067041590384 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551615
dim1: 18446744073709551614"]
	140067041588320 -> 140067041590384
	140067041588320 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041584816 -> 140067041588320
	140067041584816 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 9, 768)"]
	140067041586064 -> 140067041584816
	140067041586064 [label="ViewBackward0
-------------------------
self_sym_sizes: (27, 768)"]
	140067041583952 -> 140067041586064
	140067041583952 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (27, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041584192 -> 140067041583952
	140066770652448 [label="
 (768)" fillcolor=lightblue]
	140066770652448 -> 140067041584192
	140067041584192 [label=AccumulateGrad]
	140067041585008 -> 140067041583952
	140067041585008 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 9, 768)"]
	140067041708928 -> 140067041585008
	140067041590816 -> 140067041583952
	140067041590816 [label=TBackward0]
	140067041587840 -> 140067041590816
	140066770652368 [label="
 (768, 768)" fillcolor=lightblue]
	140066770652368 -> 140067041587840
	140067041587840 [label=AccumulateGrad]
	140067041707584 -> 140067041707488
	140067041707584 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 9, 64)"]
	140067041707152 -> 140067041707584
	140067041707152 [label=CloneBackward0]
	140067041706816 -> 140067041707152
	140067041706816 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 9, 64)"]
	140067041706720 -> 140067041706816
	140067041706720 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041707008 -> 140067041706720
	140067041707008 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 9, 768)"]
	140067041589520 -> 140067041707008
	140067041589520 [label="ViewBackward0
-------------------------
self_sym_sizes: (27, 768)"]
	140067041585728 -> 140067041589520
	140067041585728 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (27, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041579536 -> 140067041585728
	140066770652608 [label="
 (768)" fillcolor=lightblue]
	140066770652608 -> 140067041579536
	140067041579536 [label=AccumulateGrad]
	140067041589664 -> 140067041585728
	140067041589664 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 9, 768)"]
	140067041708928 -> 140067041589664
	140067041590672 -> 140067041585728
	140067041590672 [label=TBackward0]
	140066688174880 -> 140067041590672
	140066770652528 [label="
 (768, 768)" fillcolor=lightblue]
	140066770652528 -> 140066688174880
	140066688174880 [label=AccumulateGrad]
	140067041708592 -> 140067041708448
	140067041708592 [label=TBackward0]
	140067041708112 -> 140067041708592
	140066770652688 [label="
 (768, 768)" fillcolor=lightblue]
	140066770652688 -> 140067041708112
	140067041708112 [label=AccumulateGrad]
	140067041708928 -> 140067041709216
	140067041709456 -> 140067041711760
	140066770652928 [label="
 (768)" fillcolor=lightblue]
	140066770652928 -> 140067041709456
	140067041709456 [label=AccumulateGrad]
	140067041709120 -> 140067041711760
	140066770652848 [label="
 (768)" fillcolor=lightblue]
	140066770652848 -> 140067041709120
	140067041709120 [label=AccumulateGrad]
	140067041710080 -> 140067041709840
	140067041710080 [label=TBackward0]
	140067041708688 -> 140067041710080
	140066770653088 [label="
 (3072, 768)" fillcolor=lightblue]
	140066770653088 -> 140067041708688
	140067041708688 [label=AccumulateGrad]
	140067041710656 -> 140067041710464
	140067041710656 [label=TBackward0]
	140067041710176 -> 140067041710656
	140066770653248 [label="
 (768, 3072)" fillcolor=lightblue]
	140066770653248 -> 140067041710176
	140067041710176 [label=AccumulateGrad]
	140067041711760 -> 140067041711424
	140067041710800 -> 140067041710896
	140066770653488 [label="
 (768)" fillcolor=lightblue]
	140066770653488 -> 140067041710800
	140067041710800 [label=AccumulateGrad]
	140067041711808 -> 140067041710896
	140066770653408 [label="
 (768)" fillcolor=lightblue]
	140066770653408 -> 140067041711808
	140067041711808 [label=AccumulateGrad]
	140067041712240 -> 140067041717376
	140067041712240 [label=CloneBackward0]
	140067041711376 -> 140067041712240
	140067041711376 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 8, 768)
start         :                   0
step          :                   1"]
	140067041712144 -> 140067041711376
	140067041712144 [label="SelectBackward0
--------------------------------
dim           :                2
index         :                1
self_sym_sizes: (3, 8, 511, 768)"]
	140067041709408 -> 140067041712144
	140067041709408 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:    (3, 8, 511, 768)
start         :                   0
step          :                   1"]
	140067041710560 -> 140067041709408
	140067041710560 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:    (3, 8, 511, 768)
start         :                   0
step          :                   1"]
	140067041707776 -> 140067041710560
	140067041707776 [label="ViewBackward0
------------------------------
self_sym_sizes: (24, 511, 768)"]
	140067041707920 -> 140067041707776
	140067041707920 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140067041709312 -> 140067041707920
	140067041709312 [label="AddBackward0
------------
alpha: 1"]
	140067041708496 -> 140067041709312
	140067041708496 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041711664 -> 140067041708496
	140067041711664 [label="ViewBackward0
----------------------------
self_sym_sizes: (12264, 768)"]
	140067041706336 -> 140067041711664
	140067041706336 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :  (12264, 3072)
mat1_sym_strides:      (3072, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (3072, 768)
mat2_sym_strides:      (1, 3072)"]
	140067041706624 -> 140067041706336
	140066806142032 [label="
 (768)" fillcolor=lightblue]
	140066806142032 -> 140067041706624
	140067041706624 [label=AccumulateGrad]
	140067041588560 -> 140067041706336
	140067041588560 [label="ViewBackward0
-------------------------------
self_sym_sizes: (24, 511, 3072)"]
	140067041590576 -> 140067041588560
	140067041590576 [label="GeluBackward0
---------------------------
approximate:           none
self       : [saved tensor]"]
	140066688173584 -> 140067041590576
	140066688173584 [label="ViewBackward0
-----------------------------
self_sym_sizes: (12264, 3072)"]
	140066688187312 -> 140066688173584
	140066688187312 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :   (12264, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (768, 3072)
mat2_sym_strides:       (1, 768)"]
	140066688175360 -> 140066688187312
	140066806143152 [label="
 (3072)" fillcolor=lightblue]
	140066806143152 -> 140066688175360
	140066688175360 [label=AccumulateGrad]
	140066688186400 -> 140066688187312
	140066688186400 [label="ViewBackward0
------------------------------
self_sym_sizes: (24, 511, 768)"]
	140067041707392 -> 140066688186400
	140067041707392 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140066688186064 -> 140067041707392
	140066688186064 [label="AddBackward0
------------
alpha: 1"]
	140066688184864 -> 140066688186064
	140066688184864 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140066688185440 -> 140066688184864
	140066688185440 [label="ViewBackward0
----------------------------
self_sym_sizes: (12264, 768)"]
	140066688180784 -> 140066688185440
	140066688180784 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :   (12264, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140066688183616 -> 140066688180784
	140066806142752 [label="
 (768)" fillcolor=lightblue]
	140066806142752 -> 140066688183616
	140066688183616 [label=AccumulateGrad]
	140066688181360 -> 140066688180784
	140066688181360 [label="ViewBackward0
------------------------------
self_sym_sizes: (24, 511, 768)"]
	140066688181696 -> 140066688181360
	140066688181696 [label="ViewBackward0
---------------------------------
self_sym_sizes: (24, 511, 12, 64)"]
	140066688183184 -> 140066688181696
	140066688183184 [label=CloneBackward0]
	140066688186544 -> 140066688183184
	140066688186544 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140066688183040 -> 140066688186544
	140066688183040 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (288, 511, 64)"]
	140066688181552 -> 140066688183040
	140066688181552 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140066688181984 -> 140066688181552
	140066688181984 [label="ViewBackward0
----------------------------------
self_sym_sizes: (24, 12, 511, 511)"]
	140066688183232 -> 140066688181984
	140066688183232 [label="ExpandBackward0
----------------------------------
self_sym_sizes: (24, 12, 511, 511)"]
	140066688182272 -> 140066688183232
	140066688182272 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140066688188320 -> 140066688182272
	140066688188320 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140066688181264 -> 140066688188320
	140066688181264 [label="AddBackward0
------------
alpha: 1"]
	140066688183472 -> 140066688181264
	140066688183472 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140066688181408 -> 140066688183472
	140066688181408 [label="UnsafeViewBackward0
-------------------------------
self_sym_sizes: (288, 511, 511)"]
	140066688182560 -> 140066688181408
	140066688182560 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140066688188272 -> 140066688182560
	140066688188272 [label="UnsafeViewBackward0
---------------------------------
self_sym_sizes: (24, 12, 511, 64)"]
	140066688182416 -> 140066688188272
	140066688182416 [label=CloneBackward0]
	140066688183280 -> 140066688182416
	140066688183280 [label="ExpandBackward0
---------------------------------
self_sym_sizes: (24, 12, 511, 64)"]
	140066688188368 -> 140066688183280
	140066688188368 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140066688181888 -> 140066688188368
	140066688181888 [label="ViewBackward0
------------------------------
self_sym_sizes: (24, 511, 768)"]
	140067018590576 -> 140066688181888
	140067018590576 [label="ViewBackward0
----------------------------
self_sym_sizes: (12264, 768)"]
	140067018590048 -> 140067018590576
	140067018590048 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :   (12264, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067018590816 -> 140067018590048
	140066806142272 [label="
 (768)" fillcolor=lightblue]
	140066806142272 -> 140067018590816
	140067018590816 [label=AccumulateGrad]
	140067018590720 -> 140067018590048
	140067018590720 [label="ViewBackward0
------------------------------
self_sym_sizes: (24, 511, 768)"]
	140066688184000 -> 140067018590720
	140066688184000 [label="ViewBackward0
--------------------------------
self_sym_sizes: (3, 8, 511, 768)"]
	140067018590864 -> 140066688184000
	140067018590864 [label="AsStridedBackward0
-----------------------------------------
size          :          (3, 8, 511, 768)
storage_offset:                         0
stride        : (3139584, 392448, 768, 1)"]
	140067018590912 -> 140067018590864
	140067018590912 [label=CopySlices]
	140067041584864 -> 140067018590912
	140067018591440 -> 140067018590912
	140067018591440 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 8, 768)
start         :                   0
step          :                   1"]
	140067018591248 -> 140067018591440
	140067018591248 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:         (3, 9, 768)
start         :                   1
step          :                   1"]
	140067018591488 -> 140067018591248
	140067018591488 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:         (3, 9, 768)
start         :                   0
step          :                   1"]
	140067041710896 -> 140067018591488
	140067018590192 -> 140067018590048
	140067018590192 [label=TBackward0]
	140067018590672 -> 140067018590192
	140066806142192 [label="
 (768, 768)" fillcolor=lightblue]
	140066806142192 -> 140067018590672
	140067018590672 [label=AccumulateGrad]
	140066688180976 -> 140066688182560
	140066688180976 [label="UnsafeViewBackward0
---------------------------------
self_sym_sizes: (24, 12, 64, 511)"]
	140066688181744 -> 140066688180976
	140066688181744 [label=CloneBackward0]
	140066688188224 -> 140066688181744
	140066688188224 [label="ExpandBackward0
---------------------------------
self_sym_sizes: (24, 12, 64, 511)"]
	140067018590288 -> 140066688188224
	140067018590288 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551615
dim1: 18446744073709551614"]
	140067018591344 -> 140067018590288
	140067018591344 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067018591536 -> 140067018591344
	140067018591536 [label="ViewBackward0
------------------------------
self_sym_sizes: (24, 511, 768)"]
	140067018590960 -> 140067018591536
	140067018590960 [label="ViewBackward0
----------------------------
self_sym_sizes: (12264, 768)"]
	140067018590384 -> 140067018590960
	140067018590384 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :   (12264, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067018591776 -> 140067018590384
	140066806142432 [label="
 (768)" fillcolor=lightblue]
	140066806142432 -> 140067018591776
	140067018591776 [label=AccumulateGrad]
	140067018591824 -> 140067018590384
	140067018591824 [label="ViewBackward0
------------------------------
self_sym_sizes: (24, 511, 768)"]
	140066688184000 -> 140067018591824
	140067018590240 -> 140067018590384
	140067018590240 [label=TBackward0]
	140067018591632 -> 140067018590240
	140066806142352 [label="
 (768, 768)" fillcolor=lightblue]
	140066806142352 -> 140067018591632
	140067018591632 [label=AccumulateGrad]
	140066688182896 -> 140066688181552
	140066688182896 [label="UnsafeViewBackward0
---------------------------------
self_sym_sizes: (24, 12, 511, 64)"]
	140066688182608 -> 140066688182896
	140066688182608 [label=CloneBackward0]
	140066688181648 -> 140066688182608
	140066688181648 [label="ExpandBackward0
---------------------------------
self_sym_sizes: (24, 12, 511, 64)"]
	140066688180880 -> 140066688181648
	140066688180880 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140066688181840 -> 140066688180880
	140066688181840 [label="ViewBackward0
------------------------------
self_sym_sizes: (24, 511, 768)"]
	140066688182848 -> 140066688181840
	140066688182848 [label="ViewBackward0
----------------------------
self_sym_sizes: (12264, 768)"]
	140067018591008 -> 140066688182848
	140067018591008 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :   (12264, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067018591728 -> 140067018591008
	140066806142592 [label="
 (768)" fillcolor=lightblue]
	140066806142592 -> 140067018591728
	140067018591728 [label=AccumulateGrad]
	140067018590768 -> 140067018591008
	140067018590768 [label="ViewBackward0
------------------------------
self_sym_sizes: (24, 511, 768)"]
	140066688184000 -> 140067018590768
	140067018591200 -> 140067018591008
	140067018591200 [label=TBackward0]
	140067018591584 -> 140067018591200
	140066806142512 [label="
 (768, 768)" fillcolor=lightblue]
	140066806142512 -> 140067018591584
	140067018591584 [label=AccumulateGrad]
	140066688184432 -> 140066688180784
	140066688184432 [label=TBackward0]
	140066688181456 -> 140066688184432
	140066806142672 [label="
 (768, 768)" fillcolor=lightblue]
	140066806142672 -> 140066688181456
	140066688181456 [label=AccumulateGrad]
	140066688184000 -> 140066688186064
	140066688186736 -> 140067041707392
	140066806142912 [label="
 (768)" fillcolor=lightblue]
	140066806142912 -> 140066688186736
	140066688186736 [label=AccumulateGrad]
	140066688185824 -> 140067041707392
	140066806142832 [label="
 (768)" fillcolor=lightblue]
	140066806142832 -> 140066688185824
	140066688185824 [label=AccumulateGrad]
	140066688174016 -> 140066688187312
	140066688174016 [label=TBackward0]
	140066688184096 -> 140066688174016
	140066806143072 [label="
 (3072, 768)" fillcolor=lightblue]
	140066806143072 -> 140066688184096
	140066688184096 [label=AccumulateGrad]
	140067041591248 -> 140067041706336
	140067041591248 [label=TBackward0]
	140066688174544 -> 140067041591248
	140066806142992 [label="
 (768, 3072)" fillcolor=lightblue]
	140066806142992 -> 140066688174544
	140066688174544 [label=AccumulateGrad]
	140067041707392 -> 140067041709312
	140067041707632 -> 140067041707920
	140066806143392 [label="
 (768)" fillcolor=lightblue]
	140066806143392 -> 140067041707632
	140067041707632 [label=AccumulateGrad]
	140067041711856 -> 140067041707920
	140066806143232 [label="
 (768)" fillcolor=lightblue]
	140066806143232 -> 140067041711856
	140067041711856 [label=AccumulateGrad]
	140067041713440 -> 140067041712624
	140067041713440 [label=TBackward0]
	140067041711952 -> 140067041713440
	140066770653008 [label="
 (768, 768)" fillcolor=lightblue]
	140066770653008 -> 140067041711952
	140067041711952 [label=AccumulateGrad]
	140067041714256 -> 140067041713728
	140067041714256 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 64, 9)"]
	140067041713968 -> 140067041714256
	140067041713968 [label=CloneBackward0]
	140067041712720 -> 140067041713968
	140067041712720 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 64, 9)"]
	140067041712864 -> 140067041712720
	140067041712864 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551615
dim1: 18446744073709551614"]
	140067041710752 -> 140067041712864
	140067041710752 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041707872 -> 140067041710752
	140067041707872 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 9, 768)"]
	140067041709072 -> 140067041707872
	140067041709072 [label="ViewBackward0
-------------------------
self_sym_sizes: (27, 768)"]
	140067041706912 -> 140067041709072
	140067041706912 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (27, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041587216 -> 140067041706912
	140066770653728 [label="
 (768)" fillcolor=lightblue]
	140066770653728 -> 140067041587216
	140067041587216 [label=AccumulateGrad]
	140067041708064 -> 140067041706912
	140067041708064 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 9, 768)"]
	140067041717376 -> 140067041708064
	140067041709888 -> 140067041706912
	140067041709888 [label=TBackward0]
	140067041710368 -> 140067041709888
	140066770653648 [label="
 (768, 768)" fillcolor=lightblue]
	140066770653648 -> 140067041710368
	140067041710368 [label=AccumulateGrad]
	140067041715600 -> 140067041715504
	140067041715600 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 9, 64)"]
	140067041715168 -> 140067041715600
	140067041715168 [label=CloneBackward0]
	140067041714640 -> 140067041715168
	140067041714640 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 9, 64)"]
	140067041714544 -> 140067041714640
	140067041714544 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041713104 -> 140067041714544
	140067041713104 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 9, 768)"]
	140067041711904 -> 140067041713104
	140067041711904 [label="ViewBackward0
-------------------------
self_sym_sizes: (27, 768)"]
	140067041708784 -> 140067041711904
	140067041708784 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (27, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041713392 -> 140067041708784
	140066770653888 [label="
 (768)" fillcolor=lightblue]
	140066770653888 -> 140067041713392
	140067041713392 [label=AccumulateGrad]
	140067041712048 -> 140067041708784
	140067041712048 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 9, 768)"]
	140067041717376 -> 140067041712048
	140067041715024 -> 140067041708784
	140067041715024 [label=TBackward0]
	140066688186640 -> 140067041715024
	140066770653808 [label="
 (768, 768)" fillcolor=lightblue]
	140066770653808 -> 140066688186640
	140066688186640 [label=AccumulateGrad]
	140067041716800 -> 140067041716560
	140067041716800 [label=TBackward0]
	140067041716320 -> 140067041716800
	140066770653968 [label="
 (768, 768)" fillcolor=lightblue]
	140066770653968 -> 140067041716320
	140067041716320 [label=AccumulateGrad]
	140067041717376 -> 140067041717808
	140067041718144 -> 140067041720544
	140066770654208 [label="
 (768)" fillcolor=lightblue]
	140066770654208 -> 140067041718144
	140067041718144 [label=AccumulateGrad]
	140067041717712 -> 140067041720544
	140066770654128 [label="
 (768)" fillcolor=lightblue]
	140066770654128 -> 140067041717712
	140067041717712 [label=AccumulateGrad]
	140067041718720 -> 140067041718480
	140067041718720 [label=TBackward0]
	140067041716896 -> 140067041718720
	140066770654368 [label="
 (3072, 768)" fillcolor=lightblue]
	140066770654368 -> 140067041716896
	140067041716896 [label=AccumulateGrad]
	140067041719392 -> 140067041719200
	140067041719392 [label=TBackward0]
	140067041718816 -> 140067041719392
	140066770654528 [label="
 (768, 3072)" fillcolor=lightblue]
	140066770654528 -> 140067041718816
	140067041718816 [label=AccumulateGrad]
	140067041720544 -> 140067041720160
	140067041719584 -> 140067041719728
	140066770654768 [label="
 (768)" fillcolor=lightblue]
	140066770654768 -> 140067041719584
	140067041719584 [label=AccumulateGrad]
	140067041720640 -> 140067041719728
	140066770654688 [label="
 (768)" fillcolor=lightblue]
	140066770654688 -> 140067041720640
	140067041720640 [label=AccumulateGrad]
	140067041721264 -> 140067041938832
	140067041721264 [label=CloneBackward0]
	140067041720112 -> 140067041721264
	140067041720112 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 8, 768)
start         :                   0
step          :                   1"]
	140067041721120 -> 140067041720112
	140067041721120 [label="SelectBackward0
--------------------------------
dim           :                2
index         :                1
self_sym_sizes: (3, 8, 511, 768)"]
	140067041718096 -> 140067041721120
	140067041718096 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:    (3, 8, 511, 768)
start         :                   0
step          :                   1"]
	140067041719248 -> 140067041718096
	140067041719248 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:    (3, 8, 511, 768)
start         :                   0
step          :                   1"]
	140067041715984 -> 140067041719248
	140067041715984 [label="ViewBackward0
------------------------------
self_sym_sizes: (24, 511, 768)"]
	140067041716176 -> 140067041715984
	140067041716176 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140067041718000 -> 140067041716176
	140067041718000 [label="AddBackward0
------------
alpha: 1"]
	140067041716656 -> 140067041718000
	140067041716656 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041720304 -> 140067041716656
	140067041720304 [label="ViewBackward0
----------------------------
self_sym_sizes: (12264, 768)"]
	140067041711136 -> 140067041720304
	140067041711136 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :  (12264, 3072)
mat1_sym_strides:      (3072, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (3072, 768)
mat2_sym_strides:      (1, 3072)"]
	140067041713056 -> 140067041711136
	140066806143312 [label="
 (768)" fillcolor=lightblue]
	140066806143312 -> 140067041713056
	140067041713056 [label=AccumulateGrad]
	140067041713824 -> 140067041711136
	140067041713824 [label="ViewBackward0
-------------------------------
self_sym_sizes: (24, 511, 3072)"]
	140067041718528 -> 140067041713824
	140067041718528 [label="GeluBackward0
---------------------------
approximate:           none
self       : [saved tensor]"]
	140066688185920 -> 140067041718528
	140066688185920 [label="ViewBackward0
-----------------------------
self_sym_sizes: (12264, 3072)"]
	140066688184384 -> 140066688185920
	140066688184384 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :   (12264, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (768, 3072)
mat2_sym_strides:       (1, 768)"]
	140066688186832 -> 140066688184384
	140066806144432 [label="
 (3072)" fillcolor=lightblue]
	140066806144432 -> 140066688186832
	140066688186832 [label=AccumulateGrad]
	140066688183712 -> 140066688184384
	140066688183712 [label="ViewBackward0
------------------------------
self_sym_sizes: (24, 511, 768)"]
	140067041715360 -> 140066688183712
	140067041715360 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140066688183376 -> 140067041715360
	140066688183376 [label="AddBackward0
------------
alpha: 1"]
	140066688180928 -> 140066688183376
	140066688180928 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067018590528 -> 140066688180928
	140067018590528 [label="ViewBackward0
----------------------------
self_sym_sizes: (12264, 768)"]
	140067018591680 -> 140067018590528
	140067018591680 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :   (12264, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067018591920 -> 140067018591680
	140066806144032 [label="
 (768)" fillcolor=lightblue]
	140066806144032 -> 140067018591920
	140067018591920 [label=AccumulateGrad]
	140067018591296 -> 140067018591680
	140067018591296 [label="ViewBackward0
------------------------------
self_sym_sizes: (24, 511, 768)"]
	140067018592160 -> 140067018591296
	140067018592160 [label="ViewBackward0
---------------------------------
self_sym_sizes: (24, 511, 12, 64)"]
	140067018592064 -> 140067018592160
	140067018592064 [label=CloneBackward0]
	140067018592112 -> 140067018592064
	140067018592112 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067018592400 -> 140067018592112
	140067018592400 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (288, 511, 64)"]
	140067018592448 -> 140067018592400
	140067018592448 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140067018592496 -> 140067018592448
	140067018592496 [label="ViewBackward0
----------------------------------
self_sym_sizes: (24, 12, 511, 511)"]
	140067018592592 -> 140067018592496
	140067018592592 [label="ExpandBackward0
----------------------------------
self_sym_sizes: (24, 12, 511, 511)"]
	140067018592544 -> 140067018592592
	140067018592544 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067018593072 -> 140067018592544
	140067018593072 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140067018592928 -> 140067018593072
	140067018592928 [label="AddBackward0
------------
alpha: 1"]
	140067018591104 -> 140067018592928
	140067018591104 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067018593264 -> 140067018591104
	140067018593264 [label="UnsafeViewBackward0
-------------------------------
self_sym_sizes: (288, 511, 511)"]
	140067018593696 -> 140067018593264
	140067018593696 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140067018593168 -> 140067018593696
	140067018593168 [label="UnsafeViewBackward0
---------------------------------
self_sym_sizes: (24, 12, 511, 64)"]
	140067018593504 -> 140067018593168
	140067018593504 [label=CloneBackward0]
	140067018593600 -> 140067018593504
	140067018593600 [label="ExpandBackward0
---------------------------------
self_sym_sizes: (24, 12, 511, 64)"]
	140067018593888 -> 140067018593600
	140067018593888 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067018593024 -> 140067018593888
	140067018593024 [label="ViewBackward0
------------------------------
self_sym_sizes: (24, 511, 768)"]
	140067018593984 -> 140067018593024
	140067018593984 [label="ViewBackward0
----------------------------
self_sym_sizes: (12264, 768)"]
	140067018593936 -> 140067018593984
	140067018593936 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :   (12264, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067018594176 -> 140067018593936
	140066806143552 [label="
 (768)" fillcolor=lightblue]
	140066806143552 -> 140067018594176
	140067018594176 [label=AccumulateGrad]
	140067018594416 -> 140067018593936
	140067018594416 [label="ViewBackward0
------------------------------
self_sym_sizes: (24, 511, 768)"]
	140066688182752 -> 140067018594416
	140066688182752 [label="ViewBackward0
--------------------------------
self_sym_sizes: (3, 8, 511, 768)"]
	140067018594272 -> 140066688182752
	140067018594272 [label="AsStridedBackward0
-----------------------------------------
size          :          (3, 8, 511, 768)
storage_offset:                         0
stride        : (3139584, 392448, 768, 1)"]
	140067018588560 -> 140067018594272
	140067018588560 [label=CopySlices]
	140067041707920 -> 140067018588560
	140067018589040 -> 140067018588560
	140067018589040 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 8, 768)
start         :                   0
step          :                   1"]
	140067018588800 -> 140067018589040
	140067018588800 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:         (3, 9, 768)
start         :                   1
step          :                   1"]
	140067018588896 -> 140067018588800
	140067018588896 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:         (3, 9, 768)
start         :                   0
step          :                   1"]
	140067041719728 -> 140067018588896
	140067018593792 -> 140067018593936
	140067018593792 [label=TBackward0]
	140067018594032 -> 140067018593792
	140066806143472 [label="
 (768, 768)" fillcolor=lightblue]
	140066806143472 -> 140067018594032
	140067018594032 [label=AccumulateGrad]
	140067018593120 -> 140067018593696
	140067018593120 [label="UnsafeViewBackward0
---------------------------------
self_sym_sizes: (24, 12, 64, 511)"]
	140067018593312 -> 140067018593120
	140067018593312 [label=CloneBackward0]
	140067018593744 -> 140067018593312
	140067018593744 [label="ExpandBackward0
---------------------------------
self_sym_sizes: (24, 12, 64, 511)"]
	140067018593648 -> 140067018593744
	140067018593648 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551615
dim1: 18446744073709551614"]
	140067018588416 -> 140067018593648
	140067018588416 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067018589088 -> 140067018588416
	140067018589088 [label="ViewBackward0
------------------------------
self_sym_sizes: (24, 511, 768)"]
	140067018594128 -> 140067018589088
	140067018594128 [label="ViewBackward0
----------------------------
self_sym_sizes: (12264, 768)"]
	140067018588464 -> 140067018594128
	140067018588464 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :   (12264, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067018589232 -> 140067018588464
	140066806143712 [label="
 (768)" fillcolor=lightblue]
	140066806143712 -> 140067018589232
	140067018589232 [label=AccumulateGrad]
	140067018589616 -> 140067018588464
	140067018589616 [label="ViewBackward0
------------------------------
self_sym_sizes: (24, 511, 768)"]
	140066688182752 -> 140067018589616
	140067018593840 -> 140067018588464
	140067018593840 [label=TBackward0]
	140067018588752 -> 140067018593840
	140066806143632 [label="
 (768, 768)" fillcolor=lightblue]
	140066806143632 -> 140067018588752
	140067018588752 [label=AccumulateGrad]
	140067018592688 -> 140067018592448
	140067018592688 [label="UnsafeViewBackward0
---------------------------------
self_sym_sizes: (24, 12, 511, 64)"]
	140067018592784 -> 140067018592688
	140067018592784 [label=CloneBackward0]
	140067018590480 -> 140067018592784
	140067018590480 [label="ExpandBackward0
---------------------------------
self_sym_sizes: (24, 12, 511, 64)"]
	140067018592976 -> 140067018590480
	140067018592976 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067018593456 -> 140067018592976
	140067018593456 [label="ViewBackward0
------------------------------
self_sym_sizes: (24, 511, 768)"]
	140067018588512 -> 140067018593456
	140067018588512 [label="ViewBackward0
----------------------------
self_sym_sizes: (12264, 768)"]
	140067018588272 -> 140067018588512
	140067018588272 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :   (12264, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067018589136 -> 140067018588272
	140066806143872 [label="
 (768)" fillcolor=lightblue]
	140066806143872 -> 140067018589136
	140067018589136 [label=AccumulateGrad]
	140067018594320 -> 140067018588272
	140067018594320 [label="ViewBackward0
------------------------------
self_sym_sizes: (24, 511, 768)"]
	140066688182752 -> 140067018594320
	140067018592736 -> 140067018588272
	140067018592736 [label=TBackward0]
	140067018589424 -> 140067018592736
	140066806143792 [label="
 (768, 768)" fillcolor=lightblue]
	140066806143792 -> 140067018589424
	140067018589424 [label=AccumulateGrad]
	140067018588320 -> 140067018591680
	140067018588320 [label=TBackward0]
	140067018591872 -> 140067018588320
	140066806143952 [label="
 (768, 768)" fillcolor=lightblue]
	140066806143952 -> 140067018591872
	140067018591872 [label=AccumulateGrad]
	140066688182752 -> 140066688183376
	140066688181024 -> 140067041715360
	140066806144192 [label="
 (768)" fillcolor=lightblue]
	140066806144192 -> 140066688181024
	140066688181024 [label=AccumulateGrad]
	140066688181216 -> 140067041715360
	140066806144112 [label="
 (768)" fillcolor=lightblue]
	140066806144112 -> 140066688181216
	140066688181216 [label=AccumulateGrad]
	140066688186352 -> 140066688184384
	140066688186352 [label=TBackward0]
	140066688182176 -> 140066688186352
	140066806144352 [label="
 (3072, 768)" fillcolor=lightblue]
	140066806144352 -> 140066688182176
	140066688182176 [label=AccumulateGrad]
	140067041714496 -> 140067041711136
	140067041714496 [label=TBackward0]
	140066688186448 -> 140067041714496
	140066806144272 [label="
 (768, 3072)" fillcolor=lightblue]
	140066806144272 -> 140066688186448
	140066688186448 [label=AccumulateGrad]
	140067041715360 -> 140067041718000
	140067041715696 -> 140067041716176
	140066806144672 [label="
 (768)" fillcolor=lightblue]
	140066806144672 -> 140067041715696
	140067041715696 [label=AccumulateGrad]
	140067041720736 -> 140067041716176
	140066806144512 [label="
 (768)" fillcolor=lightblue]
	140066806144512 -> 140067041720736
	140067041720736 [label=AccumulateGrad]
	140067041722224 -> 140067041721552
	140067041722224 [label=TBackward0]
	140067041720832 -> 140067041722224
	140066770654288 [label="
 (768, 768)" fillcolor=lightblue]
	140066770654288 -> 140067041720832
	140067041720832 [label=AccumulateGrad]
	140067041935856 -> 140067041935424
	140067041935856 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 64, 9)"]
	140067054822368 -> 140067041935856
	140067054822368 [label=CloneBackward0]
	140067041721600 -> 140067054822368
	140067041721600 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 64, 9)"]
	140067041721792 -> 140067041721600
	140067041721792 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551615
dim1: 18446744073709551614"]
	140067041719536 -> 140067041721792
	140067041719536 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041716080 -> 140067041719536
	140067041716080 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 9, 768)"]
	140067041717568 -> 140067041716080
	140067041717568 [label="ViewBackward0
-------------------------
self_sym_sizes: (27, 768)"]
	140067041714880 -> 140067041717568
	140067041714880 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (27, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140066688183568 -> 140067041714880
	140066770655008 [label="
 (768)" fillcolor=lightblue]
	140066770655008 -> 140066688183568
	140066688183568 [label=AccumulateGrad]
	140066688184144 -> 140067041714880
	140066688184144 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 9, 768)"]
	140067041938832 -> 140066688184144
	140066688182944 -> 140067041714880
	140066688182944 [label=TBackward0]
	140067041716224 -> 140066688182944
	140066770654928 [label="
 (768, 768)" fillcolor=lightblue]
	140066770654928 -> 140067041716224
	140067041716224 [label=AccumulateGrad]
	140067041937200 -> 140067041937152
	140067041937200 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 9, 64)"]
	140067041936864 -> 140067041937200
	140067041936864 [label=CloneBackward0]
	140067041936288 -> 140067041936864
	140067041936288 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 9, 64)"]
	140067054819152 -> 140067041936288
	140067054819152 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041936576 -> 140067054819152
	140067041936576 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 9, 768)"]
	140067041936192 -> 140067041936576
	140067041936192 [label="ViewBackward0
-------------------------
self_sym_sizes: (27, 768)"]
	140067041719824 -> 140067041936192
	140067041719824 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (27, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041718336 -> 140067041719824
	140066770655168 [label="
 (768)" fillcolor=lightblue]
	140066770655168 -> 140067041718336
	140067041718336 [label=AccumulateGrad]
	140067041716992 -> 140067041719824
	140067041716992 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 9, 768)"]
	140067041938832 -> 140067041716992
	140067041721984 -> 140067041719824
	140067041721984 [label=TBackward0]
	140067041714928 -> 140067041721984
	140066770655088 [label="
 (768, 768)" fillcolor=lightblue]
	140066770655088 -> 140067041714928
	140067041714928 [label=AccumulateGrad]
	140067041938400 -> 140067041938112
	140067041938400 [label=TBackward0]
	140067041937872 -> 140067041938400
	140066770655248 [label="
 (768, 768)" fillcolor=lightblue]
	140066770655248 -> 140067041937872
	140067041937872 [label=AccumulateGrad]
	140067041938832 -> 140067041939024
	140067041939216 -> 140067041941424
	140066770655488 [label="
 (768)" fillcolor=lightblue]
	140066770655488 -> 140067041939216
	140067041939216 [label=AccumulateGrad]
	140067041938976 -> 140067041941424
	140066770655408 [label="
 (768)" fillcolor=lightblue]
	140066770655408 -> 140067041938976
	140067041938976 [label=AccumulateGrad]
	140067041939840 -> 140067041939600
	140067041939840 [label=TBackward0]
	140067041938496 -> 140067041939840
	140066770655648 [label="
 (3072, 768)" fillcolor=lightblue]
	140066770655648 -> 140067041938496
	140067041938496 [label=AccumulateGrad]
	140067041940512 -> 140067041940176
	140067041940512 [label=TBackward0]
	140067041939984 -> 140067041940512
	140066770655808 [label="
 (768, 3072)" fillcolor=lightblue]
	140066770655808 -> 140067041939984
	140067041939984 [label=AccumulateGrad]
	140067041941424 -> 140067041941136
	140067041940752 -> 140067041940800
	140066770656048 [label="
 (768)" fillcolor=lightblue]
	140066770656048 -> 140067041940752
	140067041940752 [label=AccumulateGrad]
	140067041941568 -> 140067041940800
	140066770655968 [label="
 (768)" fillcolor=lightblue]
	140066770655968 -> 140067041941568
	140067041941568 [label=AccumulateGrad]
	140067041942096 -> 140067041946368
	140067041942096 [label=CloneBackward0]
	140067041941088 -> 140067041942096
	140067041941088 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 8, 768)
start         :                   0
step          :                   1"]
	140067041941952 -> 140067041941088
	140067041941952 [label="SelectBackward0
--------------------------------
dim           :                2
index         :                1
self_sym_sizes: (3, 8, 511, 768)"]
	140067041939168 -> 140067041941952
	140067041939168 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:    (3, 8, 511, 768)
start         :                   0
step          :                   1"]
	140067041940320 -> 140067041939168
	140067041940320 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:    (3, 8, 511, 768)
start         :                   0
step          :                   1"]
	140067041937536 -> 140067041940320
	140067041937536 [label="ViewBackward0
------------------------------
self_sym_sizes: (24, 511, 768)"]
	140067041937680 -> 140067041937536
	140067041937680 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140067041939072 -> 140067041937680
	140067041939072 [label="AddBackward0
------------
alpha: 1"]
	140067041938256 -> 140067041939072
	140067041938256 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140066688182128 -> 140067041938256
	140066688182128 [label="ViewBackward0
----------------------------
self_sym_sizes: (12264, 768)"]
	140067041936096 -> 140066688182128
	140067041936096 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :  (12264, 3072)
mat1_sym_strides:      (3072, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (3072, 768)
mat2_sym_strides:      (1, 3072)"]
	140067041939696 -> 140067041936096
	140066806144592 [label="
 (768)" fillcolor=lightblue]
	140066806144592 -> 140067041939696
	140067041939696 [label=AccumulateGrad]
	140067041935520 -> 140067041936096
	140067041935520 [label="ViewBackward0
-------------------------------
self_sym_sizes: (24, 511, 3072)"]
	140067041722176 -> 140067041935520
	140067041722176 [label="GeluBackward0
---------------------------
approximate:           none
self       : [saved tensor]"]
	140067041709744 -> 140067041722176
	140067041709744 [label="ViewBackward0
-----------------------------
self_sym_sizes: (12264, 3072)"]
	140067018591056 -> 140067041709744
	140067018591056 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :   (12264, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (768, 3072)
mat2_sym_strides:       (1, 768)"]
	140067018591392 -> 140067018591056
	140066806145712 [label="
 (3072)" fillcolor=lightblue]
	140066806145712 -> 140067018591392
	140067018591392 [label=AccumulateGrad]
	140067018591968 -> 140067018591056
	140067018591968 [label="ViewBackward0
------------------------------
self_sym_sizes: (24, 511, 768)"]
	140067041937056 -> 140067018591968
	140067041937056 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140067018592880 -> 140067041937056
	140067018592880 [label="AddBackward0
------------
alpha: 1"]
	140067018593552 -> 140067018592880
	140067018593552 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067018593216 -> 140067018593552
	140067018593216 [label="ViewBackward0
----------------------------
self_sym_sizes: (12264, 768)"]
	140067018589184 -> 140067018593216
	140067018589184 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :   (12264, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067018589712 -> 140067018589184
	140066806145312 [label="
 (768)" fillcolor=lightblue]
	140066806145312 -> 140067018589712
	140067018589712 [label=AccumulateGrad]
	140067018588224 -> 140067018589184
	140067018588224 [label="ViewBackward0
------------------------------
self_sym_sizes: (24, 511, 768)"]
	140067018589520 -> 140067018588224
	140067018589520 [label="ViewBackward0
---------------------------------
self_sym_sizes: (24, 511, 12, 64)"]
	140067018589472 -> 140067018589520
	140067018589472 [label=CloneBackward0]
	140067018592256 -> 140067018589472
	140067018592256 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067018589568 -> 140067018592256
	140067018589568 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (288, 511, 64)"]
	140067018590432 -> 140067018589568
	140067018590432 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140067018589328 -> 140067018590432
	140067018589328 [label="ViewBackward0
----------------------------------
self_sym_sizes: (24, 12, 511, 511)"]
	140067018590144 -> 140067018589328
	140067018590144 [label="ExpandBackward0
----------------------------------
self_sym_sizes: (24, 12, 511, 511)"]
	140067018589904 -> 140067018590144
	140067018589904 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067018594464 -> 140067018589904
	140067018594464 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140067018595088 -> 140067018594464
	140067018595088 [label="AddBackward0
------------
alpha: 1"]
	140067018590096 -> 140067018595088
	140067018590096 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067018594992 -> 140067018590096
	140067018594992 [label="UnsafeViewBackward0
-------------------------------
self_sym_sizes: (288, 511, 511)"]
	140067018594752 -> 140067018594992
	140067018594752 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140067018594368 -> 140067018594752
	140067018594368 [label="UnsafeViewBackward0
---------------------------------
self_sym_sizes: (24, 12, 511, 64)"]
	140067018595184 -> 140067018594368
	140067018595184 [label=CloneBackward0]
	140067018594896 -> 140067018595184
	140067018594896 [label="ExpandBackward0
---------------------------------
self_sym_sizes: (24, 12, 511, 64)"]
	140067018594608 -> 140067018594896
	140067018594608 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067018595664 -> 140067018594608
	140067018595664 [label="ViewBackward0
------------------------------
self_sym_sizes: (24, 511, 768)"]
	140067018594704 -> 140067018595664
	140067018594704 [label="ViewBackward0
----------------------------
self_sym_sizes: (12264, 768)"]
	140067018595712 -> 140067018594704
	140067018595712 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :   (12264, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067018595616 -> 140067018595712
	140066806144832 [label="
 (768)" fillcolor=lightblue]
	140066806144832 -> 140067018595616
	140067018595616 [label=AccumulateGrad]
	140067018593408 -> 140067018595712
	140067018593408 [label="ViewBackward0
------------------------------
self_sym_sizes: (24, 511, 768)"]
	140067018594080 -> 140067018593408
	140067018594080 [label="ViewBackward0
--------------------------------
self_sym_sizes: (3, 8, 511, 768)"]
	140067018597104 -> 140067018594080
	140067018597104 [label="AsStridedBackward0
-----------------------------------------
size          :          (3, 8, 511, 768)
storage_offset:                         0
stride        : (3139584, 392448, 768, 1)"]
	140067018596144 -> 140067018597104
	140067018596144 [label=CopySlices]
	140067041716176 -> 140067018596144
	140067018596048 -> 140067018596144
	140067018596048 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 8, 768)
start         :                   0
step          :                   1"]
	140067018596096 -> 140067018596048
	140067018596096 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:         (3, 9, 768)
start         :                   1
step          :                   1"]
	140067018597728 -> 140067018596096
	140067018597728 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:         (3, 9, 768)
start         :                   0
step          :                   1"]
	140067041940800 -> 140067018597728
	140067018595328 -> 140067018595712
	140067018595328 [label=TBackward0]
	140067018595904 -> 140067018595328
	140066806144752 [label="
 (768, 768)" fillcolor=lightblue]
	140066806144752 -> 140067018595904
	140067018595904 [label=AccumulateGrad]
	140067018595376 -> 140067018594752
	140067018595376 [label="UnsafeViewBackward0
---------------------------------
self_sym_sizes: (24, 12, 64, 511)"]
	140067018595040 -> 140067018595376
	140067018595040 [label=CloneBackward0]
	140067018594944 -> 140067018595040
	140067018594944 [label="ExpandBackward0
---------------------------------
self_sym_sizes: (24, 12, 64, 511)"]
	140067018595472 -> 140067018594944
	140067018595472 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551615
dim1: 18446744073709551614"]
	140067018593360 -> 140067018595472
	140067018593360 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067018597344 -> 140067018593360
	140067018597344 [label="ViewBackward0
------------------------------
self_sym_sizes: (24, 511, 768)"]
	140067018595856 -> 140067018597344
	140067018595856 [label="ViewBackward0
----------------------------
self_sym_sizes: (12264, 768)"]
	140067018596000 -> 140067018595856
	140067018596000 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :   (12264, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067018597440 -> 140067018596000
	140066806144992 [label="
 (768)" fillcolor=lightblue]
	140066806144992 -> 140067018597440
	140067018597440 [label=AccumulateGrad]
	140067018596432 -> 140067018596000
	140067018596432 [label="ViewBackward0
------------------------------
self_sym_sizes: (24, 511, 768)"]
	140067018594080 -> 140067018596432
	140067018595520 -> 140067018596000
	140067018595520 [label=TBackward0]
	140067018597920 -> 140067018595520
	140066806144912 [label="
 (768, 768)" fillcolor=lightblue]
	140066806144912 -> 140067018597920
	140067018597920 [label=AccumulateGrad]
	140067018589280 -> 140067018590432
	140067018589280 [label="UnsafeViewBackward0
---------------------------------
self_sym_sizes: (24, 12, 511, 64)"]
	140067018589760 -> 140067018589280
	140067018589760 [label=CloneBackward0]
	140067018594512 -> 140067018589760
	140067018594512 [label="ExpandBackward0
---------------------------------
self_sym_sizes: (24, 12, 511, 64)"]
	140067018595136 -> 140067018594512
	140067018595136 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067018594800 -> 140067018595136
	140067018594800 [label="ViewBackward0
------------------------------
self_sym_sizes: (24, 511, 768)"]
	140067018595808 -> 140067018594800
	140067018595808 [label="ViewBackward0
----------------------------
self_sym_sizes: (12264, 768)"]
	140067018597200 -> 140067018595808
	140067018597200 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :   (12264, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067018597488 -> 140067018597200
	140066806145152 [label="
 (768)" fillcolor=lightblue]
	140066806145152 -> 140067018597488
	140067018597488 [label=AccumulateGrad]
	140067018595952 -> 140067018597200
	140067018595952 [label="ViewBackward0
------------------------------
self_sym_sizes: (24, 511, 768)"]
	140067018594080 -> 140067018595952
	140067018589952 -> 140067018597200
	140067018589952 [label=TBackward0]
	140067018597056 -> 140067018589952
	140066806145072 [label="
 (768, 768)" fillcolor=lightblue]
	140066806145072 -> 140067018597056
	140067018597056 [label=AccumulateGrad]
	140067018588368 -> 140067018589184
	140067018588368 [label=TBackward0]
	140067018588656 -> 140067018588368
	140066806145232 [label="
 (768, 768)" fillcolor=lightblue]
	140066806145232 -> 140067018588656
	140067018588656 [label=AccumulateGrad]
	140067018594080 -> 140067018592880
	140067018592016 -> 140067041937056
	140066806145472 [label="
 (768)" fillcolor=lightblue]
	140066806145472 -> 140067018592016
	140067018592016 [label=AccumulateGrad]
	140067018592640 -> 140067041937056
	140066806145392 [label="
 (768)" fillcolor=lightblue]
	140066806145392 -> 140067018592640
	140067018592640 [label=AccumulateGrad]
	140067018591152 -> 140067018591056
	140067018591152 [label=TBackward0]
	140067018588704 -> 140067018591152
	140066806145632 [label="
 (3072, 768)" fillcolor=lightblue]
	140066806145632 -> 140067018588704
	140067018588704 [label=AccumulateGrad]
	140067041941376 -> 140067041936096
	140067041941376 [label=TBackward0]
	140067041719056 -> 140067041941376
	140066806145552 [label="
 (768, 3072)" fillcolor=lightblue]
	140066806145552 -> 140067041719056
	140067041719056 [label=AccumulateGrad]
	140067041937056 -> 140067041939072
	140067041937248 -> 140067041937680
	140066806145952 [label="
 (768)" fillcolor=lightblue]
	140066806145952 -> 140067041937248
	140067041937248 [label=AccumulateGrad]
	140067041941616 -> 140067041937680
	140066806145792 [label="
 (768)" fillcolor=lightblue]
	140066806145792 -> 140067041941616
	140067041941616 [label=AccumulateGrad]
	140067041943296 -> 140067041942480
	140067041943296 [label=TBackward0]
	140067041941712 -> 140067041943296
	140066770656128 [label="
 (768, 768)" fillcolor=lightblue]
	140066770656128 -> 140067041941712
	140067041941712 [label=AccumulateGrad]
	140067041943872 -> 140067041943584
	140067041943872 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 64, 9)"]
	140067041943680 -> 140067041943872
	140067041943680 [label=CloneBackward0]
	140067041942624 -> 140067041943680
	140067041942624 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 64, 9)"]
	140067041942720 -> 140067041942624
	140067041942720 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551615
dim1: 18446744073709551614"]
	140067041940608 -> 140067041942720
	140067041940608 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041937728 -> 140067041940608
	140067041937728 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 9, 768)"]
	140067041938880 -> 140067041937728
	140067041938880 [label="ViewBackward0
-------------------------
self_sym_sizes: (27, 768)"]
	140067041936432 -> 140067041938880
	140067041936432 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (27, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041720928 -> 140067041936432
	140066770656368 [label="
 (768)" fillcolor=lightblue]
	140066770656368 -> 140067041720928
	140067041720928 [label=AccumulateGrad]
	140067041720784 -> 140067041936432
	140067041720784 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 9, 768)"]
	140067041946368 -> 140067041720784
	140067041937824 -> 140067041936432
	140067041937824 [label=TBackward0]
	140067041943248 -> 140067041937824
	140066770656288 [label="
 (768, 768)" fillcolor=lightblue]
	140066770656288 -> 140067041943248
	140067041943248 [label=AccumulateGrad]
	140067041945024 -> 140067041944976
	140067041945024 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 9, 64)"]
	140067041944784 -> 140067041945024
	140067041944784 [label=CloneBackward0]
	140067041944160 -> 140067041944784
	140067041944160 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 9, 64)"]
	140067041944112 -> 140067041944160
	140067041944112 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041943008 -> 140067041944112
	140067041943008 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 9, 768)"]
	140067041941664 -> 140067041943008
	140067041941664 [label="ViewBackward0
-------------------------
self_sym_sizes: (27, 768)"]
	140067041938544 -> 140067041941664
	140067041938544 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (27, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041936672 -> 140067041938544
	140066770656528 [label="
 (768)" fillcolor=lightblue]
	140066770656528 -> 140067041936672
	140067041936672 [label=AccumulateGrad]
	140067041941760 -> 140067041938544
	140067041941760 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 9, 768)"]
	140067041946368 -> 140067041941760
	140067041944544 -> 140067041938544
	140067041944544 [label=TBackward0]
	140067041939504 -> 140067041944544
	140066770656448 [label="
 (768, 768)" fillcolor=lightblue]
	140066770656448 -> 140067041939504
	140067041939504 [label=AccumulateGrad]
	140067041946080 -> 140067041945840
	140067041946080 [label=TBackward0]
	140067041945648 -> 140067041946080
	140066770656608 [label="
 (768, 768)" fillcolor=lightblue]
	140066770656608 -> 140067041945648
	140067041945648 [label=AccumulateGrad]
	140067041946368 -> 140067041946560
	140067041946800 -> 140067041948720
	140066770656848 [label="
 (768)" fillcolor=lightblue]
	140066770656848 -> 140067041946800
	140067041946800 [label=AccumulateGrad]
	140067041946512 -> 140067041948720
	140066770656768 [label="
 (768)" fillcolor=lightblue]
	140066770656768 -> 140067041946512
	140067041946512 [label=AccumulateGrad]
	140067041947472 -> 140067041947184
	140067041947472 [label=TBackward0]
	140067041946128 -> 140067041947472
	140066806152592 [label="
 (3072, 768)" fillcolor=lightblue]
	140066806152592 -> 140067041946128
	140067041946128 [label=AccumulateGrad]
	140067041947952 -> 140067041947760
	140067041947952 [label=TBackward0]
	140067041947520 -> 140067041947952
	140066770657088 [label="
 (768, 3072)" fillcolor=lightblue]
	140066770657088 -> 140067041947520
	140067041947520 [label=AccumulateGrad]
	140067041948720 -> 140067041948384
	140067041948096 -> 140067041948144
	140066770657328 [label="
 (768)" fillcolor=lightblue]
	140066770657328 -> 140067041948096
	140067041948096 [label=AccumulateGrad]
	140067041948768 -> 140067041948144
	140066770657248 [label="
 (768)" fillcolor=lightblue]
	140066770657248 -> 140067041948768
	140067041948768 [label=AccumulateGrad]
	140067041949152 -> 140067041628400
	140067041949152 [label=CloneBackward0]
	140067041948336 -> 140067041949152
	140067041948336 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 8, 768)
start         :                   0
step          :                   1"]
	140067041949104 -> 140067041948336
	140067041949104 [label="SelectBackward0
--------------------------------
dim           :                2
index         :                1
self_sym_sizes: (3, 8, 511, 768)"]
	140067041946752 -> 140067041949104
	140067041946752 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:    (3, 8, 511, 768)
start         :                   0
step          :                   1"]
	140067041947856 -> 140067041946752
	140067041947856 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:    (3, 8, 511, 768)
start         :                   0
step          :                   1"]
	140067041945216 -> 140067041947856
	140067041945216 [label="ViewBackward0
------------------------------
self_sym_sizes: (24, 511, 768)"]
	140067041945408 -> 140067041945216
	140067041945408 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140067041946656 -> 140067041945408
	140067041946656 [label="AddBackward0
------------
alpha: 1"]
	140067041945888 -> 140067041946656
	140067041945888 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041948576 -> 140067041945888
	140067041948576 [label="ViewBackward0
----------------------------
self_sym_sizes: (12264, 768)"]
	140067041940848 -> 140067041948576
	140067041940848 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :  (12264, 3072)
mat1_sym_strides:      (3072, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (3072, 768)
mat2_sym_strides:      (1, 3072)"]
	140067041942912 -> 140067041940848
	140066806145872 [label="
 (768)" fillcolor=lightblue]
	140066806145872 -> 140067041942912
	140067041942912 [label=AccumulateGrad]
	140067041943632 -> 140067041940848
	140067041943632 [label="ViewBackward0
-------------------------------
self_sym_sizes: (24, 511, 3072)"]
	140067041947280 -> 140067041943632
	140067041947280 [label="GeluBackward0
---------------------------
approximate:           none
self       : [saved tensor]"]
	140067018588944 -> 140067041947280
	140067018588944 [label="ViewBackward0
-----------------------------
self_sym_sizes: (12264, 3072)"]
	140067018588848 -> 140067018588944
	140067018588848 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :   (12264, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (768, 3072)
mat2_sym_strides:       (1, 768)"]
	140067018592304 -> 140067018588848
	140066806146992 [label="
 (3072)" fillcolor=lightblue]
	140066806146992 -> 140067018592304
	140067018592304 [label=AccumulateGrad]
	140067018590000 -> 140067018588848
	140067018590000 [label="ViewBackward0
------------------------------
self_sym_sizes: (24, 511, 768)"]
	140067041944880 -> 140067018590000
	140067041944880 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140067018594848 -> 140067041944880
	140067018594848 [label="AddBackward0
------------
alpha: 1"]
	140067018594656 -> 140067018594848
	140067018594656 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067018592352 -> 140067018594656
	140067018592352 [label="ViewBackward0
----------------------------
self_sym_sizes: (12264, 768)"]
	140067018597392 -> 140067018592352
	140067018597392 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :   (12264, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067018597680 -> 140067018597392
	140066806146592 [label="
 (768)" fillcolor=lightblue]
	140066806146592 -> 140067018597680
	140067018597680 [label=AccumulateGrad]
	140067018596192 -> 140067018597392
	140067018596192 [label="ViewBackward0
------------------------------
self_sym_sizes: (24, 511, 768)"]
	140067018596288 -> 140067018596192
	140067018596288 [label="ViewBackward0
---------------------------------
self_sym_sizes: (24, 511, 12, 64)"]
	140067018596384 -> 140067018596288
	140067018596384 [label=CloneBackward0]
	140067018589664 -> 140067018596384
	140067018589664 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067018596528 -> 140067018589664
	140067018596528 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (288, 511, 64)"]
	140067018595424 -> 140067018596528
	140067018595424 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140067018596768 -> 140067018595424
	140067018596768 [label="ViewBackward0
----------------------------------
self_sym_sizes: (24, 12, 511, 511)"]
	140067018598016 -> 140067018596768
	140067018598016 [label="ExpandBackward0
----------------------------------
self_sym_sizes: (24, 12, 511, 511)"]
	140067018599168 -> 140067018598016
	140067018599168 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067018597968 -> 140067018599168
	140067018597968 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140067018598064 -> 140067018597968
	140067018598064 [label="AddBackward0
------------
alpha: 1"]
	140067018595280 -> 140067018598064
	140067018595280 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067018599456 -> 140067018595280
	140067018599456 [label="UnsafeViewBackward0
-------------------------------
self_sym_sizes: (288, 511, 511)"]
	140067018598928 -> 140067018599456
	140067018598928 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140067018599648 -> 140067018598928
	140067018599648 [label="UnsafeViewBackward0
---------------------------------
self_sym_sizes: (24, 12, 511, 64)"]
	140067018594560 -> 140067018599648
	140067018594560 [label=CloneBackward0]
	140067018599888 -> 140067018594560
	140067018599888 [label="ExpandBackward0
---------------------------------
self_sym_sizes: (24, 12, 511, 64)"]
	140067018599264 -> 140067018599888
	140067018599264 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067018599936 -> 140067018599264
	140067018599936 [label="ViewBackward0
------------------------------
self_sym_sizes: (24, 511, 768)"]
	140067018599216 -> 140067018599936
	140067018599216 [label="ViewBackward0
----------------------------
self_sym_sizes: (12264, 768)"]
	140067018598448 -> 140067018599216
	140067018598448 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :   (12264, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067018598544 -> 140067018598448
	140066806146112 [label="
 (768)" fillcolor=lightblue]
	140066806146112 -> 140067018598544
	140067018598544 [label=AccumulateGrad]
	140067018598208 -> 140067018598448
	140067018598208 [label="ViewBackward0
------------------------------
self_sym_sizes: (24, 511, 768)"]
	140067018595760 -> 140067018598208
	140067018595760 [label="ViewBackward0
--------------------------------
self_sym_sizes: (3, 8, 511, 768)"]
	140067018598976 -> 140067018595760
	140067018598976 [label="AsStridedBackward0
-----------------------------------------
size          :          (3, 8, 511, 768)
storage_offset:                         0
stride        : (3139584, 392448, 768, 1)"]
	140067018598304 -> 140067018598976
	140067018598304 [label=CopySlices]
	140067041937680 -> 140067018598304
	140067018598592 -> 140067018598304
	140067018598592 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 8, 768)
start         :                   0
step          :                   1"]
	140067018598736 -> 140067018598592
	140067018598736 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:         (3, 9, 768)
start         :                   1
step          :                   1"]
	140067018599072 -> 140067018598736
	140067018599072 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:         (3, 9, 768)
start         :                   0
step          :                   1"]
	140067041948144 -> 140067018599072
	140067018599360 -> 140067018598448
	140067018599360 [label=TBackward0]
	140067018598256 -> 140067018599360
	140066806146032 [label="
 (768, 768)" fillcolor=lightblue]
	140066806146032 -> 140067018598256
	140067018598256 [label=AccumulateGrad]
	140067018598160 -> 140067018598928
	140067018598160 [label="UnsafeViewBackward0
---------------------------------
self_sym_sizes: (24, 12, 64, 511)"]
	140067018598112 -> 140067018598160
	140067018598112 [label=CloneBackward0]
	140067018599600 -> 140067018598112
	140067018599600 [label="ExpandBackward0
---------------------------------
self_sym_sizes: (24, 12, 64, 511)"]
	140067018598352 -> 140067018599600
	140067018598352 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551615
dim1: 18446744073709551614"]
	140067018596672 -> 140067018598352
	140067018596672 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067018598832 -> 140067018596672
	140067018598832 [label="ViewBackward0
------------------------------
self_sym_sizes: (24, 511, 768)"]
	140067018599744 -> 140067018598832
	140067018599744 [label="ViewBackward0
----------------------------
self_sym_sizes: (12264, 768)"]
	140067018598640 -> 140067018599744
	140067018598640 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :   (12264, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067018600080 -> 140067018598640
	140066806146272 [label="
 (768)" fillcolor=lightblue]
	140066806146272 -> 140067018600080
	140067018600080 [label=AccumulateGrad]
	140067018600464 -> 140067018598640
	140067018600464 [label="ViewBackward0
------------------------------
self_sym_sizes: (24, 511, 768)"]
	140067018595760 -> 140067018600464
	140067018599696 -> 140067018598640
	140067018599696 [label=TBackward0]
	140067018600368 -> 140067018599696
	140066806146192 [label="
 (768, 768)" fillcolor=lightblue]
	140066806146192 -> 140067018600368
	140067018600368 [label=AccumulateGrad]
	140067018596624 -> 140067018595424
	140067018596624 [label="UnsafeViewBackward0
---------------------------------
self_sym_sizes: (24, 12, 511, 64)"]
	140067018596480 -> 140067018596624
	140067018596480 [label=CloneBackward0]
	140067018596816 -> 140067018596480
	140067018596816 [label="ExpandBackward0
---------------------------------
self_sym_sizes: (24, 12, 511, 64)"]
	140067018596960 -> 140067018596816
	140067018596960 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067018599504 -> 140067018596960
	140067018599504 [label="ViewBackward0
------------------------------
self_sym_sizes: (24, 511, 768)"]
	140067018598496 -> 140067018599504
	140067018598496 [label="ViewBackward0
----------------------------
self_sym_sizes: (12264, 768)"]
	140067018599792 -> 140067018598496
	140067018599792 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :   (12264, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067018600320 -> 140067018599792
	140066806146432 [label="
 (768)" fillcolor=lightblue]
	140066806146432 -> 140067018600320
	140067018600320 [label=AccumulateGrad]
	140067018598784 -> 140067018599792
	140067018598784 [label="ViewBackward0
------------------------------
self_sym_sizes: (24, 511, 768)"]
	140067018595760 -> 140067018598784
	140067018597008 -> 140067018599792
	140067018597008 [label=TBackward0]
	140067018599984 -> 140067018597008
	140066806146352 [label="
 (768, 768)" fillcolor=lightblue]
	140066806146352 -> 140067018599984
	140067018599984 [label=AccumulateGrad]
	140067018597248 -> 140067018597392
	140067018597248 [label=TBackward0]
	140067018597776 -> 140067018597248
	140066806146512 [label="
 (768, 768)" fillcolor=lightblue]
	140066806146512 -> 140067018597776
	140067018597776 [label=AccumulateGrad]
	140067018595760 -> 140067018594848
	140067018589808 -> 140067041944880
	140066806146752 [label="
 (768)" fillcolor=lightblue]
	140066806146752 -> 140067018589808
	140067018589808 [label=AccumulateGrad]
	140067018594224 -> 140067041944880
	140066806146672 [label="
 (768)" fillcolor=lightblue]
	140066806146672 -> 140067018594224
	140067018594224 [label=AccumulateGrad]
	140067018592208 -> 140067018588848
	140067018592208 [label=TBackward0]
	140067018597296 -> 140067018592208
	140066806146912 [label="
 (3072, 768)" fillcolor=lightblue]
	140066806146912 -> 140067018597296
	140067018597296 [label=AccumulateGrad]
	140067041944064 -> 140067041940848
	140067041944064 [label=TBackward0]
	140067018592832 -> 140067041944064
	140066806146832 [label="
 (768, 3072)" fillcolor=lightblue]
	140066806146832 -> 140067018592832
	140067018592832 [label=AccumulateGrad]
	140067041944880 -> 140067041946656
	140067041945072 -> 140067041945408
	140066806147232 [label="
 (768)" fillcolor=lightblue]
	140066806147232 -> 140067041945072
	140067041945072 [label=AccumulateGrad]
	140067041948864 -> 140067041945408
	140066806147072 [label="
 (768)" fillcolor=lightblue]
	140066806147072 -> 140067041948864
	140067041948864 [label=AccumulateGrad]
	140067041950160 -> 140067041949536
	140067041950160 [label=TBackward0]
	140067041948960 -> 140067041950160
	140066770656928 [label="
 (768, 768)" fillcolor=lightblue]
	140066770656928 -> 140067041948960
	140067041948960 [label=AccumulateGrad]
	140067041950736 -> 140067041950352
	140067041950736 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 64, 9)"]
	140067041950544 -> 140067041950736
	140067041950544 [label=CloneBackward0]
	140067041949584 -> 140067041950544
	140067041949584 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 64, 9)"]
	140067041949728 -> 140067041949584
	140067041949728 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551615
dim1: 18446744073709551614"]
	140067041948048 -> 140067041949728
	140067041948048 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041945264 -> 140067041948048
	140067041945264 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 9, 768)"]
	140067041946464 -> 140067041945264
	140067041946464 [label="ViewBackward0
-------------------------
self_sym_sizes: (27, 768)"]
	140067041944304 -> 140067041946464
	140067041944304 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (27, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041944640 -> 140067041944304
	140066770657568 [label="
 (768)" fillcolor=lightblue]
	140066770657568 -> 140067041944640
	140067041944640 [label=AccumulateGrad]
	140067041945552 -> 140067041944304
	140067041945552 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 9, 768)"]
	140067041628400 -> 140067041945552
	140067041950112 -> 140067041944304
	140067041950112 [label=TBackward0]
	140067041947664 -> 140067041950112
	140066770657488 [label="
 (768, 768)" fillcolor=lightblue]
	140066770657488 -> 140067041947664
	140067041947664 [label=AccumulateGrad]
	140067041936768 -> 140067041935616
	140067041936768 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 9, 64)"]
	140067041951600 -> 140067041936768
	140067041951600 [label=CloneBackward0]
	140067041951024 -> 140067041951600
	140067041951024 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 9, 64)"]
	140067041950976 -> 140067041951024
	140067041950976 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041949920 -> 140067041950976
	140067041949920 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 9, 768)"]
	140067041948912 -> 140067041949920
	140067041948912 [label="ViewBackward0
-------------------------
self_sym_sizes: (27, 768)"]
	140067041946224 -> 140067041948912
	140067041946224 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (27, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041940128 -> 140067041946224
	140066770657728 [label="
 (768)" fillcolor=lightblue]
	140066770657728 -> 140067041940128
	140067041940128 [label=AccumulateGrad]
	140067041949008 -> 140067041946224
	140067041949008 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 9, 768)"]
	140067041628400 -> 140067041949008
	140067041951360 -> 140067041946224
	140067041951360 [label=TBackward0]
	140067018589376 -> 140067041951360
	140066770657648 [label="
 (768, 768)" fillcolor=lightblue]
	140066770657648 -> 140067018589376
	140067018589376 [label=AccumulateGrad]
	140067041627248 -> 140067041625520
	140067041627248 [label=TBackward0]
	140067041624368 -> 140067041627248
	140066770657808 [label="
 (768, 768)" fillcolor=lightblue]
	140066770657808 -> 140067041624368
	140067041624368 [label=AccumulateGrad]
	140067041628400 -> 140067041630080
	140067041629984 -> 140067041699984
	140066770658048 [label="
 (768)" fillcolor=lightblue]
	140066770658048 -> 140067041629984
	140067041629984 [label=AccumulateGrad]
	140067041629312 -> 140067041699984
	140066770657968 [label="
 (768)" fillcolor=lightblue]
	140066770657968 -> 140067041629312
	140067041629312 [label=AccumulateGrad]
	140067041639536 -> 140067041633632
	140067041639536 [label=TBackward0]
	140067041626336 -> 140067041639536
	140066770658208 [label="
 (3072, 768)" fillcolor=lightblue]
	140066770658208 -> 140067041626336
	140067041626336 [label=AccumulateGrad]
	140067041699360 -> 140067041695280
	140067041699360 [label=TBackward0]
	140067041693312 -> 140067041699360
	140066770658368 [label="
 (768, 3072)" fillcolor=lightblue]
	140066770658368 -> 140067041693312
	140067041693312 [label=AccumulateGrad]
	140067041699984 -> 140067041697728
	140067041696864 -> 140067041697008
	140066770658608 [label="
 (768)" fillcolor=lightblue]
	140066770658608 -> 140067041696864
	140067041696864 [label=AccumulateGrad]
	140067041701040 -> 140067041697008
	140066770658528 [label="
 (768)" fillcolor=lightblue]
	140066770658528 -> 140067041701040
	140067041701040 [label=AccumulateGrad]
	140067041701184 -> 140067040903568
}

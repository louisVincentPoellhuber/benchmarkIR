{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Tmp/lvpoellhuber/bmir-env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "WARNING: Using incubator modules: jdk.incubator.vector\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from transformers import AutoModel, DPRQuestionEncoder\n",
    "from datasets import load_dataset\n",
    "from beir.datasets.data_loader import GenericDataLoader\n",
    "from beir import util\n",
    "from pyserini.search.lucene import LuceneSearcher\n",
    "from torchviz import make_dot\n",
    "from losses import contrastive_loss, InBatchNegativeLoss\n",
    "from model_longtriever import Longtriever\n",
    "\n",
    "from model_biencoder import BiEncoder, LongBiEncoder\n",
    "from preprocessing.preprocess_utils import get_triplets_dataloader, get_pairs_dataloader\n",
    "\n",
    "import dotenv\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib64/openjdk-21\"\n",
    "os.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + \"/bin:\" + os.environ[\"PATH\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter Comparisons\n",
    "\n",
    "## BiEncoder\n",
    "Comparing E5's automatic implementation and mine's. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "custom_model = BiEncoder(model_path = (\"intfloat/e5-base-v2\", \"intfloat/e5-base-v2\"), sep=\" [SEP] \")\n",
    "custom_q_model = custom_model.q_model.to(\"cpu\")\n",
    "base_model = AutoModel.from_pretrained(\"intfloat/e5-base-v2\").to(\"cpu\")\n",
    "\n",
    "# Compare weights\n",
    "custom_weights = custom_q_model.state_dict().keys()\n",
    "\n",
    "for name, param in base_model.named_parameters():\n",
    "    #print(name)\n",
    "    if name in custom_weights:\n",
    "        custom_param = custom_q_model.state_dict()[name]\n",
    "        if torch.equal(param, custom_param):\n",
    "            #print(f\"Layer {name} matches\")\n",
    "            pass\n",
    "        else:\n",
    "            print(f\"Layer {name} does not match\")\n",
    "\n",
    "    else:\n",
    "        print(f\"Layer {name} not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing DPR's implementation with mine. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "custom_model = BiEncoder(model_path = (\"facebook/dpr-question_encoder-single-nq-base\", \"facebook/dpr-ctx_encoder-single-nq-base\"), sep=\" [SEP] \")\n",
    "custom_q_model = custom_model.q_model.to(\"cpu\")\n",
    "base_model = DPRQuestionEncoder.from_pretrained(\"facebook/dpr-question_encoder-single-nq-base\").to(\"cpu\")\n",
    "\n",
    "# Compare weights\n",
    "custom_weights = custom_q_model.state_dict().keys()\n",
    "\n",
    "for name, param in base_model.named_parameters():\n",
    "    #print(name)\n",
    "    if name in custom_weights:\n",
    "        custom_param = custom_q_model.state_dict()[name]\n",
    "        if torch.equal(param, custom_param):\n",
    "            #print(f\"Layer {name} matches\")\n",
    "            pass\n",
    "        else:\n",
    "            print(f\"Layer {name} does not match\")\n",
    "\n",
    "    else:\n",
    "        print(f\"Layer {name} not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Longtriever\n",
    "Why the hell does longtriever NOT WANNA INITIALIZE BERT PROPERlY??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n",
      "You are using a model of type bert to instantiate a model of type longtriever. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of Longtriever were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['doc_embeddings', 'embeddings.LayerNorm.bias', 'embeddings.LayerNorm.weight', 'embeddings.position_embeddings.weight', 'embeddings.token_type_embeddings.weight', 'embeddings.word_embeddings.weight', 'encoder.information_exchanging_layer.0.attention.output.LayerNorm.bias', 'encoder.information_exchanging_layer.0.attention.output.LayerNorm.weight', 'encoder.information_exchanging_layer.0.attention.output.dense.bias', 'encoder.information_exchanging_layer.0.attention.output.dense.weight', 'encoder.information_exchanging_layer.0.attention.self.key.bias', 'encoder.information_exchanging_layer.0.attention.self.key.weight', 'encoder.information_exchanging_layer.0.attention.self.query.bias', 'encoder.information_exchanging_layer.0.attention.self.query.weight', 'encoder.information_exchanging_layer.0.attention.self.value.bias', 'encoder.information_exchanging_layer.0.attention.self.value.weight', 'encoder.information_exchanging_layer.0.intermediate.dense.bias', 'encoder.information_exchanging_layer.0.intermediate.dense.weight', 'encoder.information_exchanging_layer.0.output.LayerNorm.bias', 'encoder.information_exchanging_layer.0.output.LayerNorm.weight', 'encoder.information_exchanging_layer.0.output.dense.bias', 'encoder.information_exchanging_layer.0.output.dense.weight', 'encoder.information_exchanging_layer.1.attention.output.LayerNorm.bias', 'encoder.information_exchanging_layer.1.attention.output.LayerNorm.weight', 'encoder.information_exchanging_layer.1.attention.output.dense.bias', 'encoder.information_exchanging_layer.1.attention.output.dense.weight', 'encoder.information_exchanging_layer.1.attention.self.key.bias', 'encoder.information_exchanging_layer.1.attention.self.key.weight', 'encoder.information_exchanging_layer.1.attention.self.query.bias', 'encoder.information_exchanging_layer.1.attention.self.query.weight', 'encoder.information_exchanging_layer.1.attention.self.value.bias', 'encoder.information_exchanging_layer.1.attention.self.value.weight', 'encoder.information_exchanging_layer.1.intermediate.dense.bias', 'encoder.information_exchanging_layer.1.intermediate.dense.weight', 'encoder.information_exchanging_layer.1.output.LayerNorm.bias', 'encoder.information_exchanging_layer.1.output.LayerNorm.weight', 'encoder.information_exchanging_layer.1.output.dense.bias', 'encoder.information_exchanging_layer.1.output.dense.weight', 'encoder.information_exchanging_layer.10.attention.output.LayerNorm.bias', 'encoder.information_exchanging_layer.10.attention.output.LayerNorm.weight', 'encoder.information_exchanging_layer.10.attention.output.dense.bias', 'encoder.information_exchanging_layer.10.attention.output.dense.weight', 'encoder.information_exchanging_layer.10.attention.self.key.bias', 'encoder.information_exchanging_layer.10.attention.self.key.weight', 'encoder.information_exchanging_layer.10.attention.self.query.bias', 'encoder.information_exchanging_layer.10.attention.self.query.weight', 'encoder.information_exchanging_layer.10.attention.self.value.bias', 'encoder.information_exchanging_layer.10.attention.self.value.weight', 'encoder.information_exchanging_layer.10.intermediate.dense.bias', 'encoder.information_exchanging_layer.10.intermediate.dense.weight', 'encoder.information_exchanging_layer.10.output.LayerNorm.bias', 'encoder.information_exchanging_layer.10.output.LayerNorm.weight', 'encoder.information_exchanging_layer.10.output.dense.bias', 'encoder.information_exchanging_layer.10.output.dense.weight', 'encoder.information_exchanging_layer.11.attention.output.LayerNorm.bias', 'encoder.information_exchanging_layer.11.attention.output.LayerNorm.weight', 'encoder.information_exchanging_layer.11.attention.output.dense.bias', 'encoder.information_exchanging_layer.11.attention.output.dense.weight', 'encoder.information_exchanging_layer.11.attention.self.key.bias', 'encoder.information_exchanging_layer.11.attention.self.key.weight', 'encoder.information_exchanging_layer.11.attention.self.query.bias', 'encoder.information_exchanging_layer.11.attention.self.query.weight', 'encoder.information_exchanging_layer.11.attention.self.value.bias', 'encoder.information_exchanging_layer.11.attention.self.value.weight', 'encoder.information_exchanging_layer.11.intermediate.dense.bias', 'encoder.information_exchanging_layer.11.intermediate.dense.weight', 'encoder.information_exchanging_layer.11.output.LayerNorm.bias', 'encoder.information_exchanging_layer.11.output.LayerNorm.weight', 'encoder.information_exchanging_layer.11.output.dense.bias', 'encoder.information_exchanging_layer.11.output.dense.weight', 'encoder.information_exchanging_layer.2.attention.output.LayerNorm.bias', 'encoder.information_exchanging_layer.2.attention.output.LayerNorm.weight', 'encoder.information_exchanging_layer.2.attention.output.dense.bias', 'encoder.information_exchanging_layer.2.attention.output.dense.weight', 'encoder.information_exchanging_layer.2.attention.self.key.bias', 'encoder.information_exchanging_layer.2.attention.self.key.weight', 'encoder.information_exchanging_layer.2.attention.self.query.bias', 'encoder.information_exchanging_layer.2.attention.self.query.weight', 'encoder.information_exchanging_layer.2.attention.self.value.bias', 'encoder.information_exchanging_layer.2.attention.self.value.weight', 'encoder.information_exchanging_layer.2.intermediate.dense.bias', 'encoder.information_exchanging_layer.2.intermediate.dense.weight', 'encoder.information_exchanging_layer.2.output.LayerNorm.bias', 'encoder.information_exchanging_layer.2.output.LayerNorm.weight', 'encoder.information_exchanging_layer.2.output.dense.bias', 'encoder.information_exchanging_layer.2.output.dense.weight', 'encoder.information_exchanging_layer.3.attention.output.LayerNorm.bias', 'encoder.information_exchanging_layer.3.attention.output.LayerNorm.weight', 'encoder.information_exchanging_layer.3.attention.output.dense.bias', 'encoder.information_exchanging_layer.3.attention.output.dense.weight', 'encoder.information_exchanging_layer.3.attention.self.key.bias', 'encoder.information_exchanging_layer.3.attention.self.key.weight', 'encoder.information_exchanging_layer.3.attention.self.query.bias', 'encoder.information_exchanging_layer.3.attention.self.query.weight', 'encoder.information_exchanging_layer.3.attention.self.value.bias', 'encoder.information_exchanging_layer.3.attention.self.value.weight', 'encoder.information_exchanging_layer.3.intermediate.dense.bias', 'encoder.information_exchanging_layer.3.intermediate.dense.weight', 'encoder.information_exchanging_layer.3.output.LayerNorm.bias', 'encoder.information_exchanging_layer.3.output.LayerNorm.weight', 'encoder.information_exchanging_layer.3.output.dense.bias', 'encoder.information_exchanging_layer.3.output.dense.weight', 'encoder.information_exchanging_layer.4.attention.output.LayerNorm.bias', 'encoder.information_exchanging_layer.4.attention.output.LayerNorm.weight', 'encoder.information_exchanging_layer.4.attention.output.dense.bias', 'encoder.information_exchanging_layer.4.attention.output.dense.weight', 'encoder.information_exchanging_layer.4.attention.self.key.bias', 'encoder.information_exchanging_layer.4.attention.self.key.weight', 'encoder.information_exchanging_layer.4.attention.self.query.bias', 'encoder.information_exchanging_layer.4.attention.self.query.weight', 'encoder.information_exchanging_layer.4.attention.self.value.bias', 'encoder.information_exchanging_layer.4.attention.self.value.weight', 'encoder.information_exchanging_layer.4.intermediate.dense.bias', 'encoder.information_exchanging_layer.4.intermediate.dense.weight', 'encoder.information_exchanging_layer.4.output.LayerNorm.bias', 'encoder.information_exchanging_layer.4.output.LayerNorm.weight', 'encoder.information_exchanging_layer.4.output.dense.bias', 'encoder.information_exchanging_layer.4.output.dense.weight', 'encoder.information_exchanging_layer.5.attention.output.LayerNorm.bias', 'encoder.information_exchanging_layer.5.attention.output.LayerNorm.weight', 'encoder.information_exchanging_layer.5.attention.output.dense.bias', 'encoder.information_exchanging_layer.5.attention.output.dense.weight', 'encoder.information_exchanging_layer.5.attention.self.key.bias', 'encoder.information_exchanging_layer.5.attention.self.key.weight', 'encoder.information_exchanging_layer.5.attention.self.query.bias', 'encoder.information_exchanging_layer.5.attention.self.query.weight', 'encoder.information_exchanging_layer.5.attention.self.value.bias', 'encoder.information_exchanging_layer.5.attention.self.value.weight', 'encoder.information_exchanging_layer.5.intermediate.dense.bias', 'encoder.information_exchanging_layer.5.intermediate.dense.weight', 'encoder.information_exchanging_layer.5.output.LayerNorm.bias', 'encoder.information_exchanging_layer.5.output.LayerNorm.weight', 'encoder.information_exchanging_layer.5.output.dense.bias', 'encoder.information_exchanging_layer.5.output.dense.weight', 'encoder.information_exchanging_layer.6.attention.output.LayerNorm.bias', 'encoder.information_exchanging_layer.6.attention.output.LayerNorm.weight', 'encoder.information_exchanging_layer.6.attention.output.dense.bias', 'encoder.information_exchanging_layer.6.attention.output.dense.weight', 'encoder.information_exchanging_layer.6.attention.self.key.bias', 'encoder.information_exchanging_layer.6.attention.self.key.weight', 'encoder.information_exchanging_layer.6.attention.self.query.bias', 'encoder.information_exchanging_layer.6.attention.self.query.weight', 'encoder.information_exchanging_layer.6.attention.self.value.bias', 'encoder.information_exchanging_layer.6.attention.self.value.weight', 'encoder.information_exchanging_layer.6.intermediate.dense.bias', 'encoder.information_exchanging_layer.6.intermediate.dense.weight', 'encoder.information_exchanging_layer.6.output.LayerNorm.bias', 'encoder.information_exchanging_layer.6.output.LayerNorm.weight', 'encoder.information_exchanging_layer.6.output.dense.bias', 'encoder.information_exchanging_layer.6.output.dense.weight', 'encoder.information_exchanging_layer.7.attention.output.LayerNorm.bias', 'encoder.information_exchanging_layer.7.attention.output.LayerNorm.weight', 'encoder.information_exchanging_layer.7.attention.output.dense.bias', 'encoder.information_exchanging_layer.7.attention.output.dense.weight', 'encoder.information_exchanging_layer.7.attention.self.key.bias', 'encoder.information_exchanging_layer.7.attention.self.key.weight', 'encoder.information_exchanging_layer.7.attention.self.query.bias', 'encoder.information_exchanging_layer.7.attention.self.query.weight', 'encoder.information_exchanging_layer.7.attention.self.value.bias', 'encoder.information_exchanging_layer.7.attention.self.value.weight', 'encoder.information_exchanging_layer.7.intermediate.dense.bias', 'encoder.information_exchanging_layer.7.intermediate.dense.weight', 'encoder.information_exchanging_layer.7.output.LayerNorm.bias', 'encoder.information_exchanging_layer.7.output.LayerNorm.weight', 'encoder.information_exchanging_layer.7.output.dense.bias', 'encoder.information_exchanging_layer.7.output.dense.weight', 'encoder.information_exchanging_layer.8.attention.output.LayerNorm.bias', 'encoder.information_exchanging_layer.8.attention.output.LayerNorm.weight', 'encoder.information_exchanging_layer.8.attention.output.dense.bias', 'encoder.information_exchanging_layer.8.attention.output.dense.weight', 'encoder.information_exchanging_layer.8.attention.self.key.bias', 'encoder.information_exchanging_layer.8.attention.self.key.weight', 'encoder.information_exchanging_layer.8.attention.self.query.bias', 'encoder.information_exchanging_layer.8.attention.self.query.weight', 'encoder.information_exchanging_layer.8.attention.self.value.bias', 'encoder.information_exchanging_layer.8.attention.self.value.weight', 'encoder.information_exchanging_layer.8.intermediate.dense.bias', 'encoder.information_exchanging_layer.8.intermediate.dense.weight', 'encoder.information_exchanging_layer.8.output.LayerNorm.bias', 'encoder.information_exchanging_layer.8.output.LayerNorm.weight', 'encoder.information_exchanging_layer.8.output.dense.bias', 'encoder.information_exchanging_layer.8.output.dense.weight', 'encoder.information_exchanging_layer.9.attention.output.LayerNorm.bias', 'encoder.information_exchanging_layer.9.attention.output.LayerNorm.weight', 'encoder.information_exchanging_layer.9.attention.output.dense.bias', 'encoder.information_exchanging_layer.9.attention.output.dense.weight', 'encoder.information_exchanging_layer.9.attention.self.key.bias', 'encoder.information_exchanging_layer.9.attention.self.key.weight', 'encoder.information_exchanging_layer.9.attention.self.query.bias', 'encoder.information_exchanging_layer.9.attention.self.query.weight', 'encoder.information_exchanging_layer.9.attention.self.value.bias', 'encoder.information_exchanging_layer.9.attention.self.value.weight', 'encoder.information_exchanging_layer.9.intermediate.dense.bias', 'encoder.information_exchanging_layer.9.intermediate.dense.weight', 'encoder.information_exchanging_layer.9.output.LayerNorm.bias', 'encoder.information_exchanging_layer.9.output.LayerNorm.weight', 'encoder.information_exchanging_layer.9.output.dense.bias', 'encoder.information_exchanging_layer.9.output.dense.weight', 'encoder.text_encoding_layer.0.attention.output.LayerNorm.bias', 'encoder.text_encoding_layer.0.attention.output.LayerNorm.weight', 'encoder.text_encoding_layer.0.attention.output.dense.bias', 'encoder.text_encoding_layer.0.attention.output.dense.weight', 'encoder.text_encoding_layer.0.attention.self.key.bias', 'encoder.text_encoding_layer.0.attention.self.key.weight', 'encoder.text_encoding_layer.0.attention.self.query.bias', 'encoder.text_encoding_layer.0.attention.self.query.weight', 'encoder.text_encoding_layer.0.attention.self.value.bias', 'encoder.text_encoding_layer.0.attention.self.value.weight', 'encoder.text_encoding_layer.0.intermediate.dense.bias', 'encoder.text_encoding_layer.0.intermediate.dense.weight', 'encoder.text_encoding_layer.0.output.LayerNorm.bias', 'encoder.text_encoding_layer.0.output.LayerNorm.weight', 'encoder.text_encoding_layer.0.output.dense.bias', 'encoder.text_encoding_layer.0.output.dense.weight', 'encoder.text_encoding_layer.1.attention.output.LayerNorm.bias', 'encoder.text_encoding_layer.1.attention.output.LayerNorm.weight', 'encoder.text_encoding_layer.1.attention.output.dense.bias', 'encoder.text_encoding_layer.1.attention.output.dense.weight', 'encoder.text_encoding_layer.1.attention.self.key.bias', 'encoder.text_encoding_layer.1.attention.self.key.weight', 'encoder.text_encoding_layer.1.attention.self.query.bias', 'encoder.text_encoding_layer.1.attention.self.query.weight', 'encoder.text_encoding_layer.1.attention.self.value.bias', 'encoder.text_encoding_layer.1.attention.self.value.weight', 'encoder.text_encoding_layer.1.intermediate.dense.bias', 'encoder.text_encoding_layer.1.intermediate.dense.weight', 'encoder.text_encoding_layer.1.output.LayerNorm.bias', 'encoder.text_encoding_layer.1.output.LayerNorm.weight', 'encoder.text_encoding_layer.1.output.dense.bias', 'encoder.text_encoding_layer.1.output.dense.weight', 'encoder.text_encoding_layer.10.attention.output.LayerNorm.bias', 'encoder.text_encoding_layer.10.attention.output.LayerNorm.weight', 'encoder.text_encoding_layer.10.attention.output.dense.bias', 'encoder.text_encoding_layer.10.attention.output.dense.weight', 'encoder.text_encoding_layer.10.attention.self.key.bias', 'encoder.text_encoding_layer.10.attention.self.key.weight', 'encoder.text_encoding_layer.10.attention.self.query.bias', 'encoder.text_encoding_layer.10.attention.self.query.weight', 'encoder.text_encoding_layer.10.attention.self.value.bias', 'encoder.text_encoding_layer.10.attention.self.value.weight', 'encoder.text_encoding_layer.10.intermediate.dense.bias', 'encoder.text_encoding_layer.10.intermediate.dense.weight', 'encoder.text_encoding_layer.10.output.LayerNorm.bias', 'encoder.text_encoding_layer.10.output.LayerNorm.weight', 'encoder.text_encoding_layer.10.output.dense.bias', 'encoder.text_encoding_layer.10.output.dense.weight', 'encoder.text_encoding_layer.11.attention.output.LayerNorm.bias', 'encoder.text_encoding_layer.11.attention.output.LayerNorm.weight', 'encoder.text_encoding_layer.11.attention.output.dense.bias', 'encoder.text_encoding_layer.11.attention.output.dense.weight', 'encoder.text_encoding_layer.11.attention.self.key.bias', 'encoder.text_encoding_layer.11.attention.self.key.weight', 'encoder.text_encoding_layer.11.attention.self.query.bias', 'encoder.text_encoding_layer.11.attention.self.query.weight', 'encoder.text_encoding_layer.11.attention.self.value.bias', 'encoder.text_encoding_layer.11.attention.self.value.weight', 'encoder.text_encoding_layer.11.intermediate.dense.bias', 'encoder.text_encoding_layer.11.intermediate.dense.weight', 'encoder.text_encoding_layer.11.output.LayerNorm.bias', 'encoder.text_encoding_layer.11.output.LayerNorm.weight', 'encoder.text_encoding_layer.11.output.dense.bias', 'encoder.text_encoding_layer.11.output.dense.weight', 'encoder.text_encoding_layer.2.attention.output.LayerNorm.bias', 'encoder.text_encoding_layer.2.attention.output.LayerNorm.weight', 'encoder.text_encoding_layer.2.attention.output.dense.bias', 'encoder.text_encoding_layer.2.attention.output.dense.weight', 'encoder.text_encoding_layer.2.attention.self.key.bias', 'encoder.text_encoding_layer.2.attention.self.key.weight', 'encoder.text_encoding_layer.2.attention.self.query.bias', 'encoder.text_encoding_layer.2.attention.self.query.weight', 'encoder.text_encoding_layer.2.attention.self.value.bias', 'encoder.text_encoding_layer.2.attention.self.value.weight', 'encoder.text_encoding_layer.2.intermediate.dense.bias', 'encoder.text_encoding_layer.2.intermediate.dense.weight', 'encoder.text_encoding_layer.2.output.LayerNorm.bias', 'encoder.text_encoding_layer.2.output.LayerNorm.weight', 'encoder.text_encoding_layer.2.output.dense.bias', 'encoder.text_encoding_layer.2.output.dense.weight', 'encoder.text_encoding_layer.3.attention.output.LayerNorm.bias', 'encoder.text_encoding_layer.3.attention.output.LayerNorm.weight', 'encoder.text_encoding_layer.3.attention.output.dense.bias', 'encoder.text_encoding_layer.3.attention.output.dense.weight', 'encoder.text_encoding_layer.3.attention.self.key.bias', 'encoder.text_encoding_layer.3.attention.self.key.weight', 'encoder.text_encoding_layer.3.attention.self.query.bias', 'encoder.text_encoding_layer.3.attention.self.query.weight', 'encoder.text_encoding_layer.3.attention.self.value.bias', 'encoder.text_encoding_layer.3.attention.self.value.weight', 'encoder.text_encoding_layer.3.intermediate.dense.bias', 'encoder.text_encoding_layer.3.intermediate.dense.weight', 'encoder.text_encoding_layer.3.output.LayerNorm.bias', 'encoder.text_encoding_layer.3.output.LayerNorm.weight', 'encoder.text_encoding_layer.3.output.dense.bias', 'encoder.text_encoding_layer.3.output.dense.weight', 'encoder.text_encoding_layer.4.attention.output.LayerNorm.bias', 'encoder.text_encoding_layer.4.attention.output.LayerNorm.weight', 'encoder.text_encoding_layer.4.attention.output.dense.bias', 'encoder.text_encoding_layer.4.attention.output.dense.weight', 'encoder.text_encoding_layer.4.attention.self.key.bias', 'encoder.text_encoding_layer.4.attention.self.key.weight', 'encoder.text_encoding_layer.4.attention.self.query.bias', 'encoder.text_encoding_layer.4.attention.self.query.weight', 'encoder.text_encoding_layer.4.attention.self.value.bias', 'encoder.text_encoding_layer.4.attention.self.value.weight', 'encoder.text_encoding_layer.4.intermediate.dense.bias', 'encoder.text_encoding_layer.4.intermediate.dense.weight', 'encoder.text_encoding_layer.4.output.LayerNorm.bias', 'encoder.text_encoding_layer.4.output.LayerNorm.weight', 'encoder.text_encoding_layer.4.output.dense.bias', 'encoder.text_encoding_layer.4.output.dense.weight', 'encoder.text_encoding_layer.5.attention.output.LayerNorm.bias', 'encoder.text_encoding_layer.5.attention.output.LayerNorm.weight', 'encoder.text_encoding_layer.5.attention.output.dense.bias', 'encoder.text_encoding_layer.5.attention.output.dense.weight', 'encoder.text_encoding_layer.5.attention.self.key.bias', 'encoder.text_encoding_layer.5.attention.self.key.weight', 'encoder.text_encoding_layer.5.attention.self.query.bias', 'encoder.text_encoding_layer.5.attention.self.query.weight', 'encoder.text_encoding_layer.5.attention.self.value.bias', 'encoder.text_encoding_layer.5.attention.self.value.weight', 'encoder.text_encoding_layer.5.intermediate.dense.bias', 'encoder.text_encoding_layer.5.intermediate.dense.weight', 'encoder.text_encoding_layer.5.output.LayerNorm.bias', 'encoder.text_encoding_layer.5.output.LayerNorm.weight', 'encoder.text_encoding_layer.5.output.dense.bias', 'encoder.text_encoding_layer.5.output.dense.weight', 'encoder.text_encoding_layer.6.attention.output.LayerNorm.bias', 'encoder.text_encoding_layer.6.attention.output.LayerNorm.weight', 'encoder.text_encoding_layer.6.attention.output.dense.bias', 'encoder.text_encoding_layer.6.attention.output.dense.weight', 'encoder.text_encoding_layer.6.attention.self.key.bias', 'encoder.text_encoding_layer.6.attention.self.key.weight', 'encoder.text_encoding_layer.6.attention.self.query.bias', 'encoder.text_encoding_layer.6.attention.self.query.weight', 'encoder.text_encoding_layer.6.attention.self.value.bias', 'encoder.text_encoding_layer.6.attention.self.value.weight', 'encoder.text_encoding_layer.6.intermediate.dense.bias', 'encoder.text_encoding_layer.6.intermediate.dense.weight', 'encoder.text_encoding_layer.6.output.LayerNorm.bias', 'encoder.text_encoding_layer.6.output.LayerNorm.weight', 'encoder.text_encoding_layer.6.output.dense.bias', 'encoder.text_encoding_layer.6.output.dense.weight', 'encoder.text_encoding_layer.7.attention.output.LayerNorm.bias', 'encoder.text_encoding_layer.7.attention.output.LayerNorm.weight', 'encoder.text_encoding_layer.7.attention.output.dense.bias', 'encoder.text_encoding_layer.7.attention.output.dense.weight', 'encoder.text_encoding_layer.7.attention.self.key.bias', 'encoder.text_encoding_layer.7.attention.self.key.weight', 'encoder.text_encoding_layer.7.attention.self.query.bias', 'encoder.text_encoding_layer.7.attention.self.query.weight', 'encoder.text_encoding_layer.7.attention.self.value.bias', 'encoder.text_encoding_layer.7.attention.self.value.weight', 'encoder.text_encoding_layer.7.intermediate.dense.bias', 'encoder.text_encoding_layer.7.intermediate.dense.weight', 'encoder.text_encoding_layer.7.output.LayerNorm.bias', 'encoder.text_encoding_layer.7.output.LayerNorm.weight', 'encoder.text_encoding_layer.7.output.dense.bias', 'encoder.text_encoding_layer.7.output.dense.weight', 'encoder.text_encoding_layer.8.attention.output.LayerNorm.bias', 'encoder.text_encoding_layer.8.attention.output.LayerNorm.weight', 'encoder.text_encoding_layer.8.attention.output.dense.bias', 'encoder.text_encoding_layer.8.attention.output.dense.weight', 'encoder.text_encoding_layer.8.attention.self.key.bias', 'encoder.text_encoding_layer.8.attention.self.key.weight', 'encoder.text_encoding_layer.8.attention.self.query.bias', 'encoder.text_encoding_layer.8.attention.self.query.weight', 'encoder.text_encoding_layer.8.attention.self.value.bias', 'encoder.text_encoding_layer.8.attention.self.value.weight', 'encoder.text_encoding_layer.8.intermediate.dense.bias', 'encoder.text_encoding_layer.8.intermediate.dense.weight', 'encoder.text_encoding_layer.8.output.LayerNorm.bias', 'encoder.text_encoding_layer.8.output.LayerNorm.weight', 'encoder.text_encoding_layer.8.output.dense.bias', 'encoder.text_encoding_layer.8.output.dense.weight', 'encoder.text_encoding_layer.9.attention.output.LayerNorm.bias', 'encoder.text_encoding_layer.9.attention.output.LayerNorm.weight', 'encoder.text_encoding_layer.9.attention.output.dense.bias', 'encoder.text_encoding_layer.9.attention.output.dense.weight', 'encoder.text_encoding_layer.9.attention.self.key.bias', 'encoder.text_encoding_layer.9.attention.self.key.weight', 'encoder.text_encoding_layer.9.attention.self.query.bias', 'encoder.text_encoding_layer.9.attention.self.query.weight', 'encoder.text_encoding_layer.9.attention.self.value.bias', 'encoder.text_encoding_layer.9.attention.self.value.weight', 'encoder.text_encoding_layer.9.intermediate.dense.bias', 'encoder.text_encoding_layer.9.intermediate.dense.weight', 'encoder.text_encoding_layer.9.output.LayerNorm.bias', 'encoder.text_encoding_layer.9.output.LayerNorm.weight', 'encoder.text_encoding_layer.9.output.dense.bias', 'encoder.text_encoding_layer.9.output.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "longtriever = Longtriever.from_pretrained(\n",
    "            \"google-bert/bert-base-uncased\",\n",
    "            torch_dtype=\"auto\",\n",
    "            trust_remote_code=True,\n",
    "            attn_implementation=\"eager\",\n",
    "            cache_dir=None\n",
    "    ).to(\"cpu\")\n",
    "base_model = AutoModel.from_pretrained(\"google-bert/bert-base-uncased\").to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Get the state_dict of the BERT encoder\n",
    "bert_state_dict = base_model.state_dict()\n",
    "\n",
    "# Get the state_dict of the Longtriever model\n",
    "longtriever_state_dict = longtriever.state_dict()\n",
    "\n",
    "# Map weights from BERT to Longtriever\n",
    "new_state_dict = {}\n",
    "for bert_key, bert_value in bert_state_dict.items():\n",
    "    # Replace layer names to match Longtriever's naming convention\n",
    "    if \"layer\" in bert_key:\n",
    "        # Example: Map BERT's \"layer.X\" to Longtriever's \"text_encoding_layers.X\"\n",
    "        new_key = bert_key.replace(\"layer\", \"text_encoding_layer\")\n",
    "        if new_key in longtriever_state_dict:\n",
    "            new_state_dict[new_key] = bert_value\n",
    "\n",
    "        # Example: Map BERT's \"layer.X\" to Longtriever's \"information_exchanging.X\"\n",
    "        new_key = bert_key.replace(\"layer\", \"information_exchanging_layer\")\n",
    "        if new_key in longtriever_state_dict:\n",
    "            new_state_dict[new_key] = bert_value\n",
    "    else:\n",
    "        if bert_key in longtriever_state_dict:\n",
    "            new_state_dict[bert_key] = bert_value\n",
    "\n",
    "# Update Longtriever's state_dict with the new weights\n",
    "longtriever_state_dict.update(new_state_dict)\n",
    "\n",
    "# Load the updated state_dict into Longtriever\n",
    "longtriever.load_state_dict(longtriever_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.modules of Longtriever(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BlockLevelContextawareEncoder(\n",
       "    (text_encoding_layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (information_exchanging_layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "longtriever.modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.modules of BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSdpaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model.modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings.word_embeddings.weight\n",
      "embeddings.position_embeddings.weight\n",
      "embeddings.token_type_embeddings.weight\n",
      "embeddings.LayerNorm.weight\n",
      "embeddings.LayerNorm.bias\n",
      "encoder.layer.0.attention.self.query.weight\n",
      "encoder.layer.0.attention.self.query.bias\n",
      "encoder.layer.0.attention.self.key.weight\n",
      "encoder.layer.0.attention.self.key.bias\n",
      "encoder.layer.0.attention.self.value.weight\n",
      "encoder.layer.0.attention.self.value.bias\n",
      "encoder.layer.0.attention.output.dense.weight\n",
      "encoder.layer.0.attention.output.dense.bias\n",
      "encoder.layer.0.attention.output.LayerNorm.weight\n",
      "encoder.layer.0.attention.output.LayerNorm.bias\n",
      "encoder.layer.0.intermediate.dense.weight\n",
      "encoder.layer.0.intermediate.dense.bias\n",
      "encoder.layer.0.output.dense.weight\n",
      "encoder.layer.0.output.dense.bias\n",
      "encoder.layer.0.output.LayerNorm.weight\n",
      "encoder.layer.0.output.LayerNorm.bias\n",
      "encoder.layer.1.attention.self.query.weight\n",
      "encoder.layer.1.attention.self.query.bias\n",
      "encoder.layer.1.attention.self.key.weight\n",
      "encoder.layer.1.attention.self.key.bias\n",
      "encoder.layer.1.attention.self.value.weight\n",
      "encoder.layer.1.attention.self.value.bias\n",
      "encoder.layer.1.attention.output.dense.weight\n",
      "encoder.layer.1.attention.output.dense.bias\n",
      "encoder.layer.1.attention.output.LayerNorm.weight\n",
      "encoder.layer.1.attention.output.LayerNorm.bias\n",
      "encoder.layer.1.intermediate.dense.weight\n",
      "encoder.layer.1.intermediate.dense.bias\n",
      "encoder.layer.1.output.dense.weight\n",
      "encoder.layer.1.output.dense.bias\n",
      "encoder.layer.1.output.LayerNorm.weight\n",
      "encoder.layer.1.output.LayerNorm.bias\n",
      "encoder.layer.2.attention.self.query.weight\n",
      "encoder.layer.2.attention.self.query.bias\n",
      "encoder.layer.2.attention.self.key.weight\n",
      "encoder.layer.2.attention.self.key.bias\n",
      "encoder.layer.2.attention.self.value.weight\n",
      "encoder.layer.2.attention.self.value.bias\n",
      "encoder.layer.2.attention.output.dense.weight\n",
      "encoder.layer.2.attention.output.dense.bias\n",
      "encoder.layer.2.attention.output.LayerNorm.weight\n",
      "encoder.layer.2.attention.output.LayerNorm.bias\n",
      "encoder.layer.2.intermediate.dense.weight\n",
      "encoder.layer.2.intermediate.dense.bias\n",
      "encoder.layer.2.output.dense.weight\n",
      "encoder.layer.2.output.dense.bias\n",
      "encoder.layer.2.output.LayerNorm.weight\n",
      "encoder.layer.2.output.LayerNorm.bias\n",
      "encoder.layer.3.attention.self.query.weight\n",
      "encoder.layer.3.attention.self.query.bias\n",
      "encoder.layer.3.attention.self.key.weight\n",
      "encoder.layer.3.attention.self.key.bias\n",
      "encoder.layer.3.attention.self.value.weight\n",
      "encoder.layer.3.attention.self.value.bias\n",
      "encoder.layer.3.attention.output.dense.weight\n",
      "encoder.layer.3.attention.output.dense.bias\n",
      "encoder.layer.3.attention.output.LayerNorm.weight\n",
      "encoder.layer.3.attention.output.LayerNorm.bias\n",
      "encoder.layer.3.intermediate.dense.weight\n",
      "encoder.layer.3.intermediate.dense.bias\n",
      "encoder.layer.3.output.dense.weight\n",
      "encoder.layer.3.output.dense.bias\n",
      "encoder.layer.3.output.LayerNorm.weight\n",
      "encoder.layer.3.output.LayerNorm.bias\n",
      "encoder.layer.4.attention.self.query.weight\n",
      "encoder.layer.4.attention.self.query.bias\n",
      "encoder.layer.4.attention.self.key.weight\n",
      "encoder.layer.4.attention.self.key.bias\n",
      "encoder.layer.4.attention.self.value.weight\n",
      "encoder.layer.4.attention.self.value.bias\n",
      "encoder.layer.4.attention.output.dense.weight\n",
      "encoder.layer.4.attention.output.dense.bias\n",
      "encoder.layer.4.attention.output.LayerNorm.weight\n",
      "encoder.layer.4.attention.output.LayerNorm.bias\n",
      "encoder.layer.4.intermediate.dense.weight\n",
      "encoder.layer.4.intermediate.dense.bias\n",
      "encoder.layer.4.output.dense.weight\n",
      "encoder.layer.4.output.dense.bias\n",
      "encoder.layer.4.output.LayerNorm.weight\n",
      "encoder.layer.4.output.LayerNorm.bias\n",
      "encoder.layer.5.attention.self.query.weight\n",
      "encoder.layer.5.attention.self.query.bias\n",
      "encoder.layer.5.attention.self.key.weight\n",
      "encoder.layer.5.attention.self.key.bias\n",
      "encoder.layer.5.attention.self.value.weight\n",
      "encoder.layer.5.attention.self.value.bias\n",
      "encoder.layer.5.attention.output.dense.weight\n",
      "encoder.layer.5.attention.output.dense.bias\n",
      "encoder.layer.5.attention.output.LayerNorm.weight\n",
      "encoder.layer.5.attention.output.LayerNorm.bias\n",
      "encoder.layer.5.intermediate.dense.weight\n",
      "encoder.layer.5.intermediate.dense.bias\n",
      "encoder.layer.5.output.dense.weight\n",
      "encoder.layer.5.output.dense.bias\n",
      "encoder.layer.5.output.LayerNorm.weight\n",
      "encoder.layer.5.output.LayerNorm.bias\n",
      "encoder.layer.6.attention.self.query.weight\n",
      "encoder.layer.6.attention.self.query.bias\n",
      "encoder.layer.6.attention.self.key.weight\n",
      "encoder.layer.6.attention.self.key.bias\n",
      "encoder.layer.6.attention.self.value.weight\n",
      "encoder.layer.6.attention.self.value.bias\n",
      "encoder.layer.6.attention.output.dense.weight\n",
      "encoder.layer.6.attention.output.dense.bias\n",
      "encoder.layer.6.attention.output.LayerNorm.weight\n",
      "encoder.layer.6.attention.output.LayerNorm.bias\n",
      "encoder.layer.6.intermediate.dense.weight\n",
      "encoder.layer.6.intermediate.dense.bias\n",
      "encoder.layer.6.output.dense.weight\n",
      "encoder.layer.6.output.dense.bias\n",
      "encoder.layer.6.output.LayerNorm.weight\n",
      "encoder.layer.6.output.LayerNorm.bias\n",
      "encoder.layer.7.attention.self.query.weight\n",
      "encoder.layer.7.attention.self.query.bias\n",
      "encoder.layer.7.attention.self.key.weight\n",
      "encoder.layer.7.attention.self.key.bias\n",
      "encoder.layer.7.attention.self.value.weight\n",
      "encoder.layer.7.attention.self.value.bias\n",
      "encoder.layer.7.attention.output.dense.weight\n",
      "encoder.layer.7.attention.output.dense.bias\n",
      "encoder.layer.7.attention.output.LayerNorm.weight\n",
      "encoder.layer.7.attention.output.LayerNorm.bias\n",
      "encoder.layer.7.intermediate.dense.weight\n",
      "encoder.layer.7.intermediate.dense.bias\n",
      "encoder.layer.7.output.dense.weight\n",
      "encoder.layer.7.output.dense.bias\n",
      "encoder.layer.7.output.LayerNorm.weight\n",
      "encoder.layer.7.output.LayerNorm.bias\n",
      "encoder.layer.8.attention.self.query.weight\n",
      "encoder.layer.8.attention.self.query.bias\n",
      "encoder.layer.8.attention.self.key.weight\n",
      "encoder.layer.8.attention.self.key.bias\n",
      "encoder.layer.8.attention.self.value.weight\n",
      "encoder.layer.8.attention.self.value.bias\n",
      "encoder.layer.8.attention.output.dense.weight\n",
      "encoder.layer.8.attention.output.dense.bias\n",
      "encoder.layer.8.attention.output.LayerNorm.weight\n",
      "encoder.layer.8.attention.output.LayerNorm.bias\n",
      "encoder.layer.8.intermediate.dense.weight\n",
      "encoder.layer.8.intermediate.dense.bias\n",
      "encoder.layer.8.output.dense.weight\n",
      "encoder.layer.8.output.dense.bias\n",
      "encoder.layer.8.output.LayerNorm.weight\n",
      "encoder.layer.8.output.LayerNorm.bias\n",
      "encoder.layer.9.attention.self.query.weight\n",
      "encoder.layer.9.attention.self.query.bias\n",
      "encoder.layer.9.attention.self.key.weight\n",
      "encoder.layer.9.attention.self.key.bias\n",
      "encoder.layer.9.attention.self.value.weight\n",
      "encoder.layer.9.attention.self.value.bias\n",
      "encoder.layer.9.attention.output.dense.weight\n",
      "encoder.layer.9.attention.output.dense.bias\n",
      "encoder.layer.9.attention.output.LayerNorm.weight\n",
      "encoder.layer.9.attention.output.LayerNorm.bias\n",
      "encoder.layer.9.intermediate.dense.weight\n",
      "encoder.layer.9.intermediate.dense.bias\n",
      "encoder.layer.9.output.dense.weight\n",
      "encoder.layer.9.output.dense.bias\n",
      "encoder.layer.9.output.LayerNorm.weight\n",
      "encoder.layer.9.output.LayerNorm.bias\n",
      "encoder.layer.10.attention.self.query.weight\n",
      "encoder.layer.10.attention.self.query.bias\n",
      "encoder.layer.10.attention.self.key.weight\n",
      "encoder.layer.10.attention.self.key.bias\n",
      "encoder.layer.10.attention.self.value.weight\n",
      "encoder.layer.10.attention.self.value.bias\n",
      "encoder.layer.10.attention.output.dense.weight\n",
      "encoder.layer.10.attention.output.dense.bias\n",
      "encoder.layer.10.attention.output.LayerNorm.weight\n",
      "encoder.layer.10.attention.output.LayerNorm.bias\n",
      "encoder.layer.10.intermediate.dense.weight\n",
      "encoder.layer.10.intermediate.dense.bias\n",
      "encoder.layer.10.output.dense.weight\n",
      "encoder.layer.10.output.dense.bias\n",
      "encoder.layer.10.output.LayerNorm.weight\n",
      "encoder.layer.10.output.LayerNorm.bias\n",
      "encoder.layer.11.attention.self.query.weight\n",
      "encoder.layer.11.attention.self.query.bias\n",
      "encoder.layer.11.attention.self.key.weight\n",
      "encoder.layer.11.attention.self.key.bias\n",
      "encoder.layer.11.attention.self.value.weight\n",
      "encoder.layer.11.attention.self.value.bias\n",
      "encoder.layer.11.attention.output.dense.weight\n",
      "encoder.layer.11.attention.output.dense.bias\n",
      "encoder.layer.11.attention.output.LayerNorm.weight\n",
      "encoder.layer.11.attention.output.LayerNorm.bias\n",
      "encoder.layer.11.intermediate.dense.weight\n",
      "encoder.layer.11.intermediate.dense.bias\n",
      "encoder.layer.11.output.dense.weight\n",
      "encoder.layer.11.output.dense.bias\n",
      "encoder.layer.11.output.LayerNorm.weight\n",
      "encoder.layer.11.output.LayerNorm.bias\n",
      "pooler.dense.weight\n",
      "pooler.dense.bias\n"
     ]
    }
   ],
   "source": [
    "for name, param in base_model.named_parameters():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer embeddings.word_embeddings.weight matches\n",
      "Layer embeddings.word_embeddings.weight matches\n",
      "Layer embeddings.position_embeddings.weight matches\n",
      "Layer embeddings.position_embeddings.weight matches\n",
      "Layer embeddings.token_type_embeddings.weight matches\n",
      "Layer embeddings.token_type_embeddings.weight matches\n",
      "Layer embeddings.LayerNorm.weight matches\n",
      "Layer embeddings.LayerNorm.weight matches\n",
      "Layer embeddings.LayerNorm.bias matches\n",
      "Layer embeddings.LayerNorm.bias matches\n",
      "Layer encoder.text_encoding_layer.0.attention.self.query.weight matches\n",
      "Layer encoder.information_exchanging_layer.0.attention.self.query.weight matches\n",
      "Layer encoder.text_encoding_layer.0.attention.self.query.bias matches\n",
      "Layer encoder.information_exchanging_layer.0.attention.self.query.bias matches\n",
      "Layer encoder.text_encoding_layer.0.attention.self.key.weight matches\n",
      "Layer encoder.information_exchanging_layer.0.attention.self.key.weight matches\n",
      "Layer encoder.text_encoding_layer.0.attention.self.key.bias matches\n",
      "Layer encoder.information_exchanging_layer.0.attention.self.key.bias matches\n",
      "Layer encoder.text_encoding_layer.0.attention.self.value.weight matches\n",
      "Layer encoder.information_exchanging_layer.0.attention.self.value.weight matches\n",
      "Layer encoder.text_encoding_layer.0.attention.self.value.bias matches\n",
      "Layer encoder.information_exchanging_layer.0.attention.self.value.bias matches\n",
      "Layer encoder.text_encoding_layer.0.attention.output.dense.weight matches\n",
      "Layer encoder.information_exchanging_layer.0.attention.output.dense.weight matches\n",
      "Layer encoder.text_encoding_layer.0.attention.output.dense.bias matches\n",
      "Layer encoder.information_exchanging_layer.0.attention.output.dense.bias matches\n",
      "Layer encoder.text_encoding_layer.0.attention.output.LayerNorm.weight matches\n",
      "Layer encoder.information_exchanging_layer.0.attention.output.LayerNorm.weight matches\n",
      "Layer encoder.text_encoding_layer.0.attention.output.LayerNorm.bias matches\n",
      "Layer encoder.information_exchanging_layer.0.attention.output.LayerNorm.bias matches\n",
      "Layer encoder.text_encoding_layer.0.intermediate.dense.weight matches\n",
      "Layer encoder.information_exchanging_layer.0.intermediate.dense.weight matches\n",
      "Layer encoder.text_encoding_layer.0.intermediate.dense.bias matches\n",
      "Layer encoder.information_exchanging_layer.0.intermediate.dense.bias matches\n",
      "Layer encoder.text_encoding_layer.0.output.dense.weight matches\n",
      "Layer encoder.information_exchanging_layer.0.output.dense.weight matches\n",
      "Layer encoder.text_encoding_layer.0.output.dense.bias matches\n",
      "Layer encoder.information_exchanging_layer.0.output.dense.bias matches\n",
      "Layer encoder.text_encoding_layer.0.output.LayerNorm.weight matches\n",
      "Layer encoder.information_exchanging_layer.0.output.LayerNorm.weight matches\n",
      "Layer encoder.text_encoding_layer.0.output.LayerNorm.bias matches\n",
      "Layer encoder.information_exchanging_layer.0.output.LayerNorm.bias matches\n",
      "Layer encoder.text_encoding_layer.1.attention.self.query.weight matches\n",
      "Layer encoder.information_exchanging_layer.1.attention.self.query.weight matches\n",
      "Layer encoder.text_encoding_layer.1.attention.self.query.bias matches\n",
      "Layer encoder.information_exchanging_layer.1.attention.self.query.bias matches\n",
      "Layer encoder.text_encoding_layer.1.attention.self.key.weight matches\n",
      "Layer encoder.information_exchanging_layer.1.attention.self.key.weight matches\n",
      "Layer encoder.text_encoding_layer.1.attention.self.key.bias matches\n",
      "Layer encoder.information_exchanging_layer.1.attention.self.key.bias matches\n",
      "Layer encoder.text_encoding_layer.1.attention.self.value.weight matches\n",
      "Layer encoder.information_exchanging_layer.1.attention.self.value.weight matches\n",
      "Layer encoder.text_encoding_layer.1.attention.self.value.bias matches\n",
      "Layer encoder.information_exchanging_layer.1.attention.self.value.bias matches\n",
      "Layer encoder.text_encoding_layer.1.attention.output.dense.weight matches\n",
      "Layer encoder.information_exchanging_layer.1.attention.output.dense.weight matches\n",
      "Layer encoder.text_encoding_layer.1.attention.output.dense.bias matches\n",
      "Layer encoder.information_exchanging_layer.1.attention.output.dense.bias matches\n",
      "Layer encoder.text_encoding_layer.1.attention.output.LayerNorm.weight matches\n",
      "Layer encoder.information_exchanging_layer.1.attention.output.LayerNorm.weight matches\n",
      "Layer encoder.text_encoding_layer.1.attention.output.LayerNorm.bias matches\n",
      "Layer encoder.information_exchanging_layer.1.attention.output.LayerNorm.bias matches\n",
      "Layer encoder.text_encoding_layer.1.intermediate.dense.weight matches\n",
      "Layer encoder.information_exchanging_layer.1.intermediate.dense.weight matches\n",
      "Layer encoder.text_encoding_layer.1.intermediate.dense.bias matches\n",
      "Layer encoder.information_exchanging_layer.1.intermediate.dense.bias matches\n",
      "Layer encoder.text_encoding_layer.1.output.dense.weight matches\n",
      "Layer encoder.information_exchanging_layer.1.output.dense.weight matches\n",
      "Layer encoder.text_encoding_layer.1.output.dense.bias matches\n",
      "Layer encoder.information_exchanging_layer.1.output.dense.bias matches\n",
      "Layer encoder.text_encoding_layer.1.output.LayerNorm.weight matches\n",
      "Layer encoder.information_exchanging_layer.1.output.LayerNorm.weight matches\n",
      "Layer encoder.text_encoding_layer.1.output.LayerNorm.bias matches\n",
      "Layer encoder.information_exchanging_layer.1.output.LayerNorm.bias matches\n",
      "Layer encoder.text_encoding_layer.2.attention.self.query.weight matches\n",
      "Layer encoder.information_exchanging_layer.2.attention.self.query.weight matches\n",
      "Layer encoder.text_encoding_layer.2.attention.self.query.bias matches\n",
      "Layer encoder.information_exchanging_layer.2.attention.self.query.bias matches\n",
      "Layer encoder.text_encoding_layer.2.attention.self.key.weight matches\n",
      "Layer encoder.information_exchanging_layer.2.attention.self.key.weight matches\n",
      "Layer encoder.text_encoding_layer.2.attention.self.key.bias matches\n",
      "Layer encoder.information_exchanging_layer.2.attention.self.key.bias matches\n",
      "Layer encoder.text_encoding_layer.2.attention.self.value.weight matches\n",
      "Layer encoder.information_exchanging_layer.2.attention.self.value.weight matches\n",
      "Layer encoder.text_encoding_layer.2.attention.self.value.bias matches\n",
      "Layer encoder.information_exchanging_layer.2.attention.self.value.bias matches\n",
      "Layer encoder.text_encoding_layer.2.attention.output.dense.weight matches\n",
      "Layer encoder.information_exchanging_layer.2.attention.output.dense.weight matches\n",
      "Layer encoder.text_encoding_layer.2.attention.output.dense.bias matches\n",
      "Layer encoder.information_exchanging_layer.2.attention.output.dense.bias matches\n",
      "Layer encoder.text_encoding_layer.2.attention.output.LayerNorm.weight matches\n",
      "Layer encoder.information_exchanging_layer.2.attention.output.LayerNorm.weight matches\n",
      "Layer encoder.text_encoding_layer.2.attention.output.LayerNorm.bias matches\n",
      "Layer encoder.information_exchanging_layer.2.attention.output.LayerNorm.bias matches\n",
      "Layer encoder.text_encoding_layer.2.intermediate.dense.weight matches\n",
      "Layer encoder.information_exchanging_layer.2.intermediate.dense.weight matches\n",
      "Layer encoder.text_encoding_layer.2.intermediate.dense.bias matches\n",
      "Layer encoder.information_exchanging_layer.2.intermediate.dense.bias matches\n",
      "Layer encoder.text_encoding_layer.2.output.dense.weight matches\n",
      "Layer encoder.information_exchanging_layer.2.output.dense.weight matches\n",
      "Layer encoder.text_encoding_layer.2.output.dense.bias matches\n",
      "Layer encoder.information_exchanging_layer.2.output.dense.bias matches\n",
      "Layer encoder.text_encoding_layer.2.output.LayerNorm.weight matches\n",
      "Layer encoder.information_exchanging_layer.2.output.LayerNorm.weight matches\n",
      "Layer encoder.text_encoding_layer.2.output.LayerNorm.bias matches\n",
      "Layer encoder.information_exchanging_layer.2.output.LayerNorm.bias matches\n",
      "Layer encoder.text_encoding_layer.3.attention.self.query.weight matches\n",
      "Layer encoder.information_exchanging_layer.3.attention.self.query.weight matches\n",
      "Layer encoder.text_encoding_layer.3.attention.self.query.bias matches\n",
      "Layer encoder.information_exchanging_layer.3.attention.self.query.bias matches\n",
      "Layer encoder.text_encoding_layer.3.attention.self.key.weight matches\n",
      "Layer encoder.information_exchanging_layer.3.attention.self.key.weight matches\n",
      "Layer encoder.text_encoding_layer.3.attention.self.key.bias matches\n",
      "Layer encoder.information_exchanging_layer.3.attention.self.key.bias matches\n",
      "Layer encoder.text_encoding_layer.3.attention.self.value.weight matches\n",
      "Layer encoder.information_exchanging_layer.3.attention.self.value.weight matches\n",
      "Layer encoder.text_encoding_layer.3.attention.self.value.bias matches\n",
      "Layer encoder.information_exchanging_layer.3.attention.self.value.bias matches\n",
      "Layer encoder.text_encoding_layer.3.attention.output.dense.weight matches\n",
      "Layer encoder.information_exchanging_layer.3.attention.output.dense.weight matches\n",
      "Layer encoder.text_encoding_layer.3.attention.output.dense.bias matches\n",
      "Layer encoder.information_exchanging_layer.3.attention.output.dense.bias matches\n",
      "Layer encoder.text_encoding_layer.3.attention.output.LayerNorm.weight matches\n",
      "Layer encoder.information_exchanging_layer.3.attention.output.LayerNorm.weight matches\n",
      "Layer encoder.text_encoding_layer.3.attention.output.LayerNorm.bias matches\n",
      "Layer encoder.information_exchanging_layer.3.attention.output.LayerNorm.bias matches\n",
      "Layer encoder.text_encoding_layer.3.intermediate.dense.weight matches\n",
      "Layer encoder.information_exchanging_layer.3.intermediate.dense.weight matches\n",
      "Layer encoder.text_encoding_layer.3.intermediate.dense.bias matches\n",
      "Layer encoder.information_exchanging_layer.3.intermediate.dense.bias matches\n",
      "Layer encoder.text_encoding_layer.3.output.dense.weight matches\n",
      "Layer encoder.information_exchanging_layer.3.output.dense.weight matches\n",
      "Layer encoder.text_encoding_layer.3.output.dense.bias matches\n",
      "Layer encoder.information_exchanging_layer.3.output.dense.bias matches\n",
      "Layer encoder.text_encoding_layer.3.output.LayerNorm.weight matches\n",
      "Layer encoder.information_exchanging_layer.3.output.LayerNorm.weight matches\n",
      "Layer encoder.text_encoding_layer.3.output.LayerNorm.bias matches\n",
      "Layer encoder.information_exchanging_layer.3.output.LayerNorm.bias matches\n",
      "Layer encoder.text_encoding_layer.4.attention.self.query.weight matches\n",
      "Layer encoder.information_exchanging_layer.4.attention.self.query.weight matches\n",
      "Layer encoder.text_encoding_layer.4.attention.self.query.bias matches\n",
      "Layer encoder.information_exchanging_layer.4.attention.self.query.bias matches\n",
      "Layer encoder.text_encoding_layer.4.attention.self.key.weight matches\n",
      "Layer encoder.information_exchanging_layer.4.attention.self.key.weight matches\n",
      "Layer encoder.text_encoding_layer.4.attention.self.key.bias matches\n",
      "Layer encoder.information_exchanging_layer.4.attention.self.key.bias matches\n",
      "Layer encoder.text_encoding_layer.4.attention.self.value.weight matches\n",
      "Layer encoder.information_exchanging_layer.4.attention.self.value.weight matches\n",
      "Layer encoder.text_encoding_layer.4.attention.self.value.bias matches\n",
      "Layer encoder.information_exchanging_layer.4.attention.self.value.bias matches\n",
      "Layer encoder.text_encoding_layer.4.attention.output.dense.weight matches\n",
      "Layer encoder.information_exchanging_layer.4.attention.output.dense.weight matches\n",
      "Layer encoder.text_encoding_layer.4.attention.output.dense.bias matches\n",
      "Layer encoder.information_exchanging_layer.4.attention.output.dense.bias matches\n",
      "Layer encoder.text_encoding_layer.4.attention.output.LayerNorm.weight matches\n",
      "Layer encoder.information_exchanging_layer.4.attention.output.LayerNorm.weight matches\n",
      "Layer encoder.text_encoding_layer.4.attention.output.LayerNorm.bias matches\n",
      "Layer encoder.information_exchanging_layer.4.attention.output.LayerNorm.bias matches\n",
      "Layer encoder.text_encoding_layer.4.intermediate.dense.weight matches\n",
      "Layer encoder.information_exchanging_layer.4.intermediate.dense.weight matches\n",
      "Layer encoder.text_encoding_layer.4.intermediate.dense.bias matches\n",
      "Layer encoder.information_exchanging_layer.4.intermediate.dense.bias matches\n",
      "Layer encoder.text_encoding_layer.4.output.dense.weight matches\n",
      "Layer encoder.information_exchanging_layer.4.output.dense.weight matches\n",
      "Layer encoder.text_encoding_layer.4.output.dense.bias matches\n",
      "Layer encoder.information_exchanging_layer.4.output.dense.bias matches\n",
      "Layer encoder.text_encoding_layer.4.output.LayerNorm.weight matches\n",
      "Layer encoder.information_exchanging_layer.4.output.LayerNorm.weight matches\n",
      "Layer encoder.text_encoding_layer.4.output.LayerNorm.bias matches\n",
      "Layer encoder.information_exchanging_layer.4.output.LayerNorm.bias matches\n",
      "Layer encoder.text_encoding_layer.5.attention.self.query.weight matches\n",
      "Layer encoder.information_exchanging_layer.5.attention.self.query.weight matches\n",
      "Layer encoder.text_encoding_layer.5.attention.self.query.bias matches\n",
      "Layer encoder.information_exchanging_layer.5.attention.self.query.bias matches\n",
      "Layer encoder.text_encoding_layer.5.attention.self.key.weight matches\n",
      "Layer encoder.information_exchanging_layer.5.attention.self.key.weight matches\n",
      "Layer encoder.text_encoding_layer.5.attention.self.key.bias matches\n",
      "Layer encoder.information_exchanging_layer.5.attention.self.key.bias matches\n",
      "Layer encoder.text_encoding_layer.5.attention.self.value.weight matches\n",
      "Layer encoder.information_exchanging_layer.5.attention.self.value.weight matches\n",
      "Layer encoder.text_encoding_layer.5.attention.self.value.bias matches\n",
      "Layer encoder.information_exchanging_layer.5.attention.self.value.bias matches\n",
      "Layer encoder.text_encoding_layer.5.attention.output.dense.weight matches\n",
      "Layer encoder.information_exchanging_layer.5.attention.output.dense.weight matches\n",
      "Layer encoder.text_encoding_layer.5.attention.output.dense.bias matches\n",
      "Layer encoder.information_exchanging_layer.5.attention.output.dense.bias matches\n",
      "Layer encoder.text_encoding_layer.5.attention.output.LayerNorm.weight matches\n",
      "Layer encoder.information_exchanging_layer.5.attention.output.LayerNorm.weight matches\n",
      "Layer encoder.text_encoding_layer.5.attention.output.LayerNorm.bias matches\n",
      "Layer encoder.information_exchanging_layer.5.attention.output.LayerNorm.bias matches\n",
      "Layer encoder.text_encoding_layer.5.intermediate.dense.weight matches\n",
      "Layer encoder.information_exchanging_layer.5.intermediate.dense.weight matches\n",
      "Layer encoder.text_encoding_layer.5.intermediate.dense.bias matches\n",
      "Layer encoder.information_exchanging_layer.5.intermediate.dense.bias matches\n",
      "Layer encoder.text_encoding_layer.5.output.dense.weight matches\n",
      "Layer encoder.information_exchanging_layer.5.output.dense.weight matches\n",
      "Layer encoder.text_encoding_layer.5.output.dense.bias matches\n",
      "Layer encoder.information_exchanging_layer.5.output.dense.bias matches\n",
      "Layer encoder.text_encoding_layer.5.output.LayerNorm.weight matches\n",
      "Layer encoder.information_exchanging_layer.5.output.LayerNorm.weight matches\n",
      "Layer encoder.text_encoding_layer.5.output.LayerNorm.bias matches\n",
      "Layer encoder.information_exchanging_layer.5.output.LayerNorm.bias matches\n",
      "Layer encoder.text_encoding_layer.6.attention.self.query.weight matches\n",
      "Layer encoder.information_exchanging_layer.6.attention.self.query.weight matches\n",
      "Layer encoder.text_encoding_layer.6.attention.self.query.bias matches\n",
      "Layer encoder.information_exchanging_layer.6.attention.self.query.bias matches\n",
      "Layer encoder.text_encoding_layer.6.attention.self.key.weight matches\n",
      "Layer encoder.information_exchanging_layer.6.attention.self.key.weight matches\n",
      "Layer encoder.text_encoding_layer.6.attention.self.key.bias matches\n",
      "Layer encoder.information_exchanging_layer.6.attention.self.key.bias matches\n",
      "Layer encoder.text_encoding_layer.6.attention.self.value.weight matches\n",
      "Layer encoder.information_exchanging_layer.6.attention.self.value.weight matches\n",
      "Layer encoder.text_encoding_layer.6.attention.self.value.bias matches\n",
      "Layer encoder.information_exchanging_layer.6.attention.self.value.bias matches\n",
      "Layer encoder.text_encoding_layer.6.attention.output.dense.weight matches\n",
      "Layer encoder.information_exchanging_layer.6.attention.output.dense.weight matches\n",
      "Layer encoder.text_encoding_layer.6.attention.output.dense.bias matches\n",
      "Layer encoder.information_exchanging_layer.6.attention.output.dense.bias matches\n",
      "Layer encoder.text_encoding_layer.6.attention.output.LayerNorm.weight matches\n",
      "Layer encoder.information_exchanging_layer.6.attention.output.LayerNorm.weight matches\n",
      "Layer encoder.text_encoding_layer.6.attention.output.LayerNorm.bias matches\n",
      "Layer encoder.information_exchanging_layer.6.attention.output.LayerNorm.bias matches\n",
      "Layer encoder.text_encoding_layer.6.intermediate.dense.weight matches\n",
      "Layer encoder.information_exchanging_layer.6.intermediate.dense.weight matches\n",
      "Layer encoder.text_encoding_layer.6.intermediate.dense.bias matches\n",
      "Layer encoder.information_exchanging_layer.6.intermediate.dense.bias matches\n",
      "Layer encoder.text_encoding_layer.6.output.dense.weight matches\n",
      "Layer encoder.information_exchanging_layer.6.output.dense.weight matches\n",
      "Layer encoder.text_encoding_layer.6.output.dense.bias matches\n",
      "Layer encoder.information_exchanging_layer.6.output.dense.bias matches\n",
      "Layer encoder.text_encoding_layer.6.output.LayerNorm.weight matches\n",
      "Layer encoder.information_exchanging_layer.6.output.LayerNorm.weight matches\n",
      "Layer encoder.text_encoding_layer.6.output.LayerNorm.bias matches\n",
      "Layer encoder.information_exchanging_layer.6.output.LayerNorm.bias matches\n",
      "Layer encoder.text_encoding_layer.7.attention.self.query.weight matches\n",
      "Layer encoder.information_exchanging_layer.7.attention.self.query.weight matches\n",
      "Layer encoder.text_encoding_layer.7.attention.self.query.bias matches\n",
      "Layer encoder.information_exchanging_layer.7.attention.self.query.bias matches\n",
      "Layer encoder.text_encoding_layer.7.attention.self.key.weight matches\n",
      "Layer encoder.information_exchanging_layer.7.attention.self.key.weight matches\n",
      "Layer encoder.text_encoding_layer.7.attention.self.key.bias matches\n",
      "Layer encoder.information_exchanging_layer.7.attention.self.key.bias matches\n",
      "Layer encoder.text_encoding_layer.7.attention.self.value.weight matches\n",
      "Layer encoder.information_exchanging_layer.7.attention.self.value.weight matches\n",
      "Layer encoder.text_encoding_layer.7.attention.self.value.bias matches\n",
      "Layer encoder.information_exchanging_layer.7.attention.self.value.bias matches\n",
      "Layer encoder.text_encoding_layer.7.attention.output.dense.weight matches\n",
      "Layer encoder.information_exchanging_layer.7.attention.output.dense.weight matches\n",
      "Layer encoder.text_encoding_layer.7.attention.output.dense.bias matches\n",
      "Layer encoder.information_exchanging_layer.7.attention.output.dense.bias matches\n",
      "Layer encoder.text_encoding_layer.7.attention.output.LayerNorm.weight matches\n",
      "Layer encoder.information_exchanging_layer.7.attention.output.LayerNorm.weight matches\n",
      "Layer encoder.text_encoding_layer.7.attention.output.LayerNorm.bias matches\n",
      "Layer encoder.information_exchanging_layer.7.attention.output.LayerNorm.bias matches\n",
      "Layer encoder.text_encoding_layer.7.intermediate.dense.weight matches\n",
      "Layer encoder.information_exchanging_layer.7.intermediate.dense.weight matches\n",
      "Layer encoder.text_encoding_layer.7.intermediate.dense.bias matches\n",
      "Layer encoder.information_exchanging_layer.7.intermediate.dense.bias matches\n",
      "Layer encoder.text_encoding_layer.7.output.dense.weight matches\n",
      "Layer encoder.information_exchanging_layer.7.output.dense.weight matches\n",
      "Layer encoder.text_encoding_layer.7.output.dense.bias matches\n",
      "Layer encoder.information_exchanging_layer.7.output.dense.bias matches\n",
      "Layer encoder.text_encoding_layer.7.output.LayerNorm.weight matches\n",
      "Layer encoder.information_exchanging_layer.7.output.LayerNorm.weight matches\n",
      "Layer encoder.text_encoding_layer.7.output.LayerNorm.bias matches\n",
      "Layer encoder.information_exchanging_layer.7.output.LayerNorm.bias matches\n",
      "Layer encoder.text_encoding_layer.8.attention.self.query.weight matches\n",
      "Layer encoder.information_exchanging_layer.8.attention.self.query.weight matches\n",
      "Layer encoder.text_encoding_layer.8.attention.self.query.bias matches\n",
      "Layer encoder.information_exchanging_layer.8.attention.self.query.bias matches\n",
      "Layer encoder.text_encoding_layer.8.attention.self.key.weight matches\n",
      "Layer encoder.information_exchanging_layer.8.attention.self.key.weight matches\n",
      "Layer encoder.text_encoding_layer.8.attention.self.key.bias matches\n",
      "Layer encoder.information_exchanging_layer.8.attention.self.key.bias matches\n",
      "Layer encoder.text_encoding_layer.8.attention.self.value.weight matches\n",
      "Layer encoder.information_exchanging_layer.8.attention.self.value.weight matches\n",
      "Layer encoder.text_encoding_layer.8.attention.self.value.bias matches\n",
      "Layer encoder.information_exchanging_layer.8.attention.self.value.bias matches\n",
      "Layer encoder.text_encoding_layer.8.attention.output.dense.weight matches\n",
      "Layer encoder.information_exchanging_layer.8.attention.output.dense.weight matches\n",
      "Layer encoder.text_encoding_layer.8.attention.output.dense.bias matches\n",
      "Layer encoder.information_exchanging_layer.8.attention.output.dense.bias matches\n",
      "Layer encoder.text_encoding_layer.8.attention.output.LayerNorm.weight matches\n",
      "Layer encoder.information_exchanging_layer.8.attention.output.LayerNorm.weight matches\n",
      "Layer encoder.text_encoding_layer.8.attention.output.LayerNorm.bias matches\n",
      "Layer encoder.information_exchanging_layer.8.attention.output.LayerNorm.bias matches\n",
      "Layer encoder.text_encoding_layer.8.intermediate.dense.weight matches\n",
      "Layer encoder.information_exchanging_layer.8.intermediate.dense.weight matches\n",
      "Layer encoder.text_encoding_layer.8.intermediate.dense.bias matches\n",
      "Layer encoder.information_exchanging_layer.8.intermediate.dense.bias matches\n",
      "Layer encoder.text_encoding_layer.8.output.dense.weight matches\n",
      "Layer encoder.information_exchanging_layer.8.output.dense.weight matches\n",
      "Layer encoder.text_encoding_layer.8.output.dense.bias matches\n",
      "Layer encoder.information_exchanging_layer.8.output.dense.bias matches\n",
      "Layer encoder.text_encoding_layer.8.output.LayerNorm.weight matches\n",
      "Layer encoder.information_exchanging_layer.8.output.LayerNorm.weight matches\n",
      "Layer encoder.text_encoding_layer.8.output.LayerNorm.bias matches\n",
      "Layer encoder.information_exchanging_layer.8.output.LayerNorm.bias matches\n",
      "Layer encoder.text_encoding_layer.9.attention.self.query.weight matches\n",
      "Layer encoder.information_exchanging_layer.9.attention.self.query.weight matches\n",
      "Layer encoder.text_encoding_layer.9.attention.self.query.bias matches\n",
      "Layer encoder.information_exchanging_layer.9.attention.self.query.bias matches\n",
      "Layer encoder.text_encoding_layer.9.attention.self.key.weight matches\n",
      "Layer encoder.information_exchanging_layer.9.attention.self.key.weight matches\n",
      "Layer encoder.text_encoding_layer.9.attention.self.key.bias matches\n",
      "Layer encoder.information_exchanging_layer.9.attention.self.key.bias matches\n",
      "Layer encoder.text_encoding_layer.9.attention.self.value.weight matches\n",
      "Layer encoder.information_exchanging_layer.9.attention.self.value.weight matches\n",
      "Layer encoder.text_encoding_layer.9.attention.self.value.bias matches\n",
      "Layer encoder.information_exchanging_layer.9.attention.self.value.bias matches\n",
      "Layer encoder.text_encoding_layer.9.attention.output.dense.weight matches\n",
      "Layer encoder.information_exchanging_layer.9.attention.output.dense.weight matches\n",
      "Layer encoder.text_encoding_layer.9.attention.output.dense.bias matches\n",
      "Layer encoder.information_exchanging_layer.9.attention.output.dense.bias matches\n",
      "Layer encoder.text_encoding_layer.9.attention.output.LayerNorm.weight matches\n",
      "Layer encoder.information_exchanging_layer.9.attention.output.LayerNorm.weight matches\n",
      "Layer encoder.text_encoding_layer.9.attention.output.LayerNorm.bias matches\n",
      "Layer encoder.information_exchanging_layer.9.attention.output.LayerNorm.bias matches\n",
      "Layer encoder.text_encoding_layer.9.intermediate.dense.weight matches\n",
      "Layer encoder.information_exchanging_layer.9.intermediate.dense.weight matches\n",
      "Layer encoder.text_encoding_layer.9.intermediate.dense.bias matches\n",
      "Layer encoder.information_exchanging_layer.9.intermediate.dense.bias matches\n",
      "Layer encoder.text_encoding_layer.9.output.dense.weight matches\n",
      "Layer encoder.information_exchanging_layer.9.output.dense.weight matches\n",
      "Layer encoder.text_encoding_layer.9.output.dense.bias matches\n",
      "Layer encoder.information_exchanging_layer.9.output.dense.bias matches\n",
      "Layer encoder.text_encoding_layer.9.output.LayerNorm.weight matches\n",
      "Layer encoder.information_exchanging_layer.9.output.LayerNorm.weight matches\n",
      "Layer encoder.text_encoding_layer.9.output.LayerNorm.bias matches\n",
      "Layer encoder.information_exchanging_layer.9.output.LayerNorm.bias matches\n",
      "Layer encoder.text_encoding_layer.10.attention.self.query.weight matches\n",
      "Layer encoder.information_exchanging_layer.10.attention.self.query.weight matches\n",
      "Layer encoder.text_encoding_layer.10.attention.self.query.bias matches\n",
      "Layer encoder.information_exchanging_layer.10.attention.self.query.bias matches\n",
      "Layer encoder.text_encoding_layer.10.attention.self.key.weight matches\n",
      "Layer encoder.information_exchanging_layer.10.attention.self.key.weight matches\n",
      "Layer encoder.text_encoding_layer.10.attention.self.key.bias matches\n",
      "Layer encoder.information_exchanging_layer.10.attention.self.key.bias matches\n",
      "Layer encoder.text_encoding_layer.10.attention.self.value.weight matches\n",
      "Layer encoder.information_exchanging_layer.10.attention.self.value.weight matches\n",
      "Layer encoder.text_encoding_layer.10.attention.self.value.bias matches\n",
      "Layer encoder.information_exchanging_layer.10.attention.self.value.bias matches\n",
      "Layer encoder.text_encoding_layer.10.attention.output.dense.weight matches\n",
      "Layer encoder.information_exchanging_layer.10.attention.output.dense.weight matches\n",
      "Layer encoder.text_encoding_layer.10.attention.output.dense.bias matches\n",
      "Layer encoder.information_exchanging_layer.10.attention.output.dense.bias matches\n",
      "Layer encoder.text_encoding_layer.10.attention.output.LayerNorm.weight matches\n",
      "Layer encoder.information_exchanging_layer.10.attention.output.LayerNorm.weight matches\n",
      "Layer encoder.text_encoding_layer.10.attention.output.LayerNorm.bias matches\n",
      "Layer encoder.information_exchanging_layer.10.attention.output.LayerNorm.bias matches\n",
      "Layer encoder.text_encoding_layer.10.intermediate.dense.weight matches\n",
      "Layer encoder.information_exchanging_layer.10.intermediate.dense.weight matches\n",
      "Layer encoder.text_encoding_layer.10.intermediate.dense.bias matches\n",
      "Layer encoder.information_exchanging_layer.10.intermediate.dense.bias matches\n",
      "Layer encoder.text_encoding_layer.10.output.dense.weight matches\n",
      "Layer encoder.information_exchanging_layer.10.output.dense.weight matches\n",
      "Layer encoder.text_encoding_layer.10.output.dense.bias matches\n",
      "Layer encoder.information_exchanging_layer.10.output.dense.bias matches\n",
      "Layer encoder.text_encoding_layer.10.output.LayerNorm.weight matches\n",
      "Layer encoder.information_exchanging_layer.10.output.LayerNorm.weight matches\n",
      "Layer encoder.text_encoding_layer.10.output.LayerNorm.bias matches\n",
      "Layer encoder.information_exchanging_layer.10.output.LayerNorm.bias matches\n",
      "Layer encoder.text_encoding_layer.11.attention.self.query.weight matches\n",
      "Layer encoder.information_exchanging_layer.11.attention.self.query.weight matches\n",
      "Layer encoder.text_encoding_layer.11.attention.self.query.bias matches\n",
      "Layer encoder.information_exchanging_layer.11.attention.self.query.bias matches\n",
      "Layer encoder.text_encoding_layer.11.attention.self.key.weight matches\n",
      "Layer encoder.information_exchanging_layer.11.attention.self.key.weight matches\n",
      "Layer encoder.text_encoding_layer.11.attention.self.key.bias matches\n",
      "Layer encoder.information_exchanging_layer.11.attention.self.key.bias matches\n",
      "Layer encoder.text_encoding_layer.11.attention.self.value.weight matches\n",
      "Layer encoder.information_exchanging_layer.11.attention.self.value.weight matches\n",
      "Layer encoder.text_encoding_layer.11.attention.self.value.bias matches\n",
      "Layer encoder.information_exchanging_layer.11.attention.self.value.bias matches\n",
      "Layer encoder.text_encoding_layer.11.attention.output.dense.weight matches\n",
      "Layer encoder.information_exchanging_layer.11.attention.output.dense.weight matches\n",
      "Layer encoder.text_encoding_layer.11.attention.output.dense.bias matches\n",
      "Layer encoder.information_exchanging_layer.11.attention.output.dense.bias matches\n",
      "Layer encoder.text_encoding_layer.11.attention.output.LayerNorm.weight matches\n",
      "Layer encoder.information_exchanging_layer.11.attention.output.LayerNorm.weight matches\n",
      "Layer encoder.text_encoding_layer.11.attention.output.LayerNorm.bias matches\n",
      "Layer encoder.information_exchanging_layer.11.attention.output.LayerNorm.bias matches\n",
      "Layer encoder.text_encoding_layer.11.intermediate.dense.weight matches\n",
      "Layer encoder.information_exchanging_layer.11.intermediate.dense.weight matches\n",
      "Layer encoder.text_encoding_layer.11.intermediate.dense.bias matches\n",
      "Layer encoder.information_exchanging_layer.11.intermediate.dense.bias matches\n",
      "Layer encoder.text_encoding_layer.11.output.dense.weight matches\n",
      "Layer encoder.information_exchanging_layer.11.output.dense.weight matches\n",
      "Layer encoder.text_encoding_layer.11.output.dense.bias matches\n",
      "Layer encoder.information_exchanging_layer.11.output.dense.bias matches\n",
      "Layer encoder.text_encoding_layer.11.output.LayerNorm.weight matches\n",
      "Layer encoder.information_exchanging_layer.11.output.LayerNorm.weight matches\n",
      "Layer encoder.text_encoding_layer.11.output.LayerNorm.bias matches\n",
      "Layer encoder.information_exchanging_layer.11.output.LayerNorm.bias matches\n",
      "Layer pooler.dense.weight not found.\n",
      "Layer pooler.dense.weight not found.\n",
      "Layer pooler.dense.bias not found.\n",
      "Layer pooler.dense.bias not found.\n"
     ]
    }
   ],
   "source": [
    "lt_weights = longtriever.state_dict().keys()\n",
    "\n",
    "for name, param in base_model.named_parameters():\n",
    "    text_name = name.replace(\"layer\", \"text_encoding_layer\")\n",
    "    # print(name)\n",
    "    if text_name in lt_weights:\n",
    "        lt_text_param = longtriever.state_dict()[text_name]\n",
    "        if torch.equal(param, lt_text_param):\n",
    "            print(f\"Layer {text_name} matches\")\n",
    "            pass\n",
    "        else:\n",
    "            print(f\"Layer {text_name} does not match\")\n",
    "            pass\n",
    "    else:\n",
    "        print(f\"Layer {text_name} not found.\")\n",
    "        pass\n",
    "\n",
    "    info_name = name.replace(\"layer\", \"information_exchanging_layer\")\n",
    "    if info_name in lt_weights:\n",
    "        lt_info_param = longtriever.state_dict()[info_name]\n",
    "        if torch.equal(param, lt_info_param):\n",
    "            print(f\"Layer {info_name} matches\")\n",
    "            pass\n",
    "        else:\n",
    "            print(f\"Layer {info_name} does not match\")\n",
    "            pass\n",
    "    else:\n",
    "        print(f\"Layer {info_name} not found.\")\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Torchviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dpr_model = BiEncoder(\n",
    "    model_path=(\"google-bert/bert-base-uncased\", \"google-bert/bert-base-uncased\"),\n",
    "    normalize=False,\n",
    "    prompts={\"query\": \"\", \"passage\": \"\"},\n",
    "    attn_implementation=\"eager\", \n",
    "    sep = \" [SEP] \", \n",
    "    batch_size=batch_size\n",
    ")\n",
    "dpr_model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = os.path.join(\"/Tmp/lvpoellhuber/datasets/nq\", \"train_triplets.pt\")\n",
    "dataloader = get_triplets_dataloader(batch_size=batch_size, dataset_path=dataset_path,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = os.path.join(\"/Tmp/lvpoellhuber/datasets/nq\", \"train_pairs.pt\")\n",
    "dataloader = get_pairs_dataloader(batch_size=batch_size, dataset_path=dataset_path,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dpr_params = {}\n",
    "\n",
    "q_params = dict(dpr_model.q_model.named_parameters())\n",
    "doc_params = dict(dpr_model.doc_model.named_parameters())\n",
    "for param in q_params.keys():\n",
    "    q_param = q_params[param]\n",
    "    doc_param = doc_params[param]\n",
    "\n",
    "    dpr_params[\"q_encoder.\"+param] = q_param \n",
    "    dpr_params[\"doc_encoder.\"+param] = doc_param "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = [item for item in dataloader][0]\n",
    "\n",
    "queries = batch[\"queries\"]\n",
    "# documents = batch[\"documents\"]\n",
    "positives = batch[\"positives\"]\n",
    "negatives = batch[\"negatives\"]\n",
    "documents = positives + negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dpr_step(queries, documents):\n",
    "    q_embeddings = dpr_model.encode_queries(queries, convert_to_tensor=True) # All three 16x512\n",
    "    doc_embeddings = dpr_model.encode_corpus(documents, convert_to_tensor=True) # All three 16x512\n",
    "\n",
    "    loss = contrastive_loss(q_embeddings, doc_embeddings)\n",
    "\n",
    "    return loss\n",
    "\n",
    "# loss = dpr_step(queries, documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/u/poellhul/Documents/Masters/benchmarkIR-slurm/src/retrieval/dual_graph.pdf'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_dot(dpr_step(queries, documents), params=dpr_params, show_attrs=True).render(\"/u/poellhul/Documents/Masters/benchmarkIR-slurm/src/retrieval/dual_graph\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/u/poellhul/Documents/Masters/benchmarkIR-slurm/src/retrieval/doc_graph.pdf'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_dot(dpr_model.encode_corpus(documents, convert_to_tensor=True), params=dpr_params, show_attrs=True).render(\"/u/poellhul/Documents/Masters/benchmarkIR-slurm/src/retrieval/doc_graph\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Longtriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 3\n",
    "\n",
    "biencoder = LongBiEncoder(\n",
    "    model_path=\"/Tmp/lvpoellhuber/models/longtriever/pretrained/bert-base-uncased\",\n",
    "    normalize=False,\n",
    "    prompts={\"query\": \"\", \"passage\": \"\"},\n",
    "    attn_implementation=\"eager\", \n",
    "    sep = \" [SEP] \", \n",
    "    batch_size=batch_size,\n",
    "    max_block_length=512, \n",
    "    max_num_blocks=8,\n",
    "    model_type =  \"longtriever\"\n",
    ")\n",
    "biencoder.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = get_pairs_dataloader(batch_size=batch_size, dataset_path=\"/Tmp/lvpoellhuber/datasets/msmarco-doc/train_pairs.pt\")\n",
    "\n",
    "biencoder_params = {}\n",
    "\n",
    "q_params = dict(biencoder.q_model.named_parameters())\n",
    "# doc_params = dict(biencoder.doc_model.named_parameters())\n",
    "for param in q_params.keys():\n",
    "    q_param = q_params[param]\n",
    "    # doc_param = doc_params[param]\n",
    "\n",
    "    biencoder_params[\"q_encoder.\"+param] = q_param \n",
    "    # biencoder_params[\"doc_encoder.\"+param] = doc_param "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fct = InBatchNegativeLoss()\n",
    "def biencoder_step(queries, documents):\n",
    "    q_embeddings = biencoder.encode_queries(queries, convert_to_tensor=True) # All three 16x512\n",
    "    doc_embeddings = biencoder.encode_corpus(documents, convert_to_tensor=True) # All three 16x512\n",
    "\n",
    "    loss = loss_fct(q_embeddings, doc_embeddings)\n",
    "\n",
    "    return loss\n",
    "\n",
    "# loss = dpr_step(queries, documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = [item for item in dataloader][0]\n",
    "\n",
    "queries = batch[\"queries\"]\n",
    "documents = batch[\"documents\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/u/poellhul/Documents/Masters/benchmarkIR-slurm/src/retrieval/single_longtriever.pdf'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_dot(biencoder_step(queries, documents), params=biencoder_params, show_attrs=True).render(\"/u/poellhul/Documents/Masters/benchmarkIR-slurm/src/retrieval/single_longtriever\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloader inspection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = os.path.join(\"/Tmp/lvpoellhuber/datasets/nq/train_pairs.pt\")\n",
    "dataloader = get_pairs_dataloader(batch_size=12, dataset_path=dataset_path)\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in dataloader:\n",
    "    print(\"huh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MS Marco-doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = get_pairs_dataloader(\n",
    "        batch_size=3, \n",
    "        dataset_path=\"/Tmp/lvpoellhuber/datasets/msmarco-doc/train_pairs.pt\", \n",
    "        pin_memory=True, \n",
    "        prefetch_factor=2, \n",
    "        num_workers = 4\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "def seed_everything(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # if using multi-GPU\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "\n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hidden State Shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
       "        [ 0.1767,  0.7940,  0.0560,  ...,  0.0948,  0.9265,  0.5313],\n",
       "        [ 0.2671,  0.3312,  0.9351,  ...,  0.9478,  0.0695,  0.5010],\n",
       "        ...,\n",
       "        [ 0.5979,  0.3564,  0.4494,  ...,  0.1155,  0.8178,  0.7187],\n",
       "        [ 0.0975,  0.5869,  0.5117,  ...,  0.7252,  0.8423,  0.8690],\n",
       "        [ 0.6025,  0.1467,  0.8125,  ...,  0.7794,  0.3533,  0.2911]])"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_states= torch.rand(3, 8, 513, 768)\n",
    "\n",
    "hidden_states[:, :, 0, :] = -1\n",
    "\n",
    "hidden_states[0, 0, :, :]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right above is the hidden state for block 0 of example 0. Its first token is the DOC token, which is assigned the value -1.0000. The matrix above is of shape 513x768. The **VERTICAL** line represents the words (513), while the **horizontal** line represents the vector representations (768).\n",
    "\n",
    "The DOC token (1) will have the value -1 for its entire representation, as it represents the indexes we want to conserve. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "        [10., 10., 10.,  ..., 10., 10., 10.],\n",
       "        [ 0.,  0.,  1.,  ...,  1.,  0.,  1.],\n",
       "        ...,\n",
       "        [ 1.,  0.,  0.,  ...,  0.,  1.,  1.],\n",
       "        [ 0.,  1.,  1.,  ...,  1.,  1.,  1.],\n",
       "        [ 1.,  0.,  1.,  ...,  1.,  0.,  0.]])"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_states[:, torch.arange(8), torch.arange(8)+1, :] = ((torch.arange(8)+1)*10.0).view(8, 1).repeat(3, 1, 768)\n",
    "\n",
    "\n",
    "hidden_states[0, 0, :, :].round()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see the hidden state above with the CLS tokens. This is again block 0 of example 0. The CLS token is at position 1 for this block (it would be 0 without the DOC token). Each CLS token will have its own index value, multiplied by then when it is the current block's token. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
       "        [ 0.9359,  0.4442,  0.4246,  ...,  0.7920,  0.0916,  0.8704],\n",
       "        [20.0000, 20.0000, 20.0000,  ..., 20.0000, 20.0000, 20.0000],\n",
       "        ...,\n",
       "        [ 0.5618,  0.3202,  0.8232,  ...,  0.7665,  0.3409,  0.5628],\n",
       "        [ 0.6446,  0.5704,  0.3970,  ...,  0.7285,  0.4527,  0.9810],\n",
       "        [ 0.8799,  0.8465,  0.5363,  ...,  0.4101,  0.4948,  0.6691]])"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_states[0, 1, :, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can verify with block 1 of example 0, where the CLS token (20.0000) is at position 2. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [2., 2., 2.,  ..., 2., 2., 2.],\n",
       "         [3., 3., 3.,  ..., 3., 3., 3.],\n",
       "         ...,\n",
       "         [6., 6., 6.,  ..., 6., 6., 6.],\n",
       "         [7., 7., 7.,  ..., 7., 7., 7.],\n",
       "         [8., 8., 8.,  ..., 8., 8., 8.]],\n",
       "\n",
       "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [2., 2., 2.,  ..., 2., 2., 2.],\n",
       "         [3., 3., 3.,  ..., 3., 3., 3.],\n",
       "         ...,\n",
       "         [6., 6., 6.,  ..., 6., 6., 6.],\n",
       "         [7., 7., 7.,  ..., 7., 7., 7.],\n",
       "         [8., 8., 8.,  ..., 8., 8., 8.]],\n",
       "\n",
       "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [2., 2., 2.,  ..., 2., 2., 2.],\n",
       "         [3., 3., 3.,  ..., 3., 3., 3.],\n",
       "         ...,\n",
       "         [6., 6., 6.,  ..., 6., 6., 6.],\n",
       "         [7., 7., 7.,  ..., 7., 7., 7.],\n",
       "         [8., 8., 8.,  ..., 8., 8., 8.]]])"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls_tokens = hidden_states[:, torch.arange(8), torch.arange(8)+1, :]/10 # Divide by ten to indicate they're not the main tokens\n",
    "cls_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we verify for example 0 that the extracted CLS tokens truly are the correct ones. \n",
    "\n",
    "Now, I need to figure out how to select the lower diagonal and modify it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Triangular matrices\n",
    "### With 3x8x513x768\n",
    "\n",
    "Unfortunately I coded this assuming the 3x8 were separate, rather than 24. It's more logical and easier to code, but it won't fit with the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 8, 513, 768)"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B, N, L_, D = hidden_states.shape\n",
    "B, N, L_, D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre\n",
    "pre_indices = torch.tril_indices(N, L_)\n",
    "mask = pre_indices[1] != 0\n",
    "filtered_indices = pre_indices[:, mask]\n",
    "\n",
    "extended_cls_tokens = cls_tokens.repeat(N, 1, 1, 1).view(B, N, N, D)\n",
    "pre_cls_indices = torch.tril_indices(N, N, offset=-1)\n",
    "\n",
    "hidden_states[:, filtered_indices[0], filtered_indices[1], :] = extended_cls_tokens[:, pre_cls_indices[0], pre_cls_indices[1], :]\n",
    "\n",
    "# post\n",
    "post_indices = torch.triu_indices(N, L_, offset=L_-N+1)\n",
    "mask = post_indices[1] != hidden_states.shape[1] - 1\n",
    "filtered_indices = post_indices[:, mask]\n",
    "\n",
    "post_cls_indices = torch.triu_indices(N, N, offset=1)\n",
    "\n",
    "hidden_states[:, filtered_indices[0], filtered_indices[1], :] = extended_cls_tokens[:, post_cls_indices[0], post_cls_indices[1], :]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Memory Usage\n",
    "\n",
    "Initial CPU memory usage (with loaded modules): ~2.7GB / 62.6GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dpr_model = LongBiEncoder.from_pretrained(\"/Tmp/lvpoellhuber/models/longtriever/longtriever_shared\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After loading model: 2.7GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus, queries, qrels = GenericDataLoader(data_folder=\"/Tmp/lvpoellhuber/datasets/msmarco-doc\").load(split=\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Short dataset: 5.84GB\n",
    "Long dataset: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_embeddings = np.ones((3201821, 768))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After loading dummy embeddings: 59.9GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "passage_ids = np.arange(3201821)\n",
    "index = faiss.IndexFlatIP(corpus_embeddings.shape[1])\n",
    "\n",
    "for start in range(0, len(passage_ids), 50000):\n",
    "    index.add(corpus_embeddings[start : start + 50000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bmir-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

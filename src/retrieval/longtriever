digraph {
	graph [size="1312.6499999999999,1312.6499999999999"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	140067043409696 [label="
 ()" fillcolor=darkolivegreen1]
	140067008064592 [label="NllLossBackward0
----------------------------------
ignore_index: 18446744073709551516
reduction   :                    1
self        :       [saved tensor]
target      :       [saved tensor]
total_weight:       [saved tensor]
weight      :                 None"]
	140067008054704 -> 140067008064592
	140067008054704 [label="LogSoftmaxBackward0
----------------------
dim   :              1
result: [saved tensor]"]
	140067948731104 -> 140067008054704
	140067948731104 [label="MmBackward0
--------------------------------
mat2            : [saved tensor]
mat2_sym_sizes  :       (768, 3)
mat2_sym_strides:      (1, 3072)
self            : [saved tensor]
self_sym_sizes  :       (3, 768)
self_sym_strides:      (1536, 1)"]
	140067948731008 -> 140067948731104
	140067948731008 [label="SqueezeBackward1
---------------------------
dim           :           1
self_sym_sizes: (3, 1, 768)"]
	140067053484976 -> 140067948731008
	140067053484976 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 1, 768)
start         :                   0
step          :                   1"]
	140067018948368 -> 140067053484976
	140067018948368 [label="SliceBackward0
---------------------------
dim           :           1
end           :           1
self_sym_sizes: (3, 2, 768)
start         :           0
step          :           1"]
	140067018948512 -> 140067018948368
	140067018948512 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:         (3, 2, 768)
start         :                   0
step          :                   1"]
	140067053485216 -> 140067018948512
	140067053485216 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140067053483488 -> 140067053485216
	140067053483488 [label="AddBackward0
------------
alpha: 1"]
	140067064880272 -> 140067053483488
	140067064880272 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067053412960 -> 140067064880272
	140067053412960 [label="ViewBackward0
------------------------
self_sym_sizes: (6, 768)"]
	140066688038512 -> 140067053412960
	140066688038512 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (6, 3072)
mat1_sym_strides:      (3072, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (3072, 768)
mat2_sym_strides:      (1, 3072)"]
	140066688027280 -> 140066688038512
	140066904419824 [label="q_encoder.encoder.information_exchanging_layer.11.output.dense.bias
 (768)" fillcolor=lightblue]
	140066904419824 -> 140066688027280
	140066688027280 [label=AccumulateGrad]
	140066688027760 -> 140066688038512
	140066688027760 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 2, 3072)"]
	140066688025072 -> 140066688027760
	140066688025072 [label="GeluBackward0
---------------------------
approximate:           none
self       : [saved tensor]"]
	140066688038848 -> 140066688025072
	140066688038848 [label="ViewBackward0
-------------------------
self_sym_sizes: (6, 3072)"]
	140066688039184 -> 140066688038848
	140066688039184 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (6, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (768, 3072)
mat2_sym_strides:       (1, 768)"]
	140066688030640 -> 140066688039184
	140066904419664 [label="q_encoder.encoder.information_exchanging_layer.11.intermediate.dense.bias
 (3072)" fillcolor=lightblue]
	140066904419664 -> 140066688030640
	140066688030640 [label=AccumulateGrad]
	140066688025984 -> 140066688039184
	140066688025984 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140067053472544 -> 140066688025984
	140067053472544 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140066688031024 -> 140067053472544
	140066688031024 [label="AddBackward0
------------
alpha: 1"]
	140066688026656 -> 140066688031024
	140066688026656 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140066688029968 -> 140066688026656
	140066688029968 [label="ViewBackward0
------------------------
self_sym_sizes: (6, 768)"]
	140066688028528 -> 140066688029968
	140066688028528 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (6, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140066688029392 -> 140066688028528
	140066904419264 [label="q_encoder.encoder.information_exchanging_layer.11.attention.output.dense.bias
 (768)" fillcolor=lightblue]
	140066904419264 -> 140066688029392
	140066688029392 [label=AccumulateGrad]
	140067054841008 -> 140066688028528
	140067054841008 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140067053501696 -> 140067054841008
	140067053501696 [label="ViewBackward0
------------------------------
self_sym_sizes: (3, 2, 12, 64)"]
	140067053501744 -> 140067053501696
	140067053501744 [label=CloneBackward0]
	140067053487392 -> 140067053501744
	140067053487392 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067053498000 -> 140067053487392
	140067053498000 [label="UnsafeViewBackward0
---------------------------
self_sym_sizes: (36, 2, 64)"]
	140067053486864 -> 140067053498000
	140067053486864 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140067065216592 -> 140067053486864
	140067065216592 [label="ViewBackward0
-----------------------------
self_sym_sizes: (3, 12, 2, 2)"]
	140067065216208 -> 140067065216592
	140067065216208 [label="ExpandBackward0
-----------------------------
self_sym_sizes: (3, 12, 2, 2)"]
	140067053399888 -> 140067065216208
	140067053399888 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067053402288 -> 140067053399888
	140067053402288 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140067053391632 -> 140067053402288
	140067053391632 [label="AddBackward0
------------
alpha: 1"]
	140066688174736 -> 140067053391632
	140066688174736 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140066688175120 -> 140066688174736
	140066688175120 [label="UnsafeViewBackward0
--------------------------
self_sym_sizes: (36, 2, 2)"]
	140066688178960 -> 140066688175120
	140066688178960 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140066688175792 -> 140066688178960
	140066688175792 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 2, 64)"]
	140066688174208 -> 140066688175792
	140066688174208 [label=CloneBackward0]
	140066688174448 -> 140066688174208
	140066688174448 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 2, 64)"]
	140066688173584 -> 140066688174448
	140066688173584 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140066688173680 -> 140066688173584
	140066688173680 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140066688174544 -> 140066688173680
	140066688174544 [label="ViewBackward0
------------------------
self_sym_sizes: (6, 768)"]
	140066688180016 -> 140066688174544
	140066688180016 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (6, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140066688180304 -> 140066688180016
	140066904418784 [label="q_encoder.encoder.information_exchanging_layer.11.attention.self.query.bias
 (768)" fillcolor=lightblue]
	140066904418784 -> 140066688180304
	140066688180304 [label=AccumulateGrad]
	140066688180592 -> 140066688180016
	140066688180592 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140066688039616 -> 140066688180592
	140066688039616 [label="CatBackward0
------------
dim: 1"]
	140066688180688 -> 140066688039616
	140066688180688 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 1, 768)
start         :                   0
step          :                   1"]
	140066688179920 -> 140066688180688
	140066688179920 [label="SliceBackward0
---------------------------
dim           :           1
end           :           1
self_sym_sizes: (3, 2, 768)
start         :           0
step          :           1"]
	140066688179440 -> 140066688179920
	140066688179440 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:         (3, 2, 768)
start         :                   0
step          :                   1"]
	140066688180496 -> 140066688179440
	140066688180496 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140066688178240 -> 140066688180496
	140066688178240 [label="AddBackward0
------------
alpha: 1"]
	140066688178432 -> 140066688178240
	140066688178432 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140066688177328 -> 140066688178432
	140066688177328 [label="ViewBackward0
------------------------
self_sym_sizes: (6, 768)"]
	140066688177280 -> 140066688177328
	140066688177280 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (6, 3072)
mat1_sym_strides:      (3072, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (3072, 768)
mat2_sym_strides:      (1, 3072)"]
	140066688178000 -> 140066688177280
	140066904418544 [label="q_encoder.encoder.information_exchanging_layer.10.output.dense.bias
 (768)" fillcolor=lightblue]
	140066904418544 -> 140066688178000
	140066688178000 [label=AccumulateGrad]
	140066688178672 -> 140066688177280
	140066688178672 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 2, 3072)"]
	140066688174016 -> 140066688178672
	140066688174016 [label="GeluBackward0
---------------------------
approximate:           none
self       : [saved tensor]"]
	140066688177664 -> 140066688174016
	140066688177664 [label="ViewBackward0
-------------------------
self_sym_sizes: (6, 3072)"]
	140066688176560 -> 140066688177664
	140066688176560 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (6, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (768, 3072)
mat2_sym_strides:       (1, 768)"]
	140066688176608 -> 140066688176560
	140066904418384 [label="q_encoder.encoder.information_exchanging_layer.10.intermediate.dense.bias
 (3072)" fillcolor=lightblue]
	140066904418384 -> 140066688176608
	140066688176608 [label=AccumulateGrad]
	140066688175552 -> 140066688176560
	140066688175552 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140066688178720 -> 140066688175552
	140066688178720 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140066688173392 -> 140066688178720
	140066688173392 [label="AddBackward0
------------
alpha: 1"]
	140066688173488 -> 140066688173392
	140066688173488 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140066688172144 -> 140066688173488
	140066688172144 [label="ViewBackward0
------------------------
self_sym_sizes: (6, 768)"]
	140066688182656 -> 140066688172144
	140066688182656 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (6, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140066688172384 -> 140066688182656
	140066904418064 [label="q_encoder.encoder.information_exchanging_layer.10.attention.output.dense.bias
 (768)" fillcolor=lightblue]
	140066904418064 -> 140066688172384
	140066688172384 [label=AccumulateGrad]
	140066688176128 -> 140066688182656
	140066688176128 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140066688172672 -> 140066688176128
	140066688172672 [label="ViewBackward0
------------------------------
self_sym_sizes: (3, 2, 12, 64)"]
	140066688185008 -> 140066688172672
	140066688185008 [label=CloneBackward0]
	140066688178480 -> 140066688185008
	140066688178480 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140066688175456 -> 140066688178480
	140066688175456 [label="UnsafeViewBackward0
---------------------------
self_sym_sizes: (36, 2, 64)"]
	140066688177088 -> 140066688175456
	140066688177088 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140066688173200 -> 140066688177088
	140066688173200 [label="ViewBackward0
-----------------------------
self_sym_sizes: (3, 12, 2, 2)"]
	140066688178528 -> 140066688173200
	140066688178528 [label="ExpandBackward0
-----------------------------
self_sym_sizes: (3, 12, 2, 2)"]
	140066688173008 -> 140066688178528
	140066688173008 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140066819363440 -> 140066688173008
	140066819363440 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140066819366992 -> 140066819363440
	140066819366992 [label="AddBackward0
------------
alpha: 1"]
	140066819371504 -> 140066819366992
	140066819371504 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140066819369536 -> 140066819371504
	140066819369536 [label="UnsafeViewBackward0
--------------------------
self_sym_sizes: (36, 2, 2)"]
	140066819360512 -> 140066819369536
	140066819360512 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140066819369008 -> 140066819360512
	140066819369008 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 2, 64)"]
	140066819372176 -> 140066819369008
	140066819372176 [label=CloneBackward0]
	140066819366368 -> 140066819372176
	140066819366368 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 2, 64)"]
	140066819366080 -> 140066819366368
	140066819366080 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140066819365072 -> 140066819366080
	140066819365072 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140066819374864 -> 140066819365072
	140066819374864 [label="ViewBackward0
------------------------
self_sym_sizes: (6, 768)"]
	140066819371216 -> 140066819374864
	140066819371216 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (6, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140066819365840 -> 140066819371216
	140066904417584 [label="q_encoder.encoder.information_exchanging_layer.10.attention.self.query.bias
 (768)" fillcolor=lightblue]
	140066904417584 -> 140066819365840
	140066819365840 [label=AccumulateGrad]
	140066819366656 -> 140066819371216
	140066819366656 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140066688174256 -> 140066819366656
	140066688174256 [label="CatBackward0
------------
dim: 1"]
	140066819367664 -> 140066688174256
	140066819367664 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 1, 768)
start         :                   0
step          :                   1"]
	140066819372464 -> 140066819367664
	140066819372464 [label="SliceBackward0
---------------------------
dim           :           1
end           :           1
self_sym_sizes: (3, 2, 768)
start         :           0
step          :           1"]
	140066819360944 -> 140066819372464
	140066819360944 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:         (3, 2, 768)
start         :                   0
step          :                   1"]
	140066819359168 -> 140066819360944
	140066819359168 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140066819362336 -> 140066819359168
	140066819362336 [label="AddBackward0
------------
alpha: 1"]
	140066819368864 -> 140066819362336
	140066819368864 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140066819369392 -> 140066819368864
	140066819369392 [label="ViewBackward0
------------------------
self_sym_sizes: (6, 768)"]
	140066819369248 -> 140066819369392
	140066819369248 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (6, 3072)
mat1_sym_strides:      (3072, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (3072, 768)
mat2_sym_strides:      (1, 3072)"]
	140066819363104 -> 140066819369248
	140066904417264 [label="q_encoder.encoder.information_exchanging_layer.9.output.dense.bias
 (768)" fillcolor=lightblue]
	140066904417264 -> 140066819363104
	140066819363104 [label=AccumulateGrad]
	140066819366032 -> 140066819369248
	140066819366032 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 2, 3072)"]
	140066819375056 -> 140066819366032
	140066819375056 [label="GeluBackward0
---------------------------
approximate:           none
self       : [saved tensor]"]
	140066819364160 -> 140066819375056
	140066819364160 [label="ViewBackward0
-------------------------
self_sym_sizes: (6, 3072)"]
	140066819371792 -> 140066819364160
	140066819371792 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (6, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (768, 3072)
mat2_sym_strides:       (1, 768)"]
	140066819365792 -> 140066819371792
	140066904417104 [label="q_encoder.encoder.information_exchanging_layer.9.intermediate.dense.bias
 (3072)" fillcolor=lightblue]
	140066904417104 -> 140066819365792
	140066819365792 [label=AccumulateGrad]
	140066819365024 -> 140066819371792
	140066819365024 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140066819364832 -> 140066819365024
	140066819364832 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140066819369488 -> 140066819364832
	140066819369488 [label="AddBackward0
------------
alpha: 1"]
	140066819366800 -> 140066819369488
	140066819366800 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140066819359840 -> 140066819366800
	140066819359840 [label="ViewBackward0
------------------------
self_sym_sizes: (6, 768)"]
	140066819372992 -> 140066819359840
	140066819372992 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (6, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140066819370112 -> 140066819372992
	140066904416704 [label="q_encoder.encoder.information_exchanging_layer.9.attention.output.dense.bias
 (768)" fillcolor=lightblue]
	140066904416704 -> 140066819370112
	140066819370112 [label=AccumulateGrad]
	140066819364592 -> 140066819372992
	140066819364592 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140066819363728 -> 140066819364592
	140066819363728 [label="ViewBackward0
------------------------------
self_sym_sizes: (3, 2, 12, 64)"]
	140066819362384 -> 140066819363728
	140066819362384 [label=CloneBackward0]
	140066819363536 -> 140066819362384
	140066819363536 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140066819366272 -> 140066819363536
	140066819366272 [label="UnsafeViewBackward0
---------------------------
self_sym_sizes: (36, 2, 64)"]
	140066819368000 -> 140066819366272
	140066819368000 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140066819359792 -> 140066819368000
	140066819359792 [label="ViewBackward0
-----------------------------
self_sym_sizes: (3, 12, 2, 2)"]
	140066819373952 -> 140066819359792
	140066819373952 [label="ExpandBackward0
-----------------------------
self_sym_sizes: (3, 12, 2, 2)"]
	140066819368528 -> 140066819373952
	140066819368528 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140066819364208 -> 140066819368528
	140066819364208 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140066819361184 -> 140066819364208
	140066819361184 [label="AddBackward0
------------
alpha: 1"]
	140066819371264 -> 140066819361184
	140066819371264 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140066819367328 -> 140066819371264
	140066819367328 [label="UnsafeViewBackward0
--------------------------
self_sym_sizes: (36, 2, 2)"]
	140066819364688 -> 140066819367328
	140066819364688 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140066819364400 -> 140066819364688
	140066819364400 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 2, 64)"]
	140066819374192 -> 140066819364400
	140066819374192 [label=CloneBackward0]
	140066819360464 -> 140066819374192
	140066819360464 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 2, 64)"]
	140066819374960 -> 140066819360464
	140066819374960 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140066819372080 -> 140066819374960
	140066819372080 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140066819364784 -> 140066819372080
	140066819364784 [label="ViewBackward0
------------------------
self_sym_sizes: (6, 768)"]
	140066819374720 -> 140066819364784
	140066819374720 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (6, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067054793248 -> 140066819374720
	140066904416224 [label="q_encoder.encoder.information_exchanging_layer.9.attention.self.query.bias
 (768)" fillcolor=lightblue]
	140066904416224 -> 140067054793248
	140067054793248 [label=AccumulateGrad]
	140067054788256 -> 140066819374720
	140067054788256 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140066819367568 -> 140067054788256
	140066819367568 [label="CatBackward0
------------
dim: 1"]
	140067054790608 -> 140066819367568
	140067054790608 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 1, 768)
start         :                   0
step          :                   1"]
	140067054783552 -> 140067054790608
	140067054783552 [label="SliceBackward0
---------------------------
dim           :           1
end           :           1
self_sym_sizes: (3, 2, 768)
start         :           0
step          :           1"]
	140067054786288 -> 140067054783552
	140067054786288 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:         (3, 2, 768)
start         :                   0
step          :                   1"]
	140067054783840 -> 140067054786288
	140067054783840 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140067054782544 -> 140067054783840
	140067054782544 [label="AddBackward0
------------
alpha: 1"]
	140067054781584 -> 140067054782544
	140067054781584 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067054781872 -> 140067054781584
	140067054781872 [label="ViewBackward0
------------------------
self_sym_sizes: (6, 768)"]
	140067054790176 -> 140067054781872
	140067054790176 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (6, 3072)
mat1_sym_strides:      (3072, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (3072, 768)
mat2_sym_strides:      (1, 3072)"]
	140067054786768 -> 140067054790176
	140066904415984 [label="q_encoder.encoder.information_exchanging_layer.8.output.dense.bias
 (768)" fillcolor=lightblue]
	140066904415984 -> 140067054786768
	140067054786768 [label=AccumulateGrad]
	140067054787248 -> 140067054790176
	140067054787248 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 2, 3072)"]
	140067054785328 -> 140067054787248
	140067054785328 [label="GeluBackward0
---------------------------
approximate:           none
self       : [saved tensor]"]
	140067054796560 -> 140067054785328
	140067054796560 [label="ViewBackward0
-------------------------
self_sym_sizes: (6, 3072)"]
	140067054793632 -> 140067054796560
	140067054793632 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (6, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (768, 3072)
mat2_sym_strides:       (1, 768)"]
	140067054789792 -> 140067054793632
	140066904415824 [label="q_encoder.encoder.information_exchanging_layer.8.intermediate.dense.bias
 (3072)" fillcolor=lightblue]
	140066904415824 -> 140067054789792
	140067054789792 [label=AccumulateGrad]
	140067054780960 -> 140067054793632
	140067054780960 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140067054787584 -> 140067054780960
	140067054787584 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140067054786672 -> 140067054787584
	140067054786672 [label="AddBackward0
------------
alpha: 1"]
	140067054794352 -> 140067054786672
	140067054794352 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067054790800 -> 140067054794352
	140067054790800 [label="ViewBackward0
------------------------
self_sym_sizes: (6, 768)"]
	140067054787968 -> 140067054790800
	140067054787968 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (6, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067054781296 -> 140067054787968
	140066904415424 [label="q_encoder.encoder.information_exchanging_layer.8.attention.output.dense.bias
 (768)" fillcolor=lightblue]
	140066904415424 -> 140067054781296
	140067054781296 [label=AccumulateGrad]
	140067054795360 -> 140067054787968
	140067054795360 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140067054790848 -> 140067054795360
	140067054790848 [label="ViewBackward0
------------------------------
self_sym_sizes: (3, 2, 12, 64)"]
	140067054782592 -> 140067054790848
	140067054782592 [label=CloneBackward0]
	140067054782880 -> 140067054782592
	140067054782880 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067054788400 -> 140067054782880
	140067054788400 [label="UnsafeViewBackward0
---------------------------
self_sym_sizes: (36, 2, 64)"]
	140067054782352 -> 140067054788400
	140067054782352 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140067054794880 -> 140067054782352
	140067054794880 [label="ViewBackward0
-----------------------------
self_sym_sizes: (3, 12, 2, 2)"]
	140067054780720 -> 140067054794880
	140067054780720 [label="ExpandBackward0
-----------------------------
self_sym_sizes: (3, 12, 2, 2)"]
	140067054786240 -> 140067054780720
	140067054786240 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140066879623888 -> 140067054786240
	140066879623888 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140066879633680 -> 140066879623888
	140066879633680 [label="AddBackward0
------------
alpha: 1"]
	140066879622496 -> 140066879633680
	140066879622496 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140066879626144 -> 140066879622496
	140066879626144 [label="UnsafeViewBackward0
--------------------------
self_sym_sizes: (36, 2, 2)"]
	140066879619616 -> 140066879626144
	140066879619616 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140066879620288 -> 140066879619616
	140066879620288 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 2, 64)"]
	140066879620624 -> 140066879620288
	140066879620624 [label=CloneBackward0]
	140066879619520 -> 140066879620624
	140066879619520 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 2, 64)"]
	140066879620000 -> 140066879619520
	140066879620000 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140066879620192 -> 140066879620000
	140066879620192 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140066879621680 -> 140066879620192
	140066879621680 [label="ViewBackward0
------------------------
self_sym_sizes: (6, 768)"]
	140066879622352 -> 140066879621680
	140066879622352 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (6, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140066879621920 -> 140066879622352
	140066904414944 [label="q_encoder.encoder.information_exchanging_layer.8.attention.self.query.bias
 (768)" fillcolor=lightblue]
	140066904414944 -> 140066879621920
	140066879621920 [label=AccumulateGrad]
	140066879620816 -> 140066879622352
	140066879620816 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140067054794448 -> 140066879620816
	140067054794448 [label="CatBackward0
------------
dim: 1"]
	140066879621488 -> 140067054794448
	140066879621488 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 1, 768)
start         :                   0
step          :                   1"]
	140066879622160 -> 140066879621488
	140066879622160 [label="SliceBackward0
---------------------------
dim           :           1
end           :           1
self_sym_sizes: (3, 2, 768)
start         :           0
step          :           1"]
	140066879619712 -> 140066879622160
	140066879619712 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:         (3, 2, 768)
start         :                   0
step          :                   1"]
	140066879623456 -> 140066879619712
	140066879623456 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140066879621440 -> 140066879623456
	140066879621440 [label="AddBackward0
------------
alpha: 1"]
	140066879622880 -> 140066879621440
	140066879622880 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140066879623360 -> 140066879622880
	140066879623360 [label="ViewBackward0
------------------------
self_sym_sizes: (6, 768)"]
	140066879623216 -> 140066879623360
	140066879623216 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (6, 3072)
mat1_sym_strides:      (3072, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (3072, 768)
mat2_sym_strides:      (1, 3072)"]
	140066879623936 -> 140066879623216
	140066904414704 [label="q_encoder.encoder.information_exchanging_layer.7.output.dense.bias
 (768)" fillcolor=lightblue]
	140066904414704 -> 140066879623936
	140066879623936 [label=AccumulateGrad]
	140066879623552 -> 140066879623216
	140066879623552 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 2, 3072)"]
	140066879624080 -> 140066879623552
	140066879624080 [label="GeluBackward0
---------------------------
approximate:           none
self       : [saved tensor]"]
	140066879623408 -> 140066879624080
	140066879623408 [label="ViewBackward0
-------------------------
self_sym_sizes: (6, 3072)"]
	140066879623984 -> 140066879623408
	140066879623984 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (6, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (768, 3072)
mat2_sym_strides:       (1, 768)"]
	140066879624416 -> 140066879623984
	140066904414544 [label="q_encoder.encoder.information_exchanging_layer.7.intermediate.dense.bias
 (3072)" fillcolor=lightblue]
	140066904414544 -> 140066879624416
	140066879624416 [label=AccumulateGrad]
	140066879625040 -> 140066879623984
	140066879625040 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140066879619376 -> 140066879625040
	140066879619376 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140066879623504 -> 140066879619376
	140066879623504 [label="AddBackward0
------------
alpha: 1"]
	140066879624944 -> 140066879623504
	140066879624944 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140066879626624 -> 140066879624944
	140066879626624 [label="ViewBackward0
------------------------
self_sym_sizes: (6, 768)"]
	140066879626096 -> 140066879626624
	140066879626096 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (6, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140066879626672 -> 140066879626096
	140066904414144 [label="q_encoder.encoder.information_exchanging_layer.7.attention.output.dense.bias
 (768)" fillcolor=lightblue]
	140066904414144 -> 140066879626672
	140066879626672 [label=AccumulateGrad]
	140066879623696 -> 140066879626096
	140066879623696 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140066879626576 -> 140066879623696
	140066879626576 [label="ViewBackward0
------------------------------
self_sym_sizes: (3, 2, 12, 64)"]
	140066879626480 -> 140066879626576
	140066879626480 [label=CloneBackward0]
	140066879627872 -> 140066879626480
	140066879627872 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140066879628544 -> 140066879627872
	140066879628544 [label="UnsafeViewBackward0
---------------------------
self_sym_sizes: (36, 2, 64)"]
	140066879626960 -> 140066879628544
	140066879626960 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140066879627536 -> 140066879626960
	140066879627536 [label="ViewBackward0
-----------------------------
self_sym_sizes: (3, 12, 2, 2)"]
	140066879627680 -> 140066879627536
	140066879627680 [label="ExpandBackward0
-----------------------------
self_sym_sizes: (3, 12, 2, 2)"]
	140066879628736 -> 140066879627680
	140066879628736 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140066879629168 -> 140066879628736
	140066879629168 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140066879629552 -> 140066879629168
	140066879629552 [label="AddBackward0
------------
alpha: 1"]
	140066879629216 -> 140066879629552
	140066879629216 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140066879629072 -> 140066879629216
	140066879629072 [label="UnsafeViewBackward0
--------------------------
self_sym_sizes: (36, 2, 2)"]
	140066879627968 -> 140066879629072
	140066879627968 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140066879629600 -> 140066879627968
	140066879629600 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 2, 64)"]
	140066879630320 -> 140066879629600
	140066879630320 [label=CloneBackward0]
	140066879630176 -> 140066879630320
	140066879630176 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 2, 64)"]
	140066879630272 -> 140066879630176
	140066879630272 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140066879629936 -> 140066879630272
	140066879629936 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140066879631472 -> 140066879629936
	140066879631472 [label="ViewBackward0
------------------------
self_sym_sizes: (6, 768)"]
	140066879631856 -> 140066879631472
	140066879631856 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (6, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140066879630608 -> 140066879631856
	140066904413664 [label="q_encoder.encoder.information_exchanging_layer.7.attention.self.query.bias
 (768)" fillcolor=lightblue]
	140066904413664 -> 140066879630608
	140066879630608 [label=AccumulateGrad]
	140066879630992 -> 140066879631856
	140066879630992 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140066879625904 -> 140066879630992
	140066879625904 [label="CatBackward0
------------
dim: 1"]
	140066879633056 -> 140066879625904
	140066879633056 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 1, 768)
start         :                   0
step          :                   1"]
	140066879632624 -> 140066879633056
	140066879632624 [label="SliceBackward0
---------------------------
dim           :           1
end           :           1
self_sym_sizes: (3, 2, 768)
start         :           0
step          :           1"]
	140066879632816 -> 140066879632624
	140066879632816 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:         (3, 2, 768)
start         :                   0
step          :                   1"]
	140066879628352 -> 140066879632816
	140066879628352 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140066879633296 -> 140066879628352
	140066879633296 [label="AddBackward0
------------
alpha: 1"]
	140066879631136 -> 140066879633296
	140066879631136 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140066879633584 -> 140066879631136
	140066879633584 [label="ViewBackward0
------------------------
self_sym_sizes: (6, 768)"]
	140066879634160 -> 140066879633584
	140066879634160 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (6, 3072)
mat1_sym_strides:      (3072, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (3072, 768)
mat2_sym_strides:      (1, 3072)"]
	140066879634016 -> 140066879634160
	140066904413424 [label="q_encoder.encoder.information_exchanging_layer.6.output.dense.bias
 (768)" fillcolor=lightblue]
	140066904413424 -> 140066879634016
	140066879634016 [label=AccumulateGrad]
	140066879632288 -> 140066879634160
	140066879632288 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 2, 3072)"]
	140066879634112 -> 140066879632288
	140066879634112 [label="GeluBackward0
---------------------------
approximate:           none
self       : [saved tensor]"]
	140066879631232 -> 140066879634112
	140066879631232 [label="ViewBackward0
-------------------------
self_sym_sizes: (6, 3072)"]
	140066879628976 -> 140066879631232
	140066879628976 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (6, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (768, 3072)
mat2_sym_strides:       (1, 768)"]
	140066879634976 -> 140066879628976
	140066904413264 [label="q_encoder.encoder.information_exchanging_layer.6.intermediate.dense.bias
 (3072)" fillcolor=lightblue]
	140066904413264 -> 140066879634976
	140066879634976 [label=AccumulateGrad]
	140066879634832 -> 140066879628976
	140066879634832 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140066879632432 -> 140066879634832
	140066879632432 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140066907246112 -> 140066879632432
	140066907246112 [label="AddBackward0
------------
alpha: 1"]
	140066907255952 -> 140066907246112
	140066907255952 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140066907250720 -> 140066907255952
	140066907250720 [label="ViewBackward0
------------------------
self_sym_sizes: (6, 768)"]
	140066907249712 -> 140066907250720
	140066907249712 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (6, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140066907248992 -> 140066907249712
	140066904412864 [label="q_encoder.encoder.information_exchanging_layer.6.attention.output.dense.bias
 (768)" fillcolor=lightblue]
	140066904412864 -> 140066907248992
	140066907248992 [label=AccumulateGrad]
	140066907246256 -> 140066907249712
	140066907246256 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140066907245776 -> 140066907246256
	140066907245776 [label="ViewBackward0
------------------------------
self_sym_sizes: (3, 2, 12, 64)"]
	140066907242656 -> 140066907245776
	140066907242656 [label=CloneBackward0]
	140066907243088 -> 140066907242656
	140066907243088 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140066907244336 -> 140066907243088
	140066907244336 [label="UnsafeViewBackward0
---------------------------
self_sym_sizes: (36, 2, 64)"]
	140066907243136 -> 140066907244336
	140066907243136 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140066907244816 -> 140066907243136
	140066907244816 [label="ViewBackward0
-----------------------------
self_sym_sizes: (3, 12, 2, 2)"]
	140066907244480 -> 140066907244816
	140066907244480 [label="ExpandBackward0
-----------------------------
self_sym_sizes: (3, 12, 2, 2)"]
	140066907245200 -> 140066907244480
	140066907245200 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140066907244768 -> 140066907245200
	140066907244768 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140066907245536 -> 140066907244768
	140066907245536 [label="AddBackward0
------------
alpha: 1"]
	140066907245728 -> 140066907245536
	140066907245728 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140066907245056 -> 140066907245728
	140066907245056 [label="UnsafeViewBackward0
--------------------------
self_sym_sizes: (36, 2, 2)"]
	140066907246304 -> 140066907245056
	140066907246304 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140066907244144 -> 140066907246304
	140066907244144 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 2, 64)"]
	140066907245632 -> 140066907244144
	140066907245632 [label=CloneBackward0]
	140066907243472 -> 140066907245632
	140066907243472 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 2, 64)"]
	140066907246736 -> 140066907243472
	140066907246736 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140066907246544 -> 140066907246736
	140066907246544 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140066907247504 -> 140066907246544
	140066907247504 [label="ViewBackward0
------------------------
self_sym_sizes: (6, 768)"]
	140066907247312 -> 140066907247504
	140066907247312 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (6, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140066907247744 -> 140066907247312
	140066904412384 [label="q_encoder.encoder.information_exchanging_layer.6.attention.self.query.bias
 (768)" fillcolor=lightblue]
	140066904412384 -> 140066907247744
	140066907247744 [label=AccumulateGrad]
	140066907246208 -> 140066907247312
	140066907246208 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140066907256960 -> 140066907246208
	140066907256960 [label="CatBackward0
------------
dim: 1"]
	140066907248656 -> 140066907256960
	140066907248656 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 1, 768)
start         :                   0
step          :                   1"]
	140066907248032 -> 140066907248656
	140066907248032 [label="SliceBackward0
---------------------------
dim           :           1
end           :           1
self_sym_sizes: (3, 2, 768)
start         :           0
step          :           1"]
	140066907249952 -> 140066907248032
	140066907249952 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:         (3, 2, 768)
start         :                   0
step          :                   1"]
	140066907248704 -> 140066907249952
	140066907248704 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140066907248464 -> 140066907248704
	140066907248464 [label="AddBackward0
------------
alpha: 1"]
	140066907249088 -> 140066907248464
	140066907249088 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140066907248944 -> 140066907249088
	140066907248944 [label="ViewBackward0
------------------------
self_sym_sizes: (6, 768)"]
	140066907249328 -> 140066907248944
	140066907249328 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (6, 3072)
mat1_sym_strides:      (3072, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (3072, 768)
mat2_sym_strides:      (1, 3072)"]
	140066907249808 -> 140066907249328
	140066904412144 [label="q_encoder.encoder.information_exchanging_layer.5.output.dense.bias
 (768)" fillcolor=lightblue]
	140066904412144 -> 140066907249808
	140066907249808 [label=AccumulateGrad]
	140066907248176 -> 140066907249328
	140066907248176 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 2, 3072)"]
	140066907248368 -> 140066907248176
	140066907248368 [label="GeluBackward0
---------------------------
approximate:           none
self       : [saved tensor]"]
	140066907250672 -> 140066907248368
	140066907250672 [label="ViewBackward0
-------------------------
self_sym_sizes: (6, 3072)"]
	140066907250960 -> 140066907250672
	140066907250960 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (6, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (768, 3072)
mat2_sym_strides:       (1, 768)"]
	140066907250048 -> 140066907250960
	140066904411984 [label="q_encoder.encoder.information_exchanging_layer.5.intermediate.dense.bias
 (3072)" fillcolor=lightblue]
	140066904411984 -> 140066907250048
	140066907250048 [label=AccumulateGrad]
	140066907250816 -> 140066907250960
	140066907250816 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140066907250432 -> 140066907250816
	140066907250432 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140066907252112 -> 140066907250432
	140066907252112 [label="AddBackward0
------------
alpha: 1"]
	140066907251488 -> 140066907252112
	140066907251488 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140066907253024 -> 140066907251488
	140066907253024 [label="ViewBackward0
------------------------
self_sym_sizes: (6, 768)"]
	140066907250288 -> 140066907253024
	140066907250288 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (6, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140066907252352 -> 140066907250288
	140066904411584 [label="q_encoder.encoder.information_exchanging_layer.5.attention.output.dense.bias
 (768)" fillcolor=lightblue]
	140066904411584 -> 140066907252352
	140066907252352 [label=AccumulateGrad]
	140066907251632 -> 140066907250288
	140066907251632 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140066907253552 -> 140066907251632
	140066907253552 [label="ViewBackward0
------------------------------
self_sym_sizes: (3, 2, 12, 64)"]
	140066907253984 -> 140066907253552
	140066907253984 [label=CloneBackward0]
	140066907254224 -> 140066907253984
	140066907254224 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140066907252544 -> 140066907254224
	140066907252544 [label="UnsafeViewBackward0
---------------------------
self_sym_sizes: (36, 2, 64)"]
	140066907254656 -> 140066907252544
	140066907254656 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140066907255088 -> 140066907254656
	140066907255088 [label="ViewBackward0
-----------------------------
self_sym_sizes: (3, 12, 2, 2)"]
	140066907254608 -> 140066907255088
	140066907254608 [label="ExpandBackward0
-----------------------------
self_sym_sizes: (3, 12, 2, 2)"]
	140066907255136 -> 140066907254608
	140066907255136 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140066907252832 -> 140066907255136
	140066907252832 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140066907255280 -> 140066907252832
	140066907255280 [label="AddBackward0
------------
alpha: 1"]
	140066907256336 -> 140066907255280
	140066907256336 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140066907254896 -> 140066907256336
	140066907254896 [label="UnsafeViewBackward0
--------------------------
self_sym_sizes: (36, 2, 2)"]
	140066907256240 -> 140066907254896
	140066907256240 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140066907253936 -> 140066907256240
	140066907253936 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 2, 64)"]
	140066907256528 -> 140066907253936
	140066907256528 [label=CloneBackward0]
	140066907256912 -> 140066907256528
	140066907256912 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 2, 64)"]
	140066907257536 -> 140066907256912
	140066907257536 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140066907257104 -> 140066907257536
	140066907257104 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140066907257152 -> 140066907257104
	140066907257152 [label="ViewBackward0
------------------------
self_sym_sizes: (6, 768)"]
	140066907258784 -> 140066907257152
	140066907258784 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (6, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140066907254752 -> 140066907258784
	140066853292304 [label="q_encoder.encoder.information_exchanging_layer.5.attention.self.query.bias
 (768)" fillcolor=lightblue]
	140066853292304 -> 140066907254752
	140066907254752 [label=AccumulateGrad]
	140066907257680 -> 140066907258784
	140066907257680 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140066907252064 -> 140066907257680
	140066907252064 [label="CatBackward0
------------
dim: 1"]
	140066907258592 -> 140066907252064
	140066907258592 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 1, 768)
start         :                   0
step          :                   1"]
	140066907258304 -> 140066907258592
	140066907258304 [label="SliceBackward0
---------------------------
dim           :           1
end           :           1
self_sym_sizes: (3, 2, 768)
start         :           0
step          :           1"]
	140066907254080 -> 140066907258304
	140066907254080 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:         (3, 2, 768)
start         :                   0
step          :                   1"]
	140066487830272 -> 140066907254080
	140066487830272 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140066487829456 -> 140066487830272
	140066487829456 [label="AddBackward0
------------
alpha: 1"]
	140066487829552 -> 140066487829456
	140066487829552 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140066487828832 -> 140066487829552
	140066487828832 [label="ViewBackward0
------------------------
self_sym_sizes: (6, 768)"]
	140066487828640 -> 140066487828832
	140066487828640 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (6, 3072)
mat1_sym_strides:      (3072, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (3072, 768)
mat2_sym_strides:      (1, 3072)"]
	140066487828928 -> 140066487828640
	140066853293984 [label="q_encoder.encoder.information_exchanging_layer.4.output.dense.bias
 (768)" fillcolor=lightblue]
	140066853293984 -> 140066487828928
	140066487828928 [label=AccumulateGrad]
	140066487829936 -> 140066487828640
	140066487829936 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 2, 3072)"]
	140066487829504 -> 140066487829936
	140066487829504 [label="GeluBackward0
---------------------------
approximate:           none
self       : [saved tensor]"]
	140066907015536 -> 140066487829504
	140066907015536 [label="ViewBackward0
-------------------------
self_sym_sizes: (6, 3072)"]
	140066907013616 -> 140066907015536
	140066907013616 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (6, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (768, 3072)
mat2_sym_strides:       (1, 768)"]
	140066907014432 -> 140066907013616
	140066853293744 [label="q_encoder.encoder.information_exchanging_layer.4.intermediate.dense.bias
 (3072)" fillcolor=lightblue]
	140066853293744 -> 140066907014432
	140066907014432 [label=AccumulateGrad]
	140066907013232 -> 140066907013616
	140066907013232 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140066487829312 -> 140066907013232
	140066487829312 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140066907015056 -> 140066487829312
	140066907015056 [label="AddBackward0
------------
alpha: 1"]
	140066907014912 -> 140066907015056
	140066907014912 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140066907015200 -> 140066907014912
	140066907015200 [label="ViewBackward0
------------------------
self_sym_sizes: (6, 768)"]
	140066907015728 -> 140066907015200
	140066907015728 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (6, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140066907014960 -> 140066907015728
	140066853291024 [label="q_encoder.encoder.information_exchanging_layer.4.attention.output.dense.bias
 (768)" fillcolor=lightblue]
	140066853291024 -> 140066907014960
	140066907014960 [label=AccumulateGrad]
	140066907013472 -> 140066907015728
	140066907013472 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140066907015680 -> 140066907013472
	140066907015680 [label="ViewBackward0
------------------------------
self_sym_sizes: (3, 2, 12, 64)"]
	140066907015344 -> 140066907015680
	140066907015344 [label=CloneBackward0]
	140066907015920 -> 140066907015344
	140066907015920 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140066907016208 -> 140066907015920
	140066907016208 [label="UnsafeViewBackward0
---------------------------
self_sym_sizes: (36, 2, 64)"]
	140066907016688 -> 140066907016208
	140066907016688 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140066907017216 -> 140066907016688
	140066907017216 [label="ViewBackward0
-----------------------------
self_sym_sizes: (3, 12, 2, 2)"]
	140066907017072 -> 140066907017216
	140066907017072 [label="ExpandBackward0
-----------------------------
self_sym_sizes: (3, 12, 2, 2)"]
	140066907017504 -> 140066907017072
	140066907017504 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140066907017456 -> 140066907017504
	140066907017456 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140066907018128 -> 140066907017456
	140066907018128 [label="AddBackward0
------------
alpha: 1"]
	140066907018080 -> 140066907018128
	140066907018080 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140066907018320 -> 140066907018080
	140066907018320 [label="UnsafeViewBackward0
--------------------------
self_sym_sizes: (36, 2, 2)"]
	140066907018464 -> 140066907018320
	140066907018464 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140066907018272 -> 140066907018464
	140066907018272 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 2, 64)"]
	140066907019232 -> 140066907018272
	140066907019232 [label=CloneBackward0]
	140066907018992 -> 140066907019232
	140066907018992 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 2, 64)"]
	140066907019568 -> 140066907018992
	140066907019568 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140066907019808 -> 140066907019568
	140066907019808 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140066907019664 -> 140066907019808
	140066907019664 [label="ViewBackward0
------------------------
self_sym_sizes: (6, 768)"]
	140066907020288 -> 140066907019664
	140066907020288 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (6, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140066907020000 -> 140066907020288
	140066853290784 [label="q_encoder.encoder.information_exchanging_layer.4.attention.self.query.bias
 (768)" fillcolor=lightblue]
	140066853290784 -> 140066907020000
	140066907020000 [label=AccumulateGrad]
	140066907020336 -> 140066907020288
	140066907020336 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140066907014720 -> 140066907020336
	140066907014720 [label="CatBackward0
------------
dim: 1"]
	140066907020384 -> 140066907014720
	140066907020384 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 1, 768)
start         :                   0
step          :                   1"]
	140066907020048 -> 140066907020384
	140066907020048 [label="SliceBackward0
---------------------------
dim           :           1
end           :           1
self_sym_sizes: (3, 2, 768)
start         :           0
step          :           1"]
	140066907021680 -> 140066907020048
	140066907021680 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:         (3, 2, 768)
start         :                   0
step          :                   1"]
	140066907021392 -> 140066907021680
	140066907021392 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140066907021632 -> 140066907021392
	140066907021632 [label="AddBackward0
------------
alpha: 1"]
	140066907020960 -> 140066907021632
	140066907020960 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140066907022112 -> 140066907020960
	140066907022112 [label="ViewBackward0
------------------------
self_sym_sizes: (6, 768)"]
	140066907022496 -> 140066907022112
	140066907022496 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (6, 3072)
mat1_sym_strides:      (3072, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (3072, 768)
mat2_sym_strides:      (1, 3072)"]
	140066907022400 -> 140066907022496
	140066853291504 [label="q_encoder.encoder.information_exchanging_layer.3.output.dense.bias
 (768)" fillcolor=lightblue]
	140066853291504 -> 140066907022400
	140066907022400 [label=AccumulateGrad]
	140066907021152 -> 140066907022496
	140066907021152 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 2, 3072)"]
	140066907022640 -> 140066907021152
	140066907022640 [label="GeluBackward0
---------------------------
approximate:           none
self       : [saved tensor]"]
	140066907022832 -> 140066907022640
	140066907022832 [label="ViewBackward0
-------------------------
self_sym_sizes: (6, 3072)"]
	140066907023216 -> 140066907022832
	140066907023216 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (6, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (768, 3072)
mat2_sym_strides:       (1, 768)"]
	140066907023264 -> 140066907023216
	140066853291424 [label="q_encoder.encoder.information_exchanging_layer.3.intermediate.dense.bias
 (3072)" fillcolor=lightblue]
	140066853291424 -> 140066907023264
	140066907023264 [label=AccumulateGrad]
	140066907019136 -> 140066907023216
	140066907019136 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140066907021344 -> 140066907019136
	140066907021344 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140066907025280 -> 140066907021344
	140066907025280 [label="AddBackward0
------------
alpha: 1"]
	140066907025712 -> 140066907025280
	140066907025712 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140066907026000 -> 140066907025712
	140066907026000 [label="ViewBackward0
------------------------
self_sym_sizes: (6, 768)"]
	140066907026336 -> 140066907026000
	140066907026336 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (6, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140066907026192 -> 140066907026336
	140066853294144 [label="q_encoder.encoder.information_exchanging_layer.3.attention.output.dense.bias
 (768)" fillcolor=lightblue]
	140066853294144 -> 140066907026192
	140066907026192 [label=AccumulateGrad]
	140066907023408 -> 140066907026336
	140066907023408 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140066907023648 -> 140066907023408
	140066907023648 [label="ViewBackward0
------------------------------
self_sym_sizes: (3, 2, 12, 64)"]
	140066907023600 -> 140066907023648
	140066907023600 [label=CloneBackward0]
	140066907024320 -> 140066907023600
	140066907024320 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140066907024464 -> 140066907024320
	140066907024464 [label="UnsafeViewBackward0
---------------------------
self_sym_sizes: (36, 2, 64)"]
	140066907024752 -> 140066907024464
	140066907024752 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140066907026432 -> 140066907024752
	140066907026432 [label="ViewBackward0
-----------------------------
self_sym_sizes: (3, 12, 2, 2)"]
	140066907026912 -> 140066907026432
	140066907026912 [label="ExpandBackward0
-----------------------------
self_sym_sizes: (3, 12, 2, 2)"]
	140066907023840 -> 140066907026912
	140066907023840 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140066907027248 -> 140066907023840
	140066907027248 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140066907027344 -> 140066907027248
	140066907027344 [label="AddBackward0
------------
alpha: 1"]
	140066907027824 -> 140066907027344
	140066907027824 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140066907027968 -> 140066907027824
	140066907027968 [label="UnsafeViewBackward0
--------------------------
self_sym_sizes: (36, 2, 2)"]
	140066907028304 -> 140066907027968
	140066907028304 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140066907027776 -> 140066907028304
	140066907027776 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 2, 64)"]
	140066907028592 -> 140066907027776
	140066907028592 [label=CloneBackward0]
	140066907028448 -> 140066907028592
	140066907028448 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 2, 64)"]
	140066907028928 -> 140066907028448
	140066907028928 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140066907029120 -> 140066907028928
	140066907029120 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140066907029216 -> 140066907029120
	140066907029216 [label="ViewBackward0
------------------------
self_sym_sizes: (6, 768)"]
	140066907028496 -> 140066907029216
	140066907028496 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (6, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140066907026864 -> 140066907028496
	140066853294384 [label="q_encoder.encoder.information_exchanging_layer.3.attention.self.query.bias
 (768)" fillcolor=lightblue]
	140066853294384 -> 140066907026864
	140066907026864 [label=AccumulateGrad]
	140066672787312 -> 140066907028496
	140066672787312 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140066907025472 -> 140066672787312
	140066907025472 [label="CatBackward0
------------
dim: 1"]
	140067054817568 -> 140066907025472
	140067054817568 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 1, 768)
start         :                   0
step          :                   1"]
	140067054989664 -> 140067054817568
	140067054989664 [label="SliceBackward0
---------------------------
dim           :           1
end           :           1
self_sym_sizes: (3, 2, 768)
start         :           0
step          :           1"]
	140067054992160 -> 140067054989664
	140067054992160 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:         (3, 2, 768)
start         :                   0
step          :                   1"]
	140066708589536 -> 140067054992160
	140066708589536 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140066708588864 -> 140066708589536
	140066708588864 [label="AddBackward0
------------
alpha: 1"]
	140066708590640 -> 140066708588864
	140066708590640 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140066708588240 -> 140066708590640
	140066708588240 [label="ViewBackward0
------------------------
self_sym_sizes: (6, 768)"]
	140066708596160 -> 140066708588240
	140066708596160 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (6, 3072)
mat1_sym_strides:      (3072, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (3072, 768)
mat2_sym_strides:      (1, 3072)"]
	140066708588816 -> 140066708596160
	140066853293904 [label="q_encoder.encoder.information_exchanging_layer.2.output.dense.bias
 (768)" fillcolor=lightblue]
	140066853293904 -> 140066708588816
	140066708588816 [label=AccumulateGrad]
	140066708589632 -> 140066708596160
	140066708589632 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 2, 3072)"]
	140066708596208 -> 140066708589632
	140066708596208 [label="GeluBackward0
---------------------------
approximate:           none
self       : [saved tensor]"]
	140066708592080 -> 140066708596208
	140066708592080 [label="ViewBackward0
-------------------------
self_sym_sizes: (6, 3072)"]
	140066708596400 -> 140066708592080
	140066708596400 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (6, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (768, 3072)
mat2_sym_strides:       (1, 768)"]
	140066708597312 -> 140066708596400
	140066853294464 [label="q_encoder.encoder.information_exchanging_layer.2.intermediate.dense.bias
 (3072)" fillcolor=lightblue]
	140066853294464 -> 140066708597312
	140066708597312 [label=AccumulateGrad]
	140066708596640 -> 140066708596400
	140066708596640 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140066708591888 -> 140066708596640
	140066708591888 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140066708597360 -> 140066708591888
	140066708597360 [label="AddBackward0
------------
alpha: 1"]
	140066708598032 -> 140066708597360
	140066708598032 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140066708596592 -> 140066708598032
	140066708596592 [label="ViewBackward0
------------------------
self_sym_sizes: (6, 768)"]
	140066708598608 -> 140066708596592
	140066708598608 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (6, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140066708598176 -> 140066708598608
	140066853295424 [label="q_encoder.encoder.information_exchanging_layer.2.attention.output.dense.bias
 (768)" fillcolor=lightblue]
	140066853295424 -> 140066708598176
	140066708598176 [label=AccumulateGrad]
	140066708597168 -> 140066708598608
	140066708597168 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140066708599040 -> 140066708597168
	140066708599040 [label="ViewBackward0
------------------------------
self_sym_sizes: (3, 2, 12, 64)"]
	140066708598896 -> 140066708599040
	140066708598896 [label=CloneBackward0]
	140066708599088 -> 140066708598896
	140066708599088 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140066708599664 -> 140066708599088
	140066708599664 [label="UnsafeViewBackward0
---------------------------
self_sym_sizes: (36, 2, 64)"]
	140066708599520 -> 140066708599664
	140066708599520 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140066708599568 -> 140066708599520
	140066708599568 [label="ViewBackward0
-----------------------------
self_sym_sizes: (3, 12, 2, 2)"]
	140066708599616 -> 140066708599568
	140066708599616 [label="ExpandBackward0
-----------------------------
self_sym_sizes: (3, 12, 2, 2)"]
	140066708599808 -> 140066708599616
	140066708599808 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140066708600192 -> 140066708599808
	140066708600192 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140066708597936 -> 140066708600192
	140066708597936 [label="AddBackward0
------------
alpha: 1"]
	140066708592608 -> 140066708597936
	140066708592608 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140066708599952 -> 140066708592608
	140066708599952 [label="UnsafeViewBackward0
--------------------------
self_sym_sizes: (36, 2, 2)"]
	140066708592752 -> 140066708599952
	140066708592752 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140066708592944 -> 140066708592752
	140066708592944 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 2, 64)"]
	140066708592992 -> 140066708592944
	140066708592992 [label=CloneBackward0]
	140066708593616 -> 140066708592992
	140066708593616 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 2, 64)"]
	140066708593328 -> 140066708593616
	140066708593328 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140066708595680 -> 140066708593328
	140066708595680 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140066708593952 -> 140066708595680
	140066708593952 [label="ViewBackward0
------------------------
self_sym_sizes: (6, 768)"]
	140066708596016 -> 140066708593952
	140066708596016 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (6, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140066708600384 -> 140066708596016
	140066853296624 [label="q_encoder.encoder.information_exchanging_layer.2.attention.self.query.bias
 (768)" fillcolor=lightblue]
	140066853296624 -> 140066708600384
	140066708600384 [label=AccumulateGrad]
	140066708595488 -> 140066708596016
	140066708595488 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140066708597648 -> 140066708595488
	140066708597648 [label="CatBackward0
------------
dim: 1"]
	140066708594048 -> 140066708597648
	140066708594048 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 1, 768)
start         :                   0
step          :                   1"]
	140066708592176 -> 140066708594048
	140066708592176 [label="SliceBackward0
---------------------------
dim           :           1
end           :           1
self_sym_sizes: (3, 2, 768)
start         :           0
step          :           1"]
	140066708600960 -> 140066708592176
	140066708600960 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:         (3, 2, 768)
start         :                   0
step          :                   1"]
	140066708601008 -> 140066708600960
	140066708601008 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140066708594192 -> 140066708601008
	140066708594192 [label="AddBackward0
------------
alpha: 1"]
	140066708600576 -> 140066708594192
	140066708600576 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140066708595008 -> 140066708600576
	140066708595008 [label="ViewBackward0
------------------------
self_sym_sizes: (6, 768)"]
	140066708600336 -> 140066708595008
	140066708600336 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (6, 3072)
mat1_sym_strides:      (3072, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (3072, 768)
mat2_sym_strides:      (1, 3072)"]
	140066708594768 -> 140066708600336
	140066853297184 [label="q_encoder.encoder.information_exchanging_layer.1.output.dense.bias
 (768)" fillcolor=lightblue]
	140066853297184 -> 140066708594768
	140066708594768 [label=AccumulateGrad]
	140066708601152 -> 140066708600336
	140066708601152 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 2, 3072)"]
	140066708601488 -> 140066708601152
	140066708601488 [label="GeluBackward0
---------------------------
approximate:           none
self       : [saved tensor]"]
	140066708600672 -> 140066708601488
	140066708600672 [label="ViewBackward0
-------------------------
self_sym_sizes: (6, 3072)"]
	140066708601824 -> 140066708600672
	140066708601824 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (6, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (768, 3072)
mat2_sym_strides:       (1, 768)"]
	140066708601296 -> 140066708601824
	140066853294624 [label="q_encoder.encoder.information_exchanging_layer.1.intermediate.dense.bias
 (3072)" fillcolor=lightblue]
	140066853294624 -> 140066708601296
	140066708601296 [label=AccumulateGrad]
	140066708602016 -> 140066708601824
	140066708602016 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140066708594864 -> 140066708602016
	140066708594864 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140066708601248 -> 140066708594864
	140066708601248 [label="AddBackward0
------------
alpha: 1"]
	140066708586608 -> 140066708601248
	140066708586608 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140066708587136 -> 140066708586608
	140066708587136 [label="ViewBackward0
------------------------
self_sym_sizes: (6, 768)"]
	140066708588912 -> 140066708587136
	140066708588912 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (6, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140066708591216 -> 140066708588912
	140066853292384 [label="q_encoder.encoder.information_exchanging_layer.1.attention.output.dense.bias
 (768)" fillcolor=lightblue]
	140066853292384 -> 140066708591216
	140066708591216 [label=AccumulateGrad]
	140066708602400 -> 140066708588912
	140066708602400 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140066708587760 -> 140066708602400
	140066708587760 [label="ViewBackward0
------------------------------
self_sym_sizes: (3, 2, 12, 64)"]
	140066708587040 -> 140066708587760
	140066708587040 [label=CloneBackward0]
	140066708587376 -> 140066708587040
	140066708587376 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140066708587664 -> 140066708587376
	140066708587664 [label="UnsafeViewBackward0
---------------------------
self_sym_sizes: (36, 2, 64)"]
	140066708587616 -> 140066708587664
	140066708587616 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140066708587952 -> 140066708587616
	140066708587952 [label="ViewBackward0
-----------------------------
self_sym_sizes: (3, 12, 2, 2)"]
	140066708589152 -> 140066708587952
	140066708589152 [label="ExpandBackward0
-----------------------------
self_sym_sizes: (3, 12, 2, 2)"]
	140067054457312 -> 140066708589152
	140067054457312 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067054459760 -> 140067054457312
	140067054459760 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140067054467248 -> 140067054459760
	140067054467248 [label="AddBackward0
------------
alpha: 1"]
	140067053452976 -> 140067054467248
	140067053452976 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067053451200 -> 140067053452976
	140067053451200 [label="UnsafeViewBackward0
--------------------------
self_sym_sizes: (36, 2, 2)"]
	140067053452016 -> 140067053451200
	140067053452016 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140067053450144 -> 140067053452016
	140067053450144 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 2, 64)"]
	140066673013616 -> 140067053450144
	140066673013616 [label=CloneBackward0]
	140066673014096 -> 140066673013616
	140066673014096 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 2, 64)"]
	140066673014048 -> 140066673014096
	140066673014048 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140066673014528 -> 140066673014048
	140066673014528 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140066673014624 -> 140066673014528
	140066673014624 [label="ViewBackward0
------------------------
self_sym_sizes: (6, 768)"]
	140066673014768 -> 140066673014624
	140066673014768 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (6, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140066673014960 -> 140066673014768
	140066853293424 [label="q_encoder.encoder.information_exchanging_layer.1.attention.self.query.bias
 (768)" fillcolor=lightblue]
	140066853293424 -> 140066673014960
	140066673014960 [label=AccumulateGrad]
	140066673014240 -> 140066673014768
	140066673014240 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140066708602448 -> 140066673014240
	140066708602448 [label="CatBackward0
------------
dim: 1"]
	140066673015344 -> 140066708602448
	140066673015344 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 1, 768)
start         :                   0
step          :                   1"]
	140066673015152 -> 140066673015344
	140066673015152 [label="SliceBackward0
---------------------------
dim           :           1
end           :           1
self_sym_sizes: (3, 2, 768)
start         :           0
step          :           1"]
	140066673016688 -> 140066673015152
	140066673016688 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:         (3, 2, 768)
start         :                   0
step          :                   1"]
	140066673016400 -> 140066673016688
	140066673016400 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140066673016160 -> 140066673016400
	140066673016160 [label="AddBackward0
------------
alpha: 1"]
	140066673016256 -> 140066673016160
	140066673016256 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041483248 -> 140066673016256
	140067041483248 [label="ViewBackward0
------------------------
self_sym_sizes: (6, 768)"]
	140067041476960 -> 140067041483248
	140067041476960 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (6, 3072)
mat1_sym_strides:      (3072, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (3072, 768)
mat2_sym_strides:      (1, 3072)"]
	140067041476672 -> 140067041476960
	140067041833392 [label="q_encoder.encoder.information_exchanging_layer.0.output.dense.bias
 (768)" fillcolor=lightblue]
	140067041833392 -> 140067041476672
	140067041476672 [label=AccumulateGrad]
	140067041478064 -> 140067041476960
	140067041478064 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 2, 3072)"]
	140067041477008 -> 140067041478064
	140067041477008 [label="GeluBackward0
---------------------------
approximate:           none
self       : [saved tensor]"]
	140067041477152 -> 140067041477008
	140067041477152 [label="ViewBackward0
-------------------------
self_sym_sizes: (6, 3072)"]
	140067041477776 -> 140067041477152
	140067041477776 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (6, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (768, 3072)
mat2_sym_strides:       (1, 768)"]
	140067041477680 -> 140067041477776
	140067041826592 [label="q_encoder.encoder.information_exchanging_layer.0.intermediate.dense.bias
 (3072)" fillcolor=lightblue]
	140067041826592 -> 140067041477680
	140067041477680 [label=AccumulateGrad]
	140067041477488 -> 140067041477776
	140067041477488 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140066673015872 -> 140067041477488
	140066673015872 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140067041478544 -> 140066673015872
	140067041478544 [label="AddBackward0
------------
alpha: 1"]
	140067041478640 -> 140067041478544
	140067041478640 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041478832 -> 140067041478640
	140067041478832 [label="ViewBackward0
------------------------
self_sym_sizes: (6, 768)"]
	140067041479456 -> 140067041478832
	140067041479456 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (6, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041479408 -> 140067041479456
	140067041013952 [label="q_encoder.encoder.information_exchanging_layer.0.attention.output.dense.bias
 (768)" fillcolor=lightblue]
	140067041013952 -> 140067041479408
	140067041479408 [label=AccumulateGrad]
	140067041477920 -> 140067041479456
	140067041477920 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140067041479120 -> 140067041477920
	140067041479120 [label="ViewBackward0
------------------------------
self_sym_sizes: (3, 2, 12, 64)"]
	140067041479648 -> 140067041479120
	140067041479648 [label=CloneBackward0]
	140067041480032 -> 140067041479648
	140067041480032 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041479840 -> 140067041480032
	140067041479840 [label="UnsafeViewBackward0
---------------------------
self_sym_sizes: (36, 2, 64)"]
	140067041480128 -> 140067041479840
	140067041480128 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140067041480368 -> 140067041480128
	140067041480368 [label="ViewBackward0
-----------------------------
self_sym_sizes: (3, 12, 2, 2)"]
	140067041480992 -> 140067041480368
	140067041480992 [label="ExpandBackward0
-----------------------------
self_sym_sizes: (3, 12, 2, 2)"]
	140067041481040 -> 140067041480992
	140067041481040 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041481232 -> 140067041481040
	140067041481232 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140067041481568 -> 140067041481232
	140067041481568 [label="AddBackward0
------------
alpha: 1"]
	140067041481856 -> 140067041481568
	140067041481856 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041481760 -> 140067041481856
	140067041481760 [label="UnsafeViewBackward0
--------------------------
self_sym_sizes: (36, 2, 2)"]
	140067041482240 -> 140067041481760
	140067041482240 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140067041481616 -> 140067041482240
	140067041481616 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 2, 64)"]
	140067041482672 -> 140067041481616
	140067041482672 [label=CloneBackward0]
	140067041482768 -> 140067041482672
	140067041482768 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 2, 64)"]
	140067041482960 -> 140067041482768
	140067041482960 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041483392 -> 140067041482960
	140067041483392 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140067041483536 -> 140067041483392
	140067041483536 [label="ViewBackward0
------------------------
self_sym_sizes: (6, 768)"]
	140067041483872 -> 140067041483536
	140067041483872 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (6, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041484016 -> 140067041483872
	140066852645344 [label="q_encoder.encoder.information_exchanging_layer.0.attention.self.query.bias
 (768)" fillcolor=lightblue]
	140066852645344 -> 140067041484016
	140067041484016 [label=AccumulateGrad]
	140067041484208 -> 140067041483872
	140067041484208 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140067041478592 -> 140067041484208
	140067041478592 [label="CatBackward0
------------
dim: 1"]
	140067041484736 -> 140067041478592
	140067041484736 [label="RepeatBackward0
---------------------------
repeats       :   (3, 1, 1)
self_sym_sizes: (1, 1, 768)"]
	140067041484352 -> 140067041484736
	140067041484352 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (1, 1, 768)
start         :                   0
step          :                   1"]
	140067041485024 -> 140067041484352
	140067041485024 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:         (1, 1, 768)
start         :                   0
step          :                   1"]
	140067041485312 -> 140067041485024
	140067041485312 [label="UnsqueezeBackward0
------------------
dim: 0"]
	140066688181168 -> 140067041485312
	140066898659760 [label="q_encoder.doc_embeddings
 (1, 768)" fillcolor=lightblue]
	140066898659760 -> 140066688181168
	140066688181168 [label=AccumulateGrad]
	140067041484592 -> 140067041478592
	140067041484592 [label=CloneBackward0]
	140067041484784 -> 140067041484592
	140067041484784 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 1, 768)
start         :                   0
step          :                   1"]
	140067041484880 -> 140067041484784
	140067041484880 [label="SelectBackward0
------------------------------
dim           :              2
index         :              1
self_sym_sizes: (3, 1, 8, 768)"]
	140067041485936 -> 140067041484880
	140067041485936 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:      (3, 1, 8, 768)
start         :                   0
step          :                   1"]
	140067041485264 -> 140067041485936
	140067041485264 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:      (3, 1, 8, 768)
start         :                   0
step          :                   1"]
	140067041486224 -> 140067041485264
	140067041486224 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 8, 768)"]
	140067041485168 -> 140067041486224
	140067041485168 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140067041485696 -> 140067041485168
	140067041485696 [label="AddBackward0
------------
alpha: 1"]
	140067041486080 -> 140067041485696
	140067041486080 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041486800 -> 140067041486080
	140067041486800 [label="ViewBackward0
-------------------------
self_sym_sizes: (24, 768)"]
	140067041487184 -> 140067041486800
	140067041487184 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :     (24, 3072)
mat1_sym_strides:      (3072, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (3072, 768)
mat2_sym_strides:      (1, 3072)"]
	140067041486992 -> 140067041487184
	140066904408704 [label="q_encoder.encoder.text_encoding_layer.0.output.dense.bias
 (768)" fillcolor=lightblue]
	140066904408704 -> 140067041486992
	140067041486992 [label=AccumulateGrad]
	140067041485456 -> 140067041487184
	140067041485456 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 8, 3072)"]
	140067041487424 -> 140067041485456
	140067041487424 [label="GeluBackward0
---------------------------
approximate:           none
self       : [saved tensor]"]
	140067041487328 -> 140067041487424
	140067041487328 [label="ViewBackward0
--------------------------
self_sym_sizes: (24, 3072)"]
	140067041488096 -> 140067041487328
	140067041488096 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (24, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (768, 3072)
mat2_sym_strides:       (1, 768)"]
	140067041488192 -> 140067041488096
	140066904408624 [label="q_encoder.encoder.text_encoding_layer.0.intermediate.dense.bias
 (3072)" fillcolor=lightblue]
	140066904408624 -> 140067041488192
	140067041488192 [label=AccumulateGrad]
	140067041488288 -> 140067041488096
	140067041488288 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 8, 768)"]
	140067041486464 -> 140067041488288
	140067041486464 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140067041488768 -> 140067041486464
	140067041488768 [label="AddBackward0
------------
alpha: 1"]
	140067041488816 -> 140067041488768
	140067041488816 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041489296 -> 140067041488816
	140067041489296 [label="ViewBackward0
-------------------------
self_sym_sizes: (24, 768)"]
	140067041489536 -> 140067041489296
	140067041489536 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (24, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041489440 -> 140067041489536
	140066898624448 [label="q_encoder.encoder.text_encoding_layer.0.attention.output.dense.bias
 (768)" fillcolor=lightblue]
	140066898624448 -> 140067041489440
	140067041489440 [label=AccumulateGrad]
	140067041488576 -> 140067041489536
	140067041488576 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 8, 768)"]
	140067041489776 -> 140067041488576
	140067041489776 [label="ViewBackward0
------------------------------
self_sym_sizes: (3, 8, 12, 64)"]
	140067041489728 -> 140067041489776
	140067041489728 [label=CloneBackward0]
	140067041490304 -> 140067041489728
	140067041490304 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041490688 -> 140067041490304
	140067041490688 [label="UnsafeViewBackward0
---------------------------
self_sym_sizes: (36, 8, 64)"]
	140067041490784 -> 140067041490688
	140067041490784 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140067041491024 -> 140067041490784
	140067041491024 [label="ViewBackward0
-----------------------------
self_sym_sizes: (3, 12, 8, 8)"]
	140067041491408 -> 140067041491024
	140067041491408 [label="ExpandBackward0
-----------------------------
self_sym_sizes: (3, 12, 8, 8)"]
	140067041491264 -> 140067041491408
	140067041491264 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041491648 -> 140067041491264
	140067041491648 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140067041491984 -> 140067041491648
	140067041491984 [label="AddBackward0
------------
alpha: 1"]
	140067041492272 -> 140067041491984
	140067041492272 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041492176 -> 140067041492272
	140067041492176 [label="UnsafeViewBackward0
--------------------------
self_sym_sizes: (36, 8, 8)"]
	140067041492704 -> 140067041492176
	140067041492704 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140067041492560 -> 140067041492704
	140067041492560 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 8, 64)"]
	140067041492320 -> 140067041492560
	140067041492320 [label=CloneBackward0]
	140067041689808 -> 140067041492320
	140067041689808 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 8, 64)"]
	140067041690096 -> 140067041689808
	140067041690096 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041690336 -> 140067041690096
	140067041690336 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 8, 768)"]
	140067041690528 -> 140067041690336
	140067041690528 [label="ViewBackward0
-------------------------
self_sym_sizes: (24, 768)"]
	140067041690864 -> 140067041690528
	140067041690864 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (24, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041691008 -> 140067041690864
	140066904915824 [label="q_encoder.encoder.text_encoding_layer.0.attention.self.query.bias
 (768)" fillcolor=lightblue]
	140066904915824 -> 140067041691008
	140067041691008 [label=AccumulateGrad]
	140067041691152 -> 140067041690864
	140067041691152 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 8, 768)"]
	140067041488960 -> 140067041691152
	140067041488960 [label="CatBackward0
------------
dim: 1"]
	140067041691824 -> 140067041488960
	140067041691824 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041691968 -> 140067041691824
	140067041691968 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140067041692304 -> 140067041691968
	140067041692304 [label="AddBackward0
------------
alpha: 1"]
	140067041692496 -> 140067041692304
	140067041692496 [label="AddBackward0
------------
alpha: 1"]
	140067041692400 -> 140067041692496
	140067041692400 [label="EmbeddingBackward0
------------------------------------
indices             : [saved tensor]
padding_idx         :              0
scale_grad_by_freq  :          False
sparse              :          False
weight_sym_argsize_0:          30522"]
	140067041692736 -> 140067041692400
	140066904914064 [label="q_encoder.embeddings.word_embeddings.weight
 (30522, 768)" fillcolor=lightblue]
	140066904914064 -> 140067041692736
	140067041692736 [label=AccumulateGrad]
	140067041692112 -> 140067041692496
	140067041692112 [label="EmbeddingBackward0
------------------------------------------
indices             :       [saved tensor]
padding_idx         : 18446744073709551615
scale_grad_by_freq  :                False
sparse              :                False
weight_sym_argsize_0:                    2"]
	140067041692832 -> 140067041692112
	140066904911424 [label="q_encoder.embeddings.token_type_embeddings.weight
 (2, 768)" fillcolor=lightblue]
	140066904911424 -> 140067041692832
	140067041692832 [label=AccumulateGrad]
	140067041692448 -> 140067041692304
	140067041692448 [label="EmbeddingBackward0
------------------------------------------
indices             :       [saved tensor]
padding_idx         : 18446744073709551615
scale_grad_by_freq  :                False
sparse              :                False
weight_sym_argsize_0:                  512"]
	140067041693120 -> 140067041692448
	140066904915584 [label="q_encoder.embeddings.position_embeddings.weight
 (512, 768)" fillcolor=lightblue]
	140066904915584 -> 140067041693120
	140067041693120 [label=AccumulateGrad]
	140067041692208 -> 140067041691968
	140066898671760 [label="q_encoder.embeddings.LayerNorm.weight
 (768)" fillcolor=lightblue]
	140066898671760 -> 140067041692208
	140067041692208 [label=AccumulateGrad]
	140067041692064 -> 140067041691968
	140066898673360 [label="q_encoder.embeddings.LayerNorm.bias
 (768)" fillcolor=lightblue]
	140066898673360 -> 140067041692064
	140067041692064 [label=AccumulateGrad]
	140067041691728 -> 140067041690864
	140067041691728 [label=TBackward0]
	140067041691488 -> 140067041691728
	140066904915504 [label="q_encoder.encoder.text_encoding_layer.0.attention.self.query.weight
 (768, 768)" fillcolor=lightblue]
	140066904915504 -> 140067041691488
	140067041691488 [label=AccumulateGrad]
	140067041492368 -> 140067041492704
	140067041492368 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 64, 8)"]
	140067053398352 -> 140067041492368
	140067053398352 [label=CloneBackward0]
	140067041690288 -> 140067053398352
	140067041690288 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 64, 8)"]
	140067041690384 -> 140067041690288
	140067041690384 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551615
dim1: 18446744073709551614"]
	140067041691200 -> 140067041690384
	140067041691200 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041692592 -> 140067041691200
	140067041692592 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 8, 768)"]
	140067041691248 -> 140067041692592
	140067041691248 [label="ViewBackward0
-------------------------
self_sym_sizes: (24, 768)"]
	140067041692688 -> 140067041691248
	140067041692688 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (24, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041693264 -> 140067041692688
	140066688007008 [label="q_encoder.encoder.text_encoding_layer.0.attention.self.key.bias
 (768)" fillcolor=lightblue]
	140066688007008 -> 140067041693264
	140067041693264 [label=AccumulateGrad]
	140067041693024 -> 140067041692688
	140067041693024 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 8, 768)"]
	140067041488960 -> 140067041693024
	140067041689760 -> 140067041692688
	140067041689760 [label=TBackward0]
	140067041693792 -> 140067041689760
	140066904915744 [label="q_encoder.encoder.text_encoding_layer.0.attention.self.key.weight
 (768, 768)" fillcolor=lightblue]
	140066904915744 -> 140067041693792
	140067041693792 [label=AccumulateGrad]
	140067041490976 -> 140067041490784
	140067041490976 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 8, 64)"]
	140067053365392 -> 140067041490976
	140067053365392 [label=CloneBackward0]
	140067041491888 -> 140067053365392
	140067041491888 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 8, 64)"]
	140067041492080 -> 140067041491888
	140067041492080 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041492944 -> 140067041492080
	140067041492944 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 8, 768)"]
	140067041491072 -> 140067041492944
	140067041491072 [label="ViewBackward0
-------------------------
self_sym_sizes: (24, 768)"]
	140067041691584 -> 140067041491072
	140067041691584 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (24, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041693504 -> 140067041691584
	140066889564016 [label="q_encoder.encoder.text_encoding_layer.0.attention.self.value.bias
 (768)" fillcolor=lightblue]
	140066889564016 -> 140067041693504
	140067041693504 [label=AccumulateGrad]
	140067041691296 -> 140067041691584
	140067041691296 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 8, 768)"]
	140067041488960 -> 140067041691296
	140067041689952 -> 140067041691584
	140067041689952 [label=TBackward0]
	140067041693408 -> 140067041689952
	140066889552016 [label="q_encoder.encoder.text_encoding_layer.0.attention.self.value.weight
 (768, 768)" fillcolor=lightblue]
	140066889552016 -> 140067041693408
	140067041693408 [label=AccumulateGrad]
	140067041490160 -> 140067041489536
	140067041490160 [label=TBackward0]
	140067041489824 -> 140067041490160
	140066889963536 [label="q_encoder.encoder.text_encoding_layer.0.attention.output.dense.weight
 (768, 768)" fillcolor=lightblue]
	140066889963536 -> 140067041489824
	140067041489824 [label=AccumulateGrad]
	140067041488960 -> 140067041488768
	140067041488624 -> 140067041486464
	140067041340752 [label="q_encoder.encoder.text_encoding_layer.0.attention.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	140067041340752 -> 140067041488624
	140067041488624 [label=AccumulateGrad]
	140067041489152 -> 140067041486464
	140067041003952 [label="q_encoder.encoder.text_encoding_layer.0.attention.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	140067041003952 -> 140067041489152
	140067041489152 [label=AccumulateGrad]
	140067041488720 -> 140067041488096
	140067041488720 [label=TBackward0]
	140067041489104 -> 140067041488720
	140066904408544 [label="q_encoder.encoder.text_encoding_layer.0.intermediate.dense.weight
 (3072, 768)" fillcolor=lightblue]
	140066904408544 -> 140067041489104
	140067041489104 [label=AccumulateGrad]
	140067041487856 -> 140067041487184
	140067041487856 [label=TBackward0]
	140067041487520 -> 140067041487856
	140066904408384 [label="q_encoder.encoder.text_encoding_layer.0.output.dense.weight
 (768, 3072)" fillcolor=lightblue]
	140066904408384 -> 140067041487520
	140067041487520 [label=AccumulateGrad]
	140067041486464 -> 140067041485696
	140067041485984 -> 140067041485168
	140066904408864 [label="q_encoder.encoder.text_encoding_layer.0.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	140066904408864 -> 140067041485984
	140067041485984 [label=AccumulateGrad]
	140067041486752 -> 140067041485168
	140066904408784 [label="q_encoder.encoder.text_encoding_layer.0.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	140066904408784 -> 140067041486752
	140067041486752 [label=AccumulateGrad]
	140067041484688 -> 140067041483872
	140067041484688 [label=TBackward0]
	140067041484832 -> 140067041484688
	140066852645264 [label="q_encoder.encoder.information_exchanging_layer.0.attention.self.query.weight
 (768, 768)" fillcolor=lightblue]
	140066852645264 -> 140067041484832
	140067041484832 [label=AccumulateGrad]
	140067041481904 -> 140067041482240
	140067041481904 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 64, 2)"]
	140067041483104 -> 140067041481904
	140067041483104 [label=CloneBackward0]
	140067041483440 -> 140067041483104
	140067041483440 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 64, 2)"]
	140067041483776 -> 140067041483440
	140067041483776 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551615
dim1: 18446744073709551614"]
	140067041485120 -> 140067041483776
	140067041485120 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041486128 -> 140067041485120
	140067041486128 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140067041484256 -> 140067041486128
	140067041484256 [label="ViewBackward0
------------------------
self_sym_sizes: (6, 768)"]
	140067041486704 -> 140067041484256
	140067041486704 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (6, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041486896 -> 140067041486704
	140066852645504 [label="q_encoder.encoder.information_exchanging_layer.0.attention.self.key.bias
 (768)" fillcolor=lightblue]
	140066852645504 -> 140067041486896
	140067041486896 [label=AccumulateGrad]
	140067041487088 -> 140067041486704
	140067041487088 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140067041478592 -> 140067041487088
	140067041482048 -> 140067041486704
	140067041482048 [label=TBackward0]
	140067041489488 -> 140067041482048
	140066852645424 [label="q_encoder.encoder.information_exchanging_layer.0.attention.self.key.weight
 (768, 768)" fillcolor=lightblue]
	140066852645424 -> 140067041489488
	140067041489488 [label=AccumulateGrad]
	140067041480176 -> 140067041480128
	140067041480176 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 2, 64)"]
	140067041480800 -> 140067041480176
	140067041480800 [label=CloneBackward0]
	140067041481664 -> 140067041480800
	140067041481664 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 2, 64)"]
	140067041482000 -> 140067041481664
	140067041482000 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041483296 -> 140067041482000
	140067041483296 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140067041484640 -> 140067041483296
	140067041484640 [label="ViewBackward0
------------------------
self_sym_sizes: (6, 768)"]
	140067041485792 -> 140067041484640
	140067041485792 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (6, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041488864 -> 140067041485792
	140067041017712 [label="q_encoder.encoder.information_exchanging_layer.0.attention.self.value.bias
 (768)" fillcolor=lightblue]
	140067041017712 -> 140067041488864
	140067041488864 [label=AccumulateGrad]
	140067041484448 -> 140067041485792
	140067041484448 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140067041478592 -> 140067041484448
	140067041479504 -> 140067041485792
	140067041479504 [label=TBackward0]
	140067041488432 -> 140067041479504
	140066853378384 [label="q_encoder.encoder.information_exchanging_layer.0.attention.self.value.weight
 (768, 768)" fillcolor=lightblue]
	140066853378384 -> 140067041488432
	140067041488432 [label=AccumulateGrad]
	140067041480464 -> 140067041479456
	140067041480464 [label=TBackward0]
	140067041479312 -> 140067041480464
	140067041014592 [label="q_encoder.encoder.information_exchanging_layer.0.attention.output.dense.weight
 (768, 768)" fillcolor=lightblue]
	140067041014592 -> 140067041479312
	140067041479312 [label=AccumulateGrad]
	140067041478592 -> 140067041478544
	140067041478352 -> 140066673015872
	140067041410208 [label="q_encoder.encoder.information_exchanging_layer.0.attention.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	140067041410208 -> 140067041478352
	140067041478352 [label=AccumulateGrad]
	140067041478736 -> 140066673015872
	140067041408368 [label="q_encoder.encoder.information_exchanging_layer.0.attention.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	140067041408368 -> 140067041478736
	140067041478736 [label=AccumulateGrad]
	140067041478496 -> 140067041477776
	140067041478496 [label=TBackward0]
	140067041478976 -> 140067041478496
	140067041833072 [label="q_encoder.encoder.information_exchanging_layer.0.intermediate.dense.weight
 (3072, 768)" fillcolor=lightblue]
	140067041833072 -> 140067041478976
	140067041478976 [label=AccumulateGrad]
	140067041477968 -> 140067041476960
	140067041477968 [label=TBackward0]
	140067041477104 -> 140067041477968
	140066852645104 [label="q_encoder.encoder.information_exchanging_layer.0.output.dense.weight
 (768, 3072)" fillcolor=lightblue]
	140066852645104 -> 140067041477104
	140067041477104 [label=AccumulateGrad]
	140066673015872 -> 140066673016160
	140066673015440 -> 140066673016400
	140067041836432 [label="q_encoder.encoder.information_exchanging_layer.0.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	140067041836432 -> 140066673015440
	140066673015440 [label=AccumulateGrad]
	140066673016592 -> 140066673016400
	140067041836752 [label="q_encoder.encoder.information_exchanging_layer.0.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	140067041836752 -> 140066673016592
	140066673016592 [label=AccumulateGrad]
	140066673013952 -> 140066708602448
	140066673013952 [label=CloneBackward0]
	140066673014576 -> 140066673013952
	140066673014576 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 1, 768)
start         :                   0
step          :                   1"]
	140066673014864 -> 140066673014576
	140066673014864 [label="SelectBackward0
------------------------------
dim           :              2
index         :              1
self_sym_sizes: (3, 1, 8, 768)"]
	140067041478688 -> 140066673014864
	140067041478688 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:      (3, 1, 8, 768)
start         :                   0
step          :                   1"]
	140067041477248 -> 140067041478688
	140067041477248 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:      (3, 1, 8, 768)
start         :                   0
step          :                   1"]
	140067041478880 -> 140067041477248
	140067041478880 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 8, 768)"]
	140067041479984 -> 140067041478880
	140067041479984 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140067041477728 -> 140067041479984
	140067041477728 [label="AddBackward0
------------
alpha: 1"]
	140067041477824 -> 140067041477728
	140067041477824 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041481424 -> 140067041477824
	140067041481424 [label="ViewBackward0
-------------------------
self_sym_sizes: (24, 768)"]
	140067041485408 -> 140067041481424
	140067041485408 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :     (24, 3072)
mat1_sym_strides:      (3072, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (3072, 768)
mat2_sym_strides:      (1, 3072)"]
	140067041476720 -> 140067041485408
	140066904408224 [label="q_encoder.encoder.text_encoding_layer.1.output.dense.bias
 (768)" fillcolor=lightblue]
	140066904408224 -> 140067041476720
	140067041476720 [label=AccumulateGrad]
	140067041482528 -> 140067041485408
	140067041482528 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 8, 3072)"]
	140067041487568 -> 140067041482528
	140067041487568 [label="GeluBackward0
---------------------------
approximate:           none
self       : [saved tensor]"]
	140067041486656 -> 140067041487568
	140067041486656 [label="ViewBackward0
--------------------------
self_sym_sizes: (24, 3072)"]
	140067041489392 -> 140067041486656
	140067041489392 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (24, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (768, 3072)
mat2_sym_strides:       (1, 768)"]
	140067041490400 -> 140067041489392
	140066904409904 [label="q_encoder.encoder.text_encoding_layer.1.intermediate.dense.bias
 (3072)" fillcolor=lightblue]
	140066904409904 -> 140067041490400
	140067041490400 [label=AccumulateGrad]
	140067041490496 -> 140067041489392
	140067041490496 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 8, 768)"]
	140067041480656 -> 140067041490496
	140067041480656 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140067041491120 -> 140067041480656
	140067041491120 [label="AddBackward0
------------
alpha: 1"]
	140067041492656 -> 140067041491120
	140067041492656 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041689664 -> 140067041492656
	140067041689664 [label="ViewBackward0
-------------------------
self_sym_sizes: (24, 768)"]
	140067041692928 -> 140067041689664
	140067041692928 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (24, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041693936 -> 140067041692928
	140066904409504 [label="q_encoder.encoder.text_encoding_layer.1.attention.output.dense.bias
 (768)" fillcolor=lightblue]
	140066904409504 -> 140067041693936
	140067041693936 [label=AccumulateGrad]
	140067041693312 -> 140067041692928
	140067041693312 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 8, 768)"]
	140067041694368 -> 140067041693312
	140067041694368 [label="ViewBackward0
------------------------------
self_sym_sizes: (3, 8, 12, 64)"]
	140067041694080 -> 140067041694368
	140067041694080 [label=CloneBackward0]
	140067041692640 -> 140067041694080
	140067041692640 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041694800 -> 140067041692640
	140067041694800 [label="UnsafeViewBackward0
---------------------------
self_sym_sizes: (36, 8, 64)"]
	140067041695040 -> 140067041694800
	140067041695040 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140067041695184 -> 140067041695040
	140067041695184 [label="ViewBackward0
-----------------------------
self_sym_sizes: (3, 12, 8, 8)"]
	140067041695424 -> 140067041695184
	140067041695424 [label="ExpandBackward0
-----------------------------
self_sym_sizes: (3, 12, 8, 8)"]
	140067041695520 -> 140067041695424
	140067041695520 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041695808 -> 140067041695520
	140067041695808 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140067041696048 -> 140067041695808
	140067041696048 [label="AddBackward0
------------
alpha: 1"]
	140067041696192 -> 140067041696048
	140067041696192 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041696432 -> 140067041696192
	140067041696432 [label="UnsafeViewBackward0
--------------------------
self_sym_sizes: (36, 8, 8)"]
	140067041696576 -> 140067041696432
	140067041696576 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140067041696720 -> 140067041696576
	140067041696720 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 8, 64)"]
	140067041696816 -> 140067041696720
	140067041696816 [label=CloneBackward0]
	140067041697008 -> 140067041696816
	140067041697008 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 8, 64)"]
	140067041697056 -> 140067041697008
	140067041697056 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041697296 -> 140067041697056
	140067041697296 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 8, 768)"]
	140067041697488 -> 140067041697296
	140067041697488 [label="ViewBackward0
-------------------------
self_sym_sizes: (24, 768)"]
	140067041697632 -> 140067041697488
	140067041697632 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (24, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041697776 -> 140067041697632
	140066904409024 [label="q_encoder.encoder.text_encoding_layer.1.attention.self.query.bias
 (768)" fillcolor=lightblue]
	140066904409024 -> 140067041697776
	140067041697776 [label=AccumulateGrad]
	140067041697680 -> 140067041697632
	140067041697680 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 8, 768)"]
	140067041490832 -> 140067041697680
	140067041490832 [label="ViewBackward0
------------------------------
self_sym_sizes: (3, 1, 8, 768)"]
	140067041698064 -> 140067041490832
	140067041698064 [label="AsStridedBackward0
------------------------------------
size          :       (3, 1, 8, 768)
storage_offset:                    0
stride        : (6144, 6144, 768, 1)"]
	140067041698208 -> 140067041698064
	140067041698208 [label=CopySlices]
	140067041485168 -> 140067041698208
	140067041698544 -> 140067041698208
	140067041698544 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 1, 768)
start         :                   0
step          :                   1"]
	140067041698256 -> 140067041698544
	140067041698256 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:         (3, 2, 768)
start         :                   1
step          :                   1"]
	140067041699264 -> 140067041698256
	140067041699264 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:         (3, 2, 768)
start         :                   0
step          :                   1"]
	140066673016400 -> 140067041699264
	140067041696864 -> 140067041697632
	140067041696864 [label=TBackward0]
	140067041697824 -> 140067041696864
	140066904408944 [label="q_encoder.encoder.text_encoding_layer.1.attention.self.query.weight
 (768, 768)" fillcolor=lightblue]
	140066904408944 -> 140067041697824
	140067041697824 [label=AccumulateGrad]
	140067041696336 -> 140067041696576
	140067041696336 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 64, 8)"]
	140067041696480 -> 140067041696336
	140067041696480 [label=CloneBackward0]
	140067041697584 -> 140067041696480
	140067041697584 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 64, 8)"]
	140067041697392 -> 140067041697584
	140067041697392 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551615
dim1: 18446744073709551614"]
	140067041698448 -> 140067041697392
	140067041698448 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041699456 -> 140067041698448
	140067041699456 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 8, 768)"]
	140067041697728 -> 140067041699456
	140067041697728 [label="ViewBackward0
-------------------------
self_sym_sizes: (24, 768)"]
	140067041697968 -> 140067041697728
	140067041697968 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (24, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041699504 -> 140067041697968
	140066904409184 [label="q_encoder.encoder.text_encoding_layer.1.attention.self.key.bias
 (768)" fillcolor=lightblue]
	140066904409184 -> 140067041699504
	140067041699504 [label=AccumulateGrad]
	140067041699600 -> 140067041697968
	140067041699600 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 8, 768)"]
	140067041490832 -> 140067041699600
	140067041696912 -> 140067041697968
	140067041696912 [label=TBackward0]
	140067041699792 -> 140067041696912
	140066904409104 [label="q_encoder.encoder.text_encoding_layer.1.attention.self.key.weight
 (768, 768)" fillcolor=lightblue]
	140066904409104 -> 140067041699792
	140067041699792 [label=AccumulateGrad]
	140067041694992 -> 140067041695040
	140067041694992 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 8, 64)"]
	140067041695280 -> 140067041694992
	140067041695280 [label=CloneBackward0]
	140067041695712 -> 140067041695280
	140067041695712 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 8, 64)"]
	140067041695952 -> 140067041695712
	140067041695952 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041697248 -> 140067041695952
	140067041697248 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 8, 768)"]
	140067041698112 -> 140067041697248
	140067041698112 [label="ViewBackward0
-------------------------
self_sym_sizes: (24, 768)"]
	140067041698592 -> 140067041698112
	140067041698592 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (24, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041699360 -> 140067041698592
	140066904409344 [label="q_encoder.encoder.text_encoding_layer.1.attention.self.value.bias
 (768)" fillcolor=lightblue]
	140066904409344 -> 140067041699360
	140067041699360 [label=AccumulateGrad]
	140067041697920 -> 140067041698592
	140067041697920 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 8, 768)"]
	140067041490832 -> 140067041697920
	140067041695472 -> 140067041698592
	140067041695472 [label=TBackward0]
	140067041699840 -> 140067041695472
	140066904409264 [label="q_encoder.encoder.text_encoding_layer.1.attention.self.value.weight
 (768, 768)" fillcolor=lightblue]
	140066904409264 -> 140067041699840
	140067041699840 [label=AccumulateGrad]
	140067041690816 -> 140067041692928
	140067041690816 [label=TBackward0]
	140067041694176 -> 140067041690816
	140066904409424 [label="q_encoder.encoder.text_encoding_layer.1.attention.output.dense.weight
 (768, 768)" fillcolor=lightblue]
	140066904409424 -> 140067041694176
	140067041694176 [label=AccumulateGrad]
	140067041490832 -> 140067041491120
	140067041490112 -> 140067041480656
	140066904409664 [label="q_encoder.encoder.text_encoding_layer.1.attention.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	140066904409664 -> 140067041490112
	140067041490112 [label=AccumulateGrad]
	140067041491552 -> 140067041480656
	140066904409584 [label="q_encoder.encoder.text_encoding_layer.1.attention.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	140066904409584 -> 140067041491552
	140067041491552 [label=AccumulateGrad]
	140067041488672 -> 140067041489392
	140067041488672 [label=TBackward0]
	140067041491216 -> 140067041488672
	140066904409824 [label="q_encoder.encoder.text_encoding_layer.1.intermediate.dense.weight
 (3072, 768)" fillcolor=lightblue]
	140066904409824 -> 140067041491216
	140067041491216 [label=AccumulateGrad]
	140067041480848 -> 140067041485408
	140067041480848 [label=TBackward0]
	140067041488048 -> 140067041480848
	140066904409744 [label="q_encoder.encoder.text_encoding_layer.1.output.dense.weight
 (768, 3072)" fillcolor=lightblue]
	140066904409744 -> 140067041488048
	140067041488048 [label=AccumulateGrad]
	140067041480656 -> 140067041477728
	140067041480272 -> 140067041479984
	140066904410144 [label="q_encoder.encoder.text_encoding_layer.1.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	140066904410144 -> 140067041480272
	140067041480272 [label=AccumulateGrad]
	140067041476864 -> 140067041479984
	140066904409984 [label="q_encoder.encoder.text_encoding_layer.1.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	140066904409984 -> 140067041476864
	140067041476864 [label=AccumulateGrad]
	140066673015248 -> 140066673014768
	140066673015248 [label=TBackward0]
	140066673015968 -> 140066673015248
	140067041410688 [label="q_encoder.encoder.information_exchanging_layer.1.attention.self.query.weight
 (768, 768)" fillcolor=lightblue]
	140067041410688 -> 140066673015968
	140066673015968 [label=AccumulateGrad]
	140066673016736 -> 140067053452016
	140066673016736 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 64, 2)"]
	140066673013472 -> 140066673016736
	140066673013472 [label=CloneBackward0]
	140066673014672 -> 140066673013472
	140066673014672 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 64, 2)"]
	140066673015056 -> 140066673014672
	140066673015056 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551615
dim1: 18446744073709551614"]
	140066673015296 -> 140066673015056
	140066673015296 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140066673013424 -> 140066673015296
	140066673013424 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140067041477632 -> 140066673013424
	140067041477632 [label="ViewBackward0
------------------------
self_sym_sizes: (6, 768)"]
	140067041481328 -> 140067041477632
	140067041481328 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (6, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041477392 -> 140067041481328
	140066853291344 [label="q_encoder.encoder.information_exchanging_layer.1.attention.self.key.bias
 (768)" fillcolor=lightblue]
	140066853291344 -> 140067041477392
	140067041477392 [label=AccumulateGrad]
	140067041482480 -> 140067041481328
	140067041482480 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140066708602448 -> 140067041482480
	140067041479744 -> 140067041481328
	140067041479744 [label=TBackward0]
	140067041487616 -> 140067041479744
	140066853290544 [label="q_encoder.encoder.information_exchanging_layer.1.attention.self.key.weight
 (768, 768)" fillcolor=lightblue]
	140066853290544 -> 140067041487616
	140067041487616 [label=AccumulateGrad]
	140066708587472 -> 140066708587616
	140066708587472 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 2, 64)"]
	140067054459856 -> 140066708587472
	140067054459856 [label=CloneBackward0]
	140067054459952 -> 140067054459856
	140067054459952 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 2, 64)"]
	140067053451728 -> 140067054459952
	140067053451728 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067053451296 -> 140067053451728
	140067053451296 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140066673015584 -> 140067053451296
	140066673015584 [label="ViewBackward0
------------------------
self_sym_sizes: (6, 768)"]
	140066673016304 -> 140066673015584
	140066673016304 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (6, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140066673013760 -> 140066673016304
	140066853291184 [label="q_encoder.encoder.information_exchanging_layer.1.attention.self.value.bias
 (768)" fillcolor=lightblue]
	140066853291184 -> 140066673013760
	140066673013760 [label=AccumulateGrad]
	140067041478448 -> 140066673016304
	140067041478448 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140066708602448 -> 140067041478448
	140067041489920 -> 140066673016304
	140067041489920 [label=TBackward0]
	140067041483344 -> 140067041489920
	140066853291104 [label="q_encoder.encoder.information_exchanging_layer.1.attention.self.value.weight
 (768, 768)" fillcolor=lightblue]
	140066853291104 -> 140067041483344
	140067041483344 [label=AccumulateGrad]
	140066708587856 -> 140066708588912
	140066708587856 [label=TBackward0]
	140066708589584 -> 140066708587856
	140066853290944 [label="q_encoder.encoder.information_exchanging_layer.1.attention.output.dense.weight
 (768, 768)" fillcolor=lightblue]
	140066853290944 -> 140066708589584
	140066708589584 [label=AccumulateGrad]
	140066708602448 -> 140066708601248
	140066708602784 -> 140066708594864
	140066853295184 [label="q_encoder.encoder.information_exchanging_layer.1.attention.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	140066853295184 -> 140066708602784
	140066708602784 [label=AccumulateGrad]
	140066708586752 -> 140066708594864
	140066853295104 [label="q_encoder.encoder.information_exchanging_layer.1.attention.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	140066853295104 -> 140066708586752
	140066708586752 [label=AccumulateGrad]
	140066708602256 -> 140066708601824
	140066708602256 [label=TBackward0]
	140066708587712 -> 140066708602256
	140066853292224 [label="q_encoder.encoder.information_exchanging_layer.1.intermediate.dense.weight
 (3072, 768)" fillcolor=lightblue]
	140066853292224 -> 140066708587712
	140066708587712 [label=AccumulateGrad]
	140066708601968 -> 140066708600336
	140066708601968 [label=TBackward0]
	140066708601344 -> 140066708601968
	140066853290384 [label="q_encoder.encoder.information_exchanging_layer.1.output.dense.weight
 (768, 3072)" fillcolor=lightblue]
	140066853290384 -> 140066708601344
	140066708601344 [label=AccumulateGrad]
	140066708594864 -> 140066708594192
	140066708600912 -> 140066708601008
	140066853297504 [label="q_encoder.encoder.information_exchanging_layer.1.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	140066853297504 -> 140066708600912
	140066708600912 [label=AccumulateGrad]
	140066708600480 -> 140066708601008
	140066853300944 [label="q_encoder.encoder.information_exchanging_layer.1.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	140066853300944 -> 140066708600480
	140066708600480 [label=AccumulateGrad]
	140066708593856 -> 140066708597648
	140066708593856 [label=CloneBackward0]
	140066708593808 -> 140066708593856
	140066708593808 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 1, 768)
start         :                   0
step          :                   1"]
	140066708594672 -> 140066708593808
	140066708594672 [label="SelectBackward0
------------------------------
dim           :              2
index         :              1
self_sym_sizes: (3, 1, 8, 768)"]
	140066708600288 -> 140066708594672
	140066708600288 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:      (3, 1, 8, 768)
start         :                   0
step          :                   1"]
	140066708595728 -> 140066708600288
	140066708595728 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:      (3, 1, 8, 768)
start         :                   0
step          :                   1"]
	140066708591552 -> 140066708595728
	140066708591552 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 8, 768)"]
	140066708587904 -> 140066708591552
	140066708587904 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140067054468112 -> 140066708587904
	140067054468112 [label="AddBackward0
------------
alpha: 1"]
	140066708591360 -> 140067054468112
	140066708591360 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067053442656 -> 140066708591360
	140067053442656 [label="ViewBackward0
-------------------------
self_sym_sizes: (24, 768)"]
	140066673013568 -> 140067053442656
	140066673013568 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :     (24, 3072)
mat1_sym_strides:      (3072, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (3072, 768)
mat2_sym_strides:      (1, 3072)"]
	140067041492416 -> 140066673013568
	140066904408144 [label="q_encoder.encoder.text_encoding_layer.2.output.dense.bias
 (768)" fillcolor=lightblue]
	140066904408144 -> 140067041492416
	140067041492416 [label=AccumulateGrad]
	140067041479072 -> 140066673013568
	140067041479072 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 8, 3072)"]
	140066708601392 -> 140067041479072
	140066708601392 [label="GeluBackward0
---------------------------
approximate:           none
self       : [saved tensor]"]
	140066708590448 -> 140066708601392
	140066708590448 [label="ViewBackward0
--------------------------
self_sym_sizes: (24, 3072)"]
	140067041693600 -> 140066708590448
	140067041693600 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (24, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (768, 3072)
mat2_sym_strides:       (1, 768)"]
	140067041694896 -> 140067041693600
	140066904411184 [label="q_encoder.encoder.text_encoding_layer.2.intermediate.dense.bias
 (3072)" fillcolor=lightblue]
	140066904411184 -> 140067041694896
	140067041694896 [label=AccumulateGrad]
	140067041694656 -> 140067041693600
	140067041694656 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 8, 768)"]
	140066708586944 -> 140067041694656
	140066708586944 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140067041695664 -> 140066708586944
	140067041695664 [label="AddBackward0
------------
alpha: 1"]
	140067041697104 -> 140067041695664
	140067041697104 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041695328 -> 140067041697104
	140067041695328 [label="ViewBackward0
-------------------------
self_sym_sizes: (24, 768)"]
	140067041699888 -> 140067041695328
	140067041699888 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (24, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041693696 -> 140067041699888
	140066904410784 [label="q_encoder.encoder.text_encoding_layer.2.attention.output.dense.bias
 (768)" fillcolor=lightblue]
	140066904410784 -> 140067041693696
	140067041693696 [label=AccumulateGrad]
	140067041698640 -> 140067041699888
	140067041698640 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 8, 768)"]
	140067041700080 -> 140067041698640
	140067041700080 [label="ViewBackward0
------------------------------
self_sym_sizes: (3, 8, 12, 64)"]
	140067041699984 -> 140067041700080
	140067041699984 [label=CloneBackward0]
	140067041699024 -> 140067041699984
	140067041699024 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041700368 -> 140067041699024
	140067041700368 [label="UnsafeViewBackward0
---------------------------
self_sym_sizes: (36, 8, 64)"]
	140067041700608 -> 140067041700368
	140067041700608 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140067041700944 -> 140067041700608
	140067041700944 [label="ViewBackward0
-----------------------------
self_sym_sizes: (3, 12, 8, 8)"]
	140067041701040 -> 140067041700944
	140067041701040 [label="ExpandBackward0
-----------------------------
self_sym_sizes: (3, 12, 8, 8)"]
	140067041701280 -> 140067041701040
	140067041701280 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041701472 -> 140067041701280
	140067041701472 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140067041701808 -> 140067041701472
	140067041701808 [label="AddBackward0
------------
alpha: 1"]
	140067041701856 -> 140067041701808
	140067041701856 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041701952 -> 140067041701856
	140067041701952 [label="UnsafeViewBackward0
--------------------------
self_sym_sizes: (36, 8, 8)"]
	140067041702144 -> 140067041701952
	140067041702144 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140067041702192 -> 140067041702144
	140067041702192 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 8, 64)"]
	140067041702240 -> 140067041702192
	140067041702240 [label=CloneBackward0]
	140067041702432 -> 140067041702240
	140067041702432 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 8, 64)"]
	140067041702528 -> 140067041702432
	140067041702528 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041702768 -> 140067041702528
	140067041702768 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 8, 768)"]
	140067041702912 -> 140067041702768
	140067041702912 [label="ViewBackward0
-------------------------
self_sym_sizes: (24, 768)"]
	140067041703056 -> 140067041702912
	140067041703056 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (24, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041703392 -> 140067041703056
	140066904410304 [label="q_encoder.encoder.text_encoding_layer.2.attention.self.query.bias
 (768)" fillcolor=lightblue]
	140066904410304 -> 140067041703392
	140067041703392 [label=AccumulateGrad]
	140067041703152 -> 140067041703056
	140067041703152 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 8, 768)"]
	140067041697872 -> 140067041703152
	140067041697872 [label="ViewBackward0
------------------------------
self_sym_sizes: (3, 1, 8, 768)"]
	140067041703584 -> 140067041697872
	140067041703584 [label="AsStridedBackward0
------------------------------------
size          :       (3, 1, 8, 768)
storage_offset:                    0
stride        : (6144, 6144, 768, 1)"]
	140067041703728 -> 140067041703584
	140067041703728 [label=CopySlices]
	140067041479984 -> 140067041703728
	140067041704064 -> 140067041703728
	140067041704064 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 1, 768)
start         :                   0
step          :                   1"]
	140067041703824 -> 140067041704064
	140067041703824 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:         (3, 2, 768)
start         :                   1
step          :                   1"]
	140067041704784 -> 140067041703824
	140067041704784 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:         (3, 2, 768)
start         :                   0
step          :                   1"]
	140066708601008 -> 140067041704784
	140067041702336 -> 140067041703056
	140067041702336 [label=TBackward0]
	140067041703296 -> 140067041702336
	140066904410224 [label="q_encoder.encoder.text_encoding_layer.2.attention.self.query.weight
 (768, 768)" fillcolor=lightblue]
	140066904410224 -> 140067041703296
	140067041703296 [label=AccumulateGrad]
	140067041701904 -> 140067041702144
	140067041701904 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 64, 8)"]
	140067041702000 -> 140067041701904
	140067041702000 [label=CloneBackward0]
	140067041703008 -> 140067041702000
	140067041703008 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 64, 8)"]
	140067041702864 -> 140067041703008
	140067041702864 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551615
dim1: 18446744073709551614"]
	140067041703968 -> 140067041702864
	140067041703968 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041704928 -> 140067041703968
	140067041704928 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 8, 768)"]
	140067041703200 -> 140067041704928
	140067041703200 [label="ViewBackward0
-------------------------
self_sym_sizes: (24, 768)"]
	140067041703632 -> 140067041703200
	140067041703632 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (24, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041704976 -> 140067041703632
	140066904410464 [label="q_encoder.encoder.text_encoding_layer.2.attention.self.key.bias
 (768)" fillcolor=lightblue]
	140066904410464 -> 140067041704976
	140067041704976 [label=AccumulateGrad]
	140067041705120 -> 140067041703632
	140067041705120 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 8, 768)"]
	140067041697872 -> 140067041705120
	140067041702384 -> 140067041703632
	140067041702384 [label=TBackward0]
	140067041705168 -> 140067041702384
	140066904410384 [label="q_encoder.encoder.text_encoding_layer.2.attention.self.key.weight
 (768, 768)" fillcolor=lightblue]
	140066904410384 -> 140067041705168
	140067041705168 [label=AccumulateGrad]
	140067041700560 -> 140067041700608
	140067041700560 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 8, 64)"]
	140067041700848 -> 140067041700560
	140067041700848 [label=CloneBackward0]
	140067041701616 -> 140067041700848
	140067041701616 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 8, 64)"]
	140067041701664 -> 140067041701616
	140067041701664 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041702624 -> 140067041701664
	140067041702624 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 8, 768)"]
	140067041703680 -> 140067041702624
	140067041703680 [label="ViewBackward0
-------------------------
self_sym_sizes: (24, 768)"]
	140067041704112 -> 140067041703680
	140067041704112 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (24, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041704880 -> 140067041704112
	140066904410624 [label="q_encoder.encoder.text_encoding_layer.2.attention.self.value.bias
 (768)" fillcolor=lightblue]
	140066904410624 -> 140067041704880
	140067041704880 [label=AccumulateGrad]
	140067041703536 -> 140067041704112
	140067041703536 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 8, 768)"]
	140067041697872 -> 140067041703536
	140067041701184 -> 140067041704112
	140067041701184 [label=TBackward0]
	140067041705264 -> 140067041701184
	140066904410544 [label="q_encoder.encoder.text_encoding_layer.2.attention.self.value.weight
 (768, 768)" fillcolor=lightblue]
	140066904410544 -> 140067041705264
	140067041705264 [label=AccumulateGrad]
	140067041698832 -> 140067041699888
	140067041698832 [label=TBackward0]
	140067041700032 -> 140067041698832
	140066904410704 [label="q_encoder.encoder.text_encoding_layer.2.attention.output.dense.weight
 (768, 768)" fillcolor=lightblue]
	140066904410704 -> 140067041700032
	140067041700032 [label=AccumulateGrad]
	140067041697872 -> 140067041695664
	140067041694560 -> 140066708586944
	140066904410944 [label="q_encoder.encoder.text_encoding_layer.2.attention.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	140066904410944 -> 140067041694560
	140067041694560 [label=AccumulateGrad]
	140067041696000 -> 140066708586944
	140066904410864 [label="q_encoder.encoder.text_encoding_layer.2.attention.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	140066904410864 -> 140067041696000
	140067041696000 [label=AccumulateGrad]
	140067041693888 -> 140067041693600
	140067041693888 [label=TBackward0]
	140067041698928 -> 140067041693888
	140066904411104 [label="q_encoder.encoder.text_encoding_layer.2.intermediate.dense.weight
 (3072, 768)" fillcolor=lightblue]
	140066904411104 -> 140067041698928
	140067041698928 [label=AccumulateGrad]
	140067041480944 -> 140066673013568
	140067041480944 [label=TBackward0]
	140066708601776 -> 140067041480944
	140066904411024 [label="q_encoder.encoder.text_encoding_layer.2.output.dense.weight
 (768, 3072)" fillcolor=lightblue]
	140066904411024 -> 140066708601776
	140066708601776 [label=AccumulateGrad]
	140066708586944 -> 140067054468112
	140067054461056 -> 140066708587904
	140066890266048 [label="q_encoder.encoder.text_encoding_layer.2.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	140066890266048 -> 140067054461056
	140067054461056 [label=AccumulateGrad]
	140066708592464 -> 140066708587904
	140066904410064 [label="q_encoder.encoder.text_encoding_layer.2.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	140066904410064 -> 140066708592464
	140066708592464 [label=AccumulateGrad]
	140066708594144 -> 140066708596016
	140066708594144 [label=TBackward0]
	140067041485072 -> 140066708594144
	140066853292144 [label="q_encoder.encoder.information_exchanging_layer.2.attention.self.query.weight
 (768, 768)" fillcolor=lightblue]
	140066853292144 -> 140067041485072
	140067041485072 [label=AccumulateGrad]
	140066708593040 -> 140066708592752
	140066708593040 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 64, 2)"]
	140066673015776 -> 140066708593040
	140066673015776 [label=CloneBackward0]
	140066708593568 -> 140066673015776
	140066708593568 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 64, 2)"]
	140066708592416 -> 140066708593568
	140066708592416 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551615
dim1: 18446744073709551614"]
	140066708601200 -> 140066708592416
	140066708601200 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140066708592800 -> 140066708601200
	140066708592800 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140066708590208 -> 140066708592800
	140066708590208 [label="ViewBackward0
------------------------
self_sym_sizes: (6, 768)"]
	140066708600720 -> 140066708590208
	140066708600720 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (6, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140066708602208 -> 140066708600720
	140066853296384 [label="q_encoder.encoder.information_exchanging_layer.2.attention.self.key.bias
 (768)" fillcolor=lightblue]
	140066853296384 -> 140066708602208
	140066708602208 [label=AccumulateGrad]
	140066708587520 -> 140066708600720
	140066708587520 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140066708597648 -> 140066708587520
	140066708592368 -> 140066708600720
	140066708592368 [label=TBackward0]
	140066708600048 -> 140066708592368
	140066853296704 [label="q_encoder.encoder.information_exchanging_layer.2.attention.self.key.weight
 (768, 768)" fillcolor=lightblue]
	140066853296704 -> 140066708600048
	140066708600048 [label=AccumulateGrad]
	140066708599712 -> 140066708599520
	140066708599712 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 2, 64)"]
	140066673014192 -> 140066708599712
	140066673014192 [label=CloneBackward0]
	140066708592128 -> 140066673014192
	140066708592128 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 2, 64)"]
	140066708592272 -> 140066708592128
	140066708592272 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140066708593472 -> 140066708592272
	140066708593472 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140066708595440 -> 140066708593472
	140066708595440 [label="ViewBackward0
------------------------
self_sym_sizes: (6, 768)"]
	140066708600144 -> 140066708595440
	140066708600144 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (6, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140066708595104 -> 140066708600144
	140066853293664 [label="q_encoder.encoder.information_exchanging_layer.2.attention.self.value.bias
 (768)" fillcolor=lightblue]
	140066853293664 -> 140066708595104
	140066708595104 [label=AccumulateGrad]
	140066708595872 -> 140066708600144
	140066708595872 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140066708597648 -> 140066708595872
	140066708596736 -> 140066708600144
	140066708596736 [label=TBackward0]
	140066708594960 -> 140066708596736
	140066853295344 [label="q_encoder.encoder.information_exchanging_layer.2.attention.self.value.weight
 (768, 768)" fillcolor=lightblue]
	140066853295344 -> 140066708594960
	140066708594960 [label=AccumulateGrad]
	140066708599136 -> 140066708598608
	140066708599136 [label=TBackward0]
	140066708598464 -> 140066708599136
	140066853290224 [label="q_encoder.encoder.information_exchanging_layer.2.attention.output.dense.weight
 (768, 768)" fillcolor=lightblue]
	140066853290224 -> 140066708598464
	140066708598464 [label=AccumulateGrad]
	140066708597648 -> 140066708597360
	140066708596976 -> 140066708591888
	140066853294944 [label="q_encoder.encoder.information_exchanging_layer.2.attention.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	140066853294944 -> 140066708596976
	140066708596976 [label=AccumulateGrad]
	140066708597552 -> 140066708591888
	140066853297424 [label="q_encoder.encoder.information_exchanging_layer.2.attention.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	140066853297424 -> 140066708597552
	140066708597552 [label=AccumulateGrad]
	140066708597120 -> 140066708596400
	140066708597120 [label=TBackward0]
	140066708597600 -> 140066708597120
	140066853294224 [label="q_encoder.encoder.information_exchanging_layer.2.intermediate.dense.weight
 (3072, 768)" fillcolor=lightblue]
	140066853294224 -> 140066708597600
	140066708597600 [label=AccumulateGrad]
	140066708596784 -> 140066708596160
	140066708596784 [label=TBackward0]
	140066708589104 -> 140066708596784
	140066853297024 [label="q_encoder.encoder.information_exchanging_layer.2.output.dense.weight
 (768, 3072)" fillcolor=lightblue]
	140066853297024 -> 140066708589104
	140066708589104 [label=AccumulateGrad]
	140066708591888 -> 140066708588864
	140066708588048 -> 140066708589536
	140066853300544 [label="q_encoder.encoder.information_exchanging_layer.2.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	140066853300544 -> 140066708588048
	140066708588048 [label=AccumulateGrad]
	140066708589248 -> 140066708589536
	140066853296544 [label="q_encoder.encoder.information_exchanging_layer.2.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	140066853296544 -> 140066708589248
	140066708589248 [label=AccumulateGrad]
	140067054986208 -> 140066907025472
	140067054986208 [label=CloneBackward0]
	140067054988848 -> 140067054986208
	140067054988848 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 1, 768)
start         :                   0
step          :                   1"]
	140066708587184 -> 140067054988848
	140066708587184 [label="SelectBackward0
------------------------------
dim           :              2
index         :              1
self_sym_sizes: (3, 1, 8, 768)"]
	140066708595392 -> 140066708587184
	140066708595392 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:      (3, 1, 8, 768)
start         :                   0
step          :                   1"]
	140066708588432 -> 140066708595392
	140066708588432 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:      (3, 1, 8, 768)
start         :                   0
step          :                   1"]
	140066708599184 -> 140066708588432
	140066708599184 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 8, 768)"]
	140066708598656 -> 140066708599184
	140066708598656 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140066708589008 -> 140066708598656
	140066708589008 [label="AddBackward0
------------
alpha: 1"]
	140066708590496 -> 140066708589008
	140066708590496 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140066708590112 -> 140066708590496
	140066708590112 [label="ViewBackward0
-------------------------
self_sym_sizes: (24, 768)"]
	140066708596064 -> 140066708590112
	140066708596064 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :     (24, 3072)
mat1_sym_strides:      (3072, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (3072, 768)
mat2_sym_strides:      (1, 3072)"]
	140066708589824 -> 140066708596064
	140066890265968 [label="q_encoder.encoder.text_encoding_layer.3.output.dense.bias
 (768)" fillcolor=lightblue]
	140066890265968 -> 140066708589824
	140066708589824 [label=AccumulateGrad]
	140066708591456 -> 140066708596064
	140066708591456 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 8, 3072)"]
	140066708593232 -> 140066708591456
	140066708593232 [label="GeluBackward0
---------------------------
approximate:           none
self       : [saved tensor]"]
	140067041699120 -> 140066708593232
	140067041699120 [label="ViewBackward0
--------------------------
self_sym_sizes: (24, 3072)"]
	140067041694608 -> 140067041699120
	140067041694608 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (24, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (768, 3072)
mat2_sym_strides:       (1, 768)"]
	140067041700464 -> 140067041694608
	140066890264768 [label="q_encoder.encoder.text_encoding_layer.3.intermediate.dense.bias
 (3072)" fillcolor=lightblue]
	140066890264768 -> 140067041700464
	140067041700464 [label=AccumulateGrad]
	140067041700224 -> 140067041694608
	140067041700224 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 8, 768)"]
	140066708598992 -> 140067041700224
	140066708598992 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140067041701328 -> 140066708598992
	140067041701328 [label="AddBackward0
------------
alpha: 1"]
	140067041702672 -> 140067041701328
	140067041702672 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041701088 -> 140067041702672
	140067041701088 [label="ViewBackward0
-------------------------
self_sym_sizes: (24, 768)"]
	140067041705360 -> 140067041701088
	140067041705360 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (24, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041696528 -> 140067041705360
	140066890260448 [label="q_encoder.encoder.text_encoding_layer.3.attention.output.dense.bias
 (768)" fillcolor=lightblue]
	140066890260448 -> 140067041696528
	140067041696528 [label=AccumulateGrad]
	140067041704256 -> 140067041705360
	140067041704256 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 8, 768)"]
	140067041705600 -> 140067041704256
	140067041705600 [label="ViewBackward0
------------------------------
self_sym_sizes: (3, 8, 12, 64)"]
	140067041705408 -> 140067041705600
	140067041705408 [label=CloneBackward0]
	140067041704736 -> 140067041705408
	140067041704736 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041705648 -> 140067041704736
	140067041705648 [label="UnsafeViewBackward0
---------------------------
self_sym_sizes: (36, 8, 64)"]
	140067041591600 -> 140067041705648
	140067041591600 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140067041591696 -> 140067041591600
	140067041591696 [label="ViewBackward0
-----------------------------
self_sym_sizes: (3, 12, 8, 8)"]
	140067041591840 -> 140067041591696
	140067041591840 [label="ExpandBackward0
-----------------------------
self_sym_sizes: (3, 12, 8, 8)"]
	140067041592032 -> 140067041591840
	140067041592032 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041592176 -> 140067041592032
	140067041592176 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140067041592656 -> 140067041592176
	140067041592656 [label="AddBackward0
------------
alpha: 1"]
	140067041592704 -> 140067041592656
	140067041592704 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041592896 -> 140067041592704
	140067041592896 [label="UnsafeViewBackward0
--------------------------
self_sym_sizes: (36, 8, 8)"]
	140067041593088 -> 140067041592896
	140067041593088 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140067041593136 -> 140067041593088
	140067041593136 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 8, 64)"]
	140067041593232 -> 140067041593136
	140067041593232 [label=CloneBackward0]
	140067041593520 -> 140067041593232
	140067041593520 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 8, 64)"]
	140067041593616 -> 140067041593520
	140067041593616 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041593808 -> 140067041593616
	140067041593808 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 8, 768)"]
	140067041594000 -> 140067041593808
	140067041594000 [label="ViewBackward0
-------------------------
self_sym_sizes: (24, 768)"]
	140067041594144 -> 140067041594000
	140067041594144 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (24, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041594336 -> 140067041594144
	140066890265408 [label="q_encoder.encoder.text_encoding_layer.3.attention.self.query.bias
 (768)" fillcolor=lightblue]
	140066890265408 -> 140067041594336
	140067041594336 [label=AccumulateGrad]
	140067041594240 -> 140067041594144
	140067041594240 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 8, 768)"]
	140067041703488 -> 140067041594240
	140067041703488 [label="ViewBackward0
------------------------------
self_sym_sizes: (3, 1, 8, 768)"]
	140067041594624 -> 140067041703488
	140067041594624 [label="AsStridedBackward0
------------------------------------
size          :       (3, 1, 8, 768)
storage_offset:                    0
stride        : (6144, 6144, 768, 1)"]
	140067041594816 -> 140067041594624
	140067041594816 [label=CopySlices]
	140066708587904 -> 140067041594816
	140067041595248 -> 140067041594816
	140067041595248 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 1, 768)
start         :                   0
step          :                   1"]
	140067041594912 -> 140067041595248
	140067041594912 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:         (3, 2, 768)
start         :                   1
step          :                   1"]
	140067041595776 -> 140067041594912
	140067041595776 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:         (3, 2, 768)
start         :                   0
step          :                   1"]
	140066708589536 -> 140067041595776
	140067041593328 -> 140067041594144
	140067041593328 [label=TBackward0]
	140067041594432 -> 140067041593328
	140066890266128 [label="q_encoder.encoder.text_encoding_layer.3.attention.self.query.weight
 (768, 768)" fillcolor=lightblue]
	140066890266128 -> 140067041594432
	140067041594432 [label=AccumulateGrad]
	140067041592848 -> 140067041593088
	140067041592848 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 64, 8)"]
	140067041592944 -> 140067041592848
	140067041592944 [label=CloneBackward0]
	140067041594048 -> 140067041592944
	140067041594048 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 64, 8)"]
	140067041593904 -> 140067041594048
	140067041593904 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551615
dim1: 18446744073709551614"]
	140067041595056 -> 140067041593904
	140067041595056 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041596064 -> 140067041595056
	140067041596064 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 8, 768)"]
	140067041594288 -> 140067041596064
	140067041594288 [label="ViewBackward0
-------------------------
self_sym_sizes: (24, 768)"]
	140067041594672 -> 140067041594288
	140067041594672 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (24, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041596160 -> 140067041594672
	140066890265568 [label="q_encoder.encoder.text_encoding_layer.3.attention.self.key.bias
 (768)" fillcolor=lightblue]
	140066890265568 -> 140067041596160
	140067041596160 [label=AccumulateGrad]
	140067041596256 -> 140067041594672
	140067041596256 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 8, 768)"]
	140067041703488 -> 140067041596256
	140067041593376 -> 140067041594672
	140067041593376 [label=TBackward0]
	140067041596352 -> 140067041593376
	140066890265488 [label="q_encoder.encoder.text_encoding_layer.3.attention.self.key.weight
 (768, 768)" fillcolor=lightblue]
	140066890265488 -> 140067041596352
	140067041596352 [label=AccumulateGrad]
	140067041591504 -> 140067041591600
	140067041591504 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 8, 64)"]
	140067041591744 -> 140067041591504
	140067041591744 [label=CloneBackward0]
	140067041592224 -> 140067041591744
	140067041592224 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 8, 64)"]
	140067041592416 -> 140067041592224
	140067041592416 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041593664 -> 140067041592416
	140067041593664 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 8, 768)"]
	140067041594720 -> 140067041593664
	140067041594720 [label="ViewBackward0
-------------------------
self_sym_sizes: (24, 768)"]
	140067041595296 -> 140067041594720
	140067041595296 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (24, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041596016 -> 140067041595296
	140066890265728 [label="q_encoder.encoder.text_encoding_layer.3.attention.self.value.bias
 (768)" fillcolor=lightblue]
	140066890265728 -> 140067041596016
	140067041596016 [label=AccumulateGrad]
	140067041594528 -> 140067041595296
	140067041594528 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 8, 768)"]
	140067041703488 -> 140067041594528
	140067041591888 -> 140067041595296
	140067041591888 [label=TBackward0]
	140067041596400 -> 140067041591888
	140066890265648 [label="q_encoder.encoder.text_encoding_layer.3.attention.self.value.weight
 (768, 768)" fillcolor=lightblue]
	140066890265648 -> 140067041596400
	140067041596400 [label=AccumulateGrad]
	140067041704304 -> 140067041705360
	140067041704304 [label=TBackward0]
	140067041705504 -> 140067041704304
	140066890265328 [label="q_encoder.encoder.text_encoding_layer.3.attention.output.dense.weight
 (768, 768)" fillcolor=lightblue]
	140066890265328 -> 140067041705504
	140067041705504 [label=AccumulateGrad]
	140067041703488 -> 140067041701328
	140067041700176 -> 140066708598992
	140066890265248 [label="q_encoder.encoder.text_encoding_layer.3.attention.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	140066890265248 -> 140067041700176
	140067041700176 [label=AccumulateGrad]
	140067041701712 -> 140066708598992
	140066890265168 [label="q_encoder.encoder.text_encoding_layer.3.attention.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	140066890265168 -> 140067041701712
	140067041701712 [label=AccumulateGrad]
	140067041698976 -> 140067041694608
	140067041698976 [label=TBackward0]
	140067041704448 -> 140067041698976
	140066890265088 [label="q_encoder.encoder.text_encoding_layer.3.intermediate.dense.weight
 (3072, 768)" fillcolor=lightblue]
	140066890265088 -> 140067041704448
	140067041704448 [label=AccumulateGrad]
	140066708598320 -> 140066708596064
	140066708598320 [label=TBackward0]
	140067041694464 -> 140066708598320
	140066890265008 [label="q_encoder.encoder.text_encoding_layer.3.output.dense.weight
 (768, 3072)" fillcolor=lightblue]
	140066890265008 -> 140067041694464
	140067041694464 [label=AccumulateGrad]
	140066708598992 -> 140066708589008
	140066708597792 -> 140066708598656
	140066890252368 [label="q_encoder.encoder.text_encoding_layer.3.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	140066890252368 -> 140066708597792
	140066708597792 [label=AccumulateGrad]
	140066708590256 -> 140066708598656
	140066890264848 [label="q_encoder.encoder.text_encoding_layer.3.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	140066890264848 -> 140066708590256
	140066708590256 [label=AccumulateGrad]
	140077287889680 -> 140066907028496
	140077287889680 [label=TBackward0]
	140067054819152 -> 140077287889680
	140066853294064 [label="q_encoder.encoder.information_exchanging_layer.3.attention.self.query.weight
 (768, 768)" fillcolor=lightblue]
	140066853294064 -> 140067054819152
	140067054819152 [label=AccumulateGrad]
	140066907027488 -> 140066907028304
	140066907027488 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 64, 2)"]
	140066907028208 -> 140066907027488
	140066907028208 [label=CloneBackward0]
	140066907029264 -> 140066907028208
	140066907029264 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 64, 2)"]
	140067973179104 -> 140066907029264
	140067973179104 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551615
dim1: 18446744073709551614"]
	140066907027536 -> 140067973179104
	140066907027536 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140066708598800 -> 140066907027536
	140066708598800 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140066708597696 -> 140066708598800
	140066708597696 [label="ViewBackward0
------------------------
self_sym_sizes: (6, 768)"]
	140066708599424 -> 140066708597696
	140066708599424 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (6, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140066708597264 -> 140066708599424
	140066853292704 [label="q_encoder.encoder.information_exchanging_layer.3.attention.self.key.bias
 (768)" fillcolor=lightblue]
	140066853292704 -> 140066708597264
	140066708597264 [label=AccumulateGrad]
	140066708589920 -> 140066708599424
	140066708589920 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140066907025472 -> 140066708589920
	140066708596304 -> 140066708599424
	140066708596304 [label=TBackward0]
	140066708599856 -> 140066708596304
	140066853294784 [label="q_encoder.encoder.information_exchanging_layer.3.attention.self.key.weight
 (768, 768)" fillcolor=lightblue]
	140066853294784 -> 140066708599856
	140066708599856 [label=AccumulateGrad]
	140066907026288 -> 140066907024752
	140066907026288 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 2, 64)"]
	140066907026480 -> 140066907026288
	140066907026480 [label=CloneBackward0]
	140066907028016 -> 140066907026480
	140066907028016 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 2, 64)"]
	140066907027872 -> 140066907028016
	140066907027872 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067054986976 -> 140066907027872
	140067054986976 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140066907026576 -> 140067054986976
	140066907026576 [label="ViewBackward0
------------------------
self_sym_sizes: (6, 768)"]
	140066907025424 -> 140066907026576
	140066907025424 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (6, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140066708602544 -> 140066907025424
	140066853294704 [label="q_encoder.encoder.information_exchanging_layer.3.attention.self.value.bias
 (768)" fillcolor=lightblue]
	140066853294704 -> 140066708602544
	140066708602544 [label=AccumulateGrad]
	140066708590880 -> 140066907025424
	140066708590880 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140066907025472 -> 140066708590880
	140066708598224 -> 140066907025424
	140066708598224 [label=TBackward0]
	140067041700128 -> 140066708598224
	140066853293184 [label="q_encoder.encoder.information_exchanging_layer.3.attention.self.value.weight
 (768, 768)" fillcolor=lightblue]
	140066853293184 -> 140067041700128
	140067041700128 [label=AccumulateGrad]
	140066907024176 -> 140066907026336
	140066907024176 [label=TBackward0]
	140066907024032 -> 140066907024176
	140066853296304 [label="q_encoder.encoder.information_exchanging_layer.3.attention.output.dense.weight
 (768, 768)" fillcolor=lightblue]
	140066853296304 -> 140066907024032
	140066907024032 [label=AccumulateGrad]
	140066907025472 -> 140066907025280
	140066907025040 -> 140066907021344
	140066853290864 [label="q_encoder.encoder.information_exchanging_layer.3.attention.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	140066853290864 -> 140066907025040
	140066907025040 [label=AccumulateGrad]
	140066907025856 -> 140066907021344
	140066853294544 [label="q_encoder.encoder.information_exchanging_layer.3.attention.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	140066853294544 -> 140066907025856
	140066907025856 [label=AccumulateGrad]
	140066907019712 -> 140066907023216
	140066907019712 [label=TBackward0]
	140066907025952 -> 140066907019712
	140066853291584 [label="q_encoder.encoder.information_exchanging_layer.3.intermediate.dense.weight
 (3072, 768)" fillcolor=lightblue]
	140066853291584 -> 140066907025952
	140066907025952 [label=AccumulateGrad]
	140066907023072 -> 140066907022496
	140066907023072 [label=TBackward0]
	140066907022688 -> 140066907023072
	140066853293824 [label="q_encoder.encoder.information_exchanging_layer.3.output.dense.weight
 (768, 3072)" fillcolor=lightblue]
	140066853293824 -> 140066907022688
	140066907022688 [label=AccumulateGrad]
	140066907021344 -> 140066907021632
	140066907022160 -> 140066907021392
	140066853290704 [label="q_encoder.encoder.information_exchanging_layer.3.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	140066853290704 -> 140066907022160
	140066907022160 [label=AccumulateGrad]
	140066907022256 -> 140066907021392
	140066853290624 [label="q_encoder.encoder.information_exchanging_layer.3.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	140066853290624 -> 140066907022256
	140066907022256 [label=AccumulateGrad]
	140066907020720 -> 140066907014720
	140066907020720 [label=CloneBackward0]
	140066907020864 -> 140066907020720
	140066907020864 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 1, 768)
start         :                   0
step          :                   1"]
	140066907021776 -> 140066907020864
	140066907021776 [label="SelectBackward0
------------------------------
dim           :              2
index         :              1
self_sym_sizes: (3, 1, 8, 768)"]
	140066907025520 -> 140066907021776
	140066907025520 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:      (3, 1, 8, 768)
start         :                   0
step          :                   1"]
	140066907022352 -> 140066907025520
	140066907022352 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:      (3, 1, 8, 768)
start         :                   0
step          :                   1"]
	140066907024224 -> 140066907022352
	140066907024224 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 8, 768)"]
	140066907024368 -> 140066907024224
	140066907024368 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140066907025232 -> 140066907024368
	140066907025232 [label="AddBackward0
------------
alpha: 1"]
	140066907026096 -> 140066907025232
	140066907026096 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140066907022016 -> 140066907026096
	140066907022016 [label="ViewBackward0
-------------------------
self_sym_sizes: (24, 768)"]
	140066907029408 -> 140066907022016
	140066907029408 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :     (24, 3072)
mat1_sym_strides:      (3072, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (3072, 768)
mat2_sym_strides:      (1, 3072)"]
	140066708596112 -> 140066907029408
	140066890265888 [label="q_encoder.encoder.text_encoding_layer.4.output.dense.bias
 (768)" fillcolor=lightblue]
	140066890265888 -> 140066708596112
	140066708596112 [label=AccumulateGrad]
	140066907026672 -> 140066907029408
	140066907026672 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 8, 3072)"]
	140066907029072 -> 140066907026672
	140066907029072 [label="GeluBackward0
---------------------------
approximate:           none
self       : [saved tensor]"]
	140067041702096 -> 140066907029072
	140067041702096 [label="ViewBackward0
--------------------------
self_sym_sizes: (24, 3072)"]
	140067041704496 -> 140067041702096
	140067041704496 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (24, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (768, 3072)
mat2_sym_strides:       (1, 768)"]
	140067041705744 -> 140067041704496
	140066890254848 [label="q_encoder.encoder.text_encoding_layer.4.intermediate.dense.bias
 (3072)" fillcolor=lightblue]
	140066890254848 -> 140067041705744
	140067041705744 [label=AccumulateGrad]
	140067041705840 -> 140067041704496
	140067041705840 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 8, 768)"]
	140066907024848 -> 140067041705840
	140066907024848 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140067041592512 -> 140066907024848
	140067041592512 [label="AddBackward0
------------
alpha: 1"]
	140067041593760 -> 140067041592512
	140067041593760 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041591648 -> 140067041593760
	140067041591648 [label="ViewBackward0
-------------------------
self_sym_sizes: (24, 768)"]
	140067041596448 -> 140067041591648
	140067041596448 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (24, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041591984 -> 140067041596448
	140066890254128 [label="q_encoder.encoder.text_encoding_layer.4.attention.output.dense.bias
 (768)" fillcolor=lightblue]
	140066890254128 -> 140067041591984
	140067041591984 [label=AccumulateGrad]
	140067041595344 -> 140067041596448
	140067041595344 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 8, 768)"]
	140067041596736 -> 140067041595344
	140067041596736 [label="ViewBackward0
------------------------------
self_sym_sizes: (3, 8, 12, 64)"]
	140067041596496 -> 140067041596736
	140067041596496 [label=CloneBackward0]
	140067041595920 -> 140067041596496
	140067041595920 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041597072 -> 140067041595920
	140067041597072 [label="UnsafeViewBackward0
---------------------------
self_sym_sizes: (36, 8, 64)"]
	140067041597456 -> 140067041597072
	140067041597456 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140067041597744 -> 140067041597456
	140067041597744 [label="ViewBackward0
-----------------------------
self_sym_sizes: (3, 12, 8, 8)"]
	140067041597936 -> 140067041597744
	140067041597936 [label="ExpandBackward0
-----------------------------
self_sym_sizes: (3, 12, 8, 8)"]
	140067041598128 -> 140067041597936
	140067041598128 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041598368 -> 140067041598128
	140067041598368 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140067041598608 -> 140067041598368
	140067041598608 [label="AddBackward0
------------
alpha: 1"]
	140067041598704 -> 140067041598608
	140067041598704 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041598896 -> 140067041598704
	140067041598896 [label="UnsafeViewBackward0
--------------------------
self_sym_sizes: (36, 8, 8)"]
	140067041599136 -> 140067041598896
	140067041599136 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140067041599184 -> 140067041599136
	140067041599184 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 8, 64)"]
	140067041599328 -> 140067041599184
	140067041599328 [label=CloneBackward0]
	140067041599568 -> 140067041599328
	140067041599568 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 8, 64)"]
	140067041599664 -> 140067041599568
	140067041599664 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041599808 -> 140067041599664
	140067041599808 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 8, 768)"]
	140067041599952 -> 140067041599808
	140067041599952 [label="ViewBackward0
-------------------------
self_sym_sizes: (24, 768)"]
	140067041600048 -> 140067041599952
	140067041600048 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (24, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041600384 -> 140067041600048
	140066890252528 [label="q_encoder.encoder.text_encoding_layer.4.attention.self.query.bias
 (768)" fillcolor=lightblue]
	140066890252528 -> 140067041600384
	140067041600384 [label=AccumulateGrad]
	140067041600096 -> 140067041600048
	140067041600096 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 8, 768)"]
	140067041594480 -> 140067041600096
	140067041594480 [label="ViewBackward0
------------------------------
self_sym_sizes: (3, 1, 8, 768)"]
	140067041600624 -> 140067041594480
	140067041600624 [label="AsStridedBackward0
------------------------------------
size          :       (3, 1, 8, 768)
storage_offset:                    0
stride        : (6144, 6144, 768, 1)"]
	140067041600912 -> 140067041600624
	140067041600912 [label=CopySlices]
	140066708598656 -> 140067041600912
	140067041601104 -> 140067041600912
	140067041601104 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 1, 768)
start         :                   0
step          :                   1"]
	140067041601008 -> 140067041601104
	140067041601008 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:         (3, 2, 768)
start         :                   1
step          :                   1"]
	140067041601632 -> 140067041601008
	140067041601632 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:         (3, 2, 768)
start         :                   0
step          :                   1"]
	140066907021392 -> 140067041601632
	140067041599424 -> 140067041600048
	140067041599424 [label=TBackward0]
	140067041600288 -> 140067041599424
	140066890252448 [label="q_encoder.encoder.text_encoding_layer.4.attention.self.query.weight
 (768, 768)" fillcolor=lightblue]
	140066890252448 -> 140067041600288
	140067041600288 [label=AccumulateGrad]
	140067041598800 -> 140067041599136
	140067041598800 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 64, 8)"]
	140067041598944 -> 140067041598800
	140067041598944 [label=CloneBackward0]
	140067041600000 -> 140067041598944
	140067041600000 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 64, 8)"]
	140067041599856 -> 140067041600000
	140067041599856 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551615
dim1: 18446744073709551614"]
	140067041601056 -> 140067041599856
	140067041601056 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041601824 -> 140067041601056
	140067041601824 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 8, 768)"]
	140067041600192 -> 140067041601824
	140067041600192 [label="ViewBackward0
-------------------------
self_sym_sizes: (24, 768)"]
	140067041600768 -> 140067041600192
	140067041600768 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (24, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041601872 -> 140067041600768
	140066890252688 [label="q_encoder.encoder.text_encoding_layer.4.attention.self.key.bias
 (768)" fillcolor=lightblue]
	140066890252688 -> 140067041601872
	140067041601872 [label=AccumulateGrad]
	140067041601920 -> 140067041600768
	140067041601920 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 8, 768)"]
	140067041594480 -> 140067041601920
	140067041599520 -> 140067041600768
	140067041599520 [label=TBackward0]
	140067041602016 -> 140067041599520
	140066890252608 [label="q_encoder.encoder.text_encoding_layer.4.attention.self.key.weight
 (768, 768)" fillcolor=lightblue]
	140066890252608 -> 140067041602016
	140067041602016 [label=AccumulateGrad]
	140067041597360 -> 140067041597456
	140067041597360 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 8, 64)"]
	140067041597840 -> 140067041597360
	140067041597840 [label=CloneBackward0]
	140067041598416 -> 140067041597840
	140067041598416 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 8, 64)"]
	140067041598512 -> 140067041598416
	140067041598512 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041599712 -> 140067041598512
	140067041599712 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 8, 768)"]
	140067041600864 -> 140067041599712
	140067041600864 [label="ViewBackward0
-------------------------
self_sym_sizes: (24, 768)"]
	140067041601152 -> 140067041600864
	140067041601152 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (24, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041601776 -> 140067041601152
	140066890252848 [label="q_encoder.encoder.text_encoding_layer.4.attention.self.value.bias
 (768)" fillcolor=lightblue]
	140066890252848 -> 140067041601776
	140067041601776 [label=AccumulateGrad]
	140067041600528 -> 140067041601152
	140067041600528 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 8, 768)"]
	140067041594480 -> 140067041600528
	140067041598080 -> 140067041601152
	140067041598080 [label=TBackward0]
	140067041602208 -> 140067041598080
	140066890252768 [label="q_encoder.encoder.text_encoding_layer.4.attention.self.value.weight
 (768, 768)" fillcolor=lightblue]
	140066890252768 -> 140067041602208
	140067041602208 [label=AccumulateGrad]
	140067041595440 -> 140067041596448
	140067041595440 [label=TBackward0]
	140067041596688 -> 140067041595440
	140066890253488 [label="q_encoder.encoder.text_encoding_layer.4.attention.output.dense.weight
 (768, 768)" fillcolor=lightblue]
	140066890253488 -> 140067041596688
	140067041596688 [label=AccumulateGrad]
	140067041594480 -> 140067041592512
	140067041591360 -> 140066907024848
	140066890254608 [label="q_encoder.encoder.text_encoding_layer.4.attention.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	140066890254608 -> 140067041591360
	140067041591360 [label=AccumulateGrad]
	140067041593040 -> 140066907024848
	140066890254368 [label="q_encoder.encoder.text_encoding_layer.4.attention.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	140066890254368 -> 140067041593040
	140067041593040 [label=AccumulateGrad]
	140067041704640 -> 140067041704496
	140067041704640 [label=TBackward0]
	140067041700320 -> 140067041704640
	140066890254768 [label="q_encoder.encoder.text_encoding_layer.4.intermediate.dense.weight
 (3072, 768)" fillcolor=lightblue]
	140066890254768 -> 140067041700320
	140067041700320 [label=AccumulateGrad]
	140066907022928 -> 140066907029408
	140066907022928 [label=TBackward0]
	140067041695088 -> 140066907022928
	140066890254688 [label="q_encoder.encoder.text_encoding_layer.4.output.dense.weight
 (768, 3072)" fillcolor=lightblue]
	140066890254688 -> 140067041695088
	140067041695088 [label=AccumulateGrad]
	140066907024848 -> 140066907025232
	140066907024608 -> 140066907024368
	140066890255088 [label="q_encoder.encoder.text_encoding_layer.4.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	140066890255088 -> 140066907024608
	140066907024608 [label=AccumulateGrad]
	140066907020528 -> 140066907024368
	140066890254928 [label="q_encoder.encoder.text_encoding_layer.4.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	140066890254928 -> 140066907020528
	140066907020528 [label=AccumulateGrad]
	140066907020576 -> 140066907020288
	140066907020576 [label=TBackward0]
	140066907020672 -> 140066907020576
	140066853293504 [label="q_encoder.encoder.information_exchanging_layer.4.attention.self.query.weight
 (768, 768)" fillcolor=lightblue]
	140066853293504 -> 140066907020672
	140066907020672 [label=AccumulateGrad]
	140066907018512 -> 140066907018464
	140066907018512 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 64, 2)"]
	140066907019424 -> 140066907018512
	140066907019424 [label=CloneBackward0]
	140066907019472 -> 140066907019424
	140066907019472 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 64, 2)"]
	140066907019856 -> 140066907019472
	140066907019856 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551615
dim1: 18446744073709551614"]
	140066907022448 -> 140066907019856
	140066907022448 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140066907024080 -> 140066907022448
	140066907024080 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140066907025136 -> 140066907024080
	140066907025136 [label="ViewBackward0
------------------------
self_sym_sizes: (6, 768)"]
	140066907026768 -> 140066907025136
	140066907026768 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (6, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041700704 -> 140066907026768
	140066853291264 [label="q_encoder.encoder.information_exchanging_layer.4.attention.self.key.bias
 (768)" fillcolor=lightblue]
	140066853291264 -> 140067041700704
	140067041700704 [label=AccumulateGrad]
	140067041705888 -> 140066907026768
	140067041705888 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140066907014720 -> 140067041705888
	140067041705792 -> 140066907026768
	140067041705792 [label=TBackward0]
	140066907023744 -> 140067041705792
	140066853290064 [label="q_encoder.encoder.information_exchanging_layer.4.attention.self.key.weight
 (768, 768)" fillcolor=lightblue]
	140066853290064 -> 140066907023744
	140066907023744 [label=AccumulateGrad]
	140066907016496 -> 140066907016688
	140066907016496 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 2, 64)"]
	140066907017360 -> 140066907016496
	140066907017360 [label=CloneBackward0]
	140066907017936 -> 140066907017360
	140066907017936 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 2, 64)"]
	140066907018560 -> 140066907017936
	140066907018560 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140066907019088 -> 140066907018560
	140066907019088 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140066907021056 -> 140066907019088
	140066907021056 [label="ViewBackward0
------------------------
self_sym_sizes: (6, 768)"]
	140066907026240 -> 140066907021056
	140066907026240 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (6, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140066907028256 -> 140066907026240
	140066853291824 [label="q_encoder.encoder.information_exchanging_layer.4.attention.self.value.bias
 (768)" fillcolor=lightblue]
	140066853291824 -> 140066907028256
	140066907028256 [label=AccumulateGrad]
	140066907020480 -> 140066907026240
	140066907020480 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140066907014720 -> 140066907020480
	140066907017024 -> 140066907026240
	140066907017024 [label=TBackward0]
	140066907027440 -> 140066907017024
	140066853291744 [label="q_encoder.encoder.information_exchanging_layer.4.attention.self.value.weight
 (768, 768)" fillcolor=lightblue]
	140066853291744 -> 140066907027440
	140066907027440 [label=AccumulateGrad]
	140066907015488 -> 140066907015728
	140066907015488 [label=TBackward0]
	140066907015824 -> 140066907015488
	140066853291904 [label="q_encoder.encoder.information_exchanging_layer.4.attention.output.dense.weight
 (768, 768)" fillcolor=lightblue]
	140066853291904 -> 140066907015824
	140066907015824 [label=AccumulateGrad]
	140066907014720 -> 140066907015056
	140066907013952 -> 140066487829312
	140066853290144 [label="q_encoder.encoder.information_exchanging_layer.4.attention.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	140066853290144 -> 140066907013952
	140066907013952 [label=AccumulateGrad]
	140066907015632 -> 140066487829312
	140066853293264 [label="q_encoder.encoder.information_exchanging_layer.4.attention.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	140066853293264 -> 140066907015632
	140066907015632 [label=AccumulateGrad]
	140066907013424 -> 140066907013616
	140066907013424 [label=TBackward0]
	140066907014240 -> 140066907013424
	140066853290304 [label="q_encoder.encoder.information_exchanging_layer.4.intermediate.dense.weight
 (3072, 768)" fillcolor=lightblue]
	140066853290304 -> 140066907014240
	140066907014240 [label=AccumulateGrad]
	140066487829648 -> 140066487828640
	140066487829648 [label=TBackward0]
	140066907013184 -> 140066487829648
	140066853290464 [label="q_encoder.encoder.information_exchanging_layer.4.output.dense.weight
 (768, 3072)" fillcolor=lightblue]
	140066853290464 -> 140066907013184
	140066907013184 [label=AccumulateGrad]
	140066487829312 -> 140066487829456
	140066487829984 -> 140066487830272
	140066853291984 [label="q_encoder.encoder.information_exchanging_layer.4.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	140066853291984 -> 140066487829984
	140066487829984 [label=AccumulateGrad]
	140066487830224 -> 140066487830272
	140066853293104 [label="q_encoder.encoder.information_exchanging_layer.4.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	140066853293104 -> 140066487830224
	140066487830224 [label=AccumulateGrad]
	140066907257728 -> 140066907252064
	140066907257728 [label=CloneBackward0]
	140066907258832 -> 140066907257728
	140066907258832 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 1, 768)
start         :                   0
step          :                   1"]
	140066487829360 -> 140066907258832
	140066487829360 [label="SelectBackward0
------------------------------
dim           :              2
index         :              1
self_sym_sizes: (3, 1, 8, 768)"]
	140066487829120 -> 140066487829360
	140066487829120 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:      (3, 1, 8, 768)
start         :                   0
step          :                   1"]
	140066487830080 -> 140066487829120
	140066487830080 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:      (3, 1, 8, 768)
start         :                   0
step          :                   1"]
	140066907016592 -> 140066487830080
	140066907016592 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 8, 768)"]
	140066907016016 -> 140066907016592
	140066907016016 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140066907014768 -> 140066907016016
	140066907014768 [label="AddBackward0
------------
alpha: 1"]
	140066907014288 -> 140066907014768
	140066907014288 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140066907013808 -> 140066907014288
	140066907013808 [label="ViewBackward0
-------------------------
self_sym_sizes: (24, 768)"]
	140066907021824 -> 140066907013808
	140066907021824 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :     (24, 3072)
mat1_sym_strides:      (3072, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (3072, 768)
mat2_sym_strides:      (1, 3072)"]
	140066907014672 -> 140066907021824
	140066890264928 [label="q_encoder.encoder.text_encoding_layer.5.output.dense.bias
 (768)" fillcolor=lightblue]
	140066890264928 -> 140066907014672
	140066907014672 [label=AccumulateGrad]
	140066907018704 -> 140066907021824
	140066907018704 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 8, 3072)"]
	140066907014336 -> 140066907018704
	140066907014336 [label="GeluBackward0
---------------------------
approximate:           none
self       : [saved tensor]"]
	140066907025088 -> 140066907014336
	140066907025088 [label="ViewBackward0
--------------------------
self_sym_sizes: (24, 3072)"]
	140067041595728 -> 140066907025088
	140067041595728 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (24, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (768, 3072)
mat2_sym_strides:       (1, 768)"]
	140067041597168 -> 140067041595728
	140066890264288 [label="q_encoder.encoder.text_encoding_layer.5.intermediate.dense.bias
 (3072)" fillcolor=lightblue]
	140066890264288 -> 140067041597168
	140067041597168 [label=AccumulateGrad]
	140067041596976 -> 140067041595728
	140067041596976 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 8, 768)"]
	140066907016352 -> 140067041596976
	140066907016352 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140067041598272 -> 140066907016352
	140067041598272 [label="AddBackward0
------------
alpha: 1"]
	140067041599760 -> 140067041598272
	140067041599760 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041597984 -> 140067041599760
	140067041597984 [label="ViewBackward0
-------------------------
self_sym_sizes: (24, 768)"]
	140067041602064 -> 140067041597984
	140067041602064 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (24, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041595536 -> 140067041602064
	140066890256128 [label="q_encoder.encoder.text_encoding_layer.5.attention.output.dense.bias
 (768)" fillcolor=lightblue]
	140066890256128 -> 140067041595536
	140067041595536 [label=AccumulateGrad]
	140067041601248 -> 140067041602064
	140067041601248 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 8, 768)"]
	140067041602400 -> 140067041601248
	140067041602400 [label="ViewBackward0
------------------------------
self_sym_sizes: (3, 8, 12, 64)"]
	140067041602304 -> 140067041602400
	140067041602304 [label=CloneBackward0]
	140067041601584 -> 140067041602304
	140067041601584 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041602736 -> 140067041601584
	140067041602736 [label="UnsafeViewBackward0
---------------------------
self_sym_sizes: (36, 8, 64)"]
	140067041603024 -> 140067041602736
	140067041603024 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140067041603120 -> 140067041603024
	140067041603120 [label="ViewBackward0
-----------------------------
self_sym_sizes: (3, 12, 8, 8)"]
	140067041603312 -> 140067041603120
	140067041603312 [label="ExpandBackward0
-----------------------------
self_sym_sizes: (3, 12, 8, 8)"]
	140067041603504 -> 140067041603312
	140067041603504 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041603648 -> 140067041603504
	140067041603648 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140067041603984 -> 140067041603648
	140067041603984 [label="AddBackward0
------------
alpha: 1"]
	140067041604128 -> 140067041603984
	140067041604128 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041604224 -> 140067041604128
	140067041604224 [label="UnsafeViewBackward0
--------------------------
self_sym_sizes: (36, 8, 8)"]
	140067041604416 -> 140067041604224
	140067041604416 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140067041604464 -> 140067041604416
	140067041604464 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 8, 64)"]
	140067041604560 -> 140067041604464
	140067041604560 [label=CloneBackward0]
	140067041604896 -> 140067041604560
	140067041604896 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 8, 64)"]
	140067041605040 -> 140067041604896
	140067041605040 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041605280 -> 140067041605040
	140067041605280 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 8, 768)"]
	140067041605568 -> 140067041605280
	140067041605568 [label="ViewBackward0
-------------------------
self_sym_sizes: (24, 768)"]
	140067041605856 -> 140067041605568
	140067041605856 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (24, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041606192 -> 140067041605856
	140066890265808 [label="q_encoder.encoder.text_encoding_layer.5.attention.self.query.bias
 (768)" fillcolor=lightblue]
	140066890265808 -> 140067041606192
	140067041606192 [label=AccumulateGrad]
	140067041605952 -> 140067041605856
	140067041605952 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 8, 768)"]
	140067041600432 -> 140067041605952
	140067041600432 [label="ViewBackward0
------------------------------
self_sym_sizes: (3, 1, 8, 768)"]
	140067041606336 -> 140067041600432
	140067041606336 [label="AsStridedBackward0
------------------------------------
size          :       (3, 1, 8, 768)
storage_offset:                    0
stride        : (6144, 6144, 768, 1)"]
	140067041606672 -> 140067041606336
	140067041606672 [label=CopySlices]
	140066907024368 -> 140067041606672
	140067041607152 -> 140067041606672
	140067041607152 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 1, 768)
start         :                   0
step          :                   1"]
	140067041606816 -> 140067041607152
	140067041606816 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:         (3, 2, 768)
start         :                   1
step          :                   1"]
	140067041607632 -> 140067041606816
	140067041607632 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:         (3, 2, 768)
start         :                   0
step          :                   1"]
	140066487830272 -> 140067041607632
	140067041604704 -> 140067041605856
	140067041604704 [label=TBackward0]
	140067041606240 -> 140067041604704
	140066890255168 [label="q_encoder.encoder.text_encoding_layer.5.attention.self.query.weight
 (768, 768)" fillcolor=lightblue]
	140066890255168 -> 140067041606240
	140067041606240 [label=AccumulateGrad]
	140067041604032 -> 140067041604416
	140067041604032 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 64, 8)"]
	140067041604320 -> 140067041604032
	140067041604320 [label=CloneBackward0]
	140067041605712 -> 140067041604320
	140067041605712 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 64, 8)"]
	140067041605424 -> 140067041605712
	140067041605424 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551615
dim1: 18446744073709551614"]
	140067041607008 -> 140067041605424
	140067041607008 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041606096 -> 140067041607008
	140067041606096 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 8, 768)"]
	140067041607248 -> 140067041606096
	140067041607248 [label="ViewBackward0
-------------------------
self_sym_sizes: (24, 768)"]
	140067041606576 -> 140067041607248
	140067041606576 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (24, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041607344 -> 140067041606576
	140066890255408 [label="q_encoder.encoder.text_encoding_layer.5.attention.self.key.bias
 (768)" fillcolor=lightblue]
	140066890255408 -> 140067041607344
	140067041607344 [label=AccumulateGrad]
	140067041607536 -> 140067041606576
	140067041607536 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 8, 768)"]
	140067041600432 -> 140067041607536
	140067041604800 -> 140067041606576
	140067041604800 [label=TBackward0]
	140067041673520 -> 140067041604800
	140066890255328 [label="q_encoder.encoder.text_encoding_layer.5.attention.self.key.weight
 (768, 768)" fillcolor=lightblue]
	140066890255328 -> 140067041673520
	140067041673520 [label=AccumulateGrad]
	140067041602928 -> 140067041603024
	140067041602928 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 8, 64)"]
	140067041603216 -> 140067041602928
	140067041603216 [label=CloneBackward0]
	140067041603744 -> 140067041603216
	140067041603744 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 8, 64)"]
	140067041603792 -> 140067041603744
	140067041603792 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041605136 -> 140067041603792
	140067041605136 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 8, 768)"]
	140067041606624 -> 140067041605136
	140067041606624 [label="ViewBackward0
-------------------------
self_sym_sizes: (24, 768)"]
	140067041606864 -> 140067041606624
	140067041606864 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (24, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041607392 -> 140067041606864
	140066890255568 [label="q_encoder.encoder.text_encoding_layer.5.attention.self.value.bias
 (768)" fillcolor=lightblue]
	140066890255568 -> 140067041607392
	140067041607392 [label=AccumulateGrad]
	140067041606528 -> 140067041606864
	140067041606528 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 8, 768)"]
	140067041600432 -> 140067041606528
	140067041603456 -> 140067041606864
	140067041603456 [label=TBackward0]
	140067041673952 -> 140067041603456
	140066890255488 [label="q_encoder.encoder.text_encoding_layer.5.attention.self.value.weight
 (768, 768)" fillcolor=lightblue]
	140066890255488 -> 140067041673952
	140067041673952 [label=AccumulateGrad]
	140067041601392 -> 140067041602064
	140067041601392 [label=TBackward0]
	140067041602352 -> 140067041601392
	140066890255648 [label="q_encoder.encoder.text_encoding_layer.5.attention.output.dense.weight
 (768, 768)" fillcolor=lightblue]
	140066890255648 -> 140067041602352
	140067041602352 [label=AccumulateGrad]
	140067041600432 -> 140067041598272
	140067041596880 -> 140066907016352
	140066890256928 [label="q_encoder.encoder.text_encoding_layer.5.attention.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	140066890256928 -> 140067041596880
	140067041596880 [label=AccumulateGrad]
	140067041598560 -> 140066907016352
	140066890256048 [label="q_encoder.encoder.text_encoding_layer.5.attention.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	140066890256048 -> 140067041598560
	140067041598560 [label=AccumulateGrad]
	140067041595632 -> 140067041595728
	140067041595632 [label=TBackward0]
	140067041601296 -> 140067041595632
	140066890257968 [label="q_encoder.encoder.text_encoding_layer.5.intermediate.dense.weight
 (3072, 768)" fillcolor=lightblue]
	140066890257968 -> 140067041601296
	140067041601296 [label=AccumulateGrad]
	140066907017888 -> 140066907021824
	140066907017888 [label=TBackward0]
	140066907018368 -> 140066907017888
	140066890257568 [label="q_encoder.encoder.text_encoding_layer.5.output.dense.weight
 (768, 3072)" fillcolor=lightblue]
	140066890257568 -> 140066907018368
	140066907018368 [label=AccumulateGrad]
	140066907016352 -> 140066907014768
	140066907016544 -> 140066907016016
	140066890264448 [label="q_encoder.encoder.text_encoding_layer.5.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	140066890264448 -> 140066907016544
	140066907016544 [label=AccumulateGrad]
	140066907013760 -> 140066907016016
	140066890264208 [label="q_encoder.encoder.text_encoding_layer.5.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	140066890264208 -> 140066907013760
	140066907013760 [label=AccumulateGrad]
	140066907258160 -> 140066907258784
	140066907258160 [label=TBackward0]
	140066907258352 -> 140066907258160
	140066853291664 [label="q_encoder.encoder.information_exchanging_layer.5.attention.self.query.weight
 (768, 768)" fillcolor=lightblue]
	140066853291664 -> 140066907258352
	140066907258352 [label=AccumulateGrad]
	140066907255904 -> 140066907256240
	140066907255904 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 64, 2)"]
	140066907257200 -> 140066907255904
	140066907257200 [label=CloneBackward0]
	140066907258112 -> 140066907257200
	140066907258112 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 64, 2)"]
	140066907258208 -> 140066907258112
	140066907258208 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551615
dim1: 18446744073709551614"]
	140066907255040 -> 140066907258208
	140066907255040 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140066487828976 -> 140066907255040
	140066487828976 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140066487828688 -> 140066487828976
	140066487828688 [label="ViewBackward0
------------------------
self_sym_sizes: (6, 768)"]
	140066907017264 -> 140066487828688
	140066907017264 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (6, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140066907014816 -> 140066907017264
	140066904411264 [label="q_encoder.encoder.information_exchanging_layer.5.attention.self.key.bias
 (768)" fillcolor=lightblue]
	140066904411264 -> 140066907014816
	140066907014816 [label=AccumulateGrad]
	140066907018800 -> 140066907017264
	140066907018800 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140066907252064 -> 140066907018800
	140066907015104 -> 140066907017264
	140066907015104 [label=TBackward0]
	140066907016928 -> 140066907015104
	140066853293344 [label="q_encoder.encoder.information_exchanging_layer.5.attention.self.key.weight
 (768, 768)" fillcolor=lightblue]
	140066853293344 -> 140066907016928
	140066907016928 [label=AccumulateGrad]
	140066907253072 -> 140066907254656
	140066907253072 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 2, 64)"]
	140066907254368 -> 140066907253072
	140066907254368 [label=CloneBackward0]
	140066907256144 -> 140066907254368
	140066907256144 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 2, 64)"]
	140066907256000 -> 140066907256144
	140066907256000 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140066907256576 -> 140066907256000
	140066907256576 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140066907257872 -> 140066907256576
	140066907257872 [label="ViewBackward0
------------------------
self_sym_sizes: (6, 768)"]
	140066907249472 -> 140066907257872
	140066907249472 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (6, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140066487830176 -> 140066907249472
	140066904411424 [label="q_encoder.encoder.information_exchanging_layer.5.attention.self.value.bias
 (768)" fillcolor=lightblue]
	140066904411424 -> 140066487830176
	140066487830176 [label=AccumulateGrad]
	140066487829840 -> 140066907249472
	140066487829840 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140066907252064 -> 140066487829840
	140066907022784 -> 140066907249472
	140066907022784 [label=TBackward0]
	140067041596784 -> 140066907022784
	140066904411344 [label="q_encoder.encoder.information_exchanging_layer.5.attention.self.value.weight
 (768, 768)" fillcolor=lightblue]
	140066904411344 -> 140067041596784
	140067041596784 [label=AccumulateGrad]
	140066907253696 -> 140066907250288
	140066907253696 [label=TBackward0]
	140066907252448 -> 140066907253696
	140066904411504 [label="q_encoder.encoder.information_exchanging_layer.5.attention.output.dense.weight
 (768, 768)" fillcolor=lightblue]
	140066904411504 -> 140066907252448
	140066907252448 [label=AccumulateGrad]
	140066907252064 -> 140066907252112
	140066907251680 -> 140066907250432
	140066904411744 [label="q_encoder.encoder.information_exchanging_layer.5.attention.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	140066904411744 -> 140066907251680
	140066907251680 [label=AccumulateGrad]
	140066907251296 -> 140066907250432
	140066904411664 [label="q_encoder.encoder.information_exchanging_layer.5.attention.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	140066904411664 -> 140066907251296
	140066907251296 [label=AccumulateGrad]
	140066907251008 -> 140066907250960
	140066907251008 [label=TBackward0]
	140066907251968 -> 140066907251008
	140066904411904 [label="q_encoder.encoder.information_exchanging_layer.5.intermediate.dense.weight
 (3072, 768)" fillcolor=lightblue]
	140066904411904 -> 140066907251968
	140066907251968 [label=AccumulateGrad]
	140066907251440 -> 140066907249328
	140066907251440 [label=TBackward0]
	140066907251152 -> 140066907251440
	140066904412064 [label="q_encoder.encoder.information_exchanging_layer.5.output.dense.weight
 (768, 3072)" fillcolor=lightblue]
	140066904412064 -> 140066907251152
	140066907251152 [label=AccumulateGrad]
	140066907250432 -> 140066907248464
	140066907249904 -> 140066907248704
	140066904412304 [label="q_encoder.encoder.information_exchanging_layer.5.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	140066904412304 -> 140066907249904
	140066907249904 [label=AccumulateGrad]
	140066907250912 -> 140066907248704
	140066904412224 [label="q_encoder.encoder.information_exchanging_layer.5.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	140066904412224 -> 140066907250912
	140066907250912 [label=AccumulateGrad]
	140066907248560 -> 140066907256960
	140066907248560 [label=CloneBackward0]
	140066907248608 -> 140066907248560
	140066907248608 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 1, 768)
start         :                   0
step          :                   1"]
	140066907248848 -> 140066907248608
	140066907248848 [label="SelectBackward0
------------------------------
dim           :              2
index         :              1
self_sym_sizes: (3, 1, 8, 768)"]
	140066907251920 -> 140066907248848
	140066907251920 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:      (3, 1, 8, 768)
start         :                   0
step          :                   1"]
	140066907250624 -> 140066907251920
	140066907250624 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:      (3, 1, 8, 768)
start         :                   0
step          :                   1"]
	140066907252880 -> 140066907250624
	140066907252880 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 8, 768)"]
	140066907252736 -> 140066907252880
	140066907252736 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140066907251200 -> 140066907252736
	140066907251200 [label="AddBackward0
------------
alpha: 1"]
	140066907253168 -> 140066907251200
	140066907253168 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140066907250768 -> 140066907253168
	140066907250768 [label="ViewBackward0
-------------------------
self_sym_sizes: (24, 768)"]
	140066907258400 -> 140066907250768
	140066907258400 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :     (24, 3072)
mat1_sym_strides:      (3072, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (3072, 768)
mat2_sym_strides:      (1, 3072)"]
	140066907013520 -> 140066907258400
	140066890255008 [label="q_encoder.encoder.text_encoding_layer.6.output.dense.bias
 (768)" fillcolor=lightblue]
	140066890255008 -> 140066907013520
	140066907013520 [label=AccumulateGrad]
	140066907255616 -> 140066907258400
	140066907255616 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 8, 3072)"]
	140066907257344 -> 140066907255616
	140066907257344 [label="GeluBackward0
---------------------------
approximate:           none
self       : [saved tensor]"]
	140067041599040 -> 140066907257344
	140067041599040 [label="ViewBackward0
--------------------------
self_sym_sizes: (24, 3072)"]
	140067041601440 -> 140067041599040
	140067041601440 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (24, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (768, 3072)
mat2_sym_strides:       (1, 768)"]
	140067041602832 -> 140067041601440
	140066890253008 [label="q_encoder.encoder.text_encoding_layer.6.intermediate.dense.bias
 (3072)" fillcolor=lightblue]
	140066890253008 -> 140067041602832
	140067041602832 [label=AccumulateGrad]
	140067041602640 -> 140067041601440
	140067041602640 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 8, 768)"]
	140066907252208 -> 140067041602640
	140066907252208 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140067041603600 -> 140066907252208
	140067041603600 [label="AddBackward0
------------
alpha: 1"]
	140067041596928 -> 140067041603600
	140067041596928 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041603072 -> 140067041596928
	140067041603072 [label="ViewBackward0
-------------------------
self_sym_sizes: (24, 768)"]
	140067041674000 -> 140067041603072
	140067041674000 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (24, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041673328 -> 140067041674000
	140066890253808 [label="q_encoder.encoder.text_encoding_layer.6.attention.output.dense.bias
 (768)" fillcolor=lightblue]
	140066890253808 -> 140067041673328
	140067041673328 [label=AccumulateGrad]
	140067041673472 -> 140067041674000
	140067041673472 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 8, 768)"]
	140067041674288 -> 140067041673472
	140067041674288 [label="ViewBackward0
------------------------------
self_sym_sizes: (3, 8, 12, 64)"]
	140067041674144 -> 140067041674288
	140067041674144 [label=CloneBackward0]
	140067041673856 -> 140067041674144
	140067041673856 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041674672 -> 140067041673856
	140067041674672 [label="UnsafeViewBackward0
---------------------------
self_sym_sizes: (36, 8, 64)"]
	140067041674912 -> 140067041674672
	140067041674912 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140067041675008 -> 140067041674912
	140067041675008 [label="ViewBackward0
-----------------------------
self_sym_sizes: (3, 12, 8, 8)"]
	140067041675200 -> 140067041675008
	140067041675200 [label="ExpandBackward0
-----------------------------
self_sym_sizes: (3, 12, 8, 8)"]
	140067041675536 -> 140067041675200
	140067041675536 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041675680 -> 140067041675536
	140067041675680 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140067041675968 -> 140067041675680
	140067041675968 [label="AddBackward0
------------
alpha: 1"]
	140067041676064 -> 140067041675968
	140067041676064 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041676160 -> 140067041676064
	140067041676160 [label="UnsafeViewBackward0
--------------------------
self_sym_sizes: (36, 8, 8)"]
	140067041676400 -> 140067041676160
	140067041676400 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140067041676448 -> 140067041676400
	140067041676448 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 8, 64)"]
	140067041676496 -> 140067041676448
	140067041676496 [label=CloneBackward0]
	140067041676640 -> 140067041676496
	140067041676640 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 8, 64)"]
	140067041676688 -> 140067041676640
	140067041676688 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041676832 -> 140067041676688
	140067041676832 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 8, 768)"]
	140067041676976 -> 140067041676832
	140067041676976 [label="ViewBackward0
-------------------------
self_sym_sizes: (24, 768)"]
	140067041677120 -> 140067041676976
	140067041677120 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (24, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041677264 -> 140067041677120
	140066890264608 [label="q_encoder.encoder.text_encoding_layer.6.attention.self.query.bias
 (768)" fillcolor=lightblue]
	140066890264608 -> 140067041677264
	140067041677264 [label=AccumulateGrad]
	140067041677168 -> 140067041677120
	140067041677168 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 8, 768)"]
	140067041603360 -> 140067041677168
	140067041603360 [label="ViewBackward0
------------------------------
self_sym_sizes: (3, 1, 8, 768)"]
	140067041677600 -> 140067041603360
	140067041677600 [label="AsStridedBackward0
------------------------------------
size          :       (3, 1, 8, 768)
storage_offset:                    0
stride        : (6144, 6144, 768, 1)"]
	140067041677840 -> 140067041677600
	140067041677840 [label=CopySlices]
	140066907016016 -> 140067041677840
	140067041678176 -> 140067041677840
	140067041678176 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 1, 768)
start         :                   0
step          :                   1"]
	140067041677984 -> 140067041678176
	140067041677984 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:         (3, 2, 768)
start         :                   1
step          :                   1"]
	140067041678848 -> 140067041677984
	140067041678848 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:         (3, 2, 768)
start         :                   0
step          :                   1"]
	140066907248704 -> 140067041678848
	140067041676544 -> 140067041677120
	140067041676544 [label=TBackward0]
	140067041677456 -> 140067041676544
	140066890264528 [label="q_encoder.encoder.text_encoding_layer.6.attention.self.query.weight
 (768, 768)" fillcolor=lightblue]
	140066890264528 -> 140067041677456
	140067041677456 [label=AccumulateGrad]
	140067041676112 -> 140067041676400
	140067041676112 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 64, 8)"]
	140067041676256 -> 140067041676112
	140067041676256 [label=CloneBackward0]
	140067041677024 -> 140067041676256
	140067041677024 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 64, 8)"]
	140067041676928 -> 140067041677024
	140067041676928 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551615
dim1: 18446744073709551614"]
	140067041678128 -> 140067041676928
	140067041678128 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041679040 -> 140067041678128
	140067041679040 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 8, 768)"]
	140067041677216 -> 140067041679040
	140067041677216 [label="ViewBackward0
-------------------------
self_sym_sizes: (24, 768)"]
	140067041677696 -> 140067041677216
	140067041677696 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (24, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041679088 -> 140067041677696
	140066890254448 [label="q_encoder.encoder.text_encoding_layer.6.attention.self.key.bias
 (768)" fillcolor=lightblue]
	140066890254448 -> 140067041679088
	140067041679088 [label=AccumulateGrad]
	140067041679136 -> 140067041677696
	140067041679136 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 8, 768)"]
	140067041603360 -> 140067041679136
	140067041676592 -> 140067041677696
	140067041676592 [label=TBackward0]
	140067041679232 -> 140067041676592
	140066890264688 [label="q_encoder.encoder.text_encoding_layer.6.attention.self.key.weight
 (768, 768)" fillcolor=lightblue]
	140066890264688 -> 140067041679232
	140067041679232 [label=AccumulateGrad]
	140067041674816 -> 140067041674912
	140067041674816 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 8, 64)"]
	140067041674960 -> 140067041674816
	140067041674960 [label=CloneBackward0]
	140067041675728 -> 140067041674960
	140067041675728 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 8, 64)"]
	140067041675824 -> 140067041675728
	140067041675824 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041676736 -> 140067041675824
	140067041676736 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 8, 768)"]
	140067041677792 -> 140067041676736
	140067041677792 [label="ViewBackward0
-------------------------
self_sym_sizes: (24, 768)"]
	140067041678320 -> 140067041677792
	140067041678320 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (24, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041678896 -> 140067041678320
	140066890253648 [label="q_encoder.encoder.text_encoding_layer.6.attention.self.value.bias
 (768)" fillcolor=lightblue]
	140066890253648 -> 140067041678896
	140067041678896 [label=AccumulateGrad]
	140067041677552 -> 140067041678320
	140067041677552 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 8, 768)"]
	140067041603360 -> 140067041677552
	140067041675392 -> 140067041678320
	140067041675392 [label=TBackward0]
	140067041679328 -> 140067041675392
	140066890254528 [label="q_encoder.encoder.text_encoding_layer.6.attention.self.value.weight
 (768, 768)" fillcolor=lightblue]
	140066890254528 -> 140067041679328
	140067041679328 [label=AccumulateGrad]
	140067041673760 -> 140067041674000
	140067041673760 [label=TBackward0]
	140067041674240 -> 140067041673760
	140066890253728 [label="q_encoder.encoder.text_encoding_layer.6.attention.output.dense.weight
 (768, 768)" fillcolor=lightblue]
	140066890253728 -> 140067041674240
	140067041674240 [label=AccumulateGrad]
	140067041603360 -> 140067041603600
	140067041602544 -> 140066907252208
	140066890253968 [label="q_encoder.encoder.text_encoding_layer.6.attention.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	140066890253968 -> 140067041602544
	140067041602544 [label=AccumulateGrad]
	140067041603888 -> 140066907252208
	140066890253888 [label="q_encoder.encoder.text_encoding_layer.6.attention.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	140066890253888 -> 140067041603888
	140067041603888 [label=AccumulateGrad]
	140067041601488 -> 140067041601440
	140067041601488 [label=TBackward0]
	140067041605232 -> 140067041601488
	140066890254288 [label="q_encoder.encoder.text_encoding_layer.6.intermediate.dense.weight
 (3072, 768)" fillcolor=lightblue]
	140066890254288 -> 140067041605232
	140067041605232 [label=AccumulateGrad]
	140066907250480 -> 140066907258400
	140066907250480 [label=TBackward0]
	140067041592128 -> 140066907250480
	140066890254208 [label="q_encoder.encoder.text_encoding_layer.6.output.dense.weight
 (768, 3072)" fillcolor=lightblue]
	140066890254208 -> 140067041592128
	140067041592128 [label=AccumulateGrad]
	140066907252208 -> 140066907251200
	140066907253264 -> 140066907252736
	140066890253248 [label="q_encoder.encoder.text_encoding_layer.6.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	140066890253248 -> 140066907253264
	140066907253264 [label=AccumulateGrad]
	140066907247840 -> 140066907252736
	140066890253088 [label="q_encoder.encoder.text_encoding_layer.6.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	140066890253088 -> 140066907247840
	140066907247840 [label=AccumulateGrad]
	140066907247936 -> 140066907247312
	140066907247936 [label=TBackward0]
	140066907249184 -> 140066907247936
	140066904411824 [label="q_encoder.encoder.information_exchanging_layer.6.attention.self.query.weight
 (768, 768)" fillcolor=lightblue]
	140066904411824 -> 140066907249184
	140066907249184 [label=AccumulateGrad]
	140066907245248 -> 140066907246304
	140066907245248 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 64, 2)"]
	140066907246496 -> 140066907245248
	140066907246496 [label=CloneBackward0]
	140066907247408 -> 140066907246496
	140066907247408 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 64, 2)"]
	140066907246784 -> 140066907247408
	140066907246784 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551615
dim1: 18446744073709551614"]
	140066907249232 -> 140066907246784
	140066907249232 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140066907252592 -> 140066907249232
	140066907252592 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140066907251776 -> 140066907252592
	140066907251776 [label="ViewBackward0
------------------------
self_sym_sizes: (6, 768)"]
	140066907255328 -> 140066907251776
	140066907255328 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (6, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041606288 -> 140066907255328
	140066904412544 [label="q_encoder.encoder.information_exchanging_layer.6.attention.self.key.bias
 (768)" fillcolor=lightblue]
	140066904412544 -> 140067041606288
	140067041606288 [label=AccumulateGrad]
	140067041602448 -> 140066907255328
	140067041602448 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140066907256960 -> 140067041602448
	140067041597600 -> 140066907255328
	140067041597600 [label=TBackward0]
	140067041602592 -> 140067041597600
	140066904412464 [label="q_encoder.encoder.information_exchanging_layer.6.attention.self.key.weight
 (768, 768)" fillcolor=lightblue]
	140066904412464 -> 140067041602592
	140067041602592 [label=AccumulateGrad]
	140066907244864 -> 140066907243136
	140066907244864 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 2, 64)"]
	140067041604368 -> 140066907244864
	140067041604368 [label=CloneBackward0]
	140066907244384 -> 140067041604368
	140066907244384 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 2, 64)"]
	140066907245344 -> 140066907244384
	140066907245344 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140066907246640 -> 140066907245344
	140066907246640 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140066907246832 -> 140066907246640
	140066907246832 [label="ViewBackward0
------------------------
self_sym_sizes: (6, 768)"]
	140066907248896 -> 140066907246832
	140066907248896 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (6, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140066907250240 -> 140066907248896
	140066904412704 [label="q_encoder.encoder.information_exchanging_layer.6.attention.self.value.bias
 (768)" fillcolor=lightblue]
	140066904412704 -> 140066907250240
	140066907250240 [label=AccumulateGrad]
	140066907252496 -> 140066907248896
	140066907252496 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140066907256960 -> 140066907252496
	140066907242800 -> 140066907248896
	140066907242800 [label=TBackward0]
	140066907246352 -> 140066907242800
	140066904412624 [label="q_encoder.encoder.information_exchanging_layer.6.attention.self.value.weight
 (768, 768)" fillcolor=lightblue]
	140066904412624 -> 140066907246352
	140066907246352 [label=AccumulateGrad]
	140066907242704 -> 140066907249712
	140066907242704 [label=TBackward0]
	140066907242896 -> 140066907242704
	140066904412784 [label="q_encoder.encoder.information_exchanging_layer.6.attention.output.dense.weight
 (768, 768)" fillcolor=lightblue]
	140066904412784 -> 140066907242896
	140066907242896 [label=AccumulateGrad]
	140066907256960 -> 140066907246112
	140066907258016 -> 140066879632432
	140066904413024 [label="q_encoder.encoder.information_exchanging_layer.6.attention.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	140066904413024 -> 140066907258016
	140066907258016 [label=AccumulateGrad]
	140066907252784 -> 140066879632432
	140066904412944 [label="q_encoder.encoder.information_exchanging_layer.6.attention.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	140066904412944 -> 140066907252784
	140066907252784 [label=AccumulateGrad]
	140066879634784 -> 140066879628976
	140066879634784 [label=TBackward0]
	140066879634640 -> 140066879634784
	140066904413184 [label="q_encoder.encoder.information_exchanging_layer.6.intermediate.dense.weight
 (3072, 768)" fillcolor=lightblue]
	140066904413184 -> 140066879634640
	140066879634640 [label=AccumulateGrad]
	140066879634304 -> 140066879634160
	140066879634304 [label=TBackward0]
	140066879634400 -> 140066879634304
	140066904413344 [label="q_encoder.encoder.information_exchanging_layer.6.output.dense.weight
 (768, 3072)" fillcolor=lightblue]
	140066904413344 -> 140066879634400
	140066879634400 [label=AccumulateGrad]
	140066879632432 -> 140066879633296
	140066879633200 -> 140066879628352
	140066904413584 [label="q_encoder.encoder.information_exchanging_layer.6.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	140066904413584 -> 140066879633200
	140066879633200 [label=AccumulateGrad]
	140066879633248 -> 140066879628352
	140066904413504 [label="q_encoder.encoder.information_exchanging_layer.6.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	140066904413504 -> 140066879633248
	140066879633248 [label=AccumulateGrad]
	140066879631280 -> 140066879625904
	140066879631280 [label=CloneBackward0]
	140066879630848 -> 140066879631280
	140066879630848 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 1, 768)
start         :                   0
step          :                   1"]
	140066879633536 -> 140066879630848
	140066879633536 [label="SelectBackward0
------------------------------
dim           :              2
index         :              1
self_sym_sizes: (3, 1, 8, 768)"]
	140066879633488 -> 140066879633536
	140066879633488 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:      (3, 1, 8, 768)
start         :                   0
step          :                   1"]
	140066879624032 -> 140066879633488
	140066879624032 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:      (3, 1, 8, 768)
start         :                   0
step          :                   1"]
	140066879634592 -> 140066879624032
	140066879634592 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 8, 768)"]
	140066879630800 -> 140066879634592
	140066879630800 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140066907249664 -> 140066879630800
	140066907249664 [label="AddBackward0
------------
alpha: 1"]
	140066907243232 -> 140066907249664
	140066907243232 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140066907249568 -> 140066907243232
	140066907249568 [label="ViewBackward0
-------------------------
self_sym_sizes: (24, 768)"]
	140066907247888 -> 140066907249568
	140066907247888 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :     (24, 3072)
mat1_sym_strides:      (3072, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (3072, 768)
mat2_sym_strides:      (1, 3072)"]
	140066907243520 -> 140066907247888
	140066890264368 [label="q_encoder.encoder.text_encoding_layer.7.output.dense.bias
 (768)" fillcolor=lightblue]
	140066890264368 -> 140066907243520
	140066907243520 [label=AccumulateGrad]
	140066907245824 -> 140066907247888
	140066907245824 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 8, 3072)"]
	140066907256864 -> 140066907245824
	140066907256864 [label="GeluBackward0
---------------------------
approximate:           none
self       : [saved tensor]"]
	140066907246064 -> 140066907256864
	140066907246064 [label="ViewBackward0
--------------------------
self_sym_sizes: (24, 3072)"]
	140066907248224 -> 140066907246064
	140066907248224 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (24, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (768, 3072)
mat2_sym_strides:       (1, 768)"]
	140067041674720 -> 140066907248224
	140067041802384 [label="q_encoder.encoder.text_encoding_layer.7.intermediate.dense.bias
 (3072)" fillcolor=lightblue]
	140067041802384 -> 140067041674720
	140067041674720 [label=AccumulateGrad]
	140067041674576 -> 140066907248224
	140067041674576 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 8, 768)"]
	140066907244288 -> 140067041674576
	140066907244288 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140067041675584 -> 140066907244288
	140067041675584 [label="AddBackward0
------------
alpha: 1"]
	140067041676784 -> 140067041675584
	140067041676784 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041675344 -> 140067041676784
	140067041675344 [label="ViewBackward0
-------------------------
self_sym_sizes: (24, 768)"]
	140067041679424 -> 140067041675344
	140067041679424 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (24, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041673376 -> 140067041679424
	140067041878064 [label="q_encoder.encoder.text_encoding_layer.7.attention.output.dense.bias
 (768)" fillcolor=lightblue]
	140067041878064 -> 140067041673376
	140067041673376 [label=AccumulateGrad]
	140067041678224 -> 140067041679424
	140067041678224 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 8, 768)"]
	140067041679664 -> 140067041678224
	140067041679664 [label="ViewBackward0
------------------------------
self_sym_sizes: (3, 8, 12, 64)"]
	140067041679472 -> 140067041679664
	140067041679472 [label=CloneBackward0]
	140067041678704 -> 140067041679472
	140067041678704 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041679952 -> 140067041678704
	140067041679952 [label="UnsafeViewBackward0
---------------------------
self_sym_sizes: (36, 8, 64)"]
	140067041680096 -> 140067041679952
	140067041680096 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140067041680240 -> 140067041680096
	140067041680240 [label="ViewBackward0
-----------------------------
self_sym_sizes: (3, 12, 8, 8)"]
	140067041680336 -> 140067041680240
	140067041680336 [label="ExpandBackward0
-----------------------------
self_sym_sizes: (3, 12, 8, 8)"]
	140067041680672 -> 140067041680336
	140067041680672 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041680768 -> 140067041680672
	140067041680768 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140067041681056 -> 140067041680768
	140067041681056 [label="AddBackward0
------------
alpha: 1"]
	140067041681104 -> 140067041681056
	140067041681104 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041681296 -> 140067041681104
	140067041681296 [label="UnsafeViewBackward0
--------------------------
self_sym_sizes: (36, 8, 8)"]
	140067041681536 -> 140067041681296
	140067041681536 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140067041681584 -> 140067041681536
	140067041681584 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 8, 64)"]
	140067041681728 -> 140067041681584
	140067041681728 [label=CloneBackward0]
	140067041681920 -> 140067041681728
	140067041681920 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 8, 64)"]
	140067041682064 -> 140067041681920
	140067041682064 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041682256 -> 140067041682064
	140067041682256 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 8, 768)"]
	140067041682400 -> 140067041682256
	140067041682400 [label="ViewBackward0
-------------------------
self_sym_sizes: (24, 768)"]
	140067041682592 -> 140067041682400
	140067041682592 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (24, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041682832 -> 140067041682592
	140066890253568 [label="q_encoder.encoder.text_encoding_layer.7.attention.self.query.bias
 (768)" fillcolor=lightblue]
	140066890253568 -> 140067041682832
	140067041682832 [label=AccumulateGrad]
	140067041682736 -> 140067041682592
	140067041682736 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 8, 768)"]
	140067041677504 -> 140067041682736
	140067041677504 [label="ViewBackward0
------------------------------
self_sym_sizes: (3, 1, 8, 768)"]
	140067041683168 -> 140067041677504
	140067041683168 [label="AsStridedBackward0
------------------------------------
size          :       (3, 1, 8, 768)
storage_offset:                    0
stride        : (6144, 6144, 768, 1)"]
	140067041683312 -> 140067041683168
	140067041683312 [label=CopySlices]
	140066907252736 -> 140067041683312
	140067041683552 -> 140067041683312
	140067041683552 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 1, 768)
start         :                   0
step          :                   1"]
	140067041683360 -> 140067041683552
	140067041683360 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:         (3, 2, 768)
start         :                   1
step          :                   1"]
	140067041684176 -> 140067041683360
	140067041684176 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:         (3, 2, 768)
start         :                   0
step          :                   1"]
	140066879628352 -> 140067041684176
	140067041681824 -> 140067041682592
	140067041681824 [label=TBackward0]
	140067041682880 -> 140067041681824
	140066890253328 [label="q_encoder.encoder.text_encoding_layer.7.attention.self.query.weight
 (768, 768)" fillcolor=lightblue]
	140066890253328 -> 140067041682880
	140067041682880 [label=AccumulateGrad]
	140067041681248 -> 140067041681536
	140067041681248 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 64, 8)"]
	140067041681344 -> 140067041681248
	140067041681344 [label=CloneBackward0]
	140067041682496 -> 140067041681344
	140067041682496 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 64, 8)"]
	140067041682304 -> 140067041682496
	140067041682304 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551615
dim1: 18446744073709551614"]
	140067041683408 -> 140067041682304
	140067041683408 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041684368 -> 140067041683408
	140067041684368 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 8, 768)"]
	140067041682784 -> 140067041684368
	140067041682784 [label="ViewBackward0
-------------------------
self_sym_sizes: (24, 768)"]
	140067041683072 -> 140067041682784
	140067041683072 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (24, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041684464 -> 140067041683072
	140066890255248 [label="q_encoder.encoder.text_encoding_layer.7.attention.self.key.bias
 (768)" fillcolor=lightblue]
	140066890255248 -> 140067041684464
	140067041684464 [label=AccumulateGrad]
	140067041684512 -> 140067041683072
	140067041684512 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 8, 768)"]
	140067041677504 -> 140067041684512
	140067041681872 -> 140067041683072
	140067041681872 [label=TBackward0]
	140067041684560 -> 140067041681872
	140066890254048 [label="q_encoder.encoder.text_encoding_layer.7.attention.self.key.weight
 (768, 768)" fillcolor=lightblue]
	140066890254048 -> 140067041684560
	140067041684560 [label=AccumulateGrad]
	140067041680048 -> 140067041680096
	140067041680048 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 8, 64)"]
	140067041680288 -> 140067041680048
	140067041680288 [label=CloneBackward0]
	140067041680864 -> 140067041680288
	140067041680864 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 8, 64)"]
	140067041680912 -> 140067041680864
	140067041680912 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041681968 -> 140067041680912
	140067041681968 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 8, 768)"]
	140067041683264 -> 140067041681968
	140067041683264 [label="ViewBackward0
-------------------------
self_sym_sizes: (24, 768)"]
	140067041683600 -> 140067041683264
	140067041683600 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (24, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041684272 -> 140067041683600
	140067041768016 [label="q_encoder.encoder.text_encoding_layer.7.attention.self.value.bias
 (768)" fillcolor=lightblue]
	140067041768016 -> 140067041684272
	140067041684272 [label=AccumulateGrad]
	140067041683024 -> 140067041683600
	140067041683024 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 8, 768)"]
	140067041677504 -> 140067041683024
	140067041680576 -> 140067041683600
	140067041680576 [label=TBackward0]
	140067041684608 -> 140067041680576
	140066890253408 [label="q_encoder.encoder.text_encoding_layer.7.attention.self.value.weight
 (768, 768)" fillcolor=lightblue]
	140066890253408 -> 140067041684608
	140067041684608 [label=AccumulateGrad]
	140067041678368 -> 140067041679424
	140067041678368 [label=TBackward0]
	140067041679568 -> 140067041678368
	140067041885584 [label="q_encoder.encoder.text_encoding_layer.7.attention.output.dense.weight
 (768, 768)" fillcolor=lightblue]
	140067041885584 -> 140067041679568
	140067041679568 [label=AccumulateGrad]
	140067041677504 -> 140067041675584
	140067041674384 -> 140066907244288
	140067041648768 [label="q_encoder.encoder.text_encoding_layer.7.attention.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	140067041648768 -> 140067041674384
	140067041674384 [label=AccumulateGrad]
	140067041675872 -> 140066907244288
	140067041648128 [label="q_encoder.encoder.text_encoding_layer.7.attention.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	140067041648128 -> 140067041675872
	140067041675872 [label=AccumulateGrad]
	140067041673568 -> 140066907248224
	140067041673568 [label=TBackward0]
	140067041678464 -> 140067041673568
	140067041801184 [label="q_encoder.encoder.text_encoding_layer.7.intermediate.dense.weight
 (3072, 768)" fillcolor=lightblue]
	140067041801184 -> 140067041678464
	140067041678464 [label=AccumulateGrad]
	140066907244096 -> 140066907247888
	140066907244096 [label=TBackward0]
	140066907255760 -> 140066907244096
	140067004463648 [label="q_encoder.encoder.text_encoding_layer.7.output.dense.weight
 (768, 3072)" fillcolor=lightblue]
	140067004463648 -> 140066907255760
	140066907255760 [label=AccumulateGrad]
	140066907244288 -> 140066907249664
	140066907255712 -> 140066879630800
	140066852648944 [label="q_encoder.encoder.text_encoding_layer.7.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	140066852648944 -> 140066907255712
	140066907255712 [label=AccumulateGrad]
	140066907243280 -> 140066879630800
	140066890253168 [label="q_encoder.encoder.text_encoding_layer.7.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	140066890253168 -> 140066907243280
	140066907243280 [label=AccumulateGrad]
	140066879631328 -> 140066879631856
	140066879631328 [label=TBackward0]
	140066879632672 -> 140066879631328
	140066904413104 [label="q_encoder.encoder.information_exchanging_layer.7.attention.self.query.weight
 (768, 768)" fillcolor=lightblue]
	140066904413104 -> 140066879632672
	140066879632672 [label=AccumulateGrad]
	140066879629744 -> 140066879627968
	140066879629744 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 64, 2)"]
	140066879628640 -> 140066879629744
	140066879628640 [label=CloneBackward0]
	140066879632096 -> 140066879628640
	140066879632096 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 64, 2)"]
	140066879630704 -> 140066879632096
	140066879630704 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551615
dim1: 18446744073709551614"]
	140066879634928 -> 140066879630704
	140066879634928 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140066879633872 -> 140066879634928
	140066879633872 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140066879631376 -> 140066879633872
	140066879631376 [label="ViewBackward0
------------------------
self_sym_sizes: (6, 768)"]
	140066879629360 -> 140066879631376
	140066879629360 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (6, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140066907253792 -> 140066879629360
	140066904413824 [label="q_encoder.encoder.information_exchanging_layer.7.attention.self.key.bias
 (768)" fillcolor=lightblue]
	140066904413824 -> 140066907253792
	140066907253792 [label=AccumulateGrad]
	140066907246016 -> 140066879629360
	140066907246016 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140066879625904 -> 140066907246016
	140066907243856 -> 140066879629360
	140066907243856 [label=TBackward0]
	140066907251056 -> 140066907243856
	140066904413744 [label="q_encoder.encoder.information_exchanging_layer.7.attention.self.key.weight
 (768, 768)" fillcolor=lightblue]
	140066904413744 -> 140066907251056
	140066907251056 [label=AccumulateGrad]
	140066879628688 -> 140066879626960
	140066879628688 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 2, 64)"]
	140066879628832 -> 140066879628688
	140066879628832 [label=CloneBackward0]
	140066879629504 -> 140066879628832
	140066879629504 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 2, 64)"]
	140066879629840 -> 140066879629504
	140066879629840 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140066879626768 -> 140066879629840
	140066879626768 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140066879632720 -> 140066879626768
	140066879632720 [label="ViewBackward0
------------------------
self_sym_sizes: (6, 768)"]
	140066879635120 -> 140066879632720
	140066879635120 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (6, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140066879626384 -> 140066879635120
	140066904413984 [label="q_encoder.encoder.information_exchanging_layer.7.attention.self.value.bias
 (768)" fillcolor=lightblue]
	140066904413984 -> 140066879626384
	140066879626384 [label=AccumulateGrad]
	140066879633824 -> 140066879635120
	140066879633824 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140066879625904 -> 140066879633824
	140066907255808 -> 140066879635120
	140066907255808 [label=TBackward0]
	140066907243376 -> 140066907255808
	140066904413904 [label="q_encoder.encoder.information_exchanging_layer.7.attention.self.value.weight
 (768, 768)" fillcolor=lightblue]
	140066904413904 -> 140066907243376
	140066907243376 [label=AccumulateGrad]
	140066879625808 -> 140066879626096
	140066879625808 [label=TBackward0]
	140066879626432 -> 140066879625808
	140066904414064 [label="q_encoder.encoder.information_exchanging_layer.7.attention.output.dense.weight
 (768, 768)" fillcolor=lightblue]
	140066904414064 -> 140066879626432
	140066879626432 [label=AccumulateGrad]
	140066879625904 -> 140066879623504
	140066879625088 -> 140066879619376
	140066904414304 [label="q_encoder.encoder.information_exchanging_layer.7.attention.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	140066904414304 -> 140066879625088
	140066879625088 [label=AccumulateGrad]
	140066879625472 -> 140066879619376
	140066904414224 [label="q_encoder.encoder.information_exchanging_layer.7.attention.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	140066904414224 -> 140066879625472
	140066879625472 [label=AccumulateGrad]
	140066879625184 -> 140066879623984
	140066879625184 [label=TBackward0]
	140066879625616 -> 140066879625184
	140066904414464 [label="q_encoder.encoder.information_exchanging_layer.7.intermediate.dense.weight
 (3072, 768)" fillcolor=lightblue]
	140066904414464 -> 140066879625616
	140066879625616 [label=AccumulateGrad]
	140066879623120 -> 140066879623216
	140066879623120 [label=TBackward0]
	140066879623744 -> 140066879623120
	140066904414624 [label="q_encoder.encoder.information_exchanging_layer.7.output.dense.weight
 (768, 3072)" fillcolor=lightblue]
	140066904414624 -> 140066879623744
	140066879623744 [label=AccumulateGrad]
	140066879619376 -> 140066879621440
	140066879622784 -> 140066879623456
	140066904414864 [label="q_encoder.encoder.information_exchanging_layer.7.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	140066904414864 -> 140066879622784
	140066879622784 [label=AccumulateGrad]
	140066879623600 -> 140066879623456
	140066904414784 [label="q_encoder.encoder.information_exchanging_layer.7.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	140066904414784 -> 140066879623600
	140066879623600 [label=AccumulateGrad]
	140066879621248 -> 140067054794448
	140066879621248 [label=CloneBackward0]
	140066879621392 -> 140066879621248
	140066879621392 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 1, 768)
start         :                   0
step          :                   1"]
	140066879620912 -> 140066879621392
	140066879620912 [label="SelectBackward0
------------------------------
dim           :              2
index         :              1
self_sym_sizes: (3, 1, 8, 768)"]
	140066879622640 -> 140066879620912
	140066879622640 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:      (3, 1, 8, 768)
start         :                   0
step          :                   1"]
	140066879623792 -> 140066879622640
	140066879623792 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:      (3, 1, 8, 768)
start         :                   0
step          :                   1"]
	140066879627488 -> 140066879623792
	140066879627488 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 8, 768)"]
	140066879626336 -> 140066879627488
	140066879626336 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140066879625328 -> 140066879626336
	140066879625328 [label="AddBackward0
------------
alpha: 1"]
	140066879625664 -> 140066879625328
	140066879625664 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140066879623072 -> 140066879625664
	140066879623072 [label="ViewBackward0
-------------------------
self_sym_sizes: (24, 768)"]
	140066879632480 -> 140066879623072
	140066879632480 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :     (24, 3072)
mat1_sym_strides:      (3072, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (3072, 768)
mat2_sym_strides:      (1, 3072)"]
	140066907243712 -> 140066879632480
	140066852647344 [label="q_encoder.encoder.text_encoding_layer.8.output.dense.bias
 (768)" fillcolor=lightblue]
	140066852647344 -> 140066907243712
	140066907243712 [label=AccumulateGrad]
	140066879630128 -> 140066879632480
	140066879630128 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 8, 3072)"]
	140066879632000 -> 140066879630128
	140066879632000 [label="GeluBackward0
---------------------------
approximate:           none
self       : [saved tensor]"]
	140067041678608 -> 140066879632000
	140067041678608 [label="ViewBackward0
--------------------------
self_sym_sizes: (24, 3072)"]
	140067041674432 -> 140067041678608
	140067041674432 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (24, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (768, 3072)
mat2_sym_strides:       (1, 768)"]
	140067041680000 -> 140067041674432
	140066852649904 [label="q_encoder.encoder.text_encoding_layer.8.intermediate.dense.bias
 (3072)" fillcolor=lightblue]
	140066852649904 -> 140067041680000
	140067041680000 [label=AccumulateGrad]
	140067041679904 -> 140067041674432
	140067041679904 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 8, 768)"]
	140066879627056 -> 140067041679904
	140066879627056 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140067041680720 -> 140066879627056
	140067041680720 [label="AddBackward0
------------
alpha: 1"]
	140067041682112 -> 140067041680720
	140067041682112 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041680480 -> 140067041682112
	140067041680480 [label="ViewBackward0
-------------------------
self_sym_sizes: (24, 768)"]
	140067041684656 -> 140067041680480
	140067041684656 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (24, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041676304 -> 140067041684656
	140066852649024 [label="q_encoder.encoder.text_encoding_layer.8.attention.output.dense.bias
 (768)" fillcolor=lightblue]
	140066852649024 -> 140067041676304
	140067041676304 [label=AccumulateGrad]
	140067041683744 -> 140067041684656
	140067041683744 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 8, 768)"]
	140067041684896 -> 140067041683744
	140067041684896 [label="ViewBackward0
------------------------------
self_sym_sizes: (3, 8, 12, 64)"]
	140067041684704 -> 140067041684896
	140067041684704 [label=CloneBackward0]
	140067041684128 -> 140067041684704
	140067041684128 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041685280 -> 140067041684128
	140067041685280 [label="UnsafeViewBackward0
---------------------------
self_sym_sizes: (36, 8, 64)"]
	140067041685616 -> 140067041685280
	140067041685616 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140067041685808 -> 140067041685616
	140067041685808 [label="ViewBackward0
-----------------------------
self_sym_sizes: (3, 12, 8, 8)"]
	140067041685952 -> 140067041685808
	140067041685952 [label="ExpandBackward0
-----------------------------
self_sym_sizes: (3, 12, 8, 8)"]
	140067041686192 -> 140067041685952
	140067041686192 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041686288 -> 140067041686192
	140067041686288 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140067041686528 -> 140067041686288
	140067041686528 [label="AddBackward0
------------
alpha: 1"]
	140067041686576 -> 140067041686528
	140067041686576 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041686864 -> 140067041686576
	140067041686864 [label="UnsafeViewBackward0
--------------------------
self_sym_sizes: (36, 8, 8)"]
	140067041687152 -> 140067041686864
	140067041687152 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140067041687200 -> 140067041687152
	140067041687200 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 8, 64)"]
	140067041687296 -> 140067041687200
	140067041687296 [label=CloneBackward0]
	140067041687488 -> 140067041687296
	140067041687488 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 8, 64)"]
	140067041687632 -> 140067041687488
	140067041687632 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041687920 -> 140067041687632
	140067041687920 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 8, 768)"]
	140067041688064 -> 140067041687920
	140067041688064 [label="ViewBackward0
-------------------------
self_sym_sizes: (24, 768)"]
	140067041688208 -> 140067041688064
	140067041688208 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (24, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041688544 -> 140067041688208
	140066852650224 [label="q_encoder.encoder.text_encoding_layer.8.attention.self.query.bias
 (768)" fillcolor=lightblue]
	140066852650224 -> 140067041688544
	140067041688544 [label=AccumulateGrad]
	140067041688352 -> 140067041688208
	140067041688352 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 8, 768)"]
	140067041682928 -> 140067041688352
	140067041682928 [label="ViewBackward0
------------------------------
self_sym_sizes: (3, 1, 8, 768)"]
	140067041688784 -> 140067041682928
	140067041688784 [label="AsStridedBackward0
------------------------------------
size          :       (3, 1, 8, 768)
storage_offset:                    0
stride        : (6144, 6144, 768, 1)"]
	140067041689024 -> 140067041688784
	140067041689024 [label=CopySlices]
	140066879630800 -> 140067041689024
	140067041689216 -> 140067041689024
	140067041689216 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 1, 768)
start         :                   0
step          :                   1"]
	140067041689072 -> 140067041689216
	140067041689072 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:         (3, 2, 768)
start         :                   1
step          :                   1"]
	140067041689456 -> 140067041689072
	140067041689456 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:         (3, 2, 768)
start         :                   0
step          :                   1"]
	140066879623456 -> 140067041689456
	140067041687344 -> 140067041688208
	140067041687344 [label=TBackward0]
	140067041688592 -> 140067041687344
	140066852649664 [label="q_encoder.encoder.text_encoding_layer.8.attention.self.query.weight
 (768, 768)" fillcolor=lightblue]
	140066852649664 -> 140067041688592
	140067041688592 [label=AccumulateGrad]
	140067041686672 -> 140067041687152
	140067041686672 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 64, 8)"]
	140067041686960 -> 140067041686672
	140067041686960 [label=CloneBackward0]
	140067041688112 -> 140067041686960
	140067041688112 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 64, 8)"]
	140067041688016 -> 140067041688112
	140067041688016 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551615
dim1: 18446744073709551614"]
	140067041689120 -> 140067041688016
	140067041689120 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041688448 -> 140067041689120
	140067041688448 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 8, 768)"]
	140067041689264 -> 140067041688448
	140067041689264 [label="ViewBackward0
-------------------------
self_sym_sizes: (24, 768)"]
	140067041688880 -> 140067041689264
	140067041688880 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (24, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041687440 -> 140067041688880
	140066852650704 [label="q_encoder.encoder.text_encoding_layer.8.attention.self.key.bias
 (768)" fillcolor=lightblue]
	140066852650704 -> 140067041687440
	140067041687440 [label=AccumulateGrad]
	140067041689408 -> 140067041688880
	140067041689408 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 8, 768)"]
	140067041682928 -> 140067041689408
	140067041722864 -> 140067041688880
	140067041722864 [label=TBackward0]
	140067041722768 -> 140067041722864
	140066852650464 [label="q_encoder.encoder.text_encoding_layer.8.attention.self.key.weight
 (768, 768)" fillcolor=lightblue]
	140066852650464 -> 140067041722768
	140067041722768 [label=AccumulateGrad]
	140067041685568 -> 140067041685616
	140067041685568 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 8, 64)"]
	140067041685856 -> 140067041685568
	140067041685856 [label=CloneBackward0]
	140067041686336 -> 140067041685856
	140067041686336 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 8, 64)"]
	140067041686384 -> 140067041686336
	140067041686384 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041687776 -> 140067041686384
	140067041687776 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 8, 768)"]
	140067041688976 -> 140067041687776
	140067041688976 [label="ViewBackward0
-------------------------
self_sym_sizes: (24, 768)"]
	140067041689360 -> 140067041688976
	140067041689360 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (24, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041689552 -> 140067041689360
	140066852650864 [label="q_encoder.encoder.text_encoding_layer.8.attention.self.value.bias
 (768)" fillcolor=lightblue]
	140066852650864 -> 140067041689552
	140067041689552 [label=AccumulateGrad]
	140067041688688 -> 140067041689360
	140067041688688 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 8, 768)"]
	140067041682928 -> 140067041688688
	140067041686144 -> 140067041689360
	140067041686144 [label=TBackward0]
	140067041723056 -> 140067041686144
	140066852650944 [label="q_encoder.encoder.text_encoding_layer.8.attention.self.value.weight
 (768, 768)" fillcolor=lightblue]
	140066852650944 -> 140067041723056
	140067041723056 [label=AccumulateGrad]
	140067041683888 -> 140067041684656
	140067041683888 [label=TBackward0]
	140067041684800 -> 140067041683888
	140066852650784 [label="q_encoder.encoder.text_encoding_layer.8.attention.output.dense.weight
 (768, 768)" fillcolor=lightblue]
	140066852650784 -> 140067041684800
	140067041684800 [label=AccumulateGrad]
	140067041682928 -> 140067041680720
	140067041679808 -> 140066879627056
	140066852650544 [label="q_encoder.encoder.text_encoding_layer.8.attention.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	140066852650544 -> 140067041679808
	140067041679808 [label=AccumulateGrad]
	140067041681008 -> 140066879627056
	140066852649104 [label="q_encoder.encoder.text_encoding_layer.8.attention.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	140066852649104 -> 140067041681008
	140067041681008 [label=AccumulateGrad]
	140067041678512 -> 140067041674432
	140067041678512 [label=TBackward0]
	140067041683936 -> 140067041678512
	140066852649824 [label="q_encoder.encoder.text_encoding_layer.8.intermediate.dense.weight
 (3072, 768)" fillcolor=lightblue]
	140066852649824 -> 140067041683936
	140067041683936 [label=AccumulateGrad]
	140066879624512 -> 140066879632480
	140066879624512 [label=TBackward0]
	140067041674336 -> 140066879624512
	140066852650624 [label="q_encoder.encoder.text_encoding_layer.8.output.dense.weight
 (768, 3072)" fillcolor=lightblue]
	140066852650624 -> 140067041674336
	140067041674336 [label=AccumulateGrad]
	140066879627056 -> 140066879625328
	140066879623312 -> 140066879626336
	140066852650304 [label="q_encoder.encoder.text_encoding_layer.8.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	140066852650304 -> 140066879623312
	140066879623312 [label=AccumulateGrad]
	140066879623024 -> 140066879626336
	140066852649984 [label="q_encoder.encoder.text_encoding_layer.8.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	140066852649984 -> 140066879623024
	140066879623024 [label=AccumulateGrad]
	140066879622208 -> 140066879622352
	140066879622208 [label=TBackward0]
	140066879620960 -> 140066879622208
	140066904414384 [label="q_encoder.encoder.information_exchanging_layer.8.attention.self.query.weight
 (768, 768)" fillcolor=lightblue]
	140066904414384 -> 140066879620960
	140066879620960 [label=AccumulateGrad]
	140066879622592 -> 140066879619616
	140066879622592 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 64, 2)"]
	140066879619808 -> 140066879622592
	140066879619808 [label=CloneBackward0]
	140066879620768 -> 140066879619808
	140066879620768 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 64, 2)"]
	140066879620480 -> 140066879620768
	140066879620480 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551615
dim1: 18446744073709551614"]
	140066879621632 -> 140066879620480
	140066879621632 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140066879626912 -> 140066879621632
	140066879626912 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140066879625136 -> 140066879626912
	140066879625136 [label="ViewBackward0
------------------------
self_sym_sizes: (6, 768)"]
	140066879629408 -> 140066879625136
	140066879629408 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (6, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140066879624224 -> 140066879629408
	140066904415104 [label="q_encoder.encoder.information_exchanging_layer.8.attention.self.key.bias
 (768)" fillcolor=lightblue]
	140066904415104 -> 140066879624224
	140066879624224 [label=AccumulateGrad]
	140066879628880 -> 140066879629408
	140066879628880 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140067054794448 -> 140066879628880
	140066879622544 -> 140066879629408
	140066879622544 [label=TBackward0]
	140066879627248 -> 140066879622544
	140066904415024 [label="q_encoder.encoder.information_exchanging_layer.8.attention.self.key.weight
 (768, 768)" fillcolor=lightblue]
	140066904415024 -> 140066879627248
	140066879627248 [label=AccumulateGrad]
	140067054781632 -> 140067054782352
	140067054781632 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 2, 64)"]
	140067054788496 -> 140067054781632
	140067054788496 [label=CloneBackward0]
	140066879631904 -> 140067054788496
	140066879631904 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 2, 64)"]
	140066879619184 -> 140066879631904
	140066879619184 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140066879620528 -> 140066879619184
	140066879620528 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140066879621296 -> 140066879620528
	140066879621296 [label="ViewBackward0
------------------------
self_sym_sizes: (6, 768)"]
	140066879625760 -> 140066879621296
	140066879625760 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (6, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140066879628160 -> 140066879625760
	140066904415264 [label="q_encoder.encoder.information_exchanging_layer.8.attention.self.value.bias
 (768)" fillcolor=lightblue]
	140066904415264 -> 140066879628160
	140066879628160 [label=AccumulateGrad]
	140066879622256 -> 140066879625760
	140066879622256 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140067054794448 -> 140066879622256
	140066879629648 -> 140066879625760
	140066879629648 [label=TBackward0]
	140067041679712 -> 140066879629648
	140066904415184 [label="q_encoder.encoder.information_exchanging_layer.8.attention.self.value.weight
 (768, 768)" fillcolor=lightblue]
	140066904415184 -> 140067041679712
	140067041679712 [label=AccumulateGrad]
	140067054781056 -> 140067054787968
	140067054781056 [label=TBackward0]
	140067054781824 -> 140067054781056
	140066904415344 [label="q_encoder.encoder.information_exchanging_layer.8.attention.output.dense.weight
 (768, 768)" fillcolor=lightblue]
	140066904415344 -> 140067054781824
	140067054781824 [label=AccumulateGrad]
	140067054794448 -> 140067054786672
	140067054787440 -> 140067054787584
	140066904415584 [label="q_encoder.encoder.information_exchanging_layer.8.attention.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	140066904415584 -> 140067054787440
	140067054787440 [label=AccumulateGrad]
	140067054791136 -> 140067054787584
	140066904415504 [label="q_encoder.encoder.information_exchanging_layer.8.attention.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	140066904415504 -> 140067054791136
	140067054791136 [label=AccumulateGrad]
	140067054786816 -> 140067054793632
	140067054786816 [label=TBackward0]
	140067054784752 -> 140067054786816
	140066904415744 [label="q_encoder.encoder.information_exchanging_layer.8.intermediate.dense.weight
 (3072, 768)" fillcolor=lightblue]
	140066904415744 -> 140067054784752
	140067054784752 [label=AccumulateGrad]
	140067054787152 -> 140067054790176
	140067054787152 [label=TBackward0]
	140067054790464 -> 140067054787152
	140066904415904 [label="q_encoder.encoder.information_exchanging_layer.8.output.dense.weight
 (768, 3072)" fillcolor=lightblue]
	140066904415904 -> 140067054790464
	140067054790464 [label=AccumulateGrad]
	140067054787584 -> 140067054782544
	140067054790080 -> 140067054783840
	140066904416144 [label="q_encoder.encoder.information_exchanging_layer.8.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	140066904416144 -> 140067054790080
	140067054790080 [label=AccumulateGrad]
	140067054784656 -> 140067054783840
	140066904416064 [label="q_encoder.encoder.information_exchanging_layer.8.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	140066904416064 -> 140067054784656
	140067054784656 [label=AccumulateGrad]
	140067054786048 -> 140066819367568
	140067054786048 [label=CloneBackward0]
	140067054784272 -> 140067054786048
	140067054784272 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 1, 768)
start         :                   0
step          :                   1"]
	140067054782688 -> 140067054784272
	140067054782688 [label="SelectBackward0
------------------------------
dim           :              2
index         :              1
self_sym_sizes: (3, 1, 8, 768)"]
	140067054796704 -> 140067054782688
	140067054796704 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:      (3, 1, 8, 768)
start         :                   0
step          :                   1"]
	140067054786384 -> 140067054796704
	140067054786384 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:      (3, 1, 8, 768)
start         :                   0
step          :                   1"]
	140067054795840 -> 140067054786384
	140067054795840 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 8, 768)"]
	140067054782448 -> 140067054795840
	140067054782448 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140067054784800 -> 140067054782448
	140067054784800 [label="AddBackward0
------------
alpha: 1"]
	140067054789600 -> 140067054784800
	140067054789600 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067054784032 -> 140067054789600
	140067054784032 [label="ViewBackward0
-------------------------
self_sym_sizes: (24, 768)"]
	140066879622448 -> 140067054784032
	140066879622448 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :     (24, 3072)
mat1_sym_strides:      (3072, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (3072, 768)
mat2_sym_strides:      (1, 3072)"]
	140066879634064 -> 140066879622448
	140066852650064 [label="q_encoder.encoder.text_encoding_layer.9.output.dense.bias
 (768)" fillcolor=lightblue]
	140066852650064 -> 140066879634064
	140066879634064 [label=AccumulateGrad]
	140066879619280 -> 140066879622448
	140066879619280 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 8, 3072)"]
	140066879620240 -> 140066879619280
	140066879620240 [label="GeluBackward0
---------------------------
approximate:           none
self       : [saved tensor]"]
	140067041681440 -> 140066879620240
	140067041681440 [label="ViewBackward0
--------------------------
self_sym_sizes: (24, 3072)"]
	140067041683984 -> 140067041681440
	140067041683984 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (24, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (768, 3072)
mat2_sym_strides:       (1, 768)"]
	140067041685472 -> 140067041683984
	140066852647264 [label="q_encoder.encoder.text_encoding_layer.9.intermediate.dense.bias
 (3072)" fillcolor=lightblue]
	140066852647264 -> 140067041685472
	140067041685472 [label=AccumulateGrad]
	140067041685184 -> 140067041683984
	140067041685184 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 8, 768)"]
	140067054780768 -> 140067041685184
	140067054780768 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140067041686240 -> 140067054780768
	140067041686240 [label="AddBackward0
------------
alpha: 1"]
	140067041679856 -> 140067041686240
	140067041679856 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041685712 -> 140067041679856
	140067041685712 [label="ViewBackward0
-------------------------
self_sym_sizes: (24, 768)"]
	140067041723200 -> 140067041685712
	140067041723200 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (24, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041722576 -> 140067041723200
	140066852649184 [label="q_encoder.encoder.text_encoding_layer.9.attention.output.dense.bias
 (768)" fillcolor=lightblue]
	140066852649184 -> 140067041722576
	140067041722576 [label=AccumulateGrad]
	140067041722672 -> 140067041723200
	140067041722672 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 8, 768)"]
	140067041723392 -> 140067041722672
	140067041723392 [label="ViewBackward0
------------------------------
self_sym_sizes: (3, 8, 12, 64)"]
	140067041723248 -> 140067041723392
	140067041723248 [label=CloneBackward0]
	140067041722960 -> 140067041723248
	140067041722960 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041723728 -> 140067041722960
	140067041723728 [label="UnsafeViewBackward0
---------------------------
self_sym_sizes: (36, 8, 64)"]
	140067041723920 -> 140067041723728
	140067041723920 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140067041724016 -> 140067041723920
	140067041724016 [label="ViewBackward0
-----------------------------
self_sym_sizes: (3, 12, 8, 8)"]
	140067041724208 -> 140067041724016
	140067041724208 [label="ExpandBackward0
-----------------------------
self_sym_sizes: (3, 12, 8, 8)"]
	140067041724448 -> 140067041724208
	140067041724448 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041724640 -> 140067041724448
	140067041724640 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140067041724976 -> 140067041724640
	140067041724976 [label="AddBackward0
------------
alpha: 1"]
	140067041725072 -> 140067041724976
	140067041725072 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041725216 -> 140067041725072
	140067041725216 [label="UnsafeViewBackward0
--------------------------
self_sym_sizes: (36, 8, 8)"]
	140067041725360 -> 140067041725216
	140067041725360 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140067041725552 -> 140067041725360
	140067041725552 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 8, 64)"]
	140067041725600 -> 140067041725552
	140067041725600 [label=CloneBackward0]
	140067041725792 -> 140067041725600
	140067041725792 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 8, 64)"]
	140067041725888 -> 140067041725792
	140067041725888 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041726032 -> 140067041725888
	140067041726032 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 8, 768)"]
	140067041726224 -> 140067041726032
	140067041726224 [label="ViewBackward0
-------------------------
self_sym_sizes: (24, 768)"]
	140067041726416 -> 140067041726224
	140067041726416 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (24, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041726704 -> 140067041726416
	140066852649264 [label="q_encoder.encoder.text_encoding_layer.9.attention.self.query.bias
 (768)" fillcolor=lightblue]
	140066852649264 -> 140067041726704
	140067041726704 [label=AccumulateGrad]
	140067041726560 -> 140067041726416
	140067041726560 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 8, 768)"]
	140067041686048 -> 140067041726560
	140067041686048 [label="ViewBackward0
------------------------------
self_sym_sizes: (3, 1, 8, 768)"]
	140067041727040 -> 140067041686048
	140067041727040 [label="AsStridedBackward0
------------------------------------
size          :       (3, 1, 8, 768)
storage_offset:                    0
stride        : (6144, 6144, 768, 1)"]
	140067041727376 -> 140067041727040
	140067041727376 [label=CopySlices]
	140066879626336 -> 140067041727376
	140067041727472 -> 140067041727376
	140067041727472 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 1, 768)
start         :                   0
step          :                   1"]
	140067041727424 -> 140067041727472
	140067041727424 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:         (3, 2, 768)
start         :                   1
step          :                   1"]
	140067041728288 -> 140067041727424
	140067041728288 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:         (3, 2, 768)
start         :                   0
step          :                   1"]
	140067054783840 -> 140067041728288
	140067041725648 -> 140067041726416
	140067041725648 [label=TBackward0]
	140067041726752 -> 140067041725648
	140066852650384 [label="q_encoder.encoder.text_encoding_layer.9.attention.self.query.weight
 (768, 768)" fillcolor=lightblue]
	140066852650384 -> 140067041726752
	140067041726752 [label=AccumulateGrad]
	140067041725120 -> 140067041725360
	140067041725120 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 64, 8)"]
	140067041725312 -> 140067041725120
	140067041725312 [label=CloneBackward0]
	140067041726368 -> 140067041725312
	140067041726368 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 64, 8)"]
	140067041726128 -> 140067041726368
	140067041726128 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551615
dim1: 18446744073709551614"]
	140067041727568 -> 140067041726128
	140067041727568 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041728480 -> 140067041727568
	140067041728480 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 8, 768)"]
	140067041726608 -> 140067041728480
	140067041726608 [label="ViewBackward0
-------------------------
self_sym_sizes: (24, 768)"]
	140067041727136 -> 140067041726608
	140067041727136 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (24, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041728576 -> 140067041727136
	140066852649424 [label="q_encoder.encoder.text_encoding_layer.9.attention.self.key.bias
 (768)" fillcolor=lightblue]
	140066852649424 -> 140067041728576
	140067041728576 [label=AccumulateGrad]
	140067041728624 -> 140067041727136
	140067041728624 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 8, 768)"]
	140067041686048 -> 140067041728624
	140067041725744 -> 140067041727136
	140067041725744 [label=TBackward0]
	140067041728720 -> 140067041725744
	140066852649344 [label="q_encoder.encoder.text_encoding_layer.9.attention.self.key.weight
 (768, 768)" fillcolor=lightblue]
	140066852649344 -> 140067041728720
	140067041728720 [label=AccumulateGrad]
	140067041723872 -> 140067041723920
	140067041723872 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 8, 64)"]
	140067041724160 -> 140067041723872
	140067041724160 [label=CloneBackward0]
	140067041724688 -> 140067041724160
	140067041724688 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 8, 64)"]
	140067041724784 -> 140067041724688
	140067041724784 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041725936 -> 140067041724784
	140067041725936 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 8, 768)"]
	140067041727232 -> 140067041725936
	140067041727232 [label="ViewBackward0
-------------------------
self_sym_sizes: (24, 768)"]
	140067041727760 -> 140067041727232
	140067041727760 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (24, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041728432 -> 140067041727760
	140066852649744 [label="q_encoder.encoder.text_encoding_layer.9.attention.self.value.bias
 (768)" fillcolor=lightblue]
	140066852649744 -> 140067041728432
	140067041728432 [label=AccumulateGrad]
	140067041726992 -> 140067041727760
	140067041726992 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 8, 768)"]
	140067041686048 -> 140067041726992
	140067041724352 -> 140067041727760
	140067041724352 [label=TBackward0]
	140067041728816 -> 140067041724352
	140066852649504 [label="q_encoder.encoder.text_encoding_layer.9.attention.self.value.weight
 (768, 768)" fillcolor=lightblue]
	140066852649504 -> 140067041728816
	140067041728816 [label=AccumulateGrad]
	140067041722912 -> 140067041723200
	140067041722912 [label=TBackward0]
	140067041723296 -> 140067041722912
	140066852650144 [label="q_encoder.encoder.text_encoding_layer.9.attention.output.dense.weight
 (768, 768)" fillcolor=lightblue]
	140066852650144 -> 140067041723296
	140067041723296 [label=AccumulateGrad]
	140067041686048 -> 140067041686240
	140067041685040 -> 140067054780768
	140066852648464 [label="q_encoder.encoder.text_encoding_layer.9.attention.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	140066852648464 -> 140067041685040
	140067041685040 [label=AccumulateGrad]
	140067041686480 -> 140067054780768
	140066852649584 [label="q_encoder.encoder.text_encoding_layer.9.attention.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	140066852649584 -> 140067041686480
	140067041686480 [label=AccumulateGrad]
	140067041684032 -> 140067041683984
	140067041684032 [label=TBackward0]
	140067041687872 -> 140067041684032
	140066852648544 [label="q_encoder.encoder.text_encoding_layer.9.intermediate.dense.weight
 (3072, 768)" fillcolor=lightblue]
	140066852648544 -> 140067041687872
	140067041687872 [label=AccumulateGrad]
	140066879635312 -> 140066879622448
	140066879635312 [label=TBackward0]
	140067041675152 -> 140066879635312
	140066852648064 [label="q_encoder.encoder.text_encoding_layer.9.output.dense.weight
 (768, 3072)" fillcolor=lightblue]
	140066852648064 -> 140067041675152
	140067041675152 [label=AccumulateGrad]
	140067054780768 -> 140067054784800
	140067054782640 -> 140067054782448
	140066852647584 [label="q_encoder.encoder.text_encoding_layer.9.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	140066852647584 -> 140067054782640
	140067054782640 [label=AccumulateGrad]
	140067054782256 -> 140067054782448
	140066852647424 [label="q_encoder.encoder.text_encoding_layer.9.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	140066852647424 -> 140067054782256
	140067054782256 [label=AccumulateGrad]
	140067054787344 -> 140066819374720
	140067054787344 [label=TBackward0]
	140067054790992 -> 140067054787344
	140066904415664 [label="q_encoder.encoder.information_exchanging_layer.9.attention.self.query.weight
 (768, 768)" fillcolor=lightblue]
	140066904415664 -> 140067054790992
	140067054790992 [label=AccumulateGrad]
	140066819372320 -> 140066819364688
	140066819372320 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 64, 2)"]
	140066819359984 -> 140066819372320
	140066819359984 [label=CloneBackward0]
	140066819371072 -> 140066819359984
	140066819371072 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 64, 2)"]
	140066819370160 -> 140066819371072
	140066819370160 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551615
dim1: 18446744073709551614"]
	140067054786336 -> 140066819370160
	140067054786336 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067054789936 -> 140067054786336
	140067054789936 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140067054786960 -> 140067054789936
	140067054786960 [label="ViewBackward0
------------------------
self_sym_sizes: (6, 768)"]
	140067054791232 -> 140067054786960
	140067054791232 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (6, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067054787104 -> 140067054791232
	140066904416384 [label="q_encoder.encoder.information_exchanging_layer.9.attention.self.key.bias
 (768)" fillcolor=lightblue]
	140066904416384 -> 140067054787104
	140067054787104 [label=AccumulateGrad]
	140067054787488 -> 140067054791232
	140067054787488 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140066819367568 -> 140067054787488
	140067054790224 -> 140067054791232
	140067054790224 [label=TBackward0]
	140067041688640 -> 140067054790224
	140066904416304 [label="q_encoder.encoder.information_exchanging_layer.9.attention.self.key.weight
 (768, 768)" fillcolor=lightblue]
	140066904416304 -> 140067041688640
	140067041688640 [label=AccumulateGrad]
	140066819360560 -> 140066819368000
	140066819360560 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 2, 64)"]
	140066819363680 -> 140066819360560
	140066819363680 [label=CloneBackward0]
	140066819372944 -> 140066819363680
	140066819372944 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 2, 64)"]
	140066819373376 -> 140066819372944
	140066819373376 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140066819373904 -> 140066819373376
	140066819373904 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140066819373520 -> 140066819373904
	140066819373520 [label="ViewBackward0
------------------------
self_sym_sizes: (6, 768)"]
	140066819365456 -> 140066819373520
	140066819365456 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (6, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067054785376 -> 140066819365456
	140066904416544 [label="q_encoder.encoder.information_exchanging_layer.9.attention.self.value.bias
 (768)" fillcolor=lightblue]
	140066904416544 -> 140067054785376
	140067054785376 [label=AccumulateGrad]
	140067054788112 -> 140066819365456
	140067054788112 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140066819367568 -> 140067054788112
	140067054783024 -> 140066819365456
	140067054783024 [label=TBackward0]
	140066879624176 -> 140067054783024
	140066904416464 [label="q_encoder.encoder.information_exchanging_layer.9.attention.self.value.weight
 (768, 768)" fillcolor=lightblue]
	140066904416464 -> 140066879624176
	140066879624176 [label=AccumulateGrad]
	140066819362000 -> 140066819372992
	140066819362000 [label=TBackward0]
	140066819369344 -> 140066819362000
	140066904416624 [label="q_encoder.encoder.information_exchanging_layer.9.attention.output.dense.weight
 (768, 768)" fillcolor=lightblue]
	140066904416624 -> 140066819369344
	140066819369344 [label=AccumulateGrad]
	140066819367568 -> 140066819369488
	140066819371552 -> 140066819364832
	140066904416864 [label="q_encoder.encoder.information_exchanging_layer.9.attention.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	140066904416864 -> 140066819371552
	140066819371552 [label=AccumulateGrad]
	140066819373664 -> 140066819364832
	140066904416784 [label="q_encoder.encoder.information_exchanging_layer.9.attention.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	140066904416784 -> 140066819373664
	140066819373664 [label=AccumulateGrad]
	140066819369056 -> 140066819371792
	140066819369056 [label=TBackward0]
	140066819361376 -> 140066819369056
	140066904417024 [label="q_encoder.encoder.information_exchanging_layer.9.intermediate.dense.weight
 (3072, 768)" fillcolor=lightblue]
	140066904417024 -> 140066819361376
	140066819361376 [label=AccumulateGrad]
	140066819361808 -> 140066819369248
	140066819361808 [label=TBackward0]
	140066819369920 -> 140066819361808
	140066904417184 [label="q_encoder.encoder.information_exchanging_layer.9.output.dense.weight
 (768, 3072)" fillcolor=lightblue]
	140066904417184 -> 140066819369920
	140066819369920 [label=AccumulateGrad]
	140066819364832 -> 140066819362336
	140066819360896 -> 140066819359168
	140066904417424 [label="q_encoder.encoder.information_exchanging_layer.9.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	140066904417424 -> 140066819360896
	140066819360896 [label=AccumulateGrad]
	140066819363008 -> 140066819359168
	140066904417344 [label="q_encoder.encoder.information_exchanging_layer.9.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	140066904417344 -> 140066819363008
	140066819363008 [label=AccumulateGrad]
	140066819367424 -> 140066688174256
	140066819367424 [label=CloneBackward0]
	140066819359024 -> 140066819367424
	140066819359024 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 1, 768)
start         :                   0
step          :                   1"]
	140066819361760 -> 140066819359024
	140066819361760 [label="SelectBackward0
------------------------------
dim           :              2
index         :              1
self_sym_sizes: (3, 1, 8, 768)"]
	140066819364256 -> 140066819361760
	140066819364256 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:      (3, 1, 8, 768)
start         :                   0
step          :                   1"]
	140066819370304 -> 140066819364256
	140066819370304 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:      (3, 1, 8, 768)
start         :                   0
step          :                   1"]
	140066819363968 -> 140066819370304
	140066819363968 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 8, 768)"]
	140066819364016 -> 140066819363968
	140066819364016 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140066819373232 -> 140066819364016
	140066819373232 [label="AddBackward0
------------
alpha: 1"]
	140066819358832 -> 140066819373232
	140066819358832 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067054785808 -> 140066819358832
	140067054785808 [label="ViewBackward0
-------------------------
self_sym_sizes: (24, 768)"]
	140066819372416 -> 140067054785808
	140066819372416 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :     (24, 3072)
mat1_sym_strides:      (3072, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (3072, 768)
mat2_sym_strides:      (1, 3072)"]
	140067041685088 -> 140066819372416
	140066852647504 [label="q_encoder.encoder.text_encoding_layer.10.output.dense.bias
 (768)" fillcolor=lightblue]
	140066852647504 -> 140067041685088
	140067041685088 [label=AccumulateGrad]
	140067041680192 -> 140066819372416
	140067041680192 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 8, 3072)"]
	140066879619424 -> 140067041680192
	140066879619424 [label="GeluBackward0
---------------------------
approximate:           none
self       : [saved tensor]"]
	140066819373760 -> 140066879619424
	140066819373760 [label="ViewBackward0
--------------------------
self_sym_sizes: (24, 3072)"]
	140066819368480 -> 140066819373760
	140066819368480 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (24, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (768, 3072)
mat2_sym_strides:       (1, 768)"]
	140067041723776 -> 140066819368480
	140066852646304 [label="q_encoder.encoder.text_encoding_layer.10.intermediate.dense.bias
 (3072)" fillcolor=lightblue]
	140066852646304 -> 140067041723776
	140067041723776 [label=AccumulateGrad]
	140067041723680 -> 140066819368480
	140067041723680 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 8, 768)"]
	140066819365552 -> 140067041723680
	140066819365552 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140067041724544 -> 140066819365552
	140067041724544 [label="AddBackward0
------------
alpha: 1"]
	140067041725984 -> 140067041724544
	140067041725984 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041724304 -> 140067041725984
	140067041724304 [label="ViewBackward0
-------------------------
self_sym_sizes: (24, 768)"]
	140067041729008 -> 140067041724304
	140067041729008 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (24, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041722432 -> 140067041729008
	140066852646704 [label="q_encoder.encoder.text_encoding_layer.10.attention.output.dense.bias
 (768)" fillcolor=lightblue]
	140066852646704 -> 140067041722432
	140067041722432 [label=AccumulateGrad]
	140067041727616 -> 140067041729008
	140067041727616 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 8, 768)"]
	140067041729200 -> 140067041727616
	140067041729200 [label="ViewBackward0
------------------------------
self_sym_sizes: (3, 8, 12, 64)"]
	140067041729104 -> 140067041729200
	140067041729104 [label=CloneBackward0]
	140067041728240 -> 140067041729104
	140067041728240 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041729632 -> 140067041728240
	140067041729632 [label="UnsafeViewBackward0
---------------------------
self_sym_sizes: (36, 8, 64)"]
	140067041729968 -> 140067041729632
	140067041729968 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140067041730112 -> 140067041729968
	140067041730112 [label="ViewBackward0
-----------------------------
self_sym_sizes: (3, 12, 8, 8)"]
	140067041730208 -> 140067041730112
	140067041730208 [label="ExpandBackward0
-----------------------------
self_sym_sizes: (3, 12, 8, 8)"]
	140067041730400 -> 140067041730208
	140067041730400 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041730496 -> 140067041730400
	140067041730496 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140067041730832 -> 140067041730496
	140067041730832 [label="AddBackward0
------------
alpha: 1"]
	140067041730928 -> 140067041730832
	140067041730928 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041731072 -> 140067041730928
	140067041731072 [label="UnsafeViewBackward0
--------------------------
self_sym_sizes: (36, 8, 8)"]
	140067041731312 -> 140067041731072
	140067041731312 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140067041731360 -> 140067041731312
	140067041731360 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 8, 64)"]
	140067041731456 -> 140067041731360
	140067041731456 [label=CloneBackward0]
	140067041731648 -> 140067041731456
	140067041731648 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 8, 64)"]
	140067041731792 -> 140067041731648
	140067041731792 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041732032 -> 140067041731792
	140067041732032 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 8, 768)"]
	140067041732128 -> 140067041732032
	140067041732128 [label="ViewBackward0
-------------------------
self_sym_sizes: (24, 768)"]
	140067041732224 -> 140067041732128
	140067041732224 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (24, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041732560 -> 140067041732224
	140066852647744 [label="q_encoder.encoder.text_encoding_layer.10.attention.self.query.bias
 (768)" fillcolor=lightblue]
	140066852647744 -> 140067041732560
	140067041732560 [label=AccumulateGrad]
	140067041732416 -> 140067041732224
	140067041732416 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 8, 768)"]
	140067041726848 -> 140067041732416
	140067041726848 [label="ViewBackward0
------------------------------
self_sym_sizes: (3, 1, 8, 768)"]
	140067041732992 -> 140067041726848
	140067041732992 [label="AsStridedBackward0
------------------------------------
size          :       (3, 1, 8, 768)
storage_offset:                    0
stride        : (6144, 6144, 768, 1)"]
	140067041733136 -> 140067041732992
	140067041733136 [label=CopySlices]
	140067054782448 -> 140067041733136
	140067041733424 -> 140067041733136
	140067041733424 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 1, 768)
start         :                   0
step          :                   1"]
	140067041733280 -> 140067041733424
	140067041733280 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:         (3, 2, 768)
start         :                   1
step          :                   1"]
	140067041734144 -> 140067041733280
	140067041734144 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:         (3, 2, 768)
start         :                   0
step          :                   1"]
	140066819359168 -> 140067041734144
	140067041731504 -> 140067041732224
	140067041731504 [label=TBackward0]
	140067041732704 -> 140067041731504
	140066852647664 [label="q_encoder.encoder.text_encoding_layer.10.attention.self.query.weight
 (768, 768)" fillcolor=lightblue]
	140066852647664 -> 140067041732704
	140067041732704 [label=AccumulateGrad]
	140067041731024 -> 140067041731312
	140067041731024 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 64, 8)"]
	140067041731120 -> 140067041731024
	140067041731120 [label=CloneBackward0]
	140067041732176 -> 140067041731120
	140067041732176 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 64, 8)"]
	140067041732080 -> 140067041732176
	140067041732080 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551615
dim1: 18446744073709551614"]
	140067041733376 -> 140067041732080
	140067041733376 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041734240 -> 140067041733376
	140067041734240 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 8, 768)"]
	140067041732320 -> 140067041734240
	140067041732320 [label="ViewBackward0
-------------------------
self_sym_sizes: (24, 768)"]
	140067041733184 -> 140067041732320
	140067041733184 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (24, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041734336 -> 140067041733184
	140066852648704 [label="q_encoder.encoder.text_encoding_layer.10.attention.self.key.bias
 (768)" fillcolor=lightblue]
	140066852648704 -> 140067041734336
	140067041734336 [label=AccumulateGrad]
	140067041734432 -> 140067041733184
	140067041734432 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 8, 768)"]
	140067041726848 -> 140067041734432
	140067041731552 -> 140067041733184
	140067041731552 [label=TBackward0]
	140067041734528 -> 140067041731552
	140066852648624 [label="q_encoder.encoder.text_encoding_layer.10.attention.self.key.weight
 (768, 768)" fillcolor=lightblue]
	140066852648624 -> 140067041734528
	140067041734528 [label=AccumulateGrad]
	140067041729776 -> 140067041729968
	140067041729776 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 8, 64)"]
	140067041730160 -> 140067041729776
	140067041730160 [label=CloneBackward0]
	140067041730544 -> 140067041730160
	140067041730544 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 8, 64)"]
	140067041730688 -> 140067041730544
	140067041730688 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041731840 -> 140067041730688
	140067041731840 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 8, 768)"]
	140067041733040 -> 140067041731840
	140067041733040 [label="ViewBackward0
-------------------------
self_sym_sizes: (24, 768)"]
	140067041733568 -> 140067041733040
	140067041733568 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (24, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041734192 -> 140067041733568
	140066852648864 [label="q_encoder.encoder.text_encoding_layer.10.attention.self.value.bias
 (768)" fillcolor=lightblue]
	140066852648864 -> 140067041734192
	140067041734192 [label=AccumulateGrad]
	140067041732800 -> 140067041733568
	140067041732800 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 8, 768)"]
	140067041726848 -> 140067041732800
	140067041730352 -> 140067041733568
	140067041730352 [label=TBackward0]
	140067041734624 -> 140067041730352
	140066852648784 [label="q_encoder.encoder.text_encoding_layer.10.attention.self.value.weight
 (768, 768)" fillcolor=lightblue]
	140066852648784 -> 140067041734624
	140067041734624 [label=AccumulateGrad]
	140067041727904 -> 140067041729008
	140067041727904 [label=TBackward0]
	140067041729152 -> 140067041727904
	140066852646624 [label="q_encoder.encoder.text_encoding_layer.10.attention.output.dense.weight
 (768, 768)" fillcolor=lightblue]
	140066852646624 -> 140067041729152
	140067041729152 [label=AccumulateGrad]
	140067041726848 -> 140067041724544
	140067041723536 -> 140066819365552
	140066852646864 [label="q_encoder.encoder.text_encoding_layer.10.attention.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	140066852646864 -> 140067041723536
	140067041723536 [label=AccumulateGrad]
	140067041724928 -> 140066819365552
	140066852646784 [label="q_encoder.encoder.text_encoding_layer.10.attention.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	140066852646784 -> 140067041724928
	140067041724928 [label=AccumulateGrad]
	140067041722528 -> 140066819368480
	140067041722528 [label=TBackward0]
	140067041727952 -> 140067041722528
	140066852646224 [label="q_encoder.encoder.text_encoding_layer.10.intermediate.dense.weight
 (3072, 768)" fillcolor=lightblue]
	140066852646224 -> 140067041727952
	140067041727952 [label=AccumulateGrad]
	140067041684944 -> 140066819372416
	140067041684944 [label=TBackward0]
	140066819370016 -> 140067041684944
	140066852646944 [label="q_encoder.encoder.text_encoding_layer.10.output.dense.weight
 (768, 3072)" fillcolor=lightblue]
	140066852646944 -> 140066819370016
	140066819370016 [label=AccumulateGrad]
	140066819365552 -> 140066819373232
	140066819362192 -> 140066819364016
	140066852646544 [label="q_encoder.encoder.text_encoding_layer.10.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	140066852646544 -> 140066819362192
	140066819362192 [label=AccumulateGrad]
	140066819364976 -> 140066819364016
	140066852646384 [label="q_encoder.encoder.text_encoding_layer.10.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	140066852646384 -> 140066819364976
	140066819364976 [label=AccumulateGrad]
	140066819358880 -> 140066819371216
	140066819358880 [label=TBackward0]
	140067041687008 -> 140066819358880
	140066904417504 [label="q_encoder.encoder.information_exchanging_layer.10.attention.self.query.weight
 (768, 768)" fillcolor=lightblue]
	140066904417504 -> 140067041687008
	140067041687008 [label=AccumulateGrad]
	140066819370496 -> 140066819360512
	140066819370496 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 64, 2)"]
	140066819359120 -> 140066819370496
	140066819359120 [label=CloneBackward0]
	140066819373088 -> 140066819359120
	140066819373088 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 64, 2)"]
	140066819363056 -> 140066819373088
	140066819363056 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551615
dim1: 18446744073709551614"]
	140066819359744 -> 140066819363056
	140066819359744 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140066819364304 -> 140066819359744
	140066819364304 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140066819362048 -> 140066819364304
	140066819362048 [label="ViewBackward0
------------------------
self_sym_sizes: (6, 768)"]
	140066819370784 -> 140066819362048
	140066819370784 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (6, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140066819364928 -> 140066819370784
	140066904417744 [label="q_encoder.encoder.information_exchanging_layer.10.attention.self.key.bias
 (768)" fillcolor=lightblue]
	140066904417744 -> 140066819364928
	140066819364928 [label=AccumulateGrad]
	140066819371312 -> 140066819370784
	140066819371312 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140066688174256 -> 140066819371312
	140066819367136 -> 140066819370784
	140066819367136 [label=TBackward0]
	140066819374432 -> 140066819367136
	140066904417664 [label="q_encoder.encoder.information_exchanging_layer.10.attention.self.key.weight
 (768, 768)" fillcolor=lightblue]
	140066904417664 -> 140066819374432
	140066819374432 [label=AccumulateGrad]
	140066688177568 -> 140066688177088
	140066688177568 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 2, 64)"]
	140066688173536 -> 140066688177568
	140066688173536 [label=CloneBackward0]
	140066819369728 -> 140066688173536
	140066819369728 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 2, 64)"]
	140066819359072 -> 140066819369728
	140066819359072 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140066819364544 -> 140066819359072
	140066819364544 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140066819370400 -> 140066819364544
	140066819370400 [label="ViewBackward0
------------------------
self_sym_sizes: (6, 768)"]
	140066819358976 -> 140066819370400
	140066819358976 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (6, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140066819374000 -> 140066819358976
	140066904417904 [label="q_encoder.encoder.information_exchanging_layer.10.attention.self.value.bias
 (768)" fillcolor=lightblue]
	140066904417904 -> 140066819374000
	140066819374000 [label=AccumulateGrad]
	140066819373856 -> 140066819358976
	140066819373856 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140066688174256 -> 140066819373856
	140066819361232 -> 140066819358976
	140066819361232 [label=TBackward0]
	140066819372368 -> 140066819361232
	140066904417824 [label="q_encoder.encoder.information_exchanging_layer.10.attention.self.value.weight
 (768, 768)" fillcolor=lightblue]
	140066904417824 -> 140066819372368
	140066819372368 [label=AccumulateGrad]
	140066688172096 -> 140066688182656
	140066688172096 [label=TBackward0]
	140066688172912 -> 140066688172096
	140066904417984 [label="q_encoder.encoder.information_exchanging_layer.10.attention.output.dense.weight
 (768, 768)" fillcolor=lightblue]
	140066904417984 -> 140066688172912
	140066688172912 [label=AccumulateGrad]
	140066688174256 -> 140066688173392
	140066688172816 -> 140066688178720
	140066904418224 [label="q_encoder.encoder.information_exchanging_layer.10.attention.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	140066904418224 -> 140066688172816
	140066688172816 [label=AccumulateGrad]
	140066688172864 -> 140066688178720
	140066904418144 [label="q_encoder.encoder.information_exchanging_layer.10.attention.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	140066904418144 -> 140066688172864
	140066688172864 [label=AccumulateGrad]
	140066688172432 -> 140066688176560
	140066688172432 [label=TBackward0]
	140066688172528 -> 140066688172432
	140067041376240 [label="q_encoder.encoder.information_exchanging_layer.10.intermediate.dense.weight
 (3072, 768)" fillcolor=lightblue]
	140067041376240 -> 140066688172528
	140066688172528 [label=AccumulateGrad]
	140066688177376 -> 140066688177280
	140066688177376 [label=TBackward0]
	140066688177760 -> 140066688177376
	140066904418464 [label="q_encoder.encoder.information_exchanging_layer.10.output.dense.weight
 (768, 3072)" fillcolor=lightblue]
	140066904418464 -> 140066688177760
	140066688177760 [label=AccumulateGrad]
	140066688178720 -> 140066688178240
	140066688177472 -> 140066688180496
	140066904418704 [label="q_encoder.encoder.information_exchanging_layer.10.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	140066904418704 -> 140066688177472
	140066688177472 [label=AccumulateGrad]
	140066688177952 -> 140066688180496
	140066904418624 [label="q_encoder.encoder.information_exchanging_layer.10.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	140066904418624 -> 140066688177952
	140066688177952 [label=AccumulateGrad]
	140066688180400 -> 140066688039616
	140066688180400 [label=CloneBackward0]
	140066688179296 -> 140066688180400
	140066688179296 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 1, 768)
start         :                   0
step          :                   1"]
	140066688173296 -> 140066688179296
	140066688173296 [label="SelectBackward0
------------------------------
dim           :              2
index         :              1
self_sym_sizes: (3, 1, 8, 768)"]
	140066688178864 -> 140066688173296
	140066688178864 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:      (3, 1, 8, 768)
start         :                   0
step          :                   1"]
	140066688177184 -> 140066688178864
	140066688177184 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:      (3, 1, 8, 768)
start         :                   0
step          :                   1"]
	140066688181120 -> 140066688177184
	140066688181120 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 8, 768)"]
	140066688175360 -> 140066688181120
	140066688175360 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140066688179104 -> 140066688175360
	140066688179104 [label="AddBackward0
------------
alpha: 1"]
	140066688182464 -> 140066688179104
	140066688182464 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140066688177856 -> 140066688182464
	140066688177856 [label="ViewBackward0
-------------------------
self_sym_sizes: (24, 768)"]
	140066688177040 -> 140066688177856
	140066688177040 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :     (24, 3072)
mat1_sym_strides:      (3072, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (3072, 768)
mat2_sym_strides:      (1, 3072)"]
	140066819361616 -> 140066688177040
	140066852646464 [label="q_encoder.encoder.text_encoding_layer.11.output.dense.bias
 (768)" fillcolor=lightblue]
	140066852646464 -> 140066819361616
	140066819361616 [label=AccumulateGrad]
	140066819371408 -> 140066688177040
	140066819371408 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 8, 3072)"]
	140066819371696 -> 140066819371408
	140066819371696 [label="GeluBackward0
---------------------------
approximate:           none
self       : [saved tensor]"]
	140067041728096 -> 140066819371696
	140067041728096 [label="ViewBackward0
--------------------------
self_sym_sizes: (24, 3072)"]
	140067041723584 -> 140067041728096
	140067041723584 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (24, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (768, 3072)
mat2_sym_strides:       (1, 768)"]
	140067041729680 -> 140067041723584
	140066852645024 [label="q_encoder.encoder.text_encoding_layer.11.intermediate.dense.bias
 (3072)" fillcolor=lightblue]
	140066852645024 -> 140067041729680
	140067041729680 [label=AccumulateGrad]
	140067041729536 -> 140067041723584
	140067041729536 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 8, 768)"]
	140066688173728 -> 140067041729536
	140066688173728 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140067041730448 -> 140066688173728
	140067041730448 [label="AddBackward0
------------
alpha: 1"]
	140067041731936 -> 140067041730448
	140067041731936 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041730256 -> 140067041731936
	140067041730256 [label="ViewBackward0
-------------------------
self_sym_sizes: (24, 768)"]
	140067041734720 -> 140067041730256
	140067041734720 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (24, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041725456 -> 140067041734720
	140066852645664 [label="q_encoder.encoder.text_encoding_layer.11.attention.output.dense.bias
 (768)" fillcolor=lightblue]
	140066852645664 -> 140067041725456
	140067041725456 [label=AccumulateGrad]
	140067041733472 -> 140067041734720
	140067041733472 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 8, 768)"]
	140067041734864 -> 140067041733472
	140067041734864 [label="ViewBackward0
------------------------------
self_sym_sizes: (3, 8, 12, 64)"]
	140067041734768 -> 140067041734864
	140067041734768 [label=CloneBackward0]
	140067041734048 -> 140067041734768
	140067041734048 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041735200 -> 140067041734048
	140067041735200 [label="UnsafeViewBackward0
---------------------------
self_sym_sizes: (36, 8, 64)"]
	140067041735440 -> 140067041735200
	140067041735440 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140067041735584 -> 140067041735440
	140067041735584 [label="ViewBackward0
-----------------------------
self_sym_sizes: (3, 12, 8, 8)"]
	140067041735680 -> 140067041735584
	140067041735680 [label="ExpandBackward0
-----------------------------
self_sym_sizes: (3, 12, 8, 8)"]
	140067041735824 -> 140067041735680
	140067041735824 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041735968 -> 140067041735824
	140067041735968 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140067041736256 -> 140067041735968
	140067041736256 [label="AddBackward0
------------
alpha: 1"]
	140067041736352 -> 140067041736256
	140067041736352 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041736544 -> 140067041736352
	140067041736544 [label="UnsafeViewBackward0
--------------------------
self_sym_sizes: (36, 8, 8)"]
	140067041736736 -> 140067041736544
	140067041736736 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140067041736832 -> 140067041736736
	140067041736832 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 8, 64)"]
	140067041736880 -> 140067041736832
	140067041736880 [label=CloneBackward0]
	140067041737072 -> 140067041736880
	140067041737072 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 8, 64)"]
	140067041737264 -> 140067041737072
	140067041737264 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041737504 -> 140067041737264
	140067041737504 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 8, 768)"]
	140067041737648 -> 140067041737504
	140067041737648 [label="ViewBackward0
-------------------------
self_sym_sizes: (24, 768)"]
	140067041737840 -> 140067041737648
	140067041737840 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (24, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041738032 -> 140067041737840
	140066852641184 [label="q_encoder.encoder.text_encoding_layer.11.attention.self.query.bias
 (768)" fillcolor=lightblue]
	140066852641184 -> 140067041738032
	140067041738032 [label=AccumulateGrad]
	140067041737936 -> 140067041737840
	140067041737936 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 8, 768)"]
	140067041732752 -> 140067041737936
	140067041732752 [label="ViewBackward0
------------------------------
self_sym_sizes: (3, 1, 8, 768)"]
	140067041738464 -> 140067041732752
	140067041738464 [label="AsStridedBackward0
------------------------------------
size          :       (3, 1, 8, 768)
storage_offset:                    0
stride        : (6144, 6144, 768, 1)"]
	140067041738560 -> 140067041738464
	140067041738560 [label=CopySlices]
	140066819364016 -> 140067041738560
	140067041738704 -> 140067041738560
	140067041738704 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 1, 768)
start         :                   0
step          :                   1"]
	140067041738512 -> 140067041738704
	140067041738512 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:         (3, 2, 768)
start         :                   1
step          :                   1"]
	140067042034304 -> 140067041738512
	140067042034304 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:         (3, 2, 768)
start         :                   0
step          :                   1"]
	140066688180496 -> 140067042034304
	140067041736976 -> 140067041737840
	140067041736976 [label=TBackward0]
	140067041738080 -> 140067041736976
	140066852646144 [label="q_encoder.encoder.text_encoding_layer.11.attention.self.query.weight
 (768, 768)" fillcolor=lightblue]
	140066852646144 -> 140067041738080
	140067041738080 [label=AccumulateGrad]
	140067041736400 -> 140067041736736
	140067041736400 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 64, 8)"]
	140067041736448 -> 140067041736400
	140067041736448 [label=CloneBackward0]
	140067041737792 -> 140067041736448
	140067041737792 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 64, 8)"]
	140067041737600 -> 140067041737792
	140067041737600 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551615
dim1: 18446744073709551614"]
	140067041738272 -> 140067041737600
	140067041738272 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041737168 -> 140067041738272
	140067041737168 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 8, 768)"]
	140067042034256 -> 140067041737168
	140067042034256 [label="ViewBackward0
-------------------------
self_sym_sizes: (24, 768)"]
	140067042033824 -> 140067042034256
	140067042033824 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (24, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067042034496 -> 140067042033824
	140066852646064 [label="q_encoder.encoder.text_encoding_layer.11.attention.self.key.bias
 (768)" fillcolor=lightblue]
	140066852646064 -> 140067042034496
	140067042034496 [label=AccumulateGrad]
	140067042034592 -> 140067042033824
	140067042034592 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 8, 768)"]
	140067041732752 -> 140067042034592
	140067042034448 -> 140067042033824
	140067042034448 [label=TBackward0]
	140067042034736 -> 140067042034448
	140066852645984 [label="q_encoder.encoder.text_encoding_layer.11.attention.self.key.weight
 (768, 768)" fillcolor=lightblue]
	140066852645984 -> 140067042034736
	140067042034736 [label=AccumulateGrad]
	140067041735392 -> 140067041735440
	140067041735392 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 8, 64)"]
	140067041735632 -> 140067041735392
	140067041735632 [label=CloneBackward0]
	140067041736064 -> 140067041735632
	140067041736064 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 8, 64)"]
	140067041736160 -> 140067041736064
	140067041736160 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041737456 -> 140067041736160
	140067041737456 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 8, 768)"]
	140067041738656 -> 140067041737456
	140067041738656 [label="ViewBackward0
-------------------------
self_sym_sizes: (24, 768)"]
	140067041737984 -> 140067041738656
	140067041737984 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (24, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041735776 -> 140067041737984
	140066852645904 [label="q_encoder.encoder.text_encoding_layer.11.attention.self.value.bias
 (768)" fillcolor=lightblue]
	140066852645904 -> 140067041735776
	140067041735776 [label=AccumulateGrad]
	140067042033872 -> 140067041737984
	140067042033872 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 8, 768)"]
	140067041732752 -> 140067042033872
	140067042034400 -> 140067041737984
	140067042034400 [label=TBackward0]
	140067042034784 -> 140067042034400
	140066852645824 [label="q_encoder.encoder.text_encoding_layer.11.attention.self.value.weight
 (768, 768)" fillcolor=lightblue]
	140066852645824 -> 140067042034784
	140067042034784 [label=AccumulateGrad]
	140067041733712 -> 140067041734720
	140067041733712 [label=TBackward0]
	140067041734816 -> 140067041733712
	140066852645584 [label="q_encoder.encoder.text_encoding_layer.11.attention.output.dense.weight
 (768, 768)" fillcolor=lightblue]
	140066852645584 -> 140067041734816
	140067041734816 [label=AccumulateGrad]
	140067041732752 -> 140067041730448
	140067041729344 -> 140066688173728
	140066852638224 [label="q_encoder.encoder.text_encoding_layer.11.attention.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	140066852638224 -> 140067041729344
	140067041729344 [label=AccumulateGrad]
	140067041730736 -> 140066688173728
	140066852645744 [label="q_encoder.encoder.text_encoding_layer.11.attention.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	140066852645744 -> 140067041730736
	140067041730736 [label=AccumulateGrad]
	140067041728048 -> 140067041723584
	140067041728048 [label=TBackward0]
	140067041733808 -> 140067041728048
	140066852636944 [label="q_encoder.encoder.text_encoding_layer.11.intermediate.dense.weight
 (3072, 768)" fillcolor=lightblue]
	140066852636944 -> 140067041733808
	140067041733808 [label=AccumulateGrad]
	140066819374816 -> 140066688177040
	140066819374816 [label=TBackward0]
	140067041723488 -> 140066819374816
	140066852637424 [label="q_encoder.encoder.text_encoding_layer.11.output.dense.weight
 (768, 3072)" fillcolor=lightblue]
	140066852637424 -> 140067041723488
	140067041723488 [label=AccumulateGrad]
	140066688173728 -> 140066688179104
	140066688173440 -> 140066688175360
	140066852645184 [label="q_encoder.encoder.text_encoding_layer.11.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	140066852645184 -> 140066688173440
	140066688173440 [label=AccumulateGrad]
	140066688179872 -> 140066688175360
	140066852644944 [label="q_encoder.encoder.text_encoding_layer.11.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	140066852644944 -> 140066688179872
	140066688179872 [label=AccumulateGrad]
	140066688179392 -> 140066688180016
	140066688179392 [label=TBackward0]
	140066688172288 -> 140066688179392
	140066904418304 [label="q_encoder.encoder.information_exchanging_layer.11.attention.self.query.weight
 (768, 768)" fillcolor=lightblue]
	140066904418304 -> 140066688172288
	140066688172288 [label=AccumulateGrad]
	140066688175840 -> 140066688178960
	140066688175840 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 64, 2)"]
	140066688174064 -> 140066688175840
	140066688174064 [label=CloneBackward0]
	140066688174880 -> 140066688174064
	140066688174880 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 64, 2)"]
	140066688179584 -> 140066688174880
	140066688179584 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551615
dim1: 18446744073709551614"]
	140066688177904 -> 140066688179584
	140066688177904 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140066688172480 -> 140066688177904
	140066688172480 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140066688173632 -> 140066688172480
	140066688173632 [label="ViewBackward0
------------------------
self_sym_sizes: (6, 768)"]
	140066688173056 -> 140066688173632
	140066688173056 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (6, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140066688174592 -> 140066688173056
	140066904418944 [label="q_encoder.encoder.information_exchanging_layer.11.attention.self.key.bias
 (768)" fillcolor=lightblue]
	140066904418944 -> 140066688174592
	140066688174592 [label=AccumulateGrad]
	140066688179344 -> 140066688173056
	140066688179344 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140066688039616 -> 140066688179344
	140066688175408 -> 140066688173056
	140066688175408 [label=TBackward0]
	140066819370928 -> 140066688175408
	140066904418864 [label="q_encoder.encoder.information_exchanging_layer.11.attention.self.key.weight
 (768, 768)" fillcolor=lightblue]
	140066904418864 -> 140066819370928
	140066819370928 [label=AccumulateGrad]
	140067065210448 -> 140067053486864
	140067065210448 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 2, 64)"]
	140067053393408 -> 140067065210448
	140067053393408 [label=CloneBackward0]
	140067053397488 -> 140067053393408
	140067053397488 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 2, 64)"]
	140066688175696 -> 140067053397488
	140066688175696 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140066688174784 -> 140066688175696
	140066688174784 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140066688179728 -> 140066688174784
	140066688179728 [label="ViewBackward0
------------------------
self_sym_sizes: (6, 768)"]
	140066819371936 -> 140066688179728
	140066819371936 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (6, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140066688176080 -> 140066819371936
	140066904419104 [label="q_encoder.encoder.information_exchanging_layer.11.attention.self.value.bias
 (768)" fillcolor=lightblue]
	140066904419104 -> 140066688176080
	140066688176080 [label=AccumulateGrad]
	140066688179632 -> 140066819371936
	140066688179632 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 2, 768)"]
	140066688039616 -> 140066688179632
	140066688173152 -> 140066819371936
	140066688173152 [label=TBackward0]
	140067041729296 -> 140066688173152
	140066904419024 [label="q_encoder.encoder.information_exchanging_layer.11.attention.self.value.weight
 (768, 768)" fillcolor=lightblue]
	140066904419024 -> 140067041729296
	140067041729296 [label=AccumulateGrad]
	140067054845712 -> 140066688028528
	140067054845712 [label=TBackward0]
	140067053502032 -> 140067054845712
	140066904419184 [label="q_encoder.encoder.information_exchanging_layer.11.attention.output.dense.weight
 (768, 768)" fillcolor=lightblue]
	140066904419184 -> 140067053502032
	140067053502032 [label=AccumulateGrad]
	140066688039616 -> 140066688031024
	140066688027136 -> 140067053472544
	140066904419424 [label="q_encoder.encoder.information_exchanging_layer.11.attention.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	140066904419424 -> 140066688027136
	140066688027136 [label=AccumulateGrad]
	140066688028096 -> 140067053472544
	140066904419344 [label="q_encoder.encoder.information_exchanging_layer.11.attention.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	140066904419344 -> 140066688028096
	140066688028096 [label=AccumulateGrad]
	140066688039328 -> 140066688039184
	140066688039328 [label=TBackward0]
	140066688026800 -> 140066688039328
	140066904419584 [label="q_encoder.encoder.information_exchanging_layer.11.intermediate.dense.weight
 (3072, 768)" fillcolor=lightblue]
	140066904419584 -> 140066688026800
	140066688026800 [label=AccumulateGrad]
	140066688031120 -> 140066688038512
	140066688031120 [label=TBackward0]
	140066688029152 -> 140066688031120
	140066904419744 [label="q_encoder.encoder.information_exchanging_layer.11.output.dense.weight
 (768, 3072)" fillcolor=lightblue]
	140066904419744 -> 140066688029152
	140066688029152 [label=AccumulateGrad]
	140067053472544 -> 140067053483488
	140067053470240 -> 140067053485216
	140066904419984 [label="q_encoder.encoder.information_exchanging_layer.11.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	140066904419984 -> 140067053470240
	140067053470240 [label=AccumulateGrad]
	140067053471104 -> 140067053485216
	140066904419904 [label="q_encoder.encoder.information_exchanging_layer.11.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	140066904419904 -> 140067053471104
	140067053471104 [label=AccumulateGrad]
	140067053471152 -> 140067948731104
	140067053471152 [label="TransposeBackward0
------------------
dim0: 0
dim1: 1"]
	140067018948416 -> 140067053471152
	140067018948416 [label="SqueezeBackward1
---------------------------
dim           :           1
self_sym_sizes: (3, 1, 768)"]
	140067053431648 -> 140067018948416
	140067053431648 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 1, 768)
start         :                   0
step          :                   1"]
	140067053471920 -> 140067053431648
	140067053471920 [label="SliceBackward0
---------------------------
dim           :           1
end           :           1
self_sym_sizes: (3, 4, 768)
start         :           0
step          :           1"]
	140067054845328 -> 140067053471920
	140067054845328 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:         (3, 4, 768)
start         :                   0
step          :                   1"]
	140066688028960 -> 140067054845328
	140066688028960 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140067053486144 -> 140066688028960
	140067053486144 [label="AddBackward0
------------
alpha: 1"]
	140067053498096 -> 140067053486144
	140067053498096 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067065208576 -> 140067053498096
	140067065208576 [label="ViewBackward0
-------------------------
self_sym_sizes: (12, 768)"]
	140066688027184 -> 140067065208576
	140066688027184 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :     (12, 3072)
mat1_sym_strides:      (3072, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (3072, 768)
mat2_sym_strides:      (1, 3072)"]
	140066688027280 -> 140066688027184
	140066688036112 -> 140066688027184
	140066688036112 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 4, 3072)"]
	140066688179824 -> 140066688036112
	140066688179824 [label="GeluBackward0
---------------------------
approximate:           none
self       : [saved tensor]"]
	140066688175312 -> 140066688179824
	140066688175312 [label="ViewBackward0
--------------------------
self_sym_sizes: (12, 3072)"]
	140067053394368 -> 140066688175312
	140067053394368 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (12, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (768, 3072)
mat2_sym_strides:       (1, 768)"]
	140066688030640 -> 140067053394368
	140066688176512 -> 140067053394368
	140066688176512 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 4, 768)"]
	140067053499392 -> 140066688176512
	140067053499392 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140067041735344 -> 140067053499392
	140067041735344 [label="AddBackward0
------------
alpha: 1"]
	140067041735920 -> 140067041735344
	140067041735920 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041729392 -> 140067041735920
	140067041729392 [label="ViewBackward0
-------------------------
self_sym_sizes: (12, 768)"]
	140067041736640 -> 140067041729392
	140067041736640 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (12, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140066688029392 -> 140067041736640
	140067041735488 -> 140067041736640
	140067041735488 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 4, 768)"]
	140067041738176 -> 140067041735488
	140067041738176 [label="ViewBackward0
------------------------------
self_sym_sizes: (3, 4, 12, 64)"]
	140067041737312 -> 140067041738176
	140067041737312 [label=CloneBackward0]
	140067042034208 -> 140067041737312
	140067042034208 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067042034160 -> 140067042034208
	140067042034160 [label="UnsafeViewBackward0
---------------------------
self_sym_sizes: (36, 4, 64)"]
	140067042034928 -> 140067042034160
	140067042034928 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140067042035024 -> 140067042034928
	140067042035024 [label="ViewBackward0
-----------------------------
self_sym_sizes: (3, 12, 4, 4)"]
	140067042035120 -> 140067042035024
	140067042035120 [label="ExpandBackward0
-----------------------------
self_sym_sizes: (3, 12, 4, 4)"]
	140067042035408 -> 140067042035120
	140067042035408 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067042035600 -> 140067042035408
	140067042035600 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140067042035840 -> 140067042035600
	140067042035840 [label="AddBackward0
------------
alpha: 1"]
	140067042035936 -> 140067042035840
	140067042035936 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067042036080 -> 140067042035936
	140067042036080 [label="UnsafeViewBackward0
--------------------------
self_sym_sizes: (36, 4, 4)"]
	140067042036272 -> 140067042036080
	140067042036272 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140067042036320 -> 140067042036272
	140067042036320 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 4, 64)"]
	140067042036368 -> 140067042036320
	140067042036368 [label=CloneBackward0]
	140067042036560 -> 140067042036368
	140067042036560 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 4, 64)"]
	140067042036608 -> 140067042036560
	140067042036608 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067042036752 -> 140067042036608
	140067042036752 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 4, 768)"]
	140067042036944 -> 140067042036752
	140067042036944 [label="ViewBackward0
-------------------------
self_sym_sizes: (12, 768)"]
	140067042037040 -> 140067042036944
	140067042037040 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (12, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140066688180304 -> 140067042037040
	140067042037136 -> 140067042037040
	140067042037136 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 4, 768)"]
	140067041734960 -> 140067042037136
	140067041734960 [label="CatBackward0
------------
dim: 1"]
	140067042037664 -> 140067041734960
	140067042037664 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 1, 768)
start         :                   0
step          :                   1"]
	140067042037760 -> 140067042037664
	140067042037760 [label="SliceBackward0
---------------------------
dim           :           1
end           :           1
self_sym_sizes: (3, 4, 768)
start         :           0
step          :           1"]
	140067042038384 -> 140067042037760
	140067042038384 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:         (3, 4, 768)
start         :                   0
step          :                   1"]
	140067042038624 -> 140067042038384
	140067042038624 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140067042038144 -> 140067042038624
	140067042038144 [label="AddBackward0
------------
alpha: 1"]
	140067042037472 -> 140067042038144
	140067042037472 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067042037808 -> 140067042037472
	140067042037808 [label="ViewBackward0
-------------------------
self_sym_sizes: (12, 768)"]
	140067042038864 -> 140067042037808
	140067042038864 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :     (12, 3072)
mat1_sym_strides:      (3072, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (3072, 768)
mat2_sym_strides:      (1, 3072)"]
	140066688178000 -> 140067042038864
	140067042038240 -> 140067042038864
	140067042038240 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 4, 3072)"]
	140067042039392 -> 140067042038240
	140067042039392 [label="GeluBackward0
---------------------------
approximate:           none
self       : [saved tensor]"]
	140067042039056 -> 140067042039392
	140067042039056 [label="ViewBackward0
--------------------------
self_sym_sizes: (12, 3072)"]
	140067042038960 -> 140067042039056
	140067042038960 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (12, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (768, 3072)
mat2_sym_strides:       (1, 768)"]
	140066688176608 -> 140067042038960
	140067042039776 -> 140067042038960
	140067042039776 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 4, 768)"]
	140067042038816 -> 140067042039776
	140067042038816 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140067042040208 -> 140067042038816
	140067042040208 [label="AddBackward0
------------
alpha: 1"]
	140067042040400 -> 140067042040208
	140067042040400 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067042039008 -> 140067042040400
	140067042039008 [label="ViewBackward0
-------------------------
self_sym_sizes: (12, 768)"]
	140067042040592 -> 140067042039008
	140067042040592 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (12, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140066688172384 -> 140067042040592
	140067042040304 -> 140067042040592
	140067042040304 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 4, 768)"]
	140067042040928 -> 140067042040304
	140067042040928 [label="ViewBackward0
------------------------------
self_sym_sizes: (3, 4, 12, 64)"]
	140067042040784 -> 140067042040928
	140067042040784 [label=CloneBackward0]
	140067042041168 -> 140067042040784
	140067042041168 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067042040640 -> 140067042041168
	140067042040640 [label="UnsafeViewBackward0
---------------------------
self_sym_sizes: (36, 4, 64)"]
	140067042041216 -> 140067042040640
	140067042041216 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140067042041312 -> 140067042041216
	140067042041312 [label="ViewBackward0
-----------------------------
self_sym_sizes: (3, 12, 4, 4)"]
	140067042041504 -> 140067042041312
	140067042041504 [label="ExpandBackward0
-----------------------------
self_sym_sizes: (3, 12, 4, 4)"]
	140067042041744 -> 140067042041504
	140067042041744 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067042041888 -> 140067042041744
	140067042041888 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140067042042416 -> 140067042041888
	140067042042416 [label="AddBackward0
------------
alpha: 1"]
	140067042042464 -> 140067042042416
	140067042042464 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067042042608 -> 140067042042464
	140067042042608 [label="UnsafeViewBackward0
--------------------------
self_sym_sizes: (36, 4, 4)"]
	140067042042800 -> 140067042042608
	140067042042800 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140067042042848 -> 140067042042800
	140067042042848 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 4, 64)"]
	140067042043040 -> 140067042042848
	140067042043040 [label=CloneBackward0]
	140067042043232 -> 140067042043040
	140067042043232 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 4, 64)"]
	140067042043280 -> 140067042043232
	140067042043280 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067042043616 -> 140067042043280
	140067042043616 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 4, 768)"]
	140067042043760 -> 140067042043616
	140067042043760 [label="ViewBackward0
-------------------------
self_sym_sizes: (12, 768)"]
	140067042043856 -> 140067042043760
	140067042043856 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (12, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140066819365840 -> 140067042043856
	140067042044240 -> 140067042043856
	140067042044240 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 4, 768)"]
	140067042040352 -> 140067042044240
	140067042040352 [label="CatBackward0
------------
dim: 1"]
	140067042044672 -> 140067042040352
	140067042044672 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 1, 768)
start         :                   0
step          :                   1"]
	140067042044720 -> 140067042044672
	140067042044720 [label="SliceBackward0
---------------------------
dim           :           1
end           :           1
self_sym_sizes: (3, 4, 768)
start         :           0
step          :           1"]
	140067042045296 -> 140067042044720
	140067042045296 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:         (3, 4, 768)
start         :                   0
step          :                   1"]
	140067042045536 -> 140067042045296
	140067042045536 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140067042045200 -> 140067042045536
	140067042045200 [label="AddBackward0
------------
alpha: 1"]
	140067042044432 -> 140067042045200
	140067042044432 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067042044816 -> 140067042044432
	140067042044816 [label="ViewBackward0
-------------------------
self_sym_sizes: (12, 768)"]
	140067042045776 -> 140067042044816
	140067042045776 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :     (12, 3072)
mat1_sym_strides:      (3072, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (3072, 768)
mat2_sym_strides:      (1, 3072)"]
	140066819363104 -> 140067042045776
	140067042045344 -> 140067042045776
	140067042045344 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 4, 3072)"]
	140067042046208 -> 140067042045344
	140067042046208 [label="GeluBackward0
---------------------------
approximate:           none
self       : [saved tensor]"]
	140067042046064 -> 140067042046208
	140067042046064 [label="ViewBackward0
--------------------------
self_sym_sizes: (12, 3072)"]
	140067042045968 -> 140067042046064
	140067042045968 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (12, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (768, 3072)
mat2_sym_strides:       (1, 768)"]
	140066819365792 -> 140067042045968
	140067042046496 -> 140067042045968
	140067042046496 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 4, 768)"]
	140067042045632 -> 140067042046496
	140067042045632 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140067042046928 -> 140067042045632
	140067042046928 [label="AddBackward0
------------
alpha: 1"]
	140067042047120 -> 140067042046928
	140067042047120 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067042046016 -> 140067042047120
	140067042046016 [label="ViewBackward0
-------------------------
self_sym_sizes: (12, 768)"]
	140067042047264 -> 140067042046016
	140067042047264 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (12, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140066819370112 -> 140067042047264
	140067042047024 -> 140067042047264
	140067042047024 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 4, 768)"]
	140067042047888 -> 140067042047024
	140067042047888 [label="ViewBackward0
------------------------------
self_sym_sizes: (3, 4, 12, 64)"]
	140067042047504 -> 140067042047888
	140067042047504 [label=CloneBackward0]
	140067042047984 -> 140067042047504
	140067042047984 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067042047312 -> 140067042047984
	140067042047312 [label="UnsafeViewBackward0
---------------------------
self_sym_sizes: (36, 4, 64)"]
	140067042048224 -> 140067042047312
	140067042048224 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140067042048512 -> 140067042048224
	140067042048512 [label="ViewBackward0
-----------------------------
self_sym_sizes: (3, 12, 4, 4)"]
	140067042048560 -> 140067042048512
	140067042048560 [label="ExpandBackward0
-----------------------------
self_sym_sizes: (3, 12, 4, 4)"]
	140067042048944 -> 140067042048560
	140067042048944 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067042049040 -> 140067042048944
	140067042049040 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140067042049472 -> 140067042049040
	140067042049472 [label="AddBackward0
------------
alpha: 1"]
	140067042049568 -> 140067042049472
	140067042049568 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067042049808 -> 140067042049568
	140067042049808 [label="UnsafeViewBackward0
--------------------------
self_sym_sizes: (36, 4, 4)"]
	140067042049952 -> 140067042049808
	140067042049952 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140067042048656 -> 140067042049952
	140067042048656 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 4, 64)"]
	140067042214048 -> 140067042048656
	140067042214048 [label=CloneBackward0]
	140067042214240 -> 140067042214048
	140067042214240 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 4, 64)"]
	140067042214336 -> 140067042214240
	140067042214336 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067042214624 -> 140067042214336
	140067042214624 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 4, 768)"]
	140067042214768 -> 140067042214624
	140067042214768 [label="ViewBackward0
-------------------------
self_sym_sizes: (12, 768)"]
	140067042214960 -> 140067042214768
	140067042214960 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (12, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067054793248 -> 140067042214960
	140067042215248 -> 140067042214960
	140067042215248 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 4, 768)"]
	140067042047072 -> 140067042215248
	140067042047072 [label="CatBackward0
------------
dim: 1"]
	140067042215536 -> 140067042047072
	140067042215536 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 1, 768)
start         :                   0
step          :                   1"]
	140067042215584 -> 140067042215536
	140067042215584 [label="SliceBackward0
---------------------------
dim           :           1
end           :           1
self_sym_sizes: (3, 4, 768)
start         :           0
step          :           1"]
	140067042216256 -> 140067042215584
	140067042216256 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:         (3, 4, 768)
start         :                   0
step          :                   1"]
	140067042216400 -> 140067042216256
	140067042216400 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140067042216064 -> 140067042216400
	140067042216064 [label="AddBackward0
------------
alpha: 1"]
	140067042215392 -> 140067042216064
	140067042215392 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067042215632 -> 140067042215392
	140067042215632 [label="ViewBackward0
-------------------------
self_sym_sizes: (12, 768)"]
	140067042216928 -> 140067042215632
	140067042216928 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :     (12, 3072)
mat1_sym_strides:      (3072, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (3072, 768)
mat2_sym_strides:      (1, 3072)"]
	140067054786768 -> 140067042216928
	140067042216304 -> 140067042216928
	140067042216304 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 4, 3072)"]
	140067042217312 -> 140067042216304
	140067042217312 [label="GeluBackward0
---------------------------
approximate:           none
self       : [saved tensor]"]
	140067042217120 -> 140067042217312
	140067042217120 [label="ViewBackward0
--------------------------
self_sym_sizes: (12, 3072)"]
	140067042216976 -> 140067042217120
	140067042216976 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (12, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (768, 3072)
mat2_sym_strides:       (1, 768)"]
	140067054789792 -> 140067042216976
	140067042217648 -> 140067042216976
	140067042217648 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 4, 768)"]
	140067042216592 -> 140067042217648
	140067042216592 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140067042218128 -> 140067042216592
	140067042218128 [label="AddBackward0
------------
alpha: 1"]
	140067042218416 -> 140067042218128
	140067042218416 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067042217024 -> 140067042218416
	140067042217024 [label="ViewBackward0
-------------------------
self_sym_sizes: (12, 768)"]
	140067042218608 -> 140067042217024
	140067042218608 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (12, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067054781296 -> 140067042218608
	140067042218272 -> 140067042218608
	140067042218272 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 4, 768)"]
	140067042219232 -> 140067042218272
	140067042219232 [label="ViewBackward0
------------------------------
self_sym_sizes: (3, 4, 12, 64)"]
	140067042218752 -> 140067042219232
	140067042218752 [label=CloneBackward0]
	140067042219424 -> 140067042218752
	140067042219424 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067042218848 -> 140067042219424
	140067042218848 [label="UnsafeViewBackward0
---------------------------
self_sym_sizes: (36, 4, 64)"]
	140067042219808 -> 140067042218848
	140067042219808 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140067042220000 -> 140067042219808
	140067042220000 [label="ViewBackward0
-----------------------------
self_sym_sizes: (3, 12, 4, 4)"]
	140067042220144 -> 140067042220000
	140067042220144 [label="ExpandBackward0
-----------------------------
self_sym_sizes: (3, 12, 4, 4)"]
	140067042220288 -> 140067042220144
	140067042220288 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067042220528 -> 140067042220288
	140067042220528 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140067042220768 -> 140067042220528
	140067042220768 [label="AddBackward0
------------
alpha: 1"]
	140067042220960 -> 140067042220768
	140067042220960 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067042221008 -> 140067042220960
	140067042221008 [label="UnsafeViewBackward0
--------------------------
self_sym_sizes: (36, 4, 4)"]
	140067042221200 -> 140067042221008
	140067042221200 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140067042221344 -> 140067042221200
	140067042221344 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 4, 64)"]
	140067042221392 -> 140067042221344
	140067042221392 [label=CloneBackward0]
	140067042221632 -> 140067042221392
	140067042221632 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 4, 64)"]
	140067042221728 -> 140067042221632
	140067042221728 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067042221968 -> 140067042221728
	140067042221968 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 4, 768)"]
	140067042222112 -> 140067042221968
	140067042222112 [label="ViewBackward0
-------------------------
self_sym_sizes: (12, 768)"]
	140067042222208 -> 140067042222112
	140067042222208 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (12, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140066879621920 -> 140067042222208
	140067042222352 -> 140067042222208
	140067042222352 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 4, 768)"]
	140067042218368 -> 140067042222352
	140067042218368 [label="CatBackward0
------------
dim: 1"]
	140067042222880 -> 140067042218368
	140067042222880 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 1, 768)
start         :                   0
step          :                   1"]
	140067042222928 -> 140067042222880
	140067042222928 [label="SliceBackward0
---------------------------
dim           :           1
end           :           1
self_sym_sizes: (3, 4, 768)
start         :           0
step          :           1"]
	140067042223600 -> 140067042222928
	140067042223600 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:         (3, 4, 768)
start         :                   0
step          :                   1"]
	140067042223792 -> 140067042223600
	140067042223792 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140067042223216 -> 140067042223792
	140067042223216 [label="AddBackward0
------------
alpha: 1"]
	140067042222544 -> 140067042223216
	140067042222544 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067042222976 -> 140067042222544
	140067042222976 [label="ViewBackward0
-------------------------
self_sym_sizes: (12, 768)"]
	140067042224272 -> 140067042222976
	140067042224272 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :     (12, 3072)
mat1_sym_strides:      (3072, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (3072, 768)
mat2_sym_strides:      (1, 3072)"]
	140066879623936 -> 140067042224272
	140067042223696 -> 140067042224272
	140067042223696 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 4, 3072)"]
	140067042224704 -> 140067042223696
	140067042224704 [label="GeluBackward0
---------------------------
approximate:           none
self       : [saved tensor]"]
	140067042224416 -> 140067042224704
	140067042224416 [label="ViewBackward0
--------------------------
self_sym_sizes: (12, 3072)"]
	140067042223984 -> 140067042224416
	140067042223984 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (12, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (768, 3072)
mat2_sym_strides:       (1, 768)"]
	140066879624416 -> 140067042223984
	140067042225088 -> 140067042223984
	140067042225088 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 4, 768)"]
	140067042223936 -> 140067042225088
	140067042223936 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140067042225472 -> 140067042223936
	140067042225472 [label="AddBackward0
------------
alpha: 1"]
	140067042225856 -> 140067042225472
	140067042225856 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067042224032 -> 140067042225856
	140067042224032 [label="ViewBackward0
-------------------------
self_sym_sizes: (12, 768)"]
	140067042226048 -> 140067042224032
	140067042226048 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (12, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140066879626672 -> 140067042226048
	140067042225664 -> 140067042226048
	140067042225664 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 4, 768)"]
	140067042226576 -> 140067042225664
	140067042226576 [label="ViewBackward0
------------------------------
self_sym_sizes: (3, 4, 12, 64)"]
	140067042226384 -> 140067042226576
	140067042226384 [label=CloneBackward0]
	140067042226816 -> 140067042226384
	140067042226816 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067042226096 -> 140067042226816
	140067042226096 [label="UnsafeViewBackward0
---------------------------
self_sym_sizes: (36, 4, 64)"]
	140067042227056 -> 140067042226096
	140067042227056 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140067042226960 -> 140067042227056
	140067042226960 [label="ViewBackward0
-----------------------------
self_sym_sizes: (3, 12, 4, 4)"]
	140067042227344 -> 140067042226960
	140067042227344 [label="ExpandBackward0
-----------------------------
self_sym_sizes: (3, 12, 4, 4)"]
	140067042227536 -> 140067042227344
	140067042227536 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067042227680 -> 140067042227536
	140067042227680 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140067042228016 -> 140067042227680
	140067042228016 [label="AddBackward0
------------
alpha: 1"]
	140067042228064 -> 140067042228016
	140067042228064 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067042228256 -> 140067042228064
	140067042228256 [label="UnsafeViewBackward0
--------------------------
self_sym_sizes: (36, 4, 4)"]
	140067042228448 -> 140067042228256
	140067042228448 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140067042228496 -> 140067042228448
	140067042228496 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 4, 64)"]
	140067042228544 -> 140067042228496
	140067042228544 [label=CloneBackward0]
	140067042228880 -> 140067042228544
	140067042228880 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 4, 64)"]
	140067042228928 -> 140067042228880
	140067042228928 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067042229168 -> 140067042228928
	140067042229168 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 4, 768)"]
	140067042229264 -> 140067042229168
	140067042229264 [label="ViewBackward0
-------------------------
self_sym_sizes: (12, 768)"]
	140067042229456 -> 140067042229264
	140067042229456 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (12, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140066879630608 -> 140067042229456
	140067042229696 -> 140067042229456
	140067042229696 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 4, 768)"]
	140067042225760 -> 140067042229696
	140067042225760 [label="CatBackward0
------------
dim: 1"]
	140067042230176 -> 140067042225760
	140067042230176 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 1, 768)
start         :                   0
step          :                   1"]
	140067042230224 -> 140067042230176
	140067042230224 [label="SliceBackward0
---------------------------
dim           :           1
end           :           1
self_sym_sizes: (3, 4, 768)
start         :           0
step          :           1"]
	140067042229600 -> 140067042230224
	140067042229600 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:         (3, 4, 768)
start         :                   0
step          :                   1"]
	140067041936192 -> 140067042229600
	140067041936192 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140067041935856 -> 140067041936192
	140067041935856 [label="AddBackward0
------------
alpha: 1"]
	140067041936000 -> 140067041935856
	140067041936000 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041935520 -> 140067041936000
	140067041935520 [label="ViewBackward0
-------------------------
self_sym_sizes: (12, 768)"]
	140067041936480 -> 140067041935520
	140067041936480 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :     (12, 3072)
mat1_sym_strides:      (3072, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (3072, 768)
mat2_sym_strides:      (1, 3072)"]
	140066879634016 -> 140067041936480
	140067041935424 -> 140067041936480
	140067041935424 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 4, 3072)"]
	140067041936912 -> 140067041935424
	140067041936912 [label="GeluBackward0
---------------------------
approximate:           none
self       : [saved tensor]"]
	140067041936768 -> 140067041936912
	140067041936768 [label="ViewBackward0
--------------------------
self_sym_sizes: (12, 3072)"]
	140067041936576 -> 140067041936768
	140067041936576 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (12, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (768, 3072)
mat2_sym_strides:       (1, 768)"]
	140066879634976 -> 140067041936576
	140067041937200 -> 140067041936576
	140067041937200 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 4, 768)"]
	140067041936336 -> 140067041937200
	140067041936336 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140067041937728 -> 140067041936336
	140067041937728 [label="AddBackward0
------------
alpha: 1"]
	140067041937920 -> 140067041937728
	140067041937920 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041936672 -> 140067041937920
	140067041936672 [label="ViewBackward0
-------------------------
self_sym_sizes: (12, 768)"]
	140067041938112 -> 140067041936672
	140067041938112 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (12, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140066907248992 -> 140067041938112
	140067041937824 -> 140067041938112
	140067041937824 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 4, 768)"]
	140067041938640 -> 140067041937824
	140067041938640 [label="ViewBackward0
------------------------------
self_sym_sizes: (3, 4, 12, 64)"]
	140067041938352 -> 140067041938640
	140067041938352 [label=CloneBackward0]
	140067041938832 -> 140067041938352
	140067041938832 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041938256 -> 140067041938832
	140067041938256 [label="UnsafeViewBackward0
---------------------------
self_sym_sizes: (36, 4, 64)"]
	140067041939024 -> 140067041938256
	140067041939024 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140067041939072 -> 140067041939024
	140067041939072 [label="ViewBackward0
-----------------------------
self_sym_sizes: (3, 12, 4, 4)"]
	140067041939312 -> 140067041939072
	140067041939312 [label="ExpandBackward0
-----------------------------
self_sym_sizes: (3, 12, 4, 4)"]
	140067041939504 -> 140067041939312
	140067041939504 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041939696 -> 140067041939504
	140067041939696 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140067041940032 -> 140067041939696
	140067041940032 [label="AddBackward0
------------
alpha: 1"]
	140067041940128 -> 140067041940032
	140067041940128 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041940320 -> 140067041940128
	140067041940320 [label="UnsafeViewBackward0
--------------------------
self_sym_sizes: (36, 4, 4)"]
	140067041940608 -> 140067041940320
	140067041940608 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140067041940752 -> 140067041940608
	140067041940752 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 4, 64)"]
	140067041940800 -> 140067041940752
	140067041940800 [label=CloneBackward0]
	140067041940992 -> 140067041940800
	140067041940992 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 4, 64)"]
	140067041941088 -> 140067041940992
	140067041941088 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041941328 -> 140067041941088
	140067041941328 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 4, 768)"]
	140067041941424 -> 140067041941328
	140067041941424 [label="ViewBackward0
-------------------------
self_sym_sizes: (12, 768)"]
	140067041941616 -> 140067041941424
	140067041941616 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (12, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140066907247744 -> 140067041941616
	140067041941760 -> 140067041941616
	140067041941760 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 4, 768)"]
	140067041937872 -> 140067041941760
	140067041937872 [label="CatBackward0
------------
dim: 1"]
	140067041942384 -> 140067041937872
	140067041942384 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 1, 768)
start         :                   0
step          :                   1"]
	140067041942432 -> 140067041942384
	140067041942432 [label="SliceBackward0
---------------------------
dim           :           1
end           :           1
self_sym_sizes: (3, 4, 768)
start         :           0
step          :           1"]
	140067041943008 -> 140067041942432
	140067041943008 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:         (3, 4, 768)
start         :                   0
step          :                   1"]
	140067041943248 -> 140067041943008
	140067041943248 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140067041942912 -> 140067041943248
	140067041942912 [label="AddBackward0
------------
alpha: 1"]
	140067041942000 -> 140067041942912
	140067041942000 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041942480 -> 140067041942000
	140067041942480 [label="ViewBackward0
-------------------------
self_sym_sizes: (12, 768)"]
	140067041943584 -> 140067041942480
	140067041943584 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :     (12, 3072)
mat1_sym_strides:      (3072, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (3072, 768)
mat2_sym_strides:      (1, 3072)"]
	140066907249808 -> 140067041943584
	140067041943056 -> 140067041943584
	140067041943056 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 4, 3072)"]
	140067041944016 -> 140067041943056
	140067041944016 [label="GeluBackward0
---------------------------
approximate:           none
self       : [saved tensor]"]
	140067041943728 -> 140067041944016
	140067041943728 [label="ViewBackward0
--------------------------
self_sym_sizes: (12, 3072)"]
	140067041943632 -> 140067041943728
	140067041943632 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (12, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (768, 3072)
mat2_sym_strides:       (1, 768)"]
	140066907250048 -> 140067041943632
	140067041944256 -> 140067041943632
	140067041944256 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 4, 768)"]
	140067041943392 -> 140067041944256
	140067041943392 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140067041944784 -> 140067041943392
	140067041944784 [label="AddBackward0
------------
alpha: 1"]
	140067041945024 -> 140067041944784
	140067041945024 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041943680 -> 140067041945024
	140067041943680 [label="ViewBackward0
-------------------------
self_sym_sizes: (12, 768)"]
	140067041945120 -> 140067041943680
	140067041945120 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (12, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140066907252352 -> 140067041945120
	140067041944880 -> 140067041945120
	140067041944880 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 4, 768)"]
	140067041945648 -> 140067041944880
	140067041945648 [label="ViewBackward0
------------------------------
self_sym_sizes: (3, 4, 12, 64)"]
	140067041945408 -> 140067041945648
	140067041945408 [label=CloneBackward0]
	140067041945792 -> 140067041945408
	140067041945792 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041945216 -> 140067041945792
	140067041945216 [label="UnsafeViewBackward0
---------------------------
self_sym_sizes: (36, 4, 64)"]
	140067041945984 -> 140067041945216
	140067041945984 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140067041946128 -> 140067041945984
	140067041946128 [label="ViewBackward0
-----------------------------
self_sym_sizes: (3, 12, 4, 4)"]
	140067041946272 -> 140067041946128
	140067041946272 [label="ExpandBackward0
-----------------------------
self_sym_sizes: (3, 12, 4, 4)"]
	140067041946464 -> 140067041946272
	140067041946464 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041946560 -> 140067041946464
	140067041946560 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140067041946944 -> 140067041946560
	140067041946944 [label="AddBackward0
------------
alpha: 1"]
	140067041946992 -> 140067041946944
	140067041946992 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041947088 -> 140067041946992
	140067041947088 [label="UnsafeViewBackward0
--------------------------
self_sym_sizes: (36, 4, 4)"]
	140067041947376 -> 140067041947088
	140067041947376 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140067041947472 -> 140067041947376
	140067041947472 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 4, 64)"]
	140067041947520 -> 140067041947472
	140067041947520 [label=CloneBackward0]
	140067041947760 -> 140067041947520
	140067041947760 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 4, 64)"]
	140067041947856 -> 140067041947760
	140067041947856 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041948048 -> 140067041947856
	140067041948048 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 4, 768)"]
	140067041948144 -> 140067041948048
	140067041948144 [label="ViewBackward0
-------------------------
self_sym_sizes: (12, 768)"]
	140067041948240 -> 140067041948144
	140067041948240 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (12, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140066907254752 -> 140067041948240
	140067041948384 -> 140067041948240
	140067041948384 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 4, 768)"]
	140067041944976 -> 140067041948384
	140067041944976 [label="CatBackward0
------------
dim: 1"]
	140067041948864 -> 140067041944976
	140067041948864 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 1, 768)
start         :                   0
step          :                   1"]
	140067041948912 -> 140067041948864
	140067041948912 [label="SliceBackward0
---------------------------
dim           :           1
end           :           1
self_sym_sizes: (3, 4, 768)
start         :           0
step          :           1"]
	140067041949248 -> 140067041948912
	140067041949248 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:         (3, 4, 768)
start         :                   0
step          :                   1"]
	140067041949584 -> 140067041949248
	140067041949584 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140067041949152 -> 140067041949584
	140067041949152 [label="AddBackward0
------------
alpha: 1"]
	140067041948672 -> 140067041949152
	140067041948672 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041948960 -> 140067041948672
	140067041948960 [label="ViewBackward0
-------------------------
self_sym_sizes: (12, 768)"]
	140067041949872 -> 140067041948960
	140067041949872 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :     (12, 3072)
mat1_sym_strides:      (3072, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (3072, 768)
mat2_sym_strides:      (1, 3072)"]
	140066487828928 -> 140067041949872
	140067041949440 -> 140067041949872
	140067041949440 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 4, 3072)"]
	140067041950256 -> 140067041949440
	140067041950256 [label="GeluBackward0
---------------------------
approximate:           none
self       : [saved tensor]"]
	140067041950064 -> 140067041950256
	140067041950064 [label="ViewBackward0
--------------------------
self_sym_sizes: (12, 3072)"]
	140067041949920 -> 140067041950064
	140067041949920 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (12, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (768, 3072)
mat2_sym_strides:       (1, 768)"]
	140066907014432 -> 140067041949920
	140067041950544 -> 140067041949920
	140067041950544 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 4, 768)"]
	140067041949728 -> 140067041950544
	140067041949728 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140067041950976 -> 140067041949728
	140067041950976 [label="AddBackward0
------------
alpha: 1"]
	140067041951216 -> 140067041950976
	140067041951216 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041950016 -> 140067041951216
	140067041950016 [label="ViewBackward0
-------------------------
self_sym_sizes: (12, 768)"]
	140067041951408 -> 140067041950016
	140067041951408 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (12, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140066907014960 -> 140067041951408
	140067041951072 -> 140067041951408
	140067041951072 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 4, 768)"]
	140067041951648 -> 140067041951072
	140067041951648 [label="ViewBackward0
------------------------------
self_sym_sizes: (3, 4, 12, 64)"]
	140067041951696 -> 140067041951648
	140067041951696 [label=CloneBackward0]
	140067041509440 -> 140067041951696
	140067041509440 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041509632 -> 140067041509440
	140067041509632 [label="UnsafeViewBackward0
---------------------------
self_sym_sizes: (36, 4, 64)"]
	140067041509872 -> 140067041509632
	140067041509872 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140067041510064 -> 140067041509872
	140067041510064 [label="ViewBackward0
-----------------------------
self_sym_sizes: (3, 12, 4, 4)"]
	140067041510208 -> 140067041510064
	140067041510208 [label="ExpandBackward0
-----------------------------
self_sym_sizes: (3, 12, 4, 4)"]
	140067041510448 -> 140067041510208
	140067041510448 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041510784 -> 140067041510448
	140067041510784 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140067041510976 -> 140067041510784
	140067041510976 [label="AddBackward0
------------
alpha: 1"]
	140067041511072 -> 140067041510976
	140067041511072 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041511168 -> 140067041511072
	140067041511168 [label="UnsafeViewBackward0
--------------------------
self_sym_sizes: (36, 4, 4)"]
	140067041511360 -> 140067041511168
	140067041511360 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140067041511408 -> 140067041511360
	140067041511408 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 4, 64)"]
	140067041511504 -> 140067041511408
	140067041511504 [label=CloneBackward0]
	140067041511696 -> 140067041511504
	140067041511696 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 4, 64)"]
	140067041511744 -> 140067041511696
	140067041511744 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041511984 -> 140067041511744
	140067041511984 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 4, 768)"]
	140067041512176 -> 140067041511984
	140067041512176 [label="ViewBackward0
-------------------------
self_sym_sizes: (12, 768)"]
	140067041512368 -> 140067041512176
	140067041512368 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (12, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140066907020000 -> 140067041512368
	140067041512656 -> 140067041512368
	140067041512656 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 4, 768)"]
	140067041951120 -> 140067041512656
	140067041951120 [label="CatBackward0
------------
dim: 1"]
	140067041513232 -> 140067041951120
	140067041513232 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 1, 768)
start         :                   0
step          :                   1"]
	140067041513280 -> 140067041513232
	140067041513280 [label="SliceBackward0
---------------------------
dim           :           1
end           :           1
self_sym_sizes: (3, 4, 768)
start         :           0
step          :           1"]
	140067041513952 -> 140067041513280
	140067041513952 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:         (3, 4, 768)
start         :                   0
step          :                   1"]
	140067041514096 -> 140067041513952
	140067041514096 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140067041513712 -> 140067041514096
	140067041513712 [label="AddBackward0
------------
alpha: 1"]
	140067041512944 -> 140067041513712
	140067041512944 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041513328 -> 140067041512944
	140067041513328 [label="ViewBackward0
-------------------------
self_sym_sizes: (12, 768)"]
	140067041514384 -> 140067041513328
	140067041514384 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :     (12, 3072)
mat1_sym_strides:      (3072, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (3072, 768)
mat2_sym_strides:      (1, 3072)"]
	140066907022400 -> 140067041514384
	140067041514000 -> 140067041514384
	140067041514000 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 4, 3072)"]
	140067041515056 -> 140067041514000
	140067041515056 [label="GeluBackward0
---------------------------
approximate:           none
self       : [saved tensor]"]
	140067041514768 -> 140067041515056
	140067041514768 [label="ViewBackward0
--------------------------
self_sym_sizes: (12, 3072)"]
	140067041514480 -> 140067041514768
	140067041514480 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (12, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (768, 3072)
mat2_sym_strides:       (1, 768)"]
	140066907023264 -> 140067041514480
	140067041515248 -> 140067041514480
	140067041515248 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 4, 768)"]
	140067041514240 -> 140067041515248
	140067041514240 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140067041515680 -> 140067041514240
	140067041515680 [label="AddBackward0
------------
alpha: 1"]
	140067041516016 -> 140067041515680
	140067041516016 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041514576 -> 140067041516016
	140067041514576 [label="ViewBackward0
-------------------------
self_sym_sizes: (12, 768)"]
	140067041516160 -> 140067041514576
	140067041516160 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (12, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140066907026192 -> 140067041516160
	140067041515872 -> 140067041516160
	140067041515872 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 4, 768)"]
	140067041516640 -> 140067041515872
	140067041516640 [label="ViewBackward0
------------------------------
self_sym_sizes: (3, 4, 12, 64)"]
	140067041516400 -> 140067041516640
	140067041516400 [label=CloneBackward0]
	140067041516736 -> 140067041516400
	140067041516736 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041516256 -> 140067041516736
	140067041516256 [label="UnsafeViewBackward0
---------------------------
self_sym_sizes: (36, 4, 64)"]
	140067041517024 -> 140067041516256
	140067041517024 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140067041517216 -> 140067041517024
	140067041517216 [label="ViewBackward0
-----------------------------
self_sym_sizes: (3, 12, 4, 4)"]
	140067041517264 -> 140067041517216
	140067041517264 [label="ExpandBackward0
-----------------------------
self_sym_sizes: (3, 12, 4, 4)"]
	140067041517600 -> 140067041517264
	140067041517600 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041517744 -> 140067041517600
	140067041517744 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140067041518032 -> 140067041517744
	140067041518032 [label="AddBackward0
------------
alpha: 1"]
	140067041518128 -> 140067041518032
	140067041518128 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041518320 -> 140067041518128
	140067041518320 [label="UnsafeViewBackward0
--------------------------
self_sym_sizes: (36, 4, 4)"]
	140067041518560 -> 140067041518320
	140067041518560 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140067041518656 -> 140067041518560
	140067041518656 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 4, 64)"]
	140067041518752 -> 140067041518656
	140067041518752 [label=CloneBackward0]
	140067041519040 -> 140067041518752
	140067041519040 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 4, 64)"]
	140067041519088 -> 140067041519040
	140067041519088 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041519280 -> 140067041519088
	140067041519280 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 4, 768)"]
	140067041519472 -> 140067041519280
	140067041519472 [label="ViewBackward0
-------------------------
self_sym_sizes: (12, 768)"]
	140067041519616 -> 140067041519472
	140067041519616 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (12, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140066907026864 -> 140067041519616
	140067041519856 -> 140067041519616
	140067041519856 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 4, 768)"]
	140067041515920 -> 140067041519856
	140067041515920 [label="CatBackward0
------------
dim: 1"]
	140067041520288 -> 140067041515920
	140067041520288 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 1, 768)
start         :                   0
step          :                   1"]
	140067041520384 -> 140067041520288
	140067041520384 [label="SliceBackward0
---------------------------
dim           :           1
end           :           1
self_sym_sizes: (3, 4, 768)
start         :           0
step          :           1"]
	140067041520960 -> 140067041520384
	140067041520960 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:         (3, 4, 768)
start         :                   0
step          :                   1"]
	140067041521152 -> 140067041520960
	140067041521152 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140067041520768 -> 140067041521152
	140067041520768 [label="AddBackward0
------------
alpha: 1"]
	140067041520048 -> 140067041520768
	140067041520048 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041520432 -> 140067041520048
	140067041520432 [label="ViewBackward0
-------------------------
self_sym_sizes: (12, 768)"]
	140067041521488 -> 140067041520432
	140067041521488 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :     (12, 3072)
mat1_sym_strides:      (3072, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (3072, 768)
mat2_sym_strides:      (1, 3072)"]
	140066708588816 -> 140067041521488
	140067041521008 -> 140067041521488
	140067041521008 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 4, 3072)"]
	140067041521872 -> 140067041521008
	140067041521872 [label="GeluBackward0
---------------------------
approximate:           none
self       : [saved tensor]"]
	140067041521728 -> 140067041521872
	140067041521728 [label="ViewBackward0
--------------------------
self_sym_sizes: (12, 3072)"]
	140067041521536 -> 140067041521728
	140067041521536 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (12, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (768, 3072)
mat2_sym_strides:       (1, 768)"]
	140066708597312 -> 140067041521536
	140067041522160 -> 140067041521536
	140067041522160 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 4, 768)"]
	140067041521296 -> 140067041522160
	140067041521296 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140067041522832 -> 140067041521296
	140067041522832 [label="AddBackward0
------------
alpha: 1"]
	140067041523024 -> 140067041522832
	140067041523024 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041521632 -> 140067041523024
	140067041521632 [label="ViewBackward0
-------------------------
self_sym_sizes: (12, 768)"]
	140067041523120 -> 140067041521632
	140067041523120 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (12, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140066708598176 -> 140067041523120
	140067041522880 -> 140067041523120
	140067041522880 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 4, 768)"]
	140067041523648 -> 140067041522880
	140067041523648 [label="ViewBackward0
------------------------------
self_sym_sizes: (3, 4, 12, 64)"]
	140067041523360 -> 140067041523648
	140067041523360 [label=CloneBackward0]
	140067041523744 -> 140067041523360
	140067041523744 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041523168 -> 140067041523744
	140067041523168 [label="UnsafeViewBackward0
---------------------------
self_sym_sizes: (36, 4, 64)"]
	140067041523984 -> 140067041523168
	140067041523984 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140067041524128 -> 140067041523984
	140067041524128 [label="ViewBackward0
-----------------------------
self_sym_sizes: (3, 12, 4, 4)"]
	140067041524224 -> 140067041524128
	140067041524224 [label="ExpandBackward0
-----------------------------
self_sym_sizes: (3, 12, 4, 4)"]
	140067041524656 -> 140067041524224
	140067041524656 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041524752 -> 140067041524656
	140067041524752 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140067041525088 -> 140067041524752
	140067041525088 [label="AddBackward0
------------
alpha: 1"]
	140067041525232 -> 140067041525088
	140067041525232 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041525424 -> 140067041525232
	140067041525424 [label="UnsafeViewBackward0
--------------------------
self_sym_sizes: (36, 4, 4)"]
	140067041525664 -> 140067041525424
	140067041525664 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140067041524416 -> 140067041525664
	140067041524416 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 4, 64)"]
	140067041575120 -> 140067041524416
	140067041575120 [label=CloneBackward0]
	140067041575312 -> 140067041575120
	140067041575312 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 4, 64)"]
	140067041575408 -> 140067041575312
	140067041575408 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041575552 -> 140067041575408
	140067041575552 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 4, 768)"]
	140067041575696 -> 140067041575552
	140067041575696 [label="ViewBackward0
-------------------------
self_sym_sizes: (12, 768)"]
	140067041575888 -> 140067041575696
	140067041575888 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (12, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140066708600384 -> 140067041575888
	140067041576176 -> 140067041575888
	140067041576176 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 4, 768)"]
	140067041522928 -> 140067041576176
	140067041522928 [label="CatBackward0
------------
dim: 1"]
	140067041576608 -> 140067041522928
	140067041576608 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 1, 768)
start         :                   0
step          :                   1"]
	140067041576656 -> 140067041576608
	140067041576656 [label="SliceBackward0
---------------------------
dim           :           1
end           :           1
self_sym_sizes: (3, 4, 768)
start         :           0
step          :           1"]
	140067041577088 -> 140067041576656
	140067041577088 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:         (3, 4, 768)
start         :                   0
step          :                   1"]
	140067041577376 -> 140067041577088
	140067041577376 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140067041577040 -> 140067041577376
	140067041577040 [label="AddBackward0
------------
alpha: 1"]
	140067041576368 -> 140067041577040
	140067041576368 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041576752 -> 140067041576368
	140067041576752 [label="ViewBackward0
-------------------------
self_sym_sizes: (12, 768)"]
	140067041577568 -> 140067041576752
	140067041577568 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :     (12, 3072)
mat1_sym_strides:      (3072, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (3072, 768)
mat2_sym_strides:      (1, 3072)"]
	140066708594768 -> 140067041577568
	140067041577136 -> 140067041577568
	140067041577136 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 4, 3072)"]
	140067041578096 -> 140067041577136
	140067041578096 [label="GeluBackward0
---------------------------
approximate:           none
self       : [saved tensor]"]
	140067041577856 -> 140067041578096
	140067041577856 [label="ViewBackward0
--------------------------
self_sym_sizes: (12, 3072)"]
	140067041577616 -> 140067041577856
	140067041577616 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (12, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (768, 3072)
mat2_sym_strides:       (1, 768)"]
	140066708601296 -> 140067041577616
	140067041578384 -> 140067041577616
	140067041578384 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 4, 768)"]
	140067041577472 -> 140067041578384
	140067041577472 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140067041578912 -> 140067041577472
	140067041578912 [label="AddBackward0
------------
alpha: 1"]
	140067041579296 -> 140067041578912
	140067041579296 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041577712 -> 140067041579296
	140067041577712 [label="ViewBackward0
-------------------------
self_sym_sizes: (12, 768)"]
	140067041579440 -> 140067041577712
	140067041579440 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (12, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140066708591216 -> 140067041579440
	140067041579104 -> 140067041579440
	140067041579104 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 4, 768)"]
	140067041579680 -> 140067041579104
	140067041579680 [label="ViewBackward0
------------------------------
self_sym_sizes: (3, 4, 12, 64)"]
	140067041579584 -> 140067041579680
	140067041579584 [label=CloneBackward0]
	140067041579920 -> 140067041579584
	140067041579920 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041579488 -> 140067041579920
	140067041579488 [label="UnsafeViewBackward0
---------------------------
self_sym_sizes: (36, 4, 64)"]
	140067041580112 -> 140067041579488
	140067041580112 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140067041580256 -> 140067041580112
	140067041580256 [label="ViewBackward0
-----------------------------
self_sym_sizes: (3, 12, 4, 4)"]
	140067041580352 -> 140067041580256
	140067041580352 [label="ExpandBackward0
-----------------------------
self_sym_sizes: (3, 12, 4, 4)"]
	140067041580544 -> 140067041580352
	140067041580544 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041580640 -> 140067041580544
	140067041580640 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140067041581168 -> 140067041580640
	140067041581168 [label="AddBackward0
------------
alpha: 1"]
	140067041581264 -> 140067041581168
	140067041581264 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041581360 -> 140067041581264
	140067041581360 [label="UnsafeViewBackward0
--------------------------
self_sym_sizes: (36, 4, 4)"]
	140067041581648 -> 140067041581360
	140067041581648 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140067041581696 -> 140067041581648
	140067041581696 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 4, 64)"]
	140067041581744 -> 140067041581696
	140067041581744 [label=CloneBackward0]
	140067041581984 -> 140067041581744
	140067041581984 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 4, 64)"]
	140067041582080 -> 140067041581984
	140067041582080 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041582320 -> 140067041582080
	140067041582320 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 4, 768)"]
	140067041582416 -> 140067041582320
	140067041582416 [label="ViewBackward0
-------------------------
self_sym_sizes: (12, 768)"]
	140067041582560 -> 140067041582416
	140067041582560 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (12, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140066673014960 -> 140067041582560
	140067041582752 -> 140067041582560
	140067041582752 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 4, 768)"]
	140067041579248 -> 140067041582752
	140067041579248 [label="CatBackward0
------------
dim: 1"]
	140067041583328 -> 140067041579248
	140067041583328 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 1, 768)
start         :                   0
step          :                   1"]
	140067041583424 -> 140067041583328
	140067041583424 [label="SliceBackward0
---------------------------
dim           :           1
end           :           1
self_sym_sizes: (3, 4, 768)
start         :           0
step          :           1"]
	140067041583904 -> 140067041583424
	140067041583904 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:         (3, 4, 768)
start         :                   0
step          :                   1"]
	140067041584096 -> 140067041583904
	140067041584096 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140067041583808 -> 140067041584096
	140067041583808 [label="AddBackward0
------------
alpha: 1"]
	140067041583040 -> 140067041583808
	140067041583040 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041583520 -> 140067041583040
	140067041583520 [label="ViewBackward0
-------------------------
self_sym_sizes: (12, 768)"]
	140067041584384 -> 140067041583520
	140067041584384 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :     (12, 3072)
mat1_sym_strides:      (3072, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (3072, 768)
mat2_sym_strides:      (1, 3072)"]
	140067041476672 -> 140067041584384
	140067041583952 -> 140067041584384
	140067041583952 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 4, 3072)"]
	140067041584672 -> 140067041583952
	140067041584672 [label="GeluBackward0
---------------------------
approximate:           none
self       : [saved tensor]"]
	140067041584528 -> 140067041584672
	140067041584528 [label="ViewBackward0
--------------------------
self_sym_sizes: (12, 3072)"]
	140067041584432 -> 140067041584528
	140067041584432 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (12, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (768, 3072)
mat2_sym_strides:       (1, 768)"]
	140067041477680 -> 140067041584432
	140067041585008 -> 140067041584432
	140067041585008 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 4, 768)"]
	140067041584288 -> 140067041585008
	140067041584288 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140067041585584 -> 140067041584288
	140067041585584 [label="AddBackward0
------------
alpha: 1"]
	140067041585824 -> 140067041585584
	140067041585824 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041584480 -> 140067041585824
	140067041584480 [label="ViewBackward0
-------------------------
self_sym_sizes: (12, 768)"]
	140067041586016 -> 140067041584480
	140067041586016 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (12, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041479408 -> 140067041586016
	140067041585680 -> 140067041586016
	140067041585680 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 4, 768)"]
	140067041586592 -> 140067041585680
	140067041586592 [label="ViewBackward0
------------------------------
self_sym_sizes: (3, 4, 12, 64)"]
	140067041586256 -> 140067041586592
	140067041586256 [label=CloneBackward0]
	140067041586832 -> 140067041586256
	140067041586832 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041586160 -> 140067041586832
	140067041586160 [label="UnsafeViewBackward0
---------------------------
self_sym_sizes: (36, 4, 64)"]
	140067041587360 -> 140067041586160
	140067041587360 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140067041587264 -> 140067041587360
	140067041587264 [label="ViewBackward0
-----------------------------
self_sym_sizes: (3, 12, 4, 4)"]
	140067041587552 -> 140067041587264
	140067041587552 [label="ExpandBackward0
-----------------------------
self_sym_sizes: (3, 12, 4, 4)"]
	140067041588032 -> 140067041587552
	140067041588032 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041588128 -> 140067041588032
	140067041588128 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140067041588512 -> 140067041588128
	140067041588512 [label="AddBackward0
------------
alpha: 1"]
	140067041588560 -> 140067041588512
	140067041588560 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041588704 -> 140067041588560
	140067041588704 [label="UnsafeViewBackward0
--------------------------
self_sym_sizes: (36, 4, 4)"]
	140067041588992 -> 140067041588704
	140067041588992 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140067041589088 -> 140067041588992
	140067041589088 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 4, 64)"]
	140067041589184 -> 140067041589088
	140067041589184 [label=CloneBackward0]
	140067041589424 -> 140067041589184
	140067041589424 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 4, 64)"]
	140067041589520 -> 140067041589424
	140067041589520 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041589712 -> 140067041589520
	140067041589712 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 4, 768)"]
	140067041589856 -> 140067041589712
	140067041589856 [label="ViewBackward0
-------------------------
self_sym_sizes: (12, 768)"]
	140067041590000 -> 140067041589856
	140067041590000 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (12, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041484016 -> 140067041590000
	140067041590144 -> 140067041590000
	140067041590144 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 4, 768)"]
	140067041585728 -> 140067041590144
	140067041585728 [label="CatBackward0
------------
dim: 1"]
	140067041590720 -> 140067041585728
	140067041590720 [label="RepeatBackward0
---------------------------
repeats       :   (3, 1, 1)
self_sym_sizes: (1, 1, 768)"]
	140067041590768 -> 140067041590720
	140067041590768 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (1, 1, 768)
start         :                   0
step          :                   1"]
	140067041590816 -> 140067041590768
	140067041590816 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:         (1, 1, 768)
start         :                   0
step          :                   1"]
	140067041591248 -> 140067041590816
	140067041591248 [label="UnsqueezeBackward0
------------------
dim: 0"]
	140066688181168 -> 140067041591248
	140067041590384 -> 140067041585728
	140067041590384 [label=CloneBackward0]
	140067041590912 -> 140067041590384
	140067041590912 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 3, 768)
start         :                   0
step          :                   1"]
	140067041590528 -> 140067041590912
	140067041590528 [label="SelectBackward0
--------------------------------
dim           :                2
index         :                1
self_sym_sizes: (3, 3, 504, 768)"]
	140067041590288 -> 140067041590528
	140067041590288 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:    (3, 3, 504, 768)
start         :                   0
step          :                   1"]
	140067041624272 -> 140067041590288
	140067041624272 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:    (3, 3, 504, 768)
start         :                   0
step          :                   1"]
	140067041624992 -> 140067041624272
	140067041624992 [label="ViewBackward0
-----------------------------
self_sym_sizes: (9, 504, 768)"]
	140067041625328 -> 140067041624992
	140067041625328 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140067041624368 -> 140067041625328
	140067041624368 [label="AddBackward0
------------
alpha: 1"]
	140067041625568 -> 140067041624368
	140067041625568 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041625424 -> 140067041625568
	140067041625424 [label="ViewBackward0
---------------------------
self_sym_sizes: (4536, 768)"]
	140067041625712 -> 140067041625424
	140067041625712 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :   (4536, 3072)
mat1_sym_strides:      (3072, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (3072, 768)
mat2_sym_strides:      (1, 3072)"]
	140067041486992 -> 140067041625712
	140067041625760 -> 140067041625712
	140067041625760 [label="ViewBackward0
------------------------------
self_sym_sizes: (9, 504, 3072)"]
	140067041626240 -> 140067041625760
	140067041626240 [label="GeluBackward0
---------------------------
approximate:           none
self       : [saved tensor]"]
	140067041626000 -> 140067041626240
	140067041626000 [label="ViewBackward0
----------------------------
self_sym_sizes: (4536, 3072)"]
	140067041624944 -> 140067041626000
	140067041624944 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :    (4536, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (768, 3072)
mat2_sym_strides:       (1, 768)"]
	140067041488192 -> 140067041624944
	140067041625856 -> 140067041624944
	140067041625856 [label="ViewBackward0
-----------------------------
self_sym_sizes: (9, 504, 768)"]
	140067041625520 -> 140067041625856
	140067041625520 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140067041627248 -> 140067041625520
	140067041627248 [label="AddBackward0
------------
alpha: 1"]
	140067041627536 -> 140067041627248
	140067041627536 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041626528 -> 140067041627536
	140067041626528 [label="ViewBackward0
---------------------------
self_sym_sizes: (4536, 768)"]
	140067041627584 -> 140067041626528
	140067041627584 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :    (4536, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041489440 -> 140067041627584
	140067041627776 -> 140067041627584
	140067041627776 [label="ViewBackward0
-----------------------------
self_sym_sizes: (9, 504, 768)"]
	140067041628016 -> 140067041627776
	140067041628016 [label="ViewBackward0
--------------------------------
self_sym_sizes: (9, 504, 12, 64)"]
	140067041627824 -> 140067041628016
	140067041627824 [label=CloneBackward0]
	140067041628112 -> 140067041627824
	140067041628112 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041627104 -> 140067041628112
	140067041627104 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (108, 504, 64)"]
	140067041628400 -> 140067041627104
	140067041628400 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140067041628592 -> 140067041628400
	140067041628592 [label="ViewBackward0
---------------------------------
self_sym_sizes: (9, 12, 504, 504)"]
	140067041628784 -> 140067041628592
	140067041628784 [label="ExpandBackward0
---------------------------------
self_sym_sizes: (9, 12, 504, 504)"]
	140067041629072 -> 140067041628784
	140067041629072 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041629120 -> 140067041629072
	140067041629120 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140067041629504 -> 140067041629120
	140067041629504 [label="AddBackward0
------------
alpha: 1"]
	140067041629552 -> 140067041629504
	140067041629552 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041629840 -> 140067041629552
	140067041629840 [label="UnsafeViewBackward0
-------------------------------
self_sym_sizes: (108, 504, 504)"]
	140067041630080 -> 140067041629840
	140067041630080 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140067041630128 -> 140067041630080
	140067041630128 [label="UnsafeViewBackward0
--------------------------------
self_sym_sizes: (9, 12, 504, 64)"]
	140067041630272 -> 140067041630128
	140067041630272 [label=CloneBackward0]
	140067041630416 -> 140067041630272
	140067041630416 [label="ExpandBackward0
--------------------------------
self_sym_sizes: (9, 12, 504, 64)"]
	140067041630464 -> 140067041630416
	140067041630464 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041630608 -> 140067041630464
	140067041630608 [label="ViewBackward0
-----------------------------
self_sym_sizes: (9, 504, 768)"]
	140067041630752 -> 140067041630608
	140067041630752 [label="ViewBackward0
---------------------------
self_sym_sizes: (4536, 768)"]
	140067041630944 -> 140067041630752
	140067041630944 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :    (4536, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041691008 -> 140067041630944
	140067041630992 -> 140067041630944
	140067041630992 [label="ViewBackward0
-----------------------------
self_sym_sizes: (9, 504, 768)"]
	140067041627344 -> 140067041630992
	140067041627344 [label="CatBackward0
------------
dim: 1"]
	140067041631616 -> 140067041627344
	140067041631616 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041631232 -> 140067041631616
	140067041631232 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140067041631712 -> 140067041631232
	140067041631712 [label="AddBackward0
------------
alpha: 1"]
	140067041632096 -> 140067041631712
	140067041632096 [label="AddBackward0
------------
alpha: 1"]
	140067041631904 -> 140067041632096
	140067041631904 [label="EmbeddingBackward0
------------------------------------
indices             : [saved tensor]
padding_idx         :              0
scale_grad_by_freq  :          False
sparse              :          False
weight_sym_argsize_0:          30522"]
	140067041692736 -> 140067041631904
	140067041631808 -> 140067041632096
	140067041631808 [label="EmbeddingBackward0
------------------------------------------
indices             :       [saved tensor]
padding_idx         : 18446744073709551615
scale_grad_by_freq  :                False
sparse              :                False
weight_sym_argsize_0:                    2"]
	140067041692832 -> 140067041631808
	140067041632048 -> 140067041631712
	140067041632048 [label="EmbeddingBackward0
------------------------------------------
indices             :       [saved tensor]
padding_idx         : 18446744073709551615
scale_grad_by_freq  :                False
sparse              :                False
weight_sym_argsize_0:                  512"]
	140067041693120 -> 140067041632048
	140067041692208 -> 140067041631232
	140067041692064 -> 140067041631232
	140067041631136 -> 140067041630944
	140067041631136 [label=TBackward0]
	140067041691488 -> 140067041631136
	140067041629744 -> 140067041630080
	140067041629744 [label="UnsafeViewBackward0
--------------------------------
self_sym_sizes: (9, 12, 64, 504)"]
	140067041629888 -> 140067041629744
	140067041629888 [label=CloneBackward0]
	140067041630848 -> 140067041629888
	140067041630848 [label="ExpandBackward0
--------------------------------
self_sym_sizes: (9, 12, 64, 504)"]
	140067041630656 -> 140067041630848
	140067041630656 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551615
dim1: 18446744073709551614"]
	140067041631568 -> 140067041630656
	140067041631568 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041632288 -> 140067041631568
	140067041632288 [label="ViewBackward0
-----------------------------
self_sym_sizes: (9, 504, 768)"]
	140067041631424 -> 140067041632288
	140067041631424 [label="ViewBackward0
---------------------------
self_sym_sizes: (4536, 768)"]
	140067041631040 -> 140067041631424
	140067041631040 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :    (4536, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041693264 -> 140067041631040
	140067041632432 -> 140067041631040
	140067041632432 [label="ViewBackward0
-----------------------------
self_sym_sizes: (9, 504, 768)"]
	140067041627344 -> 140067041632432
	140067041632528 -> 140067041631040
	140067041632528 [label=TBackward0]
	140067041693792 -> 140067041632528
	140067041628256 -> 140067041628400
	140067041628256 [label="UnsafeViewBackward0
--------------------------------
self_sym_sizes: (9, 12, 504, 64)"]
	140067041628688 -> 140067041628256
	140067041628688 [label=CloneBackward0]
	140067041629264 -> 140067041628688
	140067041629264 [label="ExpandBackward0
--------------------------------
self_sym_sizes: (9, 12, 504, 64)"]
	140067041629408 -> 140067041629264
	140067041629408 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041630512 -> 140067041629408
	140067041630512 [label="ViewBackward0
-----------------------------
self_sym_sizes: (9, 504, 768)"]
	140067041630176 -> 140067041630512
	140067041630176 [label="ViewBackward0
---------------------------
self_sym_sizes: (4536, 768)"]
	140067041631664 -> 140067041630176
	140067041631664 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :    (4536, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041693504 -> 140067041631664
	140067041631472 -> 140067041631664
	140067041631472 [label="ViewBackward0
-----------------------------
self_sym_sizes: (9, 504, 768)"]
	140067041627344 -> 140067041631472
	140067041631520 -> 140067041631664
	140067041631520 [label=TBackward0]
	140067041693408 -> 140067041631520
	140067041627392 -> 140067041627584
	140067041627392 [label=TBackward0]
	140067041489824 -> 140067041627392
	140067041627344 -> 140067041627248
	140067041488624 -> 140067041625520
	140067041489152 -> 140067041625520
	140067041626432 -> 140067041624944
	140067041626432 [label=TBackward0]
	140067041489104 -> 140067041626432
	140067041625616 -> 140067041625712
	140067041625616 [label=TBackward0]
	140067041487520 -> 140067041625616
	140067041625520 -> 140067041624368
	140067041485984 -> 140067041625328
	140067041486752 -> 140067041625328
	140067041590048 -> 140067041590000
	140067041590048 [label=TBackward0]
	140067041484832 -> 140067041590048
	140067041588608 -> 140067041588992
	140067041588608 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 64, 4)"]
	140067041588752 -> 140067041588608
	140067041588752 [label=CloneBackward0]
	140067041589952 -> 140067041588752
	140067041589952 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 64, 4)"]
	140067041589760 -> 140067041589952
	140067041589760 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551615
dim1: 18446744073709551614"]
	140067041591152 -> 140067041589760
	140067041591152 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041591056 -> 140067041591152
	140067041591056 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 4, 768)"]
	140067041589376 -> 140067041591056
	140067041589376 [label="ViewBackward0
-------------------------
self_sym_sizes: (12, 768)"]
	140067041624464 -> 140067041589376
	140067041624464 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (12, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041486896 -> 140067041624464
	140067041624896 -> 140067041624464
	140067041624896 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 4, 768)"]
	140067041585728 -> 140067041624896
	140067041625232 -> 140067041624464
	140067041625232 [label=TBackward0]
	140067041489488 -> 140067041625232
	140067041587216 -> 140067041587360
	140067041587216 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 4, 64)"]
	140067041587696 -> 140067041587216
	140067041587696 [label=CloneBackward0]
	140067041588224 -> 140067041587696
	140067041588224 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 4, 64)"]
	140067041588320 -> 140067041588224
	140067041588320 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041589616 -> 140067041588320
	140067041589616 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 4, 768)"]
	140067041589280 -> 140067041589616
	140067041589280 [label="ViewBackward0
-------------------------
self_sym_sizes: (12, 768)"]
	140067041590240 -> 140067041589280
	140067041590240 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (12, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041488864 -> 140067041590240
	140067041587744 -> 140067041590240
	140067041587744 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 4, 768)"]
	140067041585728 -> 140067041587744
	140067041590672 -> 140067041590240
	140067041590672 [label=TBackward0]
	140067041488432 -> 140067041590672
	140067041585968 -> 140067041586016
	140067041585968 [label=TBackward0]
	140067041479312 -> 140067041585968
	140067041585728 -> 140067041585584
	140067041478352 -> 140067041584288
	140067041478736 -> 140067041584288
	140067041584816 -> 140067041584432
	140067041584816 [label=TBackward0]
	140067041478976 -> 140067041584816
	140067041584336 -> 140067041584384
	140067041584336 [label=TBackward0]
	140067041477104 -> 140067041584336
	140067041584288 -> 140067041583808
	140066673015440 -> 140067041584096
	140066673016592 -> 140067041584096
	140067041582992 -> 140067041579248
	140067041582992 [label=CloneBackward0]
	140067041583856 -> 140067041582992
	140067041583856 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 3, 768)
start         :                   0
step          :                   1"]
	140067041584192 -> 140067041583856
	140067041584192 [label="SelectBackward0
--------------------------------
dim           :                2
index         :                1
self_sym_sizes: (3, 3, 504, 768)"]
	140067041585248 -> 140067041584192
	140067041585248 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:    (3, 3, 504, 768)
start         :                   0
step          :                   1"]
	140067041583664 -> 140067041585248
	140067041583664 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:    (3, 3, 504, 768)
start         :                   0
step          :                   1"]
	140067041585344 -> 140067041583664
	140067041585344 [label="ViewBackward0
-----------------------------
self_sym_sizes: (9, 504, 768)"]
	140067041586352 -> 140067041585344
	140067041586352 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140067041585488 -> 140067041586352
	140067041585488 [label="AddBackward0
------------
alpha: 1"]
	140067041587168 -> 140067041585488
	140067041587168 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041586064 -> 140067041587168
	140067041586064 [label="ViewBackward0
---------------------------
self_sym_sizes: (4536, 768)"]
	140067041586736 -> 140067041586064
	140067041586736 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :   (4536, 3072)
mat1_sym_strides:      (3072, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (3072, 768)
mat2_sym_strides:      (1, 3072)"]
	140067041476720 -> 140067041586736
	140067041588080 -> 140067041586736
	140067041588080 [label="ViewBackward0
------------------------------
self_sym_sizes: (9, 504, 3072)"]
	140067041590576 -> 140067041588080
	140067041590576 [label="GeluBackward0
---------------------------
approximate:           none
self       : [saved tensor]"]
	140067041588416 -> 140067041590576
	140067041588416 [label="ViewBackward0
----------------------------
self_sym_sizes: (4536, 3072)"]
	140067041589664 -> 140067041588416
	140067041589664 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :    (4536, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (768, 3072)
mat2_sym_strides:       (1, 768)"]
	140067041490400 -> 140067041589664
	140067041624416 -> 140067041589664
	140067041624416 [label="ViewBackward0
-----------------------------
self_sym_sizes: (9, 504, 768)"]
	140067041586448 -> 140067041624416
	140067041586448 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140067041626864 -> 140067041586448
	140067041626864 [label="AddBackward0
------------
alpha: 1"]
	140067041626768 -> 140067041626864
	140067041626768 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041626048 -> 140067041626768
	140067041626048 [label="ViewBackward0
---------------------------
self_sym_sizes: (4536, 768)"]
	140067041627872 -> 140067041626048
	140067041627872 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :    (4536, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041693936 -> 140067041627872
	140067041627680 -> 140067041627872
	140067041627680 [label="ViewBackward0
-----------------------------
self_sym_sizes: (9, 504, 768)"]
	140067041629216 -> 140067041627680
	140067041629216 [label="ViewBackward0
--------------------------------
self_sym_sizes: (9, 504, 12, 64)"]
	140067041628208 -> 140067041629216
	140067041628208 [label=CloneBackward0]
	140067041629312 -> 140067041628208
	140067041629312 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041626960 -> 140067041629312
	140067041626960 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (108, 504, 64)"]
	140067041630560 -> 140067041626960
	140067041630560 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140067041632192 -> 140067041630560
	140067041632192 [label="ViewBackward0
---------------------------------
self_sym_sizes: (9, 12, 504, 504)"]
	140067041632720 -> 140067041632192
	140067041632720 [label="ExpandBackward0
---------------------------------
self_sym_sizes: (9, 12, 504, 504)"]
	140067041631952 -> 140067041632720
	140067041631952 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041630320 -> 140067041631952
	140067041630320 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140067041633152 -> 140067041630320
	140067041633152 [label="AddBackward0
------------
alpha: 1"]
	140067041633248 -> 140067041633152
	140067041633248 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041633584 -> 140067041633248
	140067041633584 [label="UnsafeViewBackward0
-------------------------------
self_sym_sizes: (108, 504, 504)"]
	140067041633728 -> 140067041633584
	140067041633728 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140067041633824 -> 140067041633728
	140067041633824 [label="UnsafeViewBackward0
--------------------------------
self_sym_sizes: (9, 12, 504, 64)"]
	140067041633920 -> 140067041633824
	140067041633920 [label=CloneBackward0]
	140067041634112 -> 140067041633920
	140067041634112 [label="ExpandBackward0
--------------------------------
self_sym_sizes: (9, 12, 504, 64)"]
	140067041634208 -> 140067041634112
	140067041634208 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041634400 -> 140067041634208
	140067041634400 [label="ViewBackward0
-----------------------------
self_sym_sizes: (9, 504, 768)"]
	140067041634640 -> 140067041634400
	140067041634640 [label="ViewBackward0
---------------------------
self_sym_sizes: (4536, 768)"]
	140067041634688 -> 140067041634640
	140067041634688 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :    (4536, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041697776 -> 140067041634688
	140067041634928 -> 140067041634688
	140067041634928 [label="ViewBackward0
-----------------------------
self_sym_sizes: (9, 504, 768)"]
	140067041626624 -> 140067041634928
	140067041626624 [label="ViewBackward0
--------------------------------
self_sym_sizes: (3, 3, 504, 768)"]
	140067041635456 -> 140067041626624
	140067041635456 [label="AsStridedBackward0
-----------------------------------------
size          :          (3, 3, 504, 768)
storage_offset:                         0
stride        : (1161216, 387072, 768, 1)"]
	140067041634976 -> 140067041635456
	140067041634976 [label=CopySlices]
	140067041625328 -> 140067041634976
	140067041635552 -> 140067041634976
	140067041635552 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 3, 768)
start         :                   0
step          :                   1"]
	140067041634832 -> 140067041635552
	140067041634832 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:         (3, 4, 768)
start         :                   1
step          :                   1"]
	140067041636368 -> 140067041634832
	140067041636368 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:         (3, 4, 768)
start         :                   0
step          :                   1"]
	140067041584096 -> 140067041636368
	140067041634784 -> 140067041634688
	140067041634784 [label=TBackward0]
	140067041697824 -> 140067041634784
	140067041633440 -> 140067041633728
	140067041633440 [label="UnsafeViewBackward0
--------------------------------
self_sym_sizes: (9, 12, 64, 504)"]
	140067041633632 -> 140067041633440
	140067041633632 [label=CloneBackward0]
	140067041634544 -> 140067041633632
	140067041634544 [label="ExpandBackward0
--------------------------------
self_sym_sizes: (9, 12, 64, 504)"]
	140067041634496 -> 140067041634544
	140067041634496 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551615
dim1: 18446744073709551614"]
	140067041635024 -> 140067041634496
	140067041635024 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041636272 -> 140067041635024
	140067041636272 [label="ViewBackward0
-----------------------------
self_sym_sizes: (9, 504, 768)"]
	140067041636512 -> 140067041636272
	140067041636512 [label="ViewBackward0
---------------------------
self_sym_sizes: (4536, 768)"]
	140067041635408 -> 140067041636512
	140067041635408 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :    (4536, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041699504 -> 140067041635408
	140067041636656 -> 140067041635408
	140067041636656 [label="ViewBackward0
-----------------------------
self_sym_sizes: (9, 504, 768)"]
	140067041626624 -> 140067041636656
	140067041635264 -> 140067041635408
	140067041635264 [label=TBackward0]
	140067041699792 -> 140067041635264
	140067041631328 -> 140067041630560
	140067041631328 [label="UnsafeViewBackward0
--------------------------------
self_sym_sizes: (9, 12, 504, 64)"]
	140067041632864 -> 140067041631328
	140067041632864 [label=CloneBackward0]
	140067041632336 -> 140067041632864
	140067041632336 [label="ExpandBackward0
--------------------------------
self_sym_sizes: (9, 12, 504, 64)"]
	140067041633008 -> 140067041632336
	140067041633008 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041634256 -> 140067041633008
	140067041634256 [label="ViewBackward0
-----------------------------
self_sym_sizes: (9, 504, 768)"]
	140067041634016 -> 140067041634256
	140067041634016 [label="ViewBackward0
---------------------------
self_sym_sizes: (4536, 768)"]
	140067041635504 -> 140067041634016
	140067041635504 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :    (4536, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041699360 -> 140067041635504
	140067041635696 -> 140067041635504
	140067041635696 [label="ViewBackward0
-----------------------------
self_sym_sizes: (9, 504, 768)"]
	140067041626624 -> 140067041635696
	140067041635312 -> 140067041635504
	140067041635312 [label=TBackward0]
	140067041699840 -> 140067041635312
	140067041627056 -> 140067041627872
	140067041627056 [label=TBackward0]
	140067041694176 -> 140067041627056
	140067041626624 -> 140067041626864
	140067041490112 -> 140067041586448
	140067041491552 -> 140067041586448
	140067041624560 -> 140067041589664
	140067041624560 [label=TBackward0]
	140067041491216 -> 140067041624560
	140067041587504 -> 140067041586736
	140067041587504 [label=TBackward0]
	140067041488048 -> 140067041587504
	140067041586448 -> 140067041585488
	140067041480272 -> 140067041586352
	140067041476864 -> 140067041586352
	140067041582608 -> 140067041582560
	140067041582608 [label=TBackward0]
	140066673015968 -> 140067041582608
	140067041581312 -> 140067041581648
	140067041581312 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 64, 4)"]
	140067041581456 -> 140067041581312
	140067041581456 [label=CloneBackward0]
	140067041582464 -> 140067041581456
	140067041582464 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 64, 4)"]
	140067041582368 -> 140067041582464
	140067041582368 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551615
dim1: 18446744073709551614"]
	140067041584048 -> 140067041582368
	140067041584048 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041583760 -> 140067041584048
	140067041583760 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 4, 768)"]
	140067041584864 -> 140067041583760
	140067041584864 [label="ViewBackward0
-------------------------
self_sym_sizes: (12, 768)"]
	140067041585152 -> 140067041584864
	140067041585152 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (12, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041477392 -> 140067041585152
	140067041583568 -> 140067041585152
	140067041583568 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 4, 768)"]
	140067041579248 -> 140067041583568
	140067041585632 -> 140067041585152
	140067041585632 [label=TBackward0]
	140067041487616 -> 140067041585632
	140067041580064 -> 140067041580112
	140067041580064 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 4, 64)"]
	140067041580304 -> 140067041580064
	140067041580304 [label=CloneBackward0]
	140067041580736 -> 140067041580304
	140067041580736 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 4, 64)"]
	140067041580976 -> 140067041580736
	140067041580976 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041582128 -> 140067041580976
	140067041582128 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 4, 768)"]
	140067041581840 -> 140067041582128
	140067041581840 [label="ViewBackward0
-------------------------
self_sym_sizes: (12, 768)"]
	140067041582704 -> 140067041581840
	140067041582704 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (12, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140066673013760 -> 140067041582704
	140067041581936 -> 140067041582704
	140067041581936 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 4, 768)"]
	140067041579248 -> 140067041581936
	140067041583280 -> 140067041582704
	140067041583280 [label=TBackward0]
	140067041483344 -> 140067041583280
	140067041579344 -> 140067041579440
	140067041579344 [label=TBackward0]
	140066708589584 -> 140067041579344
	140067041579248 -> 140067041578912
	140066708602784 -> 140067041577472
	140066708586752 -> 140067041577472
	140067041578192 -> 140067041577616
	140067041578192 [label=TBackward0]
	140066708587712 -> 140067041578192
	140067041577520 -> 140067041577568
	140067041577520 [label=TBackward0]
	140066708601344 -> 140067041577520
	140067041577472 -> 140067041577040
	140066708600912 -> 140067041577376
	140066708600480 -> 140067041577376
	140067041576272 -> 140067041522928
	140067041576272 [label=CloneBackward0]
	140067041577232 -> 140067041576272
	140067041577232 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 3, 768)
start         :                   0
step          :                   1"]
	140067041577424 -> 140067041577232
	140067041577424 [label="SelectBackward0
--------------------------------
dim           :                2
index         :                1
self_sym_sizes: (3, 3, 504, 768)"]
	140067041578528 -> 140067041577424
	140067041578528 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:    (3, 3, 504, 768)
start         :                   0
step          :                   1"]
	140067041576896 -> 140067041578528
	140067041576896 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:    (3, 3, 504, 768)
start         :                   0
step          :                   1"]
	140067041578624 -> 140067041576896
	140067041578624 [label="ViewBackward0
-----------------------------
self_sym_sizes: (9, 504, 768)"]
	140067041579632 -> 140067041578624
	140067041579632 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140067041578720 -> 140067041579632
	140067041578720 [label="AddBackward0
------------
alpha: 1"]
	140067041580016 -> 140067041578720
	140067041580016 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041579536 -> 140067041580016
	140067041579536 [label="ViewBackward0
---------------------------
self_sym_sizes: (4536, 768)"]
	140067041579824 -> 140067041579536
	140067041579824 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :   (4536, 3072)
mat1_sym_strides:      (3072, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (3072, 768)
mat2_sym_strides:      (1, 3072)"]
	140067041492416 -> 140067041579824
	140067041580592 -> 140067041579824
	140067041580592 [label="ViewBackward0
------------------------------
self_sym_sizes: (9, 504, 3072)"]
	140067041582224 -> 140067041580592
	140067041582224 [label="GeluBackward0
---------------------------
approximate:           none
self       : [saved tensor]"]
	140067041581552 -> 140067041582224
	140067041581552 [label="ViewBackward0
----------------------------
self_sym_sizes: (4536, 3072)"]
	140067041578816 -> 140067041581552
	140067041578816 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :    (4536, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (768, 3072)
mat2_sym_strides:       (1, 768)"]
	140067041694896 -> 140067041578816
	140067041581120 -> 140067041578816
	140067041581120 [label="ViewBackward0
-----------------------------
self_sym_sizes: (9, 504, 768)"]
	140067041579776 -> 140067041581120
	140067041579776 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140067041584960 -> 140067041579776
	140067041584960 [label="AddBackward0
------------
alpha: 1"]
	140067041585536 -> 140067041584960
	140067041585536 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041625088 -> 140067041585536
	140067041625088 [label="ViewBackward0
---------------------------
self_sym_sizes: (4536, 768)"]
	140067041628496 -> 140067041625088
	140067041628496 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :    (4536, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041693696 -> 140067041628496
	140067041627920 -> 140067041628496
	140067041627920 [label="ViewBackward0
-----------------------------
self_sym_sizes: (9, 504, 768)"]
	140067041632768 -> 140067041627920
	140067041632768 [label="ViewBackward0
--------------------------------
self_sym_sizes: (9, 504, 12, 64)"]
	140067041628928 -> 140067041632768
	140067041628928 [label=CloneBackward0]
	140067041633056 -> 140067041628928
	140067041633056 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041626288 -> 140067041633056
	140067041626288 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (108, 504, 64)"]
	140067041634304 -> 140067041626288
	140067041634304 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140067041636176 -> 140067041634304
	140067041636176 [label="ViewBackward0
---------------------------------
self_sym_sizes: (9, 12, 504, 504)"]
	140067041635984 -> 140067041636176
	140067041635984 [label="ExpandBackward0
---------------------------------
self_sym_sizes: (9, 12, 504, 504)"]
	140067041635840 -> 140067041635984
	140067041635840 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041634064 -> 140067041635840
	140067041634064 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140067041636896 -> 140067041634064
	140067041636896 [label="AddBackward0
------------
alpha: 1"]
	140067041636944 -> 140067041636896
	140067041636944 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041637088 -> 140067041636944
	140067041637088 [label="UnsafeViewBackward0
-------------------------------
self_sym_sizes: (108, 504, 504)"]
	140067041637280 -> 140067041637088
	140067041637280 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140067041637328 -> 140067041637280
	140067041637328 [label="UnsafeViewBackward0
--------------------------------
self_sym_sizes: (9, 12, 504, 64)"]
	140067041637424 -> 140067041637328
	140067041637424 [label=CloneBackward0]
	140067041637664 -> 140067041637424
	140067041637664 [label="ExpandBackward0
--------------------------------
self_sym_sizes: (9, 12, 504, 64)"]
	140067041637760 -> 140067041637664
	140067041637760 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041638048 -> 140067041637760
	140067041638048 [label="ViewBackward0
-----------------------------
self_sym_sizes: (9, 504, 768)"]
	140067041638240 -> 140067041638048
	140067041638240 [label="ViewBackward0
---------------------------
self_sym_sizes: (4536, 768)"]
	140067041638288 -> 140067041638240
	140067041638288 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :    (4536, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041703392 -> 140067041638288
	140067041638576 -> 140067041638288
	140067041638576 [label="ViewBackward0
-----------------------------
self_sym_sizes: (9, 504, 768)"]
	140067041587840 -> 140067041638576
	140067041587840 [label="ViewBackward0
--------------------------------
self_sym_sizes: (3, 3, 504, 768)"]
	140067041639200 -> 140067041587840
	140067041639200 [label="AsStridedBackward0
-----------------------------------------
size          :          (3, 3, 504, 768)
storage_offset:                         0
stride        : (1161216, 387072, 768, 1)"]
	140067041638672 -> 140067041639200
	140067041638672 [label=CopySlices]
	140067041586352 -> 140067041638672
	140067041639296 -> 140067041638672
	140067041639296 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 3, 768)
start         :                   0
step          :                   1"]
	140067041638624 -> 140067041639296
	140067041638624 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:         (3, 4, 768)
start         :                   1
step          :                   1"]
	140067041639824 -> 140067041638624
	140067041639824 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:         (3, 4, 768)
start         :                   0
step          :                   1"]
	140067041577376 -> 140067041639824
	140067041638528 -> 140067041638288
	140067041638528 [label=TBackward0]
	140067041703296 -> 140067041638528
	140067041636992 -> 140067041637280
	140067041636992 [label="UnsafeViewBackward0
--------------------------------
self_sym_sizes: (9, 12, 64, 504)"]
	140067041637184 -> 140067041636992
	140067041637184 [label=CloneBackward0]
	140067041638384 -> 140067041637184
	140067041638384 [label="ExpandBackward0
--------------------------------
self_sym_sizes: (9, 12, 64, 504)"]
	140067041638192 -> 140067041638384
	140067041638192 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551615
dim1: 18446744073709551614"]
	140067041638960 -> 140067041638192
	140067041638960 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041639680 -> 140067041638960
	140067041639680 [label="ViewBackward0
-----------------------------
self_sym_sizes: (9, 504, 768)"]
	140067041639920 -> 140067041639680
	140067041639920 [label="ViewBackward0
---------------------------
self_sym_sizes: (4536, 768)"]
	140067041639152 -> 140067041639920
	140067041639152 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :    (4536, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041704976 -> 140067041639152
	140067041640016 -> 140067041639152
	140067041640016 [label="ViewBackward0
-----------------------------
self_sym_sizes: (9, 504, 768)"]
	140067041587840 -> 140067041640016
	140067041639008 -> 140067041639152
	140067041639008 [label=TBackward0]
	140067041705168 -> 140067041639008
	140067041635168 -> 140067041634304
	140067041635168 [label="UnsafeViewBackward0
--------------------------------
self_sym_sizes: (9, 12, 504, 64)"]
	140067041636704 -> 140067041635168
	140067041636704 [label=CloneBackward0]
	140067041636032 -> 140067041636704
	140067041636032 [label="ExpandBackward0
--------------------------------
self_sym_sizes: (9, 12, 504, 64)"]
	140067041636752 -> 140067041636032
	140067041636752 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041637904 -> 140067041636752
	140067041637904 [label="ViewBackward0
-----------------------------
self_sym_sizes: (9, 504, 768)"]
	140067041637520 -> 140067041637904
	140067041637520 [label="ViewBackward0
---------------------------
self_sym_sizes: (4536, 768)"]
	140067041639248 -> 140067041637520
	140067041639248 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :    (4536, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041704880 -> 140067041639248
	140067041639440 -> 140067041639248
	140067041639440 [label="ViewBackward0
-----------------------------
self_sym_sizes: (9, 504, 768)"]
	140067041587840 -> 140067041639440
	140067041639104 -> 140067041639248
	140067041639104 [label=TBackward0]
	140067041705264 -> 140067041639104
	140067041626720 -> 140067041628496
	140067041626720 [label=TBackward0]
	140067041700032 -> 140067041626720
	140067041587840 -> 140067041584960
	140067041694560 -> 140067041579776
	140067041696000 -> 140067041579776
	140067041584576 -> 140067041578816
	140067041584576 [label=TBackward0]
	140067041698928 -> 140067041584576
	140067041580160 -> 140067041579824
	140067041580160 [label=TBackward0]
	140066708601776 -> 140067041580160
	140067041579776 -> 140067041578720
	140067054461056 -> 140067041579632
	140066708592464 -> 140067041579632
	140067041576032 -> 140067041575888
	140067041576032 [label=TBackward0]
	140067041485072 -> 140067041576032
	140067041525568 -> 140067041525664
	140067041525568 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 64, 4)"]
	140067041575024 -> 140067041525568
	140067041575024 [label=CloneBackward0]
	140067041575840 -> 140067041575024
	140067041575840 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 64, 4)"]
	140067041575648 -> 140067041575840
	140067041575648 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551615
dim1: 18446744073709551614"]
	140067041577280 -> 140067041575648
	140067041577280 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041576992 -> 140067041577280
	140067041576992 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 4, 768)"]
	140067041578432 -> 140067041576992
	140067041578432 [label="ViewBackward0
-------------------------
self_sym_sizes: (12, 768)"]
	140067041578288 -> 140067041578432
	140067041578288 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (12, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140066708602208 -> 140067041578288
	140067041576848 -> 140067041578288
	140067041576848 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 4, 768)"]
	140067041522928 -> 140067041576848
	140067041579008 -> 140067041578288
	140067041579008 [label=TBackward0]
	140066708600048 -> 140067041579008
	140067041523888 -> 140067041523984
	140067041523888 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 4, 64)"]
	140067041524320 -> 140067041523888
	140067041524320 [label=CloneBackward0]
	140067041524848 -> 140067041524320
	140067041524848 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 4, 64)"]
	140067041524896 -> 140067041524848
	140067041524896 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041524512 -> 140067041524896
	140067041524512 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 4, 768)"]
	140067041575168 -> 140067041524512
	140067041575168 [label="ViewBackward0
-------------------------
self_sym_sizes: (12, 768)"]
	140067041576128 -> 140067041575168
	140067041576128 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (12, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140066708595104 -> 140067041576128
	140067041575216 -> 140067041576128
	140067041575216 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 4, 768)"]
	140067041522928 -> 140067041575216
	140067041576464 -> 140067041576128
	140067041576464 [label=TBackward0]
	140066708594960 -> 140067041576464
	140067041523072 -> 140067041523120
	140067041523072 [label=TBackward0]
	140066708598464 -> 140067041523072
	140067041522928 -> 140067041522832
	140066708596976 -> 140067041521296
	140066708597552 -> 140067041521296
	140067041521968 -> 140067041521536
	140067041521968 [label=TBackward0]
	140066708597600 -> 140067041521968
	140067041521344 -> 140067041521488
	140067041521344 [label=TBackward0]
	140066708589104 -> 140067041521344
	140067041521296 -> 140067041520768
	140066708588048 -> 140067041521152
	140066708589248 -> 140067041521152
	140067041520000 -> 140067041515920
	140067041520000 [label=CloneBackward0]
	140067041520864 -> 140067041520000
	140067041520864 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 3, 768)
start         :                   0
step          :                   1"]
	140067041521248 -> 140067041520864
	140067041521248 [label="SelectBackward0
--------------------------------
dim           :                2
index         :                1
self_sym_sizes: (3, 3, 504, 768)"]
	140067041522448 -> 140067041521248
	140067041522448 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:    (3, 3, 504, 768)
start         :                   0
step          :                   1"]
	140067041520576 -> 140067041522448
	140067041520576 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:    (3, 3, 504, 768)
start         :                   0
step          :                   1"]
	140067041522496 -> 140067041520576
	140067041522496 [label="ViewBackward0
-----------------------------
self_sym_sizes: (9, 504, 768)"]
	140067041523408 -> 140067041522496
	140067041523408 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140067041522544 -> 140067041523408
	140067041522544 [label="AddBackward0
------------
alpha: 1"]
	140067041523792 -> 140067041522544
	140067041523792 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041523264 -> 140067041523792
	140067041523264 [label="ViewBackward0
---------------------------
self_sym_sizes: (4536, 768)"]
	140067041523696 -> 140067041523264
	140067041523696 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :   (4536, 3072)
mat1_sym_strides:      (3072, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (3072, 768)
mat2_sym_strides:      (1, 3072)"]
	140066708589824 -> 140067041523696
	140067041524560 -> 140067041523696
	140067041524560 [label="ViewBackward0
------------------------------
self_sym_sizes: (9, 504, 3072)"]
	140067041524992 -> 140067041524560
	140067041524992 [label="GeluBackward0
---------------------------
approximate:           none
self       : [saved tensor]"]
	140067041525328 -> 140067041524992
	140067041525328 [label="ViewBackward0
----------------------------
self_sym_sizes: (4536, 3072)"]
	140067041578000 -> 140067041525328
	140067041578000 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :    (4536, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (768, 3072)
mat2_sym_strides:       (1, 768)"]
	140067041700464 -> 140067041578000
	140067041576416 -> 140067041578000
	140067041576416 [label="ViewBackward0
-----------------------------
self_sym_sizes: (9, 504, 768)"]
	140067041523504 -> 140067041576416
	140067041523504 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140067041584624 -> 140067041523504
	140067041584624 [label="AddBackward0
------------
alpha: 1"]
	140067041580400 -> 140067041584624
	140067041580400 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041582896 -> 140067041580400
	140067041582896 [label="ViewBackward0
---------------------------
self_sym_sizes: (4536, 768)"]
	140067041629024 -> 140067041582896
	140067041629024 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :    (4536, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041696528 -> 140067041629024
	140067041628160 -> 140067041629024
	140067041628160 [label="ViewBackward0
-----------------------------
self_sym_sizes: (9, 504, 768)"]
	140067041636608 -> 140067041628160
	140067041636608 [label="ViewBackward0
--------------------------------
self_sym_sizes: (9, 504, 12, 64)"]
	140067041632960 -> 140067041636608
	140067041632960 [label=CloneBackward0]
	140067041636848 -> 140067041632960
	140067041636848 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041624320 -> 140067041636848
	140067041624320 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (108, 504, 64)"]
	140067041638000 -> 140067041624320
	140067041638000 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140067041639536 -> 140067041638000
	140067041639536 [label="ViewBackward0
---------------------------------
self_sym_sizes: (9, 12, 504, 504)"]
	140067041639728 -> 140067041639536
	140067041639728 [label="ExpandBackward0
---------------------------------
self_sym_sizes: (9, 12, 504, 504)"]
	140067041639344 -> 140067041639728
	140067041639344 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041637568 -> 140067041639344
	140067041637568 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140067041640304 -> 140067041637568
	140067041640304 [label="AddBackward0
------------
alpha: 1"]
	140067041639872 -> 140067041640304
	140067041639872 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041706336 -> 140067041639872
	140067041706336 [label="UnsafeViewBackward0
-------------------------------
self_sym_sizes: (108, 504, 504)"]
	140067041706624 -> 140067041706336
	140067041706624 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140067041706720 -> 140067041706624
	140067041706720 [label="UnsafeViewBackward0
--------------------------------
self_sym_sizes: (9, 12, 504, 64)"]
	140067041706816 -> 140067041706720
	140067041706816 [label=CloneBackward0]
	140067041706960 -> 140067041706816
	140067041706960 [label="ExpandBackward0
--------------------------------
self_sym_sizes: (9, 12, 504, 64)"]
	140067041707008 -> 140067041706960
	140067041707008 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041707152 -> 140067041707008
	140067041707152 [label="ViewBackward0
-----------------------------
self_sym_sizes: (9, 504, 768)"]
	140067041707392 -> 140067041707152
	140067041707392 [label="ViewBackward0
---------------------------
self_sym_sizes: (4536, 768)"]
	140067041707584 -> 140067041707392
	140067041707584 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :    (4536, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041594336 -> 140067041707584
	140067041707680 -> 140067041707584
	140067041707680 [label="ViewBackward0
-----------------------------
self_sym_sizes: (9, 504, 768)"]
	140067041576224 -> 140067041707680
	140067041576224 [label="ViewBackward0
--------------------------------
self_sym_sizes: (3, 3, 504, 768)"]
	140067041708352 -> 140067041576224
	140067041708352 [label="AsStridedBackward0
-----------------------------------------
size          :          (3, 3, 504, 768)
storage_offset:                         0
stride        : (1161216, 387072, 768, 1)"]
	140067041707872 -> 140067041708352
	140067041707872 [label=CopySlices]
	140067041579632 -> 140067041707872
	140067041708496 -> 140067041707872
	140067041708496 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 3, 768)
start         :                   0
step          :                   1"]
	140067041707776 -> 140067041708496
	140067041707776 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:         (3, 4, 768)
start         :                   1
step          :                   1"]
	140067041709072 -> 140067041707776
	140067041709072 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:         (3, 4, 768)
start         :                   0
step          :                   1"]
	140067041521152 -> 140067041709072
	140067041707632 -> 140067041707584
	140067041707632 [label=TBackward0]
	140067041594432 -> 140067041707632
	140067041706144 -> 140067041706624
	140067041706144 [label="UnsafeViewBackward0
--------------------------------
self_sym_sizes: (9, 12, 64, 504)"]
	140067041706480 -> 140067041706144
	140067041706480 [label=CloneBackward0]
	140067041707488 -> 140067041706480
	140067041707488 [label="ExpandBackward0
--------------------------------
self_sym_sizes: (9, 12, 64, 504)"]
	140067041707296 -> 140067041707488
	140067041707296 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551615
dim1: 18446744073709551614"]
	140067041707968 -> 140067041707296
	140067041707968 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041709024 -> 140067041707968
	140067041709024 [label="ViewBackward0
-----------------------------
self_sym_sizes: (9, 504, 768)"]
	140067041709216 -> 140067041709024
	140067041709216 [label="ViewBackward0
---------------------------
self_sym_sizes: (4536, 768)"]
	140067041708256 -> 140067041709216
	140067041708256 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :    (4536, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041596160 -> 140067041708256
	140067041709408 -> 140067041708256
	140067041709408 [label="ViewBackward0
-----------------------------
self_sym_sizes: (9, 504, 768)"]
	140067041576224 -> 140067041709408
	140067041708064 -> 140067041708256
	140067041708064 [label=TBackward0]
	140067041596352 -> 140067041708064
	140067041638720 -> 140067041638000
	140067041638720 [label="UnsafeViewBackward0
--------------------------------
self_sym_sizes: (9, 12, 504, 64)"]
	140067041640160 -> 140067041638720
	140067041640160 [label=CloneBackward0]
	140067041639488 -> 140067041640160
	140067041639488 [label="ExpandBackward0
--------------------------------
self_sym_sizes: (9, 12, 504, 64)"]
	140067041639632 -> 140067041639488
	140067041639632 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041707056 -> 140067041639632
	140067041707056 [label="ViewBackward0
-----------------------------
self_sym_sizes: (9, 504, 768)"]
	140067041706864 -> 140067041707056
	140067041706864 [label="ViewBackward0
---------------------------
self_sym_sizes: (4536, 768)"]
	140067041708448 -> 140067041706864
	140067041708448 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :    (4536, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041596016 -> 140067041708448
	140067041708544 -> 140067041708448
	140067041708544 [label="ViewBackward0
-----------------------------
self_sym_sizes: (9, 504, 768)"]
	140067041576224 -> 140067041708544
	140067041708112 -> 140067041708448
	140067041708112 [label=TBackward0]
	140067041596400 -> 140067041708112
	140067041626336 -> 140067041629024
	140067041626336 [label=TBackward0]
	140067041705504 -> 140067041626336
	140067041576224 -> 140067041584624
	140067041700176 -> 140067041523504
	140067041701712 -> 140067041523504
	140067041577904 -> 140067041578000
	140067041577904 [label=TBackward0]
	140067041704448 -> 140067041577904
	140067041524080 -> 140067041523696
	140067041524080 [label=TBackward0]
	140067041694464 -> 140067041524080
	140067041523504 -> 140067041522544
	140066708597792 -> 140067041523408
	140066708590256 -> 140067041523408
	140067041519760 -> 140067041519616
	140067041519760 [label=TBackward0]
	140067054819152 -> 140067041519760
	140067041518224 -> 140067041518560
	140067041518224 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 64, 4)"]
	140067041518464 -> 140067041518224
	140067041518464 [label=CloneBackward0]
	140067041519568 -> 140067041518464
	140067041519568 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 64, 4)"]
	140067041519376 -> 140067041519568
	140067041519376 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551615
dim1: 18446744073709551614"]
	140067041521056 -> 140067041519376
	140067041521056 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041520672 -> 140067041521056
	140067041520672 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 4, 768)"]
	140067041522304 -> 140067041520672
	140067041522304 [label="ViewBackward0
-------------------------
self_sym_sizes: (12, 768)"]
	140067041522064 -> 140067041522304
	140067041522064 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (12, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140066708597264 -> 140067041522064
	140067041520480 -> 140067041522064
	140067041520480 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 4, 768)"]
	140067041515920 -> 140067041520480
	140067041522736 -> 140067041522064
	140067041522736 [label=TBackward0]
	140066708599856 -> 140067041522736
	140067041516976 -> 140067041517024
	140067041516976 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 4, 64)"]
	140067041517120 -> 140067041516976
	140067041517120 [label=CloneBackward0]
	140067041517792 -> 140067041517120
	140067041517792 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 4, 64)"]
	140067041517936 -> 140067041517792
	140067041517936 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041519136 -> 140067041517936
	140067041519136 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 4, 768)"]
	140067041518848 -> 140067041519136
	140067041518848 [label="ViewBackward0
-------------------------
self_sym_sizes: (12, 768)"]
	140067041519808 -> 140067041518848
	140067041519808 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (12, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140066708602544 -> 140067041519808
	140067041518896 -> 140067041519808
	140067041518896 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 4, 768)"]
	140067041515920 -> 140067041518896
	140067041520192 -> 140067041519808
	140067041520192 [label=TBackward0]
	140067041700128 -> 140067041520192
	140067041516064 -> 140067041516160
	140067041516064 [label=TBackward0]
	140066907024032 -> 140067041516064
	140067041515920 -> 140067041515680
	140066907025040 -> 140067041514240
	140066907025856 -> 140067041514240
	140067041514960 -> 140067041514480
	140067041514960 [label=TBackward0]
	140066907025952 -> 140067041514960
	140067041514336 -> 140067041514384
	140067041514336 [label=TBackward0]
	140066907022688 -> 140067041514336
	140067041514240 -> 140067041513712
	140066907022160 -> 140067041514096
	140066907022256 -> 140067041514096
	140067041512848 -> 140067041951120
	140067041512848 [label=CloneBackward0]
	140067041513856 -> 140067041512848
	140067041513856 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 3, 768)
start         :                   0
step          :                   1"]
	140067041514144 -> 140067041513856
	140067041514144 [label="SelectBackward0
--------------------------------
dim           :                2
index         :                1
self_sym_sizes: (3, 3, 504, 768)"]
	140067041515392 -> 140067041514144
	140067041515392 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:    (3, 3, 504, 768)
start         :                   0
step          :                   1"]
	140067041513520 -> 140067041515392
	140067041513520 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:    (3, 3, 504, 768)
start         :                   0
step          :                   1"]
	140067041515488 -> 140067041513520
	140067041515488 [label="ViewBackward0
-----------------------------
self_sym_sizes: (9, 504, 768)"]
	140067041516496 -> 140067041515488
	140067041516496 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140067041515536 -> 140067041516496
	140067041515536 [label="AddBackward0
------------
alpha: 1"]
	140067041516832 -> 140067041515536
	140067041516832 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041516304 -> 140067041516832
	140067041516304 [label="ViewBackward0
---------------------------
self_sym_sizes: (4536, 768)"]
	140067041516688 -> 140067041516304
	140067041516688 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :   (4536, 3072)
mat1_sym_strides:      (3072, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (3072, 768)
mat2_sym_strides:      (1, 3072)"]
	140066708596112 -> 140067041516688
	140067041517648 -> 140067041516688
	140067041517648 [label="ViewBackward0
------------------------------
self_sym_sizes: (9, 504, 3072)"]
	140067041519232 -> 140067041517648
	140067041519232 [label="GeluBackward0
---------------------------
approximate:           none
self       : [saved tensor]"]
	140067041518368 -> 140067041519232
	140067041518368 [label="ViewBackward0
----------------------------
self_sym_sizes: (4536, 3072)"]
	140067041515632 -> 140067041518368
	140067041515632 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :    (4536, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (768, 3072)
mat2_sym_strides:       (1, 768)"]
	140067041705744 -> 140067041515632
	140067041517984 -> 140067041515632
	140067041517984 [label="ViewBackward0
-----------------------------
self_sym_sizes: (9, 504, 768)"]
	140067041516544 -> 140067041517984
	140067041516544 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140067041521824 -> 140067041516544
	140067041521824 [label="AddBackward0
------------
alpha: 1"]
	140067041580496 -> 140067041521824
	140067041580496 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041575456 -> 140067041580496
	140067041575456 [label="ViewBackward0
---------------------------
self_sym_sizes: (4536, 768)"]
	140067041575504 -> 140067041575456
	140067041575504 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :    (4536, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041591984 -> 140067041575504
	140067041626144 -> 140067041575504
	140067041626144 [label="ViewBackward0
-----------------------------
self_sym_sizes: (9, 504, 768)"]
	140067041639968 -> 140067041626144
	140067041639968 [label="ViewBackward0
--------------------------------
self_sym_sizes: (9, 504, 12, 64)"]
	140067041636416 -> 140067041639968
	140067041636416 [label=CloneBackward0]
	140067041629984 -> 140067041636416
	140067041629984 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041640208 -> 140067041629984
	140067041640208 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (108, 504, 64)"]
	140067041707104 -> 140067041640208
	140067041707104 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140067041708880 -> 140067041707104
	140067041708880 [label="ViewBackward0
---------------------------------
self_sym_sizes: (9, 12, 504, 504)"]
	140067041708784 -> 140067041708880
	140067041708784 [label="ExpandBackward0
---------------------------------
self_sym_sizes: (9, 12, 504, 504)"]
	140067041708688 -> 140067041708784
	140067041708688 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041706912 -> 140067041708688
	140067041706912 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140067041709648 -> 140067041706912
	140067041709648 [label="AddBackward0
------------
alpha: 1"]
	140067041709840 -> 140067041709648
	140067041709840 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041709984 -> 140067041709840
	140067041709984 [label="UnsafeViewBackward0
-------------------------------
self_sym_sizes: (108, 504, 504)"]
	140067041710176 -> 140067041709984
	140067041710176 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140067041710272 -> 140067041710176
	140067041710272 [label="UnsafeViewBackward0
--------------------------------
self_sym_sizes: (9, 12, 504, 64)"]
	140067041710368 -> 140067041710272
	140067041710368 [label=CloneBackward0]
	140067041710608 -> 140067041710368
	140067041710608 [label="ExpandBackward0
--------------------------------
self_sym_sizes: (9, 12, 504, 64)"]
	140067041710656 -> 140067041710608
	140067041710656 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041710896 -> 140067041710656
	140067041710896 [label="ViewBackward0
-----------------------------
self_sym_sizes: (9, 504, 768)"]
	140067041711232 -> 140067041710896
	140067041711232 [label="ViewBackward0
---------------------------
self_sym_sizes: (4536, 768)"]
	140067041711376 -> 140067041711232
	140067041711376 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :    (4536, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041600384 -> 140067041711376
	140067041711520 -> 140067041711376
	140067041711520 [label="ViewBackward0
-----------------------------
self_sym_sizes: (9, 504, 768)"]
	140067041578240 -> 140067041711520
	140067041578240 [label="ViewBackward0
--------------------------------
self_sym_sizes: (3, 3, 504, 768)"]
	140067041712048 -> 140067041578240
	140067041712048 [label="AsStridedBackward0
-----------------------------------------
size          :          (3, 3, 504, 768)
storage_offset:                         0
stride        : (1161216, 387072, 768, 1)"]
	140067041711712 -> 140067041712048
	140067041711712 [label=CopySlices]
	140067041523408 -> 140067041711712
	140067041712144 -> 140067041711712
	140067041712144 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 3, 768)
start         :                   0
step          :                   1"]
	140067041711664 -> 140067041712144
	140067041711664 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:         (3, 4, 768)
start         :                   1
step          :                   1"]
	140067041712864 -> 140067041711664
	140067041712864 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:         (3, 4, 768)
start         :                   0
step          :                   1"]
	140067041514096 -> 140067041712864
	140067041711424 -> 140067041711376
	140067041711424 [label=TBackward0]
	140067041600288 -> 140067041711424
	140067041709744 -> 140067041710176
	140067041709744 [label="UnsafeViewBackward0
--------------------------------
self_sym_sizes: (9, 12, 64, 504)"]
	140067041709888 -> 140067041709744
	140067041709888 [label=CloneBackward0]
	140067041711328 -> 140067041709888
	140067041711328 [label="ExpandBackward0
--------------------------------
self_sym_sizes: (9, 12, 64, 504)"]
	140067041711136 -> 140067041711328
	140067041711136 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551615
dim1: 18446744073709551614"]
	140067041711808 -> 140067041711136
	140067041711808 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041712816 -> 140067041711808
	140067041712816 [label="ViewBackward0
-----------------------------
self_sym_sizes: (9, 504, 768)"]
	140067041713056 -> 140067041712816
	140067041713056 [label="ViewBackward0
---------------------------
self_sym_sizes: (4536, 768)"]
	140067041711952 -> 140067041713056
	140067041711952 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :    (4536, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041601872 -> 140067041711952
	140067041713200 -> 140067041711952
	140067041713200 [label="ViewBackward0
-----------------------------
self_sym_sizes: (9, 504, 768)"]
	140067041578240 -> 140067041713200
	140067041711856 -> 140067041711952
	140067041711856 [label=TBackward0]
	140067041602016 -> 140067041711856
	140067041707920 -> 140067041707104
	140067041707920 [label="UnsafeViewBackward0
--------------------------------
self_sym_sizes: (9, 12, 504, 64)"]
	140067041709456 -> 140067041707920
	140067041709456 [label=CloneBackward0]
	140067041708928 -> 140067041709456
	140067041708928 [label="ExpandBackward0
--------------------------------
self_sym_sizes: (9, 12, 504, 64)"]
	140067041709504 -> 140067041708928
	140067041709504 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041710752 -> 140067041709504
	140067041710752 [label="ViewBackward0
-----------------------------
self_sym_sizes: (9, 504, 768)"]
	140067041710464 -> 140067041710752
	140067041710464 [label="ViewBackward0
---------------------------
self_sym_sizes: (4536, 768)"]
	140067041712096 -> 140067041710464
	140067041712096 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :    (4536, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041601776 -> 140067041712096
	140067041712240 -> 140067041712096
	140067041712240 [label="ViewBackward0
-----------------------------
self_sym_sizes: (9, 504, 768)"]
	140067041578240 -> 140067041712240
	140067041711904 -> 140067041712096
	140067041711904 [label=TBackward0]
	140067041602208 -> 140067041711904
	140067041624800 -> 140067041575504
	140067041624800 [label=TBackward0]
	140067041596688 -> 140067041624800
	140067041578240 -> 140067041521824
	140067041591360 -> 140067041516544
	140067041593040 -> 140067041516544
	140067041522592 -> 140067041515632
	140067041522592 [label=TBackward0]
	140067041700320 -> 140067041522592
	140067041517072 -> 140067041516688
	140067041517072 [label=TBackward0]
	140067041695088 -> 140067041517072
	140067041516544 -> 140067041515536
	140066907024608 -> 140067041516496
	140066907020528 -> 140067041516496
	140067041512464 -> 140067041512368
	140067041512464 [label=TBackward0]
	140066907020672 -> 140067041512464
	140067041511120 -> 140067041511360
	140067041511120 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 64, 4)"]
	140067041511264 -> 140067041511120
	140067041511264 [label=CloneBackward0]
	140067041512272 -> 140067041511264
	140067041512272 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 64, 4)"]
	140067041512080 -> 140067041512272
	140067041512080 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551615
dim1: 18446744073709551614"]
	140067041514048 -> 140067041512080
	140067041514048 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041513664 -> 140067041514048
	140067041513664 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 4, 768)"]
	140067041515344 -> 140067041513664
	140067041515344 [label="ViewBackward0
-------------------------
self_sym_sizes: (12, 768)"]
	140067041515200 -> 140067041515344
	140067041515200 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (12, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041700704 -> 140067041515200
	140067041513424 -> 140067041515200
	140067041513424 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 4, 768)"]
	140067041951120 -> 140067041513424
	140067041515824 -> 140067041515200
	140067041515824 [label=TBackward0]
	140066907023744 -> 140067041515824
	140067041509968 -> 140067041509872
	140067041509968 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 4, 64)"]
	140067041510160 -> 140067041509968
	140067041510160 [label=CloneBackward0]
	140067041510880 -> 140067041510160
	140067041510880 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 4, 64)"]
	140067041510688 -> 140067041510880
	140067041510688 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041511840 -> 140067041510688
	140067041511840 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 4, 768)"]
	140067041511552 -> 140067041511840
	140067041511552 [label="ViewBackward0
-------------------------
self_sym_sizes: (12, 768)"]
	140067041512560 -> 140067041511552
	140067041512560 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (12, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140066907028256 -> 140067041512560
	140067041511648 -> 140067041512560
	140067041511648 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 4, 768)"]
	140067041951120 -> 140067041511648
	140067041513184 -> 140067041512560
	140067041513184 [label=TBackward0]
	140066907027440 -> 140067041513184
	140067041951360 -> 140067041951408
	140067041951360 [label=TBackward0]
	140066907015824 -> 140067041951360
	140067041951120 -> 140067041950976
	140066907013952 -> 140067041949728
	140066907015632 -> 140067041949728
	140067041950304 -> 140067041949920
	140067041950304 [label=TBackward0]
	140066907014240 -> 140067041950304
	140067041949824 -> 140067041949872
	140067041949824 [label=TBackward0]
	140066907013184 -> 140067041949824
	140067041949728 -> 140067041949152
	140066487829984 -> 140067041949584
	140066487830224 -> 140067041949584
	140067041948576 -> 140067041944976
	140067041948576 [label=CloneBackward0]
	140067041949200 -> 140067041948576
	140067041949200 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 3, 768)
start         :                   0
step          :                   1"]
	140067041949680 -> 140067041949200
	140067041949680 [label="SelectBackward0
--------------------------------
dim           :                2
index         :                1
self_sym_sizes: (3, 3, 504, 768)"]
	140067041950736 -> 140067041949680
	140067041950736 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:    (3, 3, 504, 768)
start         :                   0
step          :                   1"]
	140067041949056 -> 140067041950736
	140067041949056 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:    (3, 3, 504, 768)
start         :                   0
step          :                   1"]
	140067041950784 -> 140067041949056
	140067041950784 [label="ViewBackward0
-----------------------------
self_sym_sizes: (9, 504, 768)"]
	140067041951600 -> 140067041950784
	140067041951600 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140067041950880 -> 140067041951600
	140067041950880 [label="AddBackward0
------------
alpha: 1"]
	140067041950928 -> 140067041950880
	140067041950928 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041949008 -> 140067041950928
	140067041949008 [label="ViewBackward0
---------------------------
self_sym_sizes: (4536, 768)"]
	140067041950112 -> 140067041949008
	140067041950112 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :   (4536, 3072)
mat1_sym_strides:      (3072, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (3072, 768)
mat2_sym_strides:      (1, 3072)"]
	140066907014672 -> 140067041950112
	140067041510592 -> 140067041950112
	140067041510592 [label="ViewBackward0
------------------------------
self_sym_sizes: (9, 504, 3072)"]
	140067041511936 -> 140067041510592
	140067041511936 [label="GeluBackward0
---------------------------
approximate:           none
self       : [saved tensor]"]
	140067041511312 -> 140067041511936
	140067041511312 [label="ViewBackward0
----------------------------
self_sym_sizes: (4536, 3072)"]
	140067041509728 -> 140067041511312
	140067041509728 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :    (4536, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (768, 3072)
mat2_sym_strides:       (1, 768)"]
	140067041597168 -> 140067041509728
	140067041510640 -> 140067041509728
	140067041510640 [label="ViewBackward0
-----------------------------
self_sym_sizes: (9, 504, 768)"]
	140067041951504 -> 140067041510640
	140067041951504 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140067041588896 -> 140067041951504
	140067041588896 [label="AddBackward0
------------
alpha: 1"]
	140067041574976 -> 140067041588896
	140067041574976 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041517408 -> 140067041574976
	140067041517408 [label="ViewBackward0
---------------------------
self_sym_sizes: (4536, 768)"]
	140067041519904 -> 140067041517408
	140067041519904 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :    (4536, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041595536 -> 140067041519904
	140067041512752 -> 140067041519904
	140067041512752 [label="ViewBackward0
-----------------------------
self_sym_sizes: (9, 504, 768)"]
	140067041637232 -> 140067041512752
	140067041637232 [label="ViewBackward0
--------------------------------
self_sym_sizes: (9, 504, 12, 64)"]
	140067041706528 -> 140067041637232
	140067041706528 [label=CloneBackward0]
	140067041706048 -> 140067041706528
	140067041706048 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041709600 -> 140067041706048
	140067041709600 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (108, 504, 64)"]
	140067041710800 -> 140067041709600
	140067041710800 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140067041712624 -> 140067041710800
	140067041712624 [label="ViewBackward0
---------------------------------
self_sym_sizes: (9, 12, 504, 504)"]
	140067041712576 -> 140067041712624
	140067041712576 [label="ExpandBackward0
---------------------------------
self_sym_sizes: (9, 12, 504, 504)"]
	140067041712288 -> 140067041712576
	140067041712288 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041710560 -> 140067041712288
	140067041710560 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140067041713536 -> 140067041710560
	140067041713536 [label="AddBackward0
------------
alpha: 1"]
	140067041713632 -> 140067041713536
	140067041713632 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041713824 -> 140067041713632
	140067041713824 [label="UnsafeViewBackward0
-------------------------------
self_sym_sizes: (108, 504, 504)"]
	140067041714256 -> 140067041713824
	140067041714256 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140067041714400 -> 140067041714256
	140067041714400 [label="UnsafeViewBackward0
--------------------------------
self_sym_sizes: (9, 12, 504, 64)"]
	140067041714496 -> 140067041714400
	140067041714496 [label=CloneBackward0]
	140067041714544 -> 140067041714496
	140067041714544 [label="ExpandBackward0
--------------------------------
self_sym_sizes: (9, 12, 504, 64)"]
	140067041714736 -> 140067041714544
	140067041714736 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041714976 -> 140067041714736
	140067041714976 [label="ViewBackward0
-----------------------------
self_sym_sizes: (9, 504, 768)"]
	140067041715168 -> 140067041714976
	140067041715168 [label="ViewBackward0
---------------------------
self_sym_sizes: (4536, 768)"]
	140067041715312 -> 140067041715168
	140067041715312 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :    (4536, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041606192 -> 140067041715312
	140067041715504 -> 140067041715312
	140067041715504 [label="ViewBackward0
-----------------------------
self_sym_sizes: (9, 504, 768)"]
	140067041521776 -> 140067041715504
	140067041521776 [label="ViewBackward0
--------------------------------
self_sym_sizes: (3, 3, 504, 768)"]
	140067041716272 -> 140067041521776
	140067041716272 [label="AsStridedBackward0
-----------------------------------------
size          :          (3, 3, 504, 768)
storage_offset:                         0
stride        : (1161216, 387072, 768, 1)"]
	140067041715696 -> 140067041716272
	140067041715696 [label=CopySlices]
	140067041516496 -> 140067041715696
	140067041716368 -> 140067041715696
	140067041716368 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 3, 768)
start         :                   0
step          :                   1"]
	140067041715600 -> 140067041716368
	140067041715600 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:         (3, 4, 768)
start         :                   1
step          :                   1"]
	140067041717136 -> 140067041715600
	140067041717136 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:         (3, 4, 768)
start         :                   0
step          :                   1"]
	140067041949584 -> 140067041717136
	140067041715360 -> 140067041715312
	140067041715360 [label=TBackward0]
	140067041606240 -> 140067041715360
	140067041713728 -> 140067041714256
	140067041713728 [label="UnsafeViewBackward0
--------------------------------
self_sym_sizes: (9, 12, 64, 504)"]
	140067041713968 -> 140067041713728
	140067041713968 [label=CloneBackward0]
	140067041715072 -> 140067041713968
	140067041715072 [label="ExpandBackward0
--------------------------------
self_sym_sizes: (9, 12, 64, 504)"]
	140067041715024 -> 140067041715072
	140067041715024 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551615
dim1: 18446744073709551614"]
	140067041715984 -> 140067041715024
	140067041715984 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041716704 -> 140067041715984
	140067041716704 [label="ViewBackward0
-----------------------------
self_sym_sizes: (9, 504, 768)"]
	140067041717424 -> 140067041716704
	140067041717424 [label="ViewBackward0
---------------------------
self_sym_sizes: (4536, 768)"]
	140067041716224 -> 140067041717424
	140067041716224 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :    (4536, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041607344 -> 140067041716224
	140067041717712 -> 140067041716224
	140067041717712 [label="ViewBackward0
-----------------------------
self_sym_sizes: (9, 504, 768)"]
	140067041521776 -> 140067041717712
	140067041716080 -> 140067041716224
	140067041716080 [label=TBackward0]
	140067041673520 -> 140067041716080
	140067041711760 -> 140067041710800
	140067041711760 [label="UnsafeViewBackward0
--------------------------------
self_sym_sizes: (9, 12, 504, 64)"]
	140067041713344 -> 140067041711760
	140067041713344 [label=CloneBackward0]
	140067041712720 -> 140067041713344
	140067041712720 [label="ExpandBackward0
--------------------------------
self_sym_sizes: (9, 12, 504, 64)"]
	140067041713392 -> 140067041712720
	140067041713392 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041714880 -> 140067041713392
	140067041714880 [label="ViewBackward0
-----------------------------
self_sym_sizes: (9, 504, 768)"]
	140067041714304 -> 140067041714880
	140067041714304 [label="ViewBackward0
---------------------------
self_sym_sizes: (4536, 768)"]
	140067041716320 -> 140067041714304
	140067041716320 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :    (4536, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041607392 -> 140067041716320
	140067041716464 -> 140067041716320
	140067041716464 [label="ViewBackward0
-----------------------------
self_sym_sizes: (9, 504, 768)"]
	140067041521776 -> 140067041716464
	140067041716176 -> 140067041716320
	140067041716176 [label=TBackward0]
	140067041673952 -> 140067041716176
	140067041635888 -> 140067041519904
	140067041635888 [label=TBackward0]
	140067041602352 -> 140067041635888
	140067041521776 -> 140067041588896
	140067041596880 -> 140067041951504
	140067041598560 -> 140067041951504
	140067041514864 -> 140067041509728
	140067041514864 [label=TBackward0]
	140067041601296 -> 140067041514864
	140067041509536 -> 140067041950112
	140067041509536 [label=TBackward0]
	140066907018368 -> 140067041509536
	140067041951504 -> 140067041950880
	140066907016544 -> 140067041951600
	140066907013760 -> 140067041951600
	140067041948288 -> 140067041948240
	140067041948288 [label=TBackward0]
	140066907258352 -> 140067041948288
	140067041947040 -> 140067041947376
	140067041947040 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 64, 4)"]
	140067041947184 -> 140067041947040
	140067041947184 [label=CloneBackward0]
	140067041948192 -> 140067041947184
	140067041948192 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 64, 4)"]
	140067041948096 -> 140067041948192
	140067041948096 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551615
dim1: 18446744073709551614"]
	140067041949536 -> 140067041948096
	140067041949536 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041949104 -> 140067041949536
	140067041949104 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 4, 768)"]
	140067041950640 -> 140067041949104
	140067041950640 [label="ViewBackward0
-------------------------
self_sym_sizes: (12, 768)"]
	140067041950496 -> 140067041950640
	140067041950496 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (12, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140066907014816 -> 140067041950496
	140067041583232 -> 140067041950496
	140067041583232 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 4, 768)"]
	140067041944976 -> 140067041583232
	140067041951024 -> 140067041950496
	140067041951024 [label=TBackward0]
	140066907016928 -> 140067041951024
	140067041945888 -> 140067041945984
	140067041945888 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 4, 64)"]
	140067041946224 -> 140067041945888
	140067041946224 [label=CloneBackward0]
	140067041946656 -> 140067041946224
	140067041946656 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 4, 64)"]
	140067041946752 -> 140067041946656
	140067041946752 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041947904 -> 140067041946752
	140067041947904 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 4, 768)"]
	140067041947568 -> 140067041947904
	140067041947568 [label="ViewBackward0
-------------------------
self_sym_sizes: (12, 768)"]
	140067041948336 -> 140067041947568
	140067041948336 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (12, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140066487830176 -> 140067041948336
	140067041950160 -> 140067041948336
	140067041950160 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 4, 768)"]
	140067041944976 -> 140067041950160
	140067041948768 -> 140067041948336
	140067041948768 [label=TBackward0]
	140067041596784 -> 140067041948768
	140067041945072 -> 140067041945120
	140067041945072 [label=TBackward0]
	140066907252448 -> 140067041945072
	140067041944976 -> 140067041944784
	140066907251680 -> 140067041943392
	140066907251296 -> 140067041943392
	140067041944064 -> 140067041943632
	140067041944064 [label=TBackward0]
	140066907251968 -> 140067041944064
	140067041943536 -> 140067041943584
	140067041943536 [label=TBackward0]
	140066907251152 -> 140067041943536
	140067041943392 -> 140067041942912
	140066907249904 -> 140067041943248
	140066907250912 -> 140067041943248
	140067041941952 -> 140067041937872
	140067041941952 [label=CloneBackward0]
	140067041942720 -> 140067041941952
	140067041942720 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 3, 768)
start         :                   0
step          :                   1"]
	140067041943296 -> 140067041942720
	140067041943296 [label="SelectBackward0
--------------------------------
dim           :                2
index         :                1
self_sym_sizes: (3, 3, 504, 768)"]
	140067041944400 -> 140067041943296
	140067041944400 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:    (3, 3, 504, 768)
start         :                   0
step          :                   1"]
	140067041942672 -> 140067041944400
	140067041942672 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:    (3, 3, 504, 768)
start         :                   0
step          :                   1"]
	140067041944544 -> 140067041942672
	140067041944544 [label="ViewBackward0
-----------------------------
self_sym_sizes: (9, 504, 768)"]
	140067041945552 -> 140067041944544
	140067041945552 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140067041944640 -> 140067041945552
	140067041944640 [label="AddBackward0
------------
alpha: 1"]
	140067041945840 -> 140067041944640
	140067041945840 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041945264 -> 140067041945840
	140067041945264 [label="ViewBackward0
---------------------------
self_sym_sizes: (4536, 768)"]
	140067041945744 -> 140067041945264
	140067041945744 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :   (4536, 3072)
mat1_sym_strides:      (3072, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (3072, 768)
mat2_sym_strides:      (1, 3072)"]
	140066907013520 -> 140067041945744
	140067041946512 -> 140067041945744
	140067041946512 [label="ViewBackward0
------------------------------
self_sym_sizes: (9, 504, 3072)"]
	140067041947952 -> 140067041946512
	140067041947952 [label="GeluBackward0
---------------------------
approximate:           none
self       : [saved tensor]"]
	140067041947280 -> 140067041947952
	140067041947280 [label="ViewBackward0
----------------------------
self_sym_sizes: (4536, 3072)"]
	140067041944688 -> 140067041947280
	140067041944688 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :    (4536, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (768, 3072)
mat2_sym_strides:       (1, 768)"]
	140067041602832 -> 140067041944688
	140067041946800 -> 140067041944688
	140067041946800 [label="ViewBackward0
-----------------------------
self_sym_sizes: (9, 504, 768)"]
	140067041945456 -> 140067041946800
	140067041945456 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140067041520096 -> 140067041945456
	140067041520096 [label="AddBackward0
------------
alpha: 1"]
	140067041522016 -> 140067041520096
	140067041522016 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041632816 -> 140067041522016
	140067041632816 [label="ViewBackward0
---------------------------
self_sym_sizes: (4536, 768)"]
	140067041510544 -> 140067041632816
	140067041510544 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :    (4536, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041673328 -> 140067041510544
	140067041515152 -> 140067041510544
	140067041515152 [label="ViewBackward0
-----------------------------
self_sym_sizes: (9, 504, 768)"]
	140067041713104 -> 140067041515152
	140067041713104 [label="ViewBackward0
--------------------------------
self_sym_sizes: (9, 504, 12, 64)"]
	140067041709120 -> 140067041713104
	140067041709120 [label=CloneBackward0]
	140067041713440 -> 140067041709120
	140067041713440 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041706240 -> 140067041713440
	140067041706240 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (108, 504, 64)"]
	140067041714928 -> 140067041706240
	140067041714928 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140067041716896 -> 140067041714928
	140067041716896 [label="ViewBackward0
---------------------------------
self_sym_sizes: (9, 12, 504, 504)"]
	140067041716800 -> 140067041716896
	140067041716800 [label="ExpandBackward0
---------------------------------
self_sym_sizes: (9, 12, 504, 504)"]
	140067041716560 -> 140067041716800
	140067041716560 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041714640 -> 140067041716560
	140067041714640 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140067041718144 -> 140067041714640
	140067041718144 [label="AddBackward0
------------
alpha: 1"]
	140067041718192 -> 140067041718144
	140067041718192 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041718288 -> 140067041718192
	140067041718288 [label="UnsafeViewBackward0
-------------------------------
self_sym_sizes: (108, 504, 504)"]
	140067041718528 -> 140067041718288
	140067041718528 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140067041718624 -> 140067041718528
	140067041718624 [label="UnsafeViewBackward0
--------------------------------
self_sym_sizes: (9, 12, 504, 64)"]
	140067041718816 -> 140067041718624
	140067041718816 [label=CloneBackward0]
	140067041718864 -> 140067041718816
	140067041718864 [label="ExpandBackward0
--------------------------------
self_sym_sizes: (9, 12, 504, 64)"]
	140067041719200 -> 140067041718864
	140067041719200 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041719392 -> 140067041719200
	140067041719392 [label="ViewBackward0
-----------------------------
self_sym_sizes: (9, 504, 768)"]
	140067041719584 -> 140067041719392
	140067041719584 [label="ViewBackward0
---------------------------
self_sym_sizes: (4536, 768)"]
	140067041719824 -> 140067041719584
	140067041719824 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :    (4536, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041677264 -> 140067041719824
	140067041720112 -> 140067041719824
	140067041720112 [label="ViewBackward0
-----------------------------
self_sym_sizes: (9, 504, 768)"]
	140067041517504 -> 140067041720112
	140067041517504 [label="ViewBackward0
--------------------------------
self_sym_sizes: (3, 3, 504, 768)"]
	140067041720736 -> 140067041517504
	140067041720736 [label="AsStridedBackward0
-----------------------------------------
size          :          (3, 3, 504, 768)
storage_offset:                         0
stride        : (1161216, 387072, 768, 1)"]
	140067041720256 -> 140067041720736
	140067041720256 [label=CopySlices]
	140067041951600 -> 140067041720256
	140067041720832 -> 140067041720256
	140067041720832 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 3, 768)
start         :                   0
step          :                   1"]
	140067041720016 -> 140067041720832
	140067041720016 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:         (3, 4, 768)
start         :                   1
step          :                   1"]
	140067041721552 -> 140067041720016
	140067041721552 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:         (3, 4, 768)
start         :                   0
step          :                   1"]
	140067041943248 -> 140067041721552
	140067041719872 -> 140067041719824
	140067041719872 [label=TBackward0]
	140067041677456 -> 140067041719872
	140067041718240 -> 140067041718528
	140067041718240 [label="UnsafeViewBackward0
--------------------------------
self_sym_sizes: (9, 12, 64, 504)"]
	140067041718336 -> 140067041718240
	140067041718336 [label=CloneBackward0]
	140067041719728 -> 140067041718336
	140067041719728 [label="ExpandBackward0
--------------------------------
self_sym_sizes: (9, 12, 64, 504)"]
	140067041719536 -> 140067041719728
	140067041719536 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551615
dim1: 18446744073709551614"]
	140067041720304 -> 140067041719536
	140067041720304 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041721504 -> 140067041720304
	140067041721504 [label="ViewBackward0
-----------------------------
self_sym_sizes: (9, 504, 768)"]
	140067041721696 -> 140067041721504
	140067041721696 [label="ViewBackward0
---------------------------
self_sym_sizes: (4536, 768)"]
	140067041720640 -> 140067041721696
	140067041720640 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :    (4536, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041679088 -> 140067041720640
	140067041721984 -> 140067041720640
	140067041721984 [label="ViewBackward0
-----------------------------
self_sym_sizes: (9, 504, 768)"]
	140067041517504 -> 140067041721984
	140067041720448 -> 140067041720640
	140067041720448 [label=TBackward0]
	140067041679232 -> 140067041720448
	140067041715888 -> 140067041714928
	140067041715888 [label="UnsafeViewBackward0
--------------------------------
self_sym_sizes: (9, 12, 504, 64)"]
	140067041717808 -> 140067041715888
	140067041717808 [label=CloneBackward0]
	140067041716992 -> 140067041717808
	140067041716992 [label="ExpandBackward0
--------------------------------
self_sym_sizes: (9, 12, 504, 64)"]
	140067041718000 -> 140067041716992
	140067041718000 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041719248 -> 140067041718000
	140067041719248 [label="ViewBackward0
-----------------------------
self_sym_sizes: (9, 504, 768)"]
	140067041718720 -> 140067041719248
	140067041718720 [label="ViewBackward0
---------------------------
self_sym_sizes: (4536, 768)"]
	140067041720784 -> 140067041718720
	140067041720784 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :    (4536, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041678896 -> 140067041720784
	140067041720928 -> 140067041720784
	140067041720928 [label="ViewBackward0
-----------------------------
self_sym_sizes: (9, 504, 768)"]
	140067041517504 -> 140067041720928
	140067041720544 -> 140067041720784
	140067041720544 [label=TBackward0]
	140067041679328 -> 140067041720544
	140067041514624 -> 140067041510544
	140067041514624 [label=TBackward0]
	140067041674240 -> 140067041514624
	140067041517504 -> 140067041520096
	140067041602544 -> 140067041945456
	140067041603888 -> 140067041945456
	140067041950352 -> 140067041944688
	140067041950352 [label=TBackward0]
	140067041605232 -> 140067041950352
	140067041946080 -> 140067041945744
	140067041946080 [label=TBackward0]
	140067041592128 -> 140067041946080
	140067041945456 -> 140067041944640
	140066907253264 -> 140067041945552
	140066907247840 -> 140067041945552
	140067041941664 -> 140067041941616
	140067041941664 [label=TBackward0]
	140066907249184 -> 140067041941664
	140067041940176 -> 140067041940608
	140067041940176 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 64, 4)"]
	140067041940368 -> 140067041940176
	140067041940368 [label=CloneBackward0]
	140067041941568 -> 140067041940368
	140067041941568 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 64, 4)"]
	140067041941376 -> 140067041941568
	140067041941376 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551615
dim1: 18446744073709551614"]
	140067041943152 -> 140067041941376
	140067041943152 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041942816 -> 140067041943152
	140067041942816 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 4, 768)"]
	140067041944304 -> 140067041942816
	140067041944304 [label="ViewBackward0
-------------------------
self_sym_sizes: (12, 768)"]
	140067041944160 -> 140067041944304
	140067041944160 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (12, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041606288 -> 140067041944160
	140067041942624 -> 140067041944160
	140067041942624 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 4, 768)"]
	140067041937872 -> 140067041942624
	140067041944832 -> 140067041944160
	140067041944832 [label=TBackward0]
	140067041602592 -> 140067041944832
	140067041938976 -> 140067041939024
	140067041938976 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 4, 64)"]
	140067041939216 -> 140067041938976
	140067041939216 [label=CloneBackward0]
	140067041939792 -> 140067041939216
	140067041939792 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 4, 64)"]
	140067041939840 -> 140067041939792
	140067041939840 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041941280 -> 140067041939840
	140067041941280 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 4, 768)"]
	140067041940848 -> 140067041941280
	140067041940848 [label="ViewBackward0
-------------------------
self_sym_sizes: (12, 768)"]
	140067041941712 -> 140067041940848
	140067041941712 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (12, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140066907250240 -> 140067041941712
	140067041940896 -> 140067041941712
	140067041940896 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 4, 768)"]
	140067041937872 -> 140067041940896
	140067041942096 -> 140067041941712
	140067041942096 [label=TBackward0]
	140066907246352 -> 140067041942096
	140067041937968 -> 140067041938112
	140067041937968 [label=TBackward0]
	140066907242896 -> 140067041937968
	140067041937872 -> 140067041937728
	140066907258016 -> 140067041936336
	140066907252784 -> 140067041936336
	140067041937008 -> 140067041936576
	140067041937008 [label=TBackward0]
	140066879634640 -> 140067041937008
	140067041936432 -> 140067041936480
	140067041936432 [label=TBackward0]
	140066879634400 -> 140067041936432
	140067041936336 -> 140067041935856
	140066879633200 -> 140067041936192
	140066879633248 -> 140067041936192
	140067042229888 -> 140067042225760
	140067042229888 [label=CloneBackward0]
	140067042229840 -> 140067042229888
	140067042229840 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 3, 768)
start         :                   0
step          :                   1"]
	140067041936288 -> 140067042229840
	140067041936288 [label="SelectBackward0
--------------------------------
dim           :                2
index         :                1
self_sym_sizes: (3, 3, 504, 768)"]
	140067041937344 -> 140067041936288
	140067041937344 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:    (3, 3, 504, 768)
start         :                   0
step          :                   1"]
	140067041935616 -> 140067041937344
	140067041935616 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:    (3, 3, 504, 768)
start         :                   0
step          :                   1"]
	140067041937536 -> 140067041935616
	140067041937536 [label="ViewBackward0
-----------------------------
self_sym_sizes: (9, 504, 768)"]
	140067041938400 -> 140067041937536
	140067041938400 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140067041937632 -> 140067041938400
	140067041937632 [label="AddBackward0
------------
alpha: 1"]
	140067041938880 -> 140067041937632
	140067041938880 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041938496 -> 140067041938880
	140067041938496 [label="ViewBackward0
---------------------------
self_sym_sizes: (4536, 768)"]
	140067041938784 -> 140067041938496
	140067041938784 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :   (4536, 3072)
mat1_sym_strides:      (3072, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (3072, 768)
mat2_sym_strides:      (1, 3072)"]
	140066907243520 -> 140067041938784
	140067041939600 -> 140067041938784
	140067041939600 [label="ViewBackward0
------------------------------
self_sym_sizes: (9, 504, 3072)"]
	140067041941136 -> 140067041939600
	140067041941136 [label="GeluBackward0
---------------------------
approximate:           none
self       : [saved tensor]"]
	140067041940512 -> 140067041941136
	140067041940512 [label="ViewBackward0
----------------------------
self_sym_sizes: (4536, 3072)"]
	140067041937680 -> 140067041940512
	140067041937680 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :    (4536, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (768, 3072)
mat2_sym_strides:       (1, 768)"]
	140067041674720 -> 140067041937680
	140067041633680 -> 140067041937680
	140067041633680 [label="ViewBackward0
-----------------------------
self_sym_sizes: (9, 504, 768)"]
	140067041938544 -> 140067041633680
	140067041938544 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140067041510016 -> 140067041938544
	140067041510016 [label="AddBackward0
------------
alpha: 1"]
	140067041510304 -> 140067041510016
	140067041510304 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041947664 -> 140067041510304
	140067041947664 [label="ViewBackward0
---------------------------
self_sym_sizes: (4536, 768)"]
	140067041948480 -> 140067041947664
	140067041948480 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :    (4536, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041673376 -> 140067041948480
	140067041946368 -> 140067041948480
	140067041946368 [label="ViewBackward0
-----------------------------
self_sym_sizes: (9, 504, 768)"]
	140067041717568 -> 140067041946368
	140067041717568 [label="ViewBackward0
--------------------------------
self_sym_sizes: (9, 504, 12, 64)"]
	140067041712960 -> 140067041717568
	140067041712960 [label=CloneBackward0]
	140067041718096 -> 140067041712960
	140067041718096 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041709312 -> 140067041718096
	140067041709312 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (108, 504, 64)"]
	140067041719344 -> 140067041709312
	140067041719344 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140067041721312 -> 140067041719344
	140067041721312 [label="ViewBackward0
---------------------------------
self_sym_sizes: (9, 12, 504, 504)"]
	140067041721264 -> 140067041721312
	140067041721264 [label="ExpandBackward0
---------------------------------
self_sym_sizes: (9, 12, 504, 504)"]
	140067041721024 -> 140067041721264
	140067041721024 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041719056 -> 140067041721024
	140067041719056 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140067041722224 -> 140067041719056
	140067041722224 [label="AddBackward0
------------
alpha: 1"]
	140067041721600 -> 140067041722224
	140067041721600 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067042083168 -> 140067041721600
	140067042083168 [label="UnsafeViewBackward0
-------------------------------
self_sym_sizes: (108, 504, 504)"]
	140067042083456 -> 140067042083168
	140067042083456 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140067042083552 -> 140067042083456
	140067042083552 [label="UnsafeViewBackward0
--------------------------------
self_sym_sizes: (9, 12, 504, 64)"]
	140067042083648 -> 140067042083552
	140067042083648 [label=CloneBackward0]
	140067042083936 -> 140067042083648
	140067042083936 [label="ExpandBackward0
--------------------------------
self_sym_sizes: (9, 12, 504, 64)"]
	140067042084032 -> 140067042083936
	140067042084032 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067042084272 -> 140067042084032
	140067042084272 [label="ViewBackward0
-----------------------------
self_sym_sizes: (9, 504, 768)"]
	140067042084368 -> 140067042084272
	140067042084368 [label="ViewBackward0
---------------------------
self_sym_sizes: (4536, 768)"]
	140067042084512 -> 140067042084368
	140067042084512 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :    (4536, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041682832 -> 140067042084512
	140067042084704 -> 140067042084512
	140067042084704 [label="ViewBackward0
-----------------------------
self_sym_sizes: (9, 504, 768)"]
	140067041509584 -> 140067042084704
	140067041509584 [label="ViewBackward0
--------------------------------
self_sym_sizes: (3, 3, 504, 768)"]
	140067042085424 -> 140067041509584
	140067042085424 [label="AsStridedBackward0
-----------------------------------------
size          :          (3, 3, 504, 768)
storage_offset:                         0
stride        : (1161216, 387072, 768, 1)"]
	140067042085040 -> 140067042085424
	140067042085040 [label=CopySlices]
	140067041945552 -> 140067042085040
	140067042085520 -> 140067042085040
	140067042085520 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 3, 768)
start         :                   0
step          :                   1"]
	140067042084944 -> 140067042085520
	140067042084944 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:         (3, 4, 768)
start         :                   1
step          :                   1"]
	140067042086240 -> 140067042084944
	140067042086240 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:         (3, 4, 768)
start         :                   0
step          :                   1"]
	140067041936192 -> 140067042086240
	140067042084608 -> 140067042084512
	140067042084608 [label=TBackward0]
	140067041682880 -> 140067042084608
	140067042082928 -> 140067042083456
	140067042082928 [label="UnsafeViewBackward0
--------------------------------
self_sym_sizes: (9, 12, 64, 504)"]
	140067042083360 -> 140067042082928
	140067042083360 [label=CloneBackward0]
	140067042084416 -> 140067042083360
	140067042084416 [label="ExpandBackward0
--------------------------------
self_sym_sizes: (9, 12, 64, 504)"]
	140067042084320 -> 140067042084416
	140067042084320 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551615
dim1: 18446744073709551614"]
	140067042085136 -> 140067042084320
	140067042085136 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067042086096 -> 140067042085136
	140067042086096 [label="ViewBackward0
-----------------------------
self_sym_sizes: (9, 504, 768)"]
	140067042086384 -> 140067042086096
	140067042086384 [label="ViewBackward0
---------------------------
self_sym_sizes: (4536, 768)"]
	140067042085376 -> 140067042086384
	140067042085376 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :    (4536, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041684464 -> 140067042085376
	140067042086528 -> 140067042085376
	140067042086528 [label="ViewBackward0
-----------------------------
self_sym_sizes: (9, 504, 768)"]
	140067041509584 -> 140067042086528
	140067042085184 -> 140067042085376
	140067042085184 [label=TBackward0]
	140067041684560 -> 140067042085184
	140067041720160 -> 140067041719344
	140067041720160 [label="UnsafeViewBackward0
--------------------------------
self_sym_sizes: (9, 12, 504, 64)"]
	140067041721888 -> 140067041720160
	140067041721888 [label=CloneBackward0]
	140067041721360 -> 140067041721888
	140067041721360 [label="ExpandBackward0
--------------------------------
self_sym_sizes: (9, 12, 504, 64)"]
	140067041721120 -> 140067041721360
	140067041721120 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067042084080 -> 140067041721120
	140067042084080 [label="ViewBackward0
-----------------------------
self_sym_sizes: (9, 504, 768)"]
	140067042083840 -> 140067042084080
	140067042083840 [label="ViewBackward0
---------------------------
self_sym_sizes: (4536, 768)"]
	140067042085472 -> 140067042083840
	140067042085472 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :    (4536, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041684272 -> 140067042085472
	140067042085616 -> 140067042085472
	140067042085616 [label="ViewBackward0
-----------------------------
self_sym_sizes: (9, 504, 768)"]
	140067041509584 -> 140067042085616
	140067042085328 -> 140067042085472
	140067042085328 [label=TBackward0]
	140067041684608 -> 140067042085328
	140067041939408 -> 140067041948480
	140067041939408 [label=TBackward0]
	140067041679568 -> 140067041939408
	140067041509584 -> 140067041510016
	140067041674384 -> 140067041938544
	140067041675872 -> 140067041938544
	140067041943968 -> 140067041937680
	140067041943968 [label=TBackward0]
	140067041678464 -> 140067041943968
	140067041939168 -> 140067041938784
	140067041939168 [label=TBackward0]
	140066907255760 -> 140067041939168
	140067041938544 -> 140067041937632
	140066907255712 -> 140067041938400
	140066907243280 -> 140067041938400
	140067042229552 -> 140067042229456
	140067042229552 [label=TBackward0]
	140066879632672 -> 140067042229552
	140067042228160 -> 140067042228448
	140067042228160 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 64, 4)"]
	140067042228352 -> 140067042228160
	140067042228352 [label=CloneBackward0]
	140067042229408 -> 140067042228352
	140067042229408 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 64, 4)"]
	140067042229216 -> 140067042229408
	140067042229216 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551615
dim1: 18446744073709551614"]
	140067041513040 -> 140067042229216
	140067041513040 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067042228784 -> 140067041513040
	140067042228784 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 4, 768)"]
	140067042229936 -> 140067042228784
	140067042229936 [label="ViewBackward0
-------------------------
self_sym_sizes: (12, 768)"]
	140067041937248 -> 140067042229936
	140067041937248 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (12, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140066907253792 -> 140067041937248
	140067041936096 -> 140067041937248
	140067041936096 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 4, 768)"]
	140067042225760 -> 140067041936096
	140067041937152 -> 140067041937248
	140067041937152 [label=TBackward0]
	140066907251056 -> 140067041937152
	140067042226912 -> 140067042227056
	140067042226912 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 4, 64)"]
	140067042227248 -> 140067042226912
	140067042227248 [label=CloneBackward0]
	140067042227824 -> 140067042227248
	140067042227824 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 4, 64)"]
	140067042227968 -> 140067042227824
	140067042227968 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067042229024 -> 140067042227968
	140067042229024 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 4, 768)"]
	140067042228640 -> 140067042229024
	140067042228640 [label="ViewBackward0
-------------------------
self_sym_sizes: (12, 768)"]
	140067042230080 -> 140067042228640
	140067042230080 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (12, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140066879626384 -> 140067042230080
	140067042227392 -> 140067042230080
	140067042227392 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 4, 768)"]
	140067042225760 -> 140067042227392
	140067041935712 -> 140067042230080
	140067041935712 [label=TBackward0]
	140066907243376 -> 140067041935712
	140067042225952 -> 140067042226048
	140067042225952 [label=TBackward0]
	140066879626432 -> 140067042225952
	140067042225760 -> 140067042225472
	140066879625088 -> 140067042223936
	140066879625472 -> 140067042223936
	140067042224608 -> 140067042223984
	140067042224608 [label=TBackward0]
	140066879625616 -> 140067042224608
	140067042224176 -> 140067042224272
	140067042224176 [label=TBackward0]
	140066879623744 -> 140067042224176
	140067042223936 -> 140067042223216
	140066879622784 -> 140067042223792
	140066879623600 -> 140067042223792
	140067042222736 -> 140067042218368
	140067042222736 [label=CloneBackward0]
	140067042223360 -> 140067042222736
	140067042223360 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 3, 768)
start         :                   0
step          :                   1"]
	140067042223888 -> 140067042223360
	140067042223888 [label="SelectBackward0
--------------------------------
dim           :                2
index         :                1
self_sym_sizes: (3, 3, 504, 768)"]
	140067042225136 -> 140067042223888
	140067042225136 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:    (3, 3, 504, 768)
start         :                   0
step          :                   1"]
	140067042223024 -> 140067042225136
	140067042223024 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:    (3, 3, 504, 768)
start         :                   0
step          :                   1"]
	140067042225232 -> 140067042223024
	140067042225232 [label="ViewBackward0
-----------------------------
self_sym_sizes: (9, 504, 768)"]
	140067042226144 -> 140067042225232
	140067042226144 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140067042225328 -> 140067042226144
	140067042225328 [label="AddBackward0
------------
alpha: 1"]
	140067042226720 -> 140067042225328
	140067042226720 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067042226240 -> 140067042226720
	140067042226240 [label="ViewBackward0
---------------------------
self_sym_sizes: (4536, 768)"]
	140067042226672 -> 140067042226240
	140067042226672 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :   (4536, 3072)
mat1_sym_strides:      (3072, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (3072, 768)
mat2_sym_strides:      (1, 3072)"]
	140066907243712 -> 140067042226672
	140067042227776 -> 140067042226672
	140067042227776 [label="ViewBackward0
------------------------------
self_sym_sizes: (9, 504, 3072)"]
	140067042230032 -> 140067042227776
	140067042230032 [label="GeluBackward0
---------------------------
approximate:           none
self       : [saved tensor]"]
	140067042227872 -> 140067042230032
	140067042227872 [label="ViewBackward0
----------------------------
self_sym_sizes: (4536, 3072)"]
	140067042229120 -> 140067042227872
	140067042229120 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :    (4536, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (768, 3072)
mat2_sym_strides:       (1, 768)"]
	140067041680000 -> 140067042229120
	140067041936816 -> 140067042229120
	140067041936816 [label="ViewBackward0
-----------------------------
self_sym_sizes: (9, 504, 768)"]
	140067042226528 -> 140067041936816
	140067042226528 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140067041948720 -> 140067042226528
	140067041948720 [label="AddBackward0
------------
alpha: 1"]
	140067041946416 -> 140067041948720
	140067041946416 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041936864 -> 140067041946416
	140067041936864 [label="ViewBackward0
---------------------------
self_sym_sizes: (4536, 768)"]
	140067041939360 -> 140067041936864
	140067041939360 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :    (4536, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041676304 -> 140067041939360
	140067041937776 -> 140067041939360
	140067041937776 [label="ViewBackward0
-----------------------------
self_sym_sizes: (9, 504, 768)"]
	140067041721792 -> 140067041937776
	140067041721792 [label="ViewBackward0
--------------------------------
self_sym_sizes: (9, 504, 12, 64)"]
	140067041717376 -> 140067041721792
	140067041717376 [label=CloneBackward0]
	140067041710080 -> 140067041717376
	140067041710080 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067041722176 -> 140067041710080
	140067041722176 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (108, 504, 64)"]
	140067042084176 -> 140067041722176
	140067042084176 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140067042086000 -> 140067042084176
	140067042086000 [label="ViewBackward0
---------------------------------
self_sym_sizes: (9, 12, 504, 504)"]
	140067042085856 -> 140067042086000
	140067042085856 [label="ExpandBackward0
---------------------------------
self_sym_sizes: (9, 12, 504, 504)"]
	140067042085712 -> 140067042085856
	140067042085712 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067042083744 -> 140067042085712
	140067042083744 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140067042086864 -> 140067042083744
	140067042086864 [label="AddBackward0
------------
alpha: 1"]
	140067042086816 -> 140067042086864
	140067042086816 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067042087056 -> 140067042086816
	140067042087056 [label="UnsafeViewBackward0
-------------------------------
self_sym_sizes: (108, 504, 504)"]
	140067042087296 -> 140067042087056
	140067042087296 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140067042087440 -> 140067042087296
	140067042087440 [label="UnsafeViewBackward0
--------------------------------
self_sym_sizes: (9, 12, 504, 64)"]
	140067042087536 -> 140067042087440
	140067042087536 [label=CloneBackward0]
	140067042087824 -> 140067042087536
	140067042087824 [label="ExpandBackward0
--------------------------------
self_sym_sizes: (9, 12, 504, 64)"]
	140067042087872 -> 140067042087824
	140067042087872 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067042088160 -> 140067042087872
	140067042088160 [label="ViewBackward0
-----------------------------
self_sym_sizes: (9, 504, 768)"]
	140067042088352 -> 140067042088160
	140067042088352 [label="ViewBackward0
---------------------------
self_sym_sizes: (4536, 768)"]
	140067042088400 -> 140067042088352
	140067042088400 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :    (4536, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041688544 -> 140067042088400
	140067042088640 -> 140067042088400
	140067042088640 [label="ViewBackward0
-----------------------------
self_sym_sizes: (9, 504, 768)"]
	140067041941904 -> 140067042088640
	140067041941904 [label="ViewBackward0
--------------------------------
self_sym_sizes: (3, 3, 504, 768)"]
	140067042089216 -> 140067041941904
	140067042089216 [label="AsStridedBackward0
-----------------------------------------
size          :          (3, 3, 504, 768)
storage_offset:                         0
stride        : (1161216, 387072, 768, 1)"]
	140067042088784 -> 140067042089216
	140067042088784 [label=CopySlices]
	140067041938400 -> 140067042088784
	140067042089312 -> 140067042088784
	140067042089312 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 3, 768)
start         :                   0
step          :                   1"]
	140067042088736 -> 140067042089312
	140067042088736 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:         (3, 4, 768)
start         :                   1
step          :                   1"]
	140067042089984 -> 140067042088736
	140067042089984 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:         (3, 4, 768)
start         :                   0
step          :                   1"]
	140067042223792 -> 140067042089984
	140067042088592 -> 140067042088400
	140067042088592 [label=TBackward0]
	140067041688592 -> 140067042088592
	140067042087008 -> 140067042087296
	140067042087008 [label="UnsafeViewBackward0
--------------------------------
self_sym_sizes: (9, 12, 64, 504)"]
	140067042087152 -> 140067042087008
	140067042087152 [label=CloneBackward0]
	140067042088544 -> 140067042087152
	140067042088544 [label="ExpandBackward0
--------------------------------
self_sym_sizes: (9, 12, 64, 504)"]
	140067042088256 -> 140067042088544
	140067042088256 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551615
dim1: 18446744073709551614"]
	140067042088928 -> 140067042088256
	140067042088928 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067042089840 -> 140067042088928
	140067042089840 [label="ViewBackward0
-----------------------------
self_sym_sizes: (9, 504, 768)"]
	140067042090080 -> 140067042089840
	140067042090080 [label="ViewBackward0
---------------------------
self_sym_sizes: (4536, 768)"]
	140067042089120 -> 140067042090080
	140067042089120 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :    (4536, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041687440 -> 140067042089120
	140067042090272 -> 140067042089120
	140067042090272 [label="ViewBackward0
-----------------------------
self_sym_sizes: (9, 504, 768)"]
	140067041941904 -> 140067042090272
	140067042089024 -> 140067042089120
	140067042089024 [label=TBackward0]
	140067041722768 -> 140067042089024
	140067042085088 -> 140067042084176
	140067042085088 [label="UnsafeViewBackward0
--------------------------------
self_sym_sizes: (9, 12, 504, 64)"]
	140067042086624 -> 140067042085088
	140067042086624 [label=CloneBackward0]
	140067042086048 -> 140067042086624
	140067042086048 [label="ExpandBackward0
--------------------------------
self_sym_sizes: (9, 12, 504, 64)"]
	140067042086672 -> 140067042086048
	140067042086672 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067042087968 -> 140067042086672
	140067042087968 [label="ViewBackward0
-----------------------------
self_sym_sizes: (9, 504, 768)"]
	140067042087728 -> 140067042087968
	140067042087728 [label="ViewBackward0
---------------------------
self_sym_sizes: (4536, 768)"]
	140067042089264 -> 140067042087728
	140067042089264 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :    (4536, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041689552 -> 140067042089264
	140067042089408 -> 140067042089264
	140067042089408 [label="ViewBackward0
-----------------------------
self_sym_sizes: (9, 504, 768)"]
	140067041941904 -> 140067042089408
	140067042089072 -> 140067042089264
	140067042089072 [label=TBackward0]
	140067041723056 -> 140067042089072
	140067041712384 -> 140067041939360
	140067041712384 [label=TBackward0]
	140067041684800 -> 140067041712384
	140067041941904 -> 140067041948720
	140067041679808 -> 140067042226528
	140067041681008 -> 140067042226528
	140067041935952 -> 140067042229120
	140067041935952 [label=TBackward0]
	140067041683936 -> 140067041935952
	140067042227200 -> 140067042226672
	140067042227200 [label=TBackward0]
	140067041674336 -> 140067042227200
	140067042226528 -> 140067042225328
	140066879623312 -> 140067042226144
	140066879623024 -> 140067042226144
	140067042222256 -> 140067042222208
	140067042222256 [label=TBackward0]
	140066879620960 -> 140067042222256
	140067042221152 -> 140067042221200
	140067042221152 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 64, 4)"]
	140067042221056 -> 140067042221152
	140067042221056 [label=CloneBackward0]
	140067042222160 -> 140067042221056
	140067042222160 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 64, 4)"]
	140067042222016 -> 140067042222160
	140067042222016 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551615
dim1: 18446744073709551614"]
	140067042223744 -> 140067042222016
	140067042223744 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067042223072 -> 140067042223744
	140067042223072 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 4, 768)"]
	140067042224800 -> 140067042223072
	140067042224800 [label="ViewBackward0
-------------------------
self_sym_sizes: (12, 768)"]
	140067042224992 -> 140067042224800
	140067042224992 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (12, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140066879624224 -> 140067042224992
	140067042223168 -> 140067042224992
	140067042223168 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 4, 768)"]
	140067042218368 -> 140067042223168
	140067042225568 -> 140067042224992
	140067042225568 [label=TBackward0]
	140066879627248 -> 140067042225568
	140067042219712 -> 140067042219808
	140067042219712 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 4, 64)"]
	140067042220048 -> 140067042219712
	140067042220048 [label=CloneBackward0]
	140067042220576 -> 140067042220048
	140067042220576 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 4, 64)"]
	140067042220624 -> 140067042220576
	140067042220624 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067042221824 -> 140067042220624
	140067042221824 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 4, 768)"]
	140067042221440 -> 140067042221824
	140067042221440 [label="ViewBackward0
-------------------------
self_sym_sizes: (12, 768)"]
	140067042222304 -> 140067042221440
	140067042222304 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (12, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140066879628160 -> 140067042222304
	140067042221584 -> 140067042222304
	140067042221584 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 4, 768)"]
	140067042218368 -> 140067042221584
	140067042222832 -> 140067042222304
	140067042222832 [label=TBackward0]
	140067041679712 -> 140067042222832
	140067042218464 -> 140067042218608
	140067042218464 [label=TBackward0]
	140067054781824 -> 140067042218464
	140067042218368 -> 140067042218128
	140067054787440 -> 140067042216592
	140067054791136 -> 140067042216592
	140067042217456 -> 140067042216976
	140067042217456 [label=TBackward0]
	140067054784752 -> 140067042217456
	140067042216832 -> 140067042216928
	140067042216832 [label=TBackward0]
	140067054790464 -> 140067042216832
	140067042216592 -> 140067042216064
	140067054790080 -> 140067042216400
	140067054784656 -> 140067042216400
	140067042215344 -> 140067042047072
	140067042215344 [label=CloneBackward0]
	140067042216160 -> 140067042215344
	140067042216160 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 3, 768)
start         :                   0
step          :                   1"]
	140067042216496 -> 140067042216160
	140067042216496 [label="SelectBackward0
--------------------------------
dim           :                2
index         :                1
self_sym_sizes: (3, 3, 504, 768)"]
	140067042217840 -> 140067042216496
	140067042217840 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:    (3, 3, 504, 768)
start         :                   0
step          :                   1"]
	140067042215872 -> 140067042217840
	140067042215872 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:    (3, 3, 504, 768)
start         :                   0
step          :                   1"]
	140067042217984 -> 140067042215872
	140067042217984 [label="ViewBackward0
-----------------------------
self_sym_sizes: (9, 504, 768)"]
	140067042218992 -> 140067042217984
	140067042218992 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140067042218032 -> 140067042218992
	140067042218032 [label="AddBackward0
------------
alpha: 1"]
	140067042219568 -> 140067042218032
	140067042219568 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067042218944 -> 140067042219568
	140067042218944 [label="ViewBackward0
---------------------------
self_sym_sizes: (4536, 768)"]
	140067042219328 -> 140067042218944
	140067042219328 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :   (4536, 3072)
mat1_sym_strides:      (3072, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (3072, 768)
mat2_sym_strides:      (1, 3072)"]
	140066879634064 -> 140067042219328
	140067042220384 -> 140067042219328
	140067042220384 [label="ViewBackward0
------------------------------
self_sym_sizes: (9, 504, 3072)"]
	140067042221920 -> 140067042220384
	140067042221920 [label="GeluBackward0
---------------------------
approximate:           none
self       : [saved tensor]"]
	140067042221296 -> 140067042221920
	140067042221296 [label="ViewBackward0
----------------------------
self_sym_sizes: (4536, 3072)"]
	140067042218080 -> 140067042221296
	140067042218080 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :    (4536, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (768, 3072)
mat2_sym_strides:       (1, 768)"]
	140067041685472 -> 140067042218080
	140067042220864 -> 140067042218080
	140067042220864 [label="ViewBackward0
-----------------------------
self_sym_sizes: (9, 504, 768)"]
	140067042219088 -> 140067042220864
	140067042219088 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140067041943872 -> 140067042219088
	140067041943872 [label="AddBackward0
------------
alpha: 1"]
	140067041942336 -> 140067041943872
	140067041942336 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041939984 -> 140067041942336
	140067041939984 [label="ViewBackward0
---------------------------
self_sym_sizes: (4536, 768)"]
	140067042227488 -> 140067041939984
	140067042227488 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :    (4536, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041722576 -> 140067042227488
	140067042224896 -> 140067042227488
	140067042224896 [label="ViewBackward0
-----------------------------
self_sym_sizes: (9, 504, 768)"]
	140067041718480 -> 140067042224896
	140067041718480 [label="ViewBackward0
--------------------------------
self_sym_sizes: (9, 504, 12, 64)"]
	140067042083408 -> 140067041718480
	140067042083408 [label=CloneBackward0]
	140067042082880 -> 140067042083408
	140067042082880 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067042086768 -> 140067042082880
	140067042086768 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (108, 504, 64)"]
	140067042088064 -> 140067042086768
	140067042088064 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140067042089696 -> 140067042088064
	140067042089696 [label="ViewBackward0
---------------------------------
self_sym_sizes: (9, 12, 504, 504)"]
	140067042089600 -> 140067042089696
	140067042089600 [label="ExpandBackward0
---------------------------------
self_sym_sizes: (9, 12, 504, 504)"]
	140067042089456 -> 140067042089600
	140067042089456 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067042087776 -> 140067042089456
	140067042087776 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140067042090464 -> 140067042087776
	140067042090464 [label="AddBackward0
------------
alpha: 1"]
	140067042090656 -> 140067042090464
	140067042090656 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067042090800 -> 140067042090656
	140067042090800 [label="UnsafeViewBackward0
-------------------------------
self_sym_sizes: (108, 504, 504)"]
	140067042090992 -> 140067042090800
	140067042090992 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140067042091040 -> 140067042090992
	140067042091040 [label="UnsafeViewBackward0
--------------------------------
self_sym_sizes: (9, 12, 504, 64)"]
	140067042091136 -> 140067042091040
	140067042091136 [label=CloneBackward0]
	140067042091232 -> 140067042091136
	140067042091232 [label="ExpandBackward0
--------------------------------
self_sym_sizes: (9, 12, 504, 64)"]
	140067042091376 -> 140067042091232
	140067042091376 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067042091664 -> 140067042091376
	140067042091664 [label="ViewBackward0
-----------------------------
self_sym_sizes: (9, 504, 768)"]
	140067042091760 -> 140067042091664
	140067042091760 [label="ViewBackward0
---------------------------
self_sym_sizes: (4536, 768)"]
	140067042091904 -> 140067042091760
	140067042091904 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :    (4536, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041726704 -> 140067042091904
	140067042092000 -> 140067042091904
	140067042092000 [label="ViewBackward0
-----------------------------
self_sym_sizes: (9, 504, 768)"]
	140067041935568 -> 140067042092000
	140067041935568 [label="ViewBackward0
--------------------------------
self_sym_sizes: (3, 3, 504, 768)"]
	140067042092672 -> 140067041935568
	140067042092672 [label="AsStridedBackward0
-----------------------------------------
size          :          (3, 3, 504, 768)
storage_offset:                         0
stride        : (1161216, 387072, 768, 1)"]
	140067042092240 -> 140067042092672
	140067042092240 [label=CopySlices]
	140067042226144 -> 140067042092240
	140067042092864 -> 140067042092240
	140067042092864 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 3, 768)
start         :                   0
step          :                   1"]
	140067042092144 -> 140067042092864
	140067042092144 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:         (3, 4, 768)
start         :                   1
step          :                   1"]
	140067042093728 -> 140067042092144
	140067042093728 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:         (3, 4, 768)
start         :                   0
step          :                   1"]
	140067042216400 -> 140067042093728
	140067042092096 -> 140067042091904
	140067042092096 [label=TBackward0]
	140067041726752 -> 140067042092096
	140067042090704 -> 140067042090992
	140067042090704 [label="UnsafeViewBackward0
--------------------------------
self_sym_sizes: (9, 12, 64, 504)"]
	140067042090848 -> 140067042090704
	140067042090848 [label=CloneBackward0]
	140067042091808 -> 140067042090848
	140067042091808 [label="ExpandBackward0
--------------------------------
self_sym_sizes: (9, 12, 64, 504)"]
	140067042091712 -> 140067042091808
	140067042091712 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551615
dim1: 18446744073709551614"]
	140067042092288 -> 140067042091712
	140067042092288 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067042093584 -> 140067042092288
	140067042093584 [label="ViewBackward0
-----------------------------
self_sym_sizes: (9, 504, 768)"]
	140067042093920 -> 140067042093584
	140067042093920 [label="ViewBackward0
---------------------------
self_sym_sizes: (4536, 768)"]
	140067042092576 -> 140067042093920
	140067042092576 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :    (4536, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041728576 -> 140067042092576
	140067042094064 -> 140067042092576
	140067042094064 [label="ViewBackward0
-----------------------------
self_sym_sizes: (9, 504, 768)"]
	140067041935568 -> 140067042094064
	140067042092480 -> 140067042092576
	140067042092480 [label=TBackward0]
	140067041728720 -> 140067042092480
	140067042088832 -> 140067042088064
	140067042088832 [label="UnsafeViewBackward0
--------------------------------
self_sym_sizes: (9, 12, 504, 64)"]
	140067042090320 -> 140067042088832
	140067042090320 [label=CloneBackward0]
	140067042089792 -> 140067042090320
	140067042089792 [label="ExpandBackward0
--------------------------------
self_sym_sizes: (9, 12, 504, 64)"]
	140067042090560 -> 140067042089792
	140067042090560 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067042091568 -> 140067042090560
	140067042091568 [label="ViewBackward0
-----------------------------
self_sym_sizes: (9, 504, 768)"]
	140067042091184 -> 140067042091568
	140067042091184 [label="ViewBackward0
---------------------------
self_sym_sizes: (4536, 768)"]
	140067042092768 -> 140067042091184
	140067042092768 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :    (4536, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041728432 -> 140067042092768
	140067042092960 -> 140067042092768
	140067042092960 [label="ViewBackward0
-----------------------------
self_sym_sizes: (9, 504, 768)"]
	140067041935568 -> 140067042092960
	140067042092528 -> 140067042092768
	140067042092528 [label=TBackward0]
	140067041728816 -> 140067042092528
	140067041716656 -> 140067042227488
	140067041716656 [label=TBackward0]
	140067041723296 -> 140067041716656
	140067041935568 -> 140067041943872
	140067041685040 -> 140067042219088
	140067041686480 -> 140067042219088
	140067042224464 -> 140067042218080
	140067042224464 [label=TBackward0]
	140067041687872 -> 140067042224464
	140067042219904 -> 140067042219328
	140067042219904 [label=TBackward0]
	140067041675152 -> 140067042219904
	140067042219088 -> 140067042218032
	140067054782640 -> 140067042218992
	140067054782256 -> 140067042218992
	140067042215056 -> 140067042214960
	140067042215056 [label=TBackward0]
	140067054790992 -> 140067042215056
	140067042049856 -> 140067042049952
	140067042049856 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 64, 4)"]
	140067041944112 -> 140067042049856
	140067041944112 [label=CloneBackward0]
	140067042214432 -> 140067041944112
	140067042214432 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 64, 4)"]
	140067042214576 -> 140067042214432
	140067042214576 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551615
dim1: 18446744073709551614"]
	140067042214096 -> 140067042214576
	140067042214096 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067042215104 -> 140067042214096
	140067042215104 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 4, 768)"]
	140067042217504 -> 140067042215104
	140067042217504 [label="ViewBackward0
-------------------------
self_sym_sizes: (12, 768)"]
	140067042215968 -> 140067042217504
	140067042215968 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (12, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067054787104 -> 140067042215968
	140067042218224 -> 140067042215968
	140067042218224 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 4, 768)"]
	140067042047072 -> 140067042218224
	140067042217744 -> 140067042215968
	140067042217744 [label=TBackward0]
	140067041688640 -> 140067042217744
	140067042048176 -> 140067042048224
	140067042048176 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 4, 64)"]
	140067042048320 -> 140067042048176
	140067042048320 [label=CloneBackward0]
	140067042049184 -> 140067042048320
	140067042049184 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 4, 64)"]
	140067042049088 -> 140067042049184
	140067042049088 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067042048800 -> 140067042049088
	140067042048800 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 4, 768)"]
	140067042214864 -> 140067042048800
	140067042214864 [label="ViewBackward0
-------------------------
self_sym_sizes: (12, 768)"]
	140067042215440 -> 140067042214864
	140067042215440 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (12, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067054785376 -> 140067042215440
	140067042214192 -> 140067042215440
	140067042214192 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 4, 768)"]
	140067042047072 -> 140067042214192
	140067042215488 -> 140067042215440
	140067042215488 [label=TBackward0]
	140066879624176 -> 140067042215488
	140067042047216 -> 140067042047264
	140067042047216 [label=TBackward0]
	140066819369344 -> 140067042047216
	140067042047072 -> 140067042046928
	140066819371552 -> 140067042045632
	140066819373664 -> 140067042045632
	140067042046304 -> 140067042045968
	140067042046304 [label=TBackward0]
	140066819361376 -> 140067042046304
	140067042045680 -> 140067042045776
	140067042045680 [label=TBackward0]
	140066819369920 -> 140067042045680
	140067042045632 -> 140067042045200
	140066819360896 -> 140067042045536
	140066819363008 -> 140067042045536
	140067042044336 -> 140067042040352
	140067042044336 [label=CloneBackward0]
	140067042045248 -> 140067042044336
	140067042045248 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 3, 768)
start         :                   0
step          :                   1"]
	140067042045584 -> 140067042045248
	140067042045584 [label="SelectBackward0
--------------------------------
dim           :                2
index         :                1
self_sym_sizes: (3, 3, 504, 768)"]
	140067042046640 -> 140067042045584
	140067042046640 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:    (3, 3, 504, 768)
start         :                   0
step          :                   1"]
	140067042045008 -> 140067042046640
	140067042045008 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:    (3, 3, 504, 768)
start         :                   0
step          :                   1"]
	140067042046784 -> 140067042045008
	140067042046784 [label="ViewBackward0
-----------------------------
self_sym_sizes: (9, 504, 768)"]
	140067042047648 -> 140067042046784
	140067042047648 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140067042046688 -> 140067042047648
	140067042046688 [label="AddBackward0
------------
alpha: 1"]
	140067042048080 -> 140067042046688
	140067042048080 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067042047408 -> 140067042048080
	140067042047408 [label="ViewBackward0
---------------------------
self_sym_sizes: (4536, 768)"]
	140067042047792 -> 140067042047408
	140067042047792 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :   (4536, 3072)
mat1_sym_strides:      (3072, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (3072, 768)
mat2_sym_strides:      (1, 3072)"]
	140067041685088 -> 140067042047792
	140067041937056 -> 140067042047792
	140067041937056 [label="ViewBackward0
------------------------------
self_sym_sizes: (9, 504, 3072)"]
	140067042049664 -> 140067041937056
	140067042049664 [label="GeluBackward0
---------------------------
approximate:           none
self       : [saved tensor]"]
	140067042046160 -> 140067042049664
	140067042046160 [label="ViewBackward0
----------------------------
self_sym_sizes: (4536, 3072)"]
	140067042215296 -> 140067042046160
	140067042215296 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :    (4536, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (768, 3072)
mat2_sym_strides:       (1, 768)"]
	140067041723776 -> 140067042215296
	140067042214000 -> 140067042215296
	140067042214000 [label="ViewBackward0
-----------------------------
self_sym_sizes: (9, 504, 768)"]
	140067042047744 -> 140067042214000
	140067042047744 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140067042220240 -> 140067042047744
	140067042220240 [label="AddBackward0
------------
alpha: 1"]
	140067042225424 -> 140067042220240
	140067042225424 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067041708592 -> 140067042225424
	140067041708592 [label="ViewBackward0
---------------------------
self_sym_sizes: (4536, 768)"]
	140067042217216 -> 140067041708592
	140067042217216 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :    (4536, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041722432 -> 140067042217216
	140067042228400 -> 140067042217216
	140067042228400 [label="ViewBackward0
-----------------------------
self_sym_sizes: (9, 504, 768)"]
	140067042090224 -> 140067042228400
	140067042090224 [label="ViewBackward0
--------------------------------
self_sym_sizes: (9, 504, 12, 64)"]
	140067042086336 -> 140067042090224
	140067042086336 [label=CloneBackward0]
	140067042090416 -> 140067042086336
	140067042090416 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067042083072 -> 140067042090416
	140067042083072 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (108, 504, 64)"]
	140067042091472 -> 140067042083072
	140067042091472 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140067042093296 -> 140067042091472
	140067042093296 [label="ViewBackward0
---------------------------------
self_sym_sizes: (9, 12, 504, 504)"]
	140067042093200 -> 140067042093296
	140067042093200 [label="ExpandBackward0
---------------------------------
self_sym_sizes: (9, 12, 504, 504)"]
	140067042093152 -> 140067042093200
	140067042093152 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067042091328 -> 140067042093152
	140067042091328 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140067042094400 -> 140067042091328
	140067042094400 [label="AddBackward0
------------
alpha: 1"]
	140067042094448 -> 140067042094400
	140067042094448 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067042094592 -> 140067042094448
	140067042094592 [label="UnsafeViewBackward0
-------------------------------
self_sym_sizes: (108, 504, 504)"]
	140067042094736 -> 140067042094592
	140067042094736 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140067042094784 -> 140067042094736
	140067042094784 [label="UnsafeViewBackward0
--------------------------------
self_sym_sizes: (9, 12, 504, 64)"]
	140067042094832 -> 140067042094784
	140067042094832 [label=CloneBackward0]
	140067042095072 -> 140067042094832
	140067042095072 [label="ExpandBackward0
--------------------------------
self_sym_sizes: (9, 12, 504, 64)"]
	140067042095120 -> 140067042095072
	140067042095120 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067042095264 -> 140067042095120
	140067042095264 [label="ViewBackward0
-----------------------------
self_sym_sizes: (9, 504, 768)"]
	140067042095360 -> 140067042095264
	140067042095360 [label="ViewBackward0
---------------------------
self_sym_sizes: (4536, 768)"]
	140067042095600 -> 140067042095360
	140067042095600 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :    (4536, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041732560 -> 140067042095600
	140067042095648 -> 140067042095600
	140067042095648 [label="ViewBackward0
-----------------------------
self_sym_sizes: (9, 504, 768)"]
	140067042224512 -> 140067042095648
	140067042224512 [label="ViewBackward0
--------------------------------
self_sym_sizes: (3, 3, 504, 768)"]
	140067042096272 -> 140067042224512
	140067042096272 [label="AsStridedBackward0
-----------------------------------------
size          :          (3, 3, 504, 768)
storage_offset:                         0
stride        : (1161216, 387072, 768, 1)"]
	140067042095888 -> 140067042096272
	140067042095888 [label=CopySlices]
	140067042218992 -> 140067042095888
	140067042096512 -> 140067042095888
	140067042096512 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 3, 768)
start         :                   0
step          :                   1"]
	140067042095744 -> 140067042096512
	140067042095744 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:         (3, 4, 768)
start         :                   1
step          :                   1"]
	140067042097136 -> 140067042095744
	140067042097136 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:         (3, 4, 768)
start         :                   0
step          :                   1"]
	140067042045536 -> 140067042097136
	140067042095504 -> 140067042095600
	140067042095504 [label=TBackward0]
	140067041732704 -> 140067042095504
	140067042094496 -> 140067042094736
	140067042094496 [label="UnsafeViewBackward0
--------------------------------
self_sym_sizes: (9, 12, 64, 504)"]
	140067042094640 -> 140067042094496
	140067042094640 [label=CloneBackward0]
	140067042095456 -> 140067042094640
	140067042095456 [label="ExpandBackward0
--------------------------------
self_sym_sizes: (9, 12, 64, 504)"]
	140067042095312 -> 140067042095456
	140067042095312 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551615
dim1: 18446744073709551614"]
	140067042095984 -> 140067042095312
	140067042095984 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067042097040 -> 140067042095984
	140067042097040 [label="ViewBackward0
-----------------------------
self_sym_sizes: (9, 504, 768)"]
	140067042097280 -> 140067042097040
	140067042097280 [label="ViewBackward0
---------------------------
self_sym_sizes: (4536, 768)"]
	140067042096176 -> 140067042097280
	140067042096176 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :    (4536, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041734336 -> 140067042096176
	140067042097376 -> 140067042096176
	140067042097376 [label="ViewBackward0
-----------------------------
self_sym_sizes: (9, 504, 768)"]
	140067042224512 -> 140067042097376
	140067042096032 -> 140067042096176
	140067042096032 [label=TBackward0]
	140067041734528 -> 140067042096032
	140067042092432 -> 140067042091472
	140067042092432 [label="UnsafeViewBackward0
--------------------------------
self_sym_sizes: (9, 12, 504, 64)"]
	140067042094112 -> 140067042092432
	140067042094112 [label=CloneBackward0]
	140067042093440 -> 140067042094112
	140067042093440 [label="ExpandBackward0
--------------------------------
self_sym_sizes: (9, 12, 504, 64)"]
	140067042094208 -> 140067042093440
	140067042094208 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067042095168 -> 140067042094208
	140067042095168 [label="ViewBackward0
-----------------------------
self_sym_sizes: (9, 504, 768)"]
	140067042094928 -> 140067042095168
	140067042094928 [label="ViewBackward0
---------------------------
self_sym_sizes: (4536, 768)"]
	140067042096320 -> 140067042094928
	140067042096320 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :    (4536, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041734192 -> 140067042096320
	140067042096368 -> 140067042096320
	140067042096368 [label="ViewBackward0
-----------------------------
self_sym_sizes: (9, 504, 768)"]
	140067042224512 -> 140067042096368
	140067042096128 -> 140067042096320
	140067042096128 [label=TBackward0]
	140067041734624 -> 140067042096128
	140067042217264 -> 140067042217216
	140067042217264 [label=TBackward0]
	140067041729152 -> 140067042217264
	140067042224512 -> 140067042220240
	140067041723536 -> 140067042047744
	140067041724928 -> 140067042047744
	140067042215776 -> 140067042215296
	140067042215776 [label=TBackward0]
	140067041727952 -> 140067042215776
	140067042048272 -> 140067042047792
	140067042048272 [label=TBackward0]
	140066819370016 -> 140067042048272
	140067042047744 -> 140067042046688
	140066819362192 -> 140067042047648
	140066819364976 -> 140067042047648
	140067042043952 -> 140067042043856
	140067042043952 [label=TBackward0]
	140067041687008 -> 140067042043952
	140067042042560 -> 140067042042800
	140067042042560 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 64, 4)"]
	140067041714064 -> 140067042042560
	140067041714064 [label=CloneBackward0]
	140067042043520 -> 140067041714064
	140067042043520 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 64, 4)"]
	140067042043808 -> 140067042043520
	140067042043808 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551615
dim1: 18446744073709551614"]
	140067042044528 -> 140067042043808
	140067042044528 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067042046400 -> 140067042044528
	140067042046400 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 4, 768)"]
	140067042045104 -> 140067042046400
	140067042045104 [label="ViewBackward0
-------------------------
self_sym_sizes: (12, 768)"]
	140067042046592 -> 140067042045104
	140067042046592 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (12, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140066819364928 -> 140067042046592
	140067042044288 -> 140067042046592
	140067042044288 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 4, 768)"]
	140067042040352 -> 140067042044288
	140067042046448 -> 140067042046592
	140067042046448 [label=TBackward0]
	140066819374432 -> 140067042046448
	140067042041072 -> 140067042041216
	140067042041072 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 4, 64)"]
	140067042041360 -> 140067042041072
	140067042041360 [label=CloneBackward0]
	140067042041936 -> 140067042041360
	140067042041936 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 4, 64)"]
	140067042042272 -> 140067042041936
	140067042042272 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067042042656 -> 140067042042272
	140067042042656 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 4, 768)"]
	140067042043712 -> 140067042042656
	140067042043712 [label="ViewBackward0
-------------------------
self_sym_sizes: (12, 768)"]
	140067042045392 -> 140067042043712
	140067042045392 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (12, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140066819374000 -> 140067042045392
	140067042043136 -> 140067042045392
	140067042043136 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 4, 768)"]
	140067042040352 -> 140067042043136
	140067042044576 -> 140067042045392
	140067042044576 [label=TBackward0]
	140066819372368 -> 140067042044576
	140067042040448 -> 140067042040592
	140067042040448 [label=TBackward0]
	140066688172912 -> 140067042040448
	140067042040352 -> 140067042040208
	140066688172816 -> 140067042038816
	140066688172864 -> 140067042038816
	140067042039440 -> 140067042038960
	140067042039440 [label=TBackward0]
	140066688172528 -> 140067042039440
	140067042038720 -> 140067042038864
	140067042038720 [label=TBackward0]
	140066688177760 -> 140067042038720
	140067042038816 -> 140067042038144
	140066688177472 -> 140067042038624
	140066688177952 -> 140067042038624
	140067042037376 -> 140067041734960
	140067042037376 [label=CloneBackward0]
	140067042038192 -> 140067042037376
	140067042038192 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 3, 768)
start         :                   0
step          :                   1"]
	140067042038672 -> 140067042038192
	140067042038672 [label="SelectBackward0
--------------------------------
dim           :                2
index         :                1
self_sym_sizes: (3, 3, 504, 768)"]
	140067042039872 -> 140067042038672
	140067042039872 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:    (3, 3, 504, 768)
start         :                   0
step          :                   1"]
	140067042038000 -> 140067042039872
	140067042038000 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:    (3, 3, 504, 768)
start         :                   0
step          :                   1"]
	140067042039968 -> 140067042038000
	140067042039968 [label="ViewBackward0
-----------------------------
self_sym_sizes: (9, 504, 768)"]
	140067042040832 -> 140067042039968
	140067042040832 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140067042040064 -> 140067042040832
	140067042040064 [label="AddBackward0
------------
alpha: 1"]
	140067042041024 -> 140067042040064
	140067042041024 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067042040736 -> 140067042041024
	140067042040736 [label="ViewBackward0
---------------------------
self_sym_sizes: (4536, 768)"]
	140067042040976 -> 140067042040736
	140067042040976 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :   (4536, 3072)
mat1_sym_strides:      (3072, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (3072, 768)
mat2_sym_strides:      (1, 3072)"]
	140066819361616 -> 140067042040976
	140067042041792 -> 140067042040976
	140067042041792 [label="ViewBackward0
------------------------------
self_sym_sizes: (9, 504, 3072)"]
	140067042043424 -> 140067042041792
	140067042043424 [label="GeluBackward0
---------------------------
approximate:           none
self       : [saved tensor]"]
	140067042042704 -> 140067042043424
	140067042042704 [label="ViewBackward0
----------------------------
self_sym_sizes: (4536, 3072)"]
	140067042040112 -> 140067042042704
	140067042040112 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :    (4536, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (768, 3072)
mat2_sym_strides:       (1, 768)"]
	140067041729680 -> 140067042040112
	140067042042368 -> 140067042040112
	140067042042368 [label="ViewBackward0
-----------------------------
self_sym_sizes: (9, 504, 768)"]
	140067042040880 -> 140067042042368
	140067042040880 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140067042222496 -> 140067042040880
	140067042222496 [label="AddBackward0
------------
alpha: 1"]
	140067042213952 -> 140067042222496
	140067042213952 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067042220192 -> 140067042213952
	140067042220192 [label="ViewBackward0
---------------------------
self_sym_sizes: (4536, 768)"]
	140067042049376 -> 140067042220192
	140067042049376 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :    (4536, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041725456 -> 140067042049376
	140067042046112 -> 140067042049376
	140067042046112 [label="ViewBackward0
-----------------------------
self_sym_sizes: (9, 504, 768)"]
	140067042094016 -> 140067042046112
	140067042094016 [label="ViewBackward0
--------------------------------
self_sym_sizes: (9, 504, 12, 64)"]
	140067042089888 -> 140067042094016
	140067042089888 [label=CloneBackward0]
	140067042094304 -> 140067042089888
	140067042094304 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067042086432 -> 140067042094304
	140067042086432 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (108, 504, 64)"]
	140067042095216 -> 140067042086432
	140067042095216 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140067042096800 -> 140067042095216
	140067042096800 [label="ViewBackward0
---------------------------------
self_sym_sizes: (9, 12, 504, 504)"]
	140067042096752 -> 140067042096800
	140067042096752 [label="ExpandBackward0
---------------------------------
self_sym_sizes: (9, 12, 504, 504)"]
	140067042096560 -> 140067042096752
	140067042096560 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067042095024 -> 140067042096560
	140067042095024 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140067042097664 -> 140067042095024
	140067042097664 [label="AddBackward0
------------
alpha: 1"]
	140067042097760 -> 140067042097664
	140067042097760 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140067042097856 -> 140067042097760
	140067042097856 [label="UnsafeViewBackward0
-------------------------------
self_sym_sizes: (108, 504, 504)"]
	140067042098000 -> 140067042097856
	140067042098000 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140067042098048 -> 140067042098000
	140067042098048 [label="UnsafeViewBackward0
--------------------------------
self_sym_sizes: (9, 12, 504, 64)"]
	140067042098144 -> 140067042098048
	140067042098144 [label=CloneBackward0]
	140067042098336 -> 140067042098144
	140067042098336 [label="ExpandBackward0
--------------------------------
self_sym_sizes: (9, 12, 504, 64)"]
	140067042098480 -> 140067042098336
	140067042098480 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067042098768 -> 140067042098480
	140067042098768 [label="ViewBackward0
-----------------------------
self_sym_sizes: (9, 504, 768)"]
	140067042098912 -> 140067042098768
	140067042098912 [label="ViewBackward0
---------------------------
self_sym_sizes: (4536, 768)"]
	140067042099152 -> 140067042098912
	140067042099152 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :    (4536, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041738032 -> 140067042099152
	140067042098240 -> 140067042099152
	140067042098240 [label="ViewBackward0
-----------------------------
self_sym_sizes: (9, 504, 768)"]
	140067042214720 -> 140067042098240
	140067042214720 [label="ViewBackward0
--------------------------------
self_sym_sizes: (3, 3, 504, 768)"]
	140067042165376 -> 140067042214720
	140067042165376 [label="AsStridedBackward0
-----------------------------------------
size          :          (3, 3, 504, 768)
storage_offset:                         0
stride        : (1161216, 387072, 768, 1)"]
	140067042165040 -> 140067042165376
	140067042165040 [label=CopySlices]
	140067042047648 -> 140067042165040
	140067042165664 -> 140067042165040
	140067042165664 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 3, 768)
start         :                   0
step          :                   1"]
	140067042164992 -> 140067042165664
	140067042164992 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:         (3, 4, 768)
start         :                   1
step          :                   1"]
	140067042166240 -> 140067042164992
	140067042166240 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:         (3, 4, 768)
start         :                   0
step          :                   1"]
	140067042038624 -> 140067042166240
	140067042164800 -> 140067042099152
	140067042164800 [label=TBackward0]
	140067041738080 -> 140067042164800
	140067042097808 -> 140067042098000
	140067042097808 [label="UnsafeViewBackward0
--------------------------------
self_sym_sizes: (9, 12, 64, 504)"]
	140067042097904 -> 140067042097808
	140067042097904 [label=CloneBackward0]
	140067042099104 -> 140067042097904
	140067042099104 [label="ExpandBackward0
--------------------------------
self_sym_sizes: (9, 12, 64, 504)"]
	140067042098288 -> 140067042099104
	140067042098288 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551615
dim1: 18446744073709551614"]
	140067042165184 -> 140067042098288
	140067042165184 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067042166144 -> 140067042165184
	140067042166144 [label="ViewBackward0
-----------------------------
self_sym_sizes: (9, 504, 768)"]
	140067042166336 -> 140067042166144
	140067042166336 [label="ViewBackward0
---------------------------
self_sym_sizes: (4536, 768)"]
	140067042165328 -> 140067042166336
	140067042165328 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :    (4536, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067042034496 -> 140067042165328
	140067042166432 -> 140067042165328
	140067042166432 [label="ViewBackward0
-----------------------------
self_sym_sizes: (9, 504, 768)"]
	140067042214720 -> 140067042166432
	140067042165232 -> 140067042165328
	140067042165232 [label=TBackward0]
	140067042034736 -> 140067042165232
	140067042095936 -> 140067042095216
	140067042095936 [label="UnsafeViewBackward0
--------------------------------
self_sym_sizes: (9, 12, 504, 64)"]
	140067042097424 -> 140067042095936
	140067042097424 [label=CloneBackward0]
	140067042096944 -> 140067042097424
	140067042096944 [label="ExpandBackward0
--------------------------------
self_sym_sizes: (9, 12, 504, 64)"]
	140067042097520 -> 140067042096944
	140067042097520 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067042098576 -> 140067042097520
	140067042098576 [label="ViewBackward0
-----------------------------
self_sym_sizes: (9, 504, 768)"]
	140067042098864 -> 140067042098576
	140067042098864 [label="ViewBackward0
---------------------------
self_sym_sizes: (4536, 768)"]
	140067042096704 -> 140067042098864
	140067042096704 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :    (4536, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140067041735776 -> 140067042096704
	140067042165568 -> 140067042096704
	140067042165568 [label="ViewBackward0
-----------------------------
self_sym_sizes: (9, 504, 768)"]
	140067042214720 -> 140067042165568
	140067042165280 -> 140067042096704
	140067042165280 [label=TBackward0]
	140067042034784 -> 140067042165280
	140067042044000 -> 140067042049376
	140067042044000 [label=TBackward0]
	140067041734816 -> 140067042044000
	140067042214720 -> 140067042222496
	140067041729344 -> 140067042040880
	140067041730736 -> 140067042040880
	140067042044864 -> 140067042040112
	140067042044864 [label=TBackward0]
	140067041733808 -> 140067042044864
	140067042041264 -> 140067042040976
	140067042041264 [label=TBackward0]
	140067041723488 -> 140067042041264
	140067042040880 -> 140067042040064
	140066688173440 -> 140067042040832
	140066688179872 -> 140067042040832
	140067042037088 -> 140067042037040
	140067042037088 [label=TBackward0]
	140066688172288 -> 140067042037088
	140067042036032 -> 140067042036272
	140067042036032 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 64, 4)"]
	140067042216352 -> 140067042036032
	140067042216352 [label=CloneBackward0]
	140067042217600 -> 140067042216352
	140067042217600 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 64, 4)"]
	140067042036656 -> 140067042217600
	140067042036656 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551615
dim1: 18446744073709551614"]
	140067042036896 -> 140067042036656
	140067042036896 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067042038432 -> 140067042036896
	140067042038432 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 4, 768)"]
	140067042037280 -> 140067042038432
	140067042037280 [label="ViewBackward0
-------------------------
self_sym_sizes: (12, 768)"]
	140067042039488 -> 140067042037280
	140067042039488 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (12, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140066688174592 -> 140067042039488
	140067042039632 -> 140067042039488
	140067042039632 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 4, 768)"]
	140067041734960 -> 140067042039632
	140067042038048 -> 140067042039488
	140067042038048 [label=TBackward0]
	140066819370928 -> 140067042038048
	140067042033728 -> 140067042034928
	140067042033728 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 4, 64)"]
	140067042222400 -> 140067042033728
	140067042222400 [label=CloneBackward0]
	140067042035792 -> 140067042222400
	140067042035792 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 4, 64)"]
	140067042035696 -> 140067042035792
	140067042035696 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140067042035216 -> 140067042035696
	140067042035216 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 4, 768)"]
	140067042036704 -> 140067042035216
	140067042036704 [label="ViewBackward0
-------------------------
self_sym_sizes: (12, 768)"]
	140067042037616 -> 140067042036704
	140067042037616 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (12, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140066688176080 -> 140067042037616
	140067042036128 -> 140067042037616
	140067042036128 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 4, 768)"]
	140067041734960 -> 140067042036128
	140067042036416 -> 140067042037616
	140067042036416 [label=TBackward0]
	140067041729296 -> 140067042036416
	140067041736208 -> 140067041736640
	140067041736208 [label=TBackward0]
	140067053502032 -> 140067041736208
	140067041734960 -> 140067041735344
	140066688027136 -> 140067053499392
	140066688028096 -> 140067053499392
	140067041731168 -> 140067053394368
	140067041731168 [label=TBackward0]
	140066688026800 -> 140067041731168
	140066688025792 -> 140066688027184
	140066688025792 [label=TBackward0]
	140066688029152 -> 140066688025792
	140067053499392 -> 140067053486144
	140067053470240 -> 140066688028960
	140067053471104 -> 140066688028960
	140067008064592 -> 140067043409696
}

digraph {
	graph [size="744.75,744.75"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	140490044736608 [label="
 ()" fillcolor=darkolivegreen1]
	140490044785424 [label="NllLossBackward0
----------------------------------
ignore_index: 18446744073709551516
reduction   :                    1
self        :       [saved tensor]
target      :       [saved tensor]
total_weight:       [saved tensor]
weight      :                 None"]
	140490040874256 -> 140490044785424
	140490040874256 [label="LogSoftmaxBackward0
----------------------
dim   :              1
result: [saved tensor]"]
	140490034891168 -> 140490040874256
	140490034891168 [label="MmBackward0
--------------------------------
mat2            :           None
mat2_sym_sizes  :       (768, 3)
mat2_sym_strides:      (1, 2304)
self            : [saved tensor]
self_sym_sizes  :       (3, 768)
self_sym_strides:             ()"]
	140490034890640 -> 140490034891168
	140490034890640 [label="TransposeBackward0
------------------
dim0: 0
dim1: 1"]
	140490034892224 -> 140490034890640
	140490034892224 [label="SqueezeBackward1
---------------------------
dim           :           1
self_sym_sizes: (3, 1, 768)"]
	140490034891792 -> 140490034892224
	140490034891792 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 1, 768)
start         :                   0
step          :                   1"]
	140490034891888 -> 140490034891792
	140490034891888 [label="SliceBackward0
---------------------------
dim           :           1
end           :           1
self_sym_sizes: (3, 3, 768)
start         :           0
step          :           1"]
	140490034891984 -> 140490034891888
	140490034891984 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:         (3, 3, 768)
start         :                   0
step          :                   1"]
	140490034890928 -> 140490034891984
	140490034890928 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140490034891072 -> 140490034890928
	140490034891072 [label="AddBackward0
------------
alpha: 1"]
	140490034895680 -> 140490034891072
	140490034895680 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140490034892272 -> 140490034895680
	140490034892272 [label="ViewBackward0
------------------------
self_sym_sizes: (9, 768)"]
	140490034893280 -> 140490034892272
	140490034893280 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (9, 3072)
mat1_sym_strides:      (3072, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (3072, 768)
mat2_sym_strides:      (1, 3072)"]
	140490034891216 -> 140490034893280
	140490033699952 [label="q_encoder.encoder.information_exchanging_layer.11.output.dense.bias
 (768)" fillcolor=lightblue]
	140490033699952 -> 140490034891216
	140490034891216 [label=AccumulateGrad]
	140490034892704 -> 140490034893280
	140490034892704 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 3, 3072)"]
	140490034892368 -> 140490034892704
	140490034892368 [label="GeluBackward0
---------------------------
approximate:           none
self       : [saved tensor]"]
	140490034892800 -> 140490034892368
	140490034892800 [label="ViewBackward0
-------------------------
self_sym_sizes: (9, 3072)"]
	140490034893568 -> 140490034892800
	140490034893568 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (9, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (768, 3072)
mat2_sym_strides:       (1, 768)"]
	140490034893712 -> 140490034893568
	140490033699792 [label="q_encoder.encoder.information_exchanging_layer.11.intermediate.dense.bias
 (3072)" fillcolor=lightblue]
	140490033699792 -> 140490034893712
	140490034893712 [label=AccumulateGrad]
	140490034892944 -> 140490034893568
	140490034892944 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 3, 768)"]
	140490034891360 -> 140490034892944
	140490034891360 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140490034894336 -> 140490034891360
	140490034894336 [label="AddBackward0
------------
alpha: 1"]
	140490034894480 -> 140490034894336
	140490034894480 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140490034894720 -> 140490034894480
	140490034894720 [label="ViewBackward0
------------------------
self_sym_sizes: (9, 768)"]
	140490034894816 -> 140490034894720
	140490034894816 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (9, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140490034894864 -> 140490034894816
	140490033699392 [label="q_encoder.encoder.information_exchanging_layer.11.attention.output.dense.bias
 (768)" fillcolor=lightblue]
	140490033699392 -> 140490034894864
	140490034894864 [label=AccumulateGrad]
	140490034894000 -> 140490034894816
	140490034894000 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 3, 768)"]
	140490034895152 -> 140490034894000
	140490034895152 [label="ViewBackward0
------------------------------
self_sym_sizes: (3, 3, 12, 64)"]
	140490034895104 -> 140490034895152
	140490034895104 [label=CloneBackward0]
	140490034895824 -> 140490034895104
	140490034895824 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140490034896496 -> 140490034895824
	140490034896496 [label="UnsafeViewBackward0
---------------------------
self_sym_sizes: (36, 3, 64)"]
	140490034896448 -> 140490034896496
	140490034896448 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140490034896832 -> 140490034896448
	140490034896832 [label="ViewBackward0
-----------------------------
self_sym_sizes: (3, 12, 3, 3)"]
	140490034897072 -> 140490034896832
	140490034897072 [label="ExpandBackward0
-----------------------------
self_sym_sizes: (3, 12, 3, 3)"]
	140490034897264 -> 140490034897072
	140490034897264 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140490034897648 -> 140490034897264
	140490034897648 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140490034897792 -> 140490034897648
	140490034897792 [label="AddBackward0
------------
alpha: 1"]
	140490034898080 -> 140490034897792
	140490034898080 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140490034897888 -> 140490034898080
	140490034897888 [label="UnsafeViewBackward0
--------------------------
self_sym_sizes: (36, 3, 3)"]
	140490034898368 -> 140490034897888
	140490034898368 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140490034898224 -> 140490034898368
	140490034898224 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 3, 64)"]
	140490034898704 -> 140490034898224
	140490034898704 [label=CloneBackward0]
	140490034898800 -> 140490034898704
	140490034898800 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 3, 64)"]
	140490034899088 -> 140490034898800
	140490034899088 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140490034899328 -> 140490034899088
	140490034899328 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 3, 768)"]
	140490034899424 -> 140490034899328
	140490034899424 [label="ViewBackward0
------------------------
self_sym_sizes: (9, 768)"]
	140490034899808 -> 140490034899424
	140490034899808 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (9, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140490034899904 -> 140490034899808
	140490033698912 [label="q_encoder.encoder.information_exchanging_layer.11.attention.self.query.bias
 (768)" fillcolor=lightblue]
	140490033698912 -> 140490034899904
	140490034899904 [label=AccumulateGrad]
	140490034900048 -> 140490034899808
	140490034900048 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 3, 768)"]
	140490034894384 -> 140490034900048
	140490034894384 [label="CatBackward0
------------
dim: 1"]
	140490034900624 -> 140490034894384
	140490034900624 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 1, 768)
start         :                   0
step          :                   1"]
	140490034900096 -> 140490034900624
	140490034900096 [label="SliceBackward0
---------------------------
dim           :           1
end           :           1
self_sym_sizes: (3, 3, 768)
start         :           0
step          :           1"]
	140490034901200 -> 140490034900096
	140490034901200 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:         (3, 3, 768)
start         :                   0
step          :                   1"]
	140490034900768 -> 140490034901200
	140490034900768 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140490034901680 -> 140490034900768
	140490034901680 [label="AddBackward0
------------
alpha: 1"]
	140490034900816 -> 140490034901680
	140490034900816 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140490034901872 -> 140490034900816
	140490034901872 [label="ViewBackward0
------------------------
self_sym_sizes: (9, 768)"]
	140490034902304 -> 140490034901872
	140490034902304 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (9, 3072)
mat1_sym_strides:      (3072, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (3072, 768)
mat2_sym_strides:      (1, 3072)"]
	140490034902112 -> 140490034902304
	140490033698672 [label="q_encoder.encoder.information_exchanging_layer.10.output.dense.bias
 (768)" fillcolor=lightblue]
	140490033698672 -> 140490034902112
	140490034902112 [label=AccumulateGrad]
	140490034901056 -> 140490034902304
	140490034901056 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 3, 3072)"]
	140490034902544 -> 140490034901056
	140490034902544 [label="GeluBackward0
---------------------------
approximate:           none
self       : [saved tensor]"]
	140490034902352 -> 140490034902544
	140490034902352 [label="ViewBackward0
-------------------------
self_sym_sizes: (9, 3072)"]
	140490034903120 -> 140490034902352
	140490034903120 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (9, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (768, 3072)
mat2_sym_strides:       (1, 768)"]
	140490034903216 -> 140490034903120
	140490033698512 [label="q_encoder.encoder.information_exchanging_layer.10.intermediate.dense.bias
 (3072)" fillcolor=lightblue]
	140490033698512 -> 140490034903216
	140490034903216 [label=AccumulateGrad]
	140490034903408 -> 140490034903120
	140490034903408 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 3, 768)"]
	140490034900912 -> 140490034903408
	140490034900912 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140490034903984 -> 140490034900912
	140490034903984 [label="AddBackward0
------------
alpha: 1"]
	140490034904128 -> 140490034903984
	140490034904128 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140490034904464 -> 140490034904128
	140490034904464 [label="ViewBackward0
------------------------
self_sym_sizes: (9, 768)"]
	140490034904704 -> 140490034904464
	140490034904704 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (9, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140490034904560 -> 140490034904704
	140490033698192 [label="q_encoder.encoder.information_exchanging_layer.10.attention.output.dense.bias
 (768)" fillcolor=lightblue]
	140490033698192 -> 140490034904560
	140490034904560 [label=AccumulateGrad]
	140490034903600 -> 140490034904704
	140490034903600 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 3, 768)"]
	140490034904752 -> 140490034903600
	140490034904752 [label="ViewBackward0
------------------------------
self_sym_sizes: (3, 3, 12, 64)"]
	140490034495792 -> 140490034904752
	140490034495792 [label=CloneBackward0]
	140490034495936 -> 140490034495792
	140490034495936 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140490034496176 -> 140490034495936
	140490034496176 [label="UnsafeViewBackward0
---------------------------
self_sym_sizes: (36, 3, 64)"]
	140490034496224 -> 140490034496176
	140490034496224 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140490034496656 -> 140490034496224
	140490034496656 [label="ViewBackward0
-----------------------------
self_sym_sizes: (3, 12, 3, 3)"]
	140490034497040 -> 140490034496656
	140490034497040 [label="ExpandBackward0
-----------------------------
self_sym_sizes: (3, 12, 3, 3)"]
	140490034497088 -> 140490034497040
	140490034497088 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140490034497328 -> 140490034497088
	140490034497328 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140490034497520 -> 140490034497328
	140490034497520 [label="AddBackward0
------------
alpha: 1"]
	140490034497664 -> 140490034497520
	140490034497664 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140490034497616 -> 140490034497664
	140490034497616 [label="UnsafeViewBackward0
--------------------------
self_sym_sizes: (36, 3, 3)"]
	140490034498192 -> 140490034497616
	140490034498192 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140490034497952 -> 140490034498192
	140490034497952 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 3, 64)"]
	140490034498384 -> 140490034497952
	140490034498384 [label=CloneBackward0]
	140490034498432 -> 140490034498384
	140490034498432 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 3, 64)"]
	140490034498768 -> 140490034498432
	140490034498768 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140490034498864 -> 140490034498768
	140490034498864 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 3, 768)"]
	140490034499008 -> 140490034498864
	140490034499008 [label="ViewBackward0
------------------------
self_sym_sizes: (9, 768)"]
	140490034499488 -> 140490034499008
	140490034499488 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (9, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140490034499632 -> 140490034499488
	140490033697712 [label="q_encoder.encoder.information_exchanging_layer.10.attention.self.query.bias
 (768)" fillcolor=lightblue]
	140490033697712 -> 140490034499632
	140490034499632 [label=AccumulateGrad]
	140490034499728 -> 140490034499488
	140490034499728 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 3, 768)"]
	140490034904032 -> 140490034499728
	140490034904032 [label="CatBackward0
------------
dim: 1"]
	140490034500496 -> 140490034904032
	140490034500496 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 1, 768)
start         :                   0
step          :                   1"]
	140490034499776 -> 140490034500496
	140490034499776 [label="SliceBackward0
---------------------------
dim           :           1
end           :           1
self_sym_sizes: (3, 3, 768)
start         :           0
step          :           1"]
	140490034501168 -> 140490034499776
	140490034501168 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:         (3, 3, 768)
start         :                   0
step          :                   1"]
	140490034500736 -> 140490034501168
	140490034500736 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140490034501648 -> 140490034500736
	140490034501648 [label="AddBackward0
------------
alpha: 1"]
	140490034500832 -> 140490034501648
	140490034500832 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140490034501936 -> 140490034500832
	140490034501936 [label="ViewBackward0
------------------------
self_sym_sizes: (9, 768)"]
	140490034502272 -> 140490034501936
	140490034502272 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (9, 3072)
mat1_sym_strides:      (3072, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (3072, 768)
mat2_sym_strides:      (1, 3072)"]
	140490034502128 -> 140490034502272
	140490033697392 [label="q_encoder.encoder.information_exchanging_layer.9.output.dense.bias
 (768)" fillcolor=lightblue]
	140490033697392 -> 140490034502128
	140490034502128 [label=AccumulateGrad]
	140490034501024 -> 140490034502272
	140490034501024 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 3, 3072)"]
	140490034502464 -> 140490034501024
	140490034502464 [label="GeluBackward0
---------------------------
approximate:           none
self       : [saved tensor]"]
	140490034502416 -> 140490034502464
	140490034502416 [label="ViewBackward0
-------------------------
self_sym_sizes: (9, 3072)"]
	140490034503040 -> 140490034502416
	140490034503040 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (9, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (768, 3072)
mat2_sym_strides:       (1, 768)"]
	140490034503088 -> 140490034503040
	140490033697232 [label="q_encoder.encoder.information_exchanging_layer.9.intermediate.dense.bias
 (3072)" fillcolor=lightblue]
	140490033697232 -> 140490034503088
	140490034503088 [label=AccumulateGrad]
	140490034503136 -> 140490034503040
	140490034503136 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 3, 768)"]
	140490034500880 -> 140490034503136
	140490034500880 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140490034503664 -> 140490034500880
	140490034503664 [label="AddBackward0
------------
alpha: 1"]
	140490034503808 -> 140490034503664
	140490034503808 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140490034504144 -> 140490034503808
	140490034504144 [label="ViewBackward0
------------------------
self_sym_sizes: (9, 768)"]
	140490034504432 -> 140490034504144
	140490034504432 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (9, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140490034504336 -> 140490034504432
	140490033696832 [label="q_encoder.encoder.information_exchanging_layer.9.attention.output.dense.bias
 (768)" fillcolor=lightblue]
	140490033696832 -> 140490034504336
	140490034504336 [label=AccumulateGrad]
	140490034503232 -> 140490034504432
	140490034503232 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 3, 768)"]
	140490034504720 -> 140490034503232
	140490034504720 [label="ViewBackward0
------------------------------
self_sym_sizes: (3, 3, 12, 64)"]
	140490034504672 -> 140490034504720
	140490034504672 [label=CloneBackward0]
	140490034505296 -> 140490034504672
	140490034505296 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140490034505488 -> 140490034505296
	140490034505488 [label="UnsafeViewBackward0
---------------------------
self_sym_sizes: (36, 3, 64)"]
	140490034505536 -> 140490034505488
	140490034505536 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140490034505968 -> 140490034505536
	140490034505968 [label="ViewBackward0
-----------------------------
self_sym_sizes: (3, 12, 3, 3)"]
	140490034506400 -> 140490034505968
	140490034506400 [label="ExpandBackward0
-----------------------------
self_sym_sizes: (3, 12, 3, 3)"]
	140490034506304 -> 140490034506400
	140490034506304 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140490034506736 -> 140490034506304
	140490034506736 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140490034507072 -> 140490034506736
	140490034507072 [label="AddBackward0
------------
alpha: 1"]
	140490034507408 -> 140490034507072
	140490034507408 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140490034507312 -> 140490034507408
	140490034507312 [label="UnsafeViewBackward0
--------------------------
self_sym_sizes: (36, 3, 3)"]
	140490034507744 -> 140490034507312
	140490034507744 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140490034507648 -> 140490034507744
	140490034507648 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 3, 64)"]
	140490034507984 -> 140490034507648
	140490034507984 [label=CloneBackward0]
	140490034508080 -> 140490034507984
	140490034508080 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 3, 64)"]
	140490034508368 -> 140490034508080
	140490034508368 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140490034508608 -> 140490034508368
	140490034508608 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 3, 768)"]
	140490034508656 -> 140490034508608
	140490034508656 [label="ViewBackward0
------------------------
self_sym_sizes: (9, 768)"]
	140490034509040 -> 140490034508656
	140490034509040 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (9, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140490034509232 -> 140490034509040
	140490033696352 [label="q_encoder.encoder.information_exchanging_layer.9.attention.self.query.bias
 (768)" fillcolor=lightblue]
	140490033696352 -> 140490034509232
	140490034509232 [label=AccumulateGrad]
	140490034509328 -> 140490034509040
	140490034509328 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 3, 768)"]
	140490034503712 -> 140490034509328
	140490034503712 [label="CatBackward0
------------
dim: 1"]
	140490034509808 -> 140490034503712
	140490034509808 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 1, 768)
start         :                   0
step          :                   1"]
	140490034509376 -> 140490034509808
	140490034509376 [label="SliceBackward0
---------------------------
dim           :           1
end           :           1
self_sym_sizes: (3, 3, 768)
start         :           0
step          :           1"]
	140490034510528 -> 140490034509376
	140490034510528 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:         (3, 3, 768)
start         :                   0
step          :                   1"]
	140490034510000 -> 140490034510528
	140490034510000 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140490034511008 -> 140490034510000
	140490034511008 [label="AddBackward0
------------
alpha: 1"]
	140490034510096 -> 140490034511008
	140490034510096 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140490034511248 -> 140490034510096
	140490034511248 [label="ViewBackward0
------------------------
self_sym_sizes: (9, 768)"]
	140490034511632 -> 140490034511248
	140490034511632 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (9, 3072)
mat1_sym_strides:      (3072, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (3072, 768)
mat2_sym_strides:      (1, 3072)"]
	140490034511440 -> 140490034511632
	140490033696112 [label="q_encoder.encoder.information_exchanging_layer.8.output.dense.bias
 (768)" fillcolor=lightblue]
	140490033696112 -> 140490034511440
	140490034511440 [label=AccumulateGrad]
	140490034510336 -> 140490034511632
	140490034510336 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 3, 3072)"]
	140490034462976 -> 140490034510336
	140490034462976 [label="GeluBackward0
---------------------------
approximate:           none
self       : [saved tensor]"]
	140490034462880 -> 140490034462976
	140490034462880 [label="ViewBackward0
-------------------------
self_sym_sizes: (9, 3072)"]
	140490034463552 -> 140490034462880
	140490034463552 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (9, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (768, 3072)
mat2_sym_strides:       (1, 768)"]
	140490034463456 -> 140490034463552
	140490033695952 [label="q_encoder.encoder.information_exchanging_layer.8.intermediate.dense.bias
 (3072)" fillcolor=lightblue]
	140490033695952 -> 140490034463456
	140490034463456 [label=AccumulateGrad]
	140490034463600 -> 140490034463552
	140490034463600 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 3, 768)"]
	140490034510192 -> 140490034463600
	140490034510192 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140490034464320 -> 140490034510192
	140490034464320 [label="AddBackward0
------------
alpha: 1"]
	140490034464464 -> 140490034464320
	140490034464464 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140490034464800 -> 140490034464464
	140490034464800 [label="ViewBackward0
------------------------
self_sym_sizes: (9, 768)"]
	140490034465184 -> 140490034464800
	140490034465184 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (9, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140490034464992 -> 140490034465184
	140490033695552 [label="q_encoder.encoder.information_exchanging_layer.8.attention.output.dense.bias
 (768)" fillcolor=lightblue]
	140490033695552 -> 140490034464992
	140490034464992 [label=AccumulateGrad]
	140490034463744 -> 140490034465184
	140490034463744 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 3, 768)"]
	140490034465424 -> 140490034463744
	140490034465424 [label="ViewBackward0
------------------------------
self_sym_sizes: (3, 3, 12, 64)"]
	140490034465376 -> 140490034465424
	140490034465376 [label=CloneBackward0]
	140490034466000 -> 140490034465376
	140490034466000 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140490034466144 -> 140490034466000
	140490034466144 [label="UnsafeViewBackward0
---------------------------
self_sym_sizes: (36, 3, 64)"]
	140490034466192 -> 140490034466144
	140490034466192 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140490034466528 -> 140490034466192
	140490034466528 [label="ViewBackward0
-----------------------------
self_sym_sizes: (3, 12, 3, 3)"]
	140490034466768 -> 140490034466528
	140490034466768 [label="ExpandBackward0
-----------------------------
self_sym_sizes: (3, 12, 3, 3)"]
	140490034466816 -> 140490034466768
	140490034466816 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140490034467008 -> 140490034466816
	140490034467008 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140490034467344 -> 140490034467008
	140490034467344 [label="AddBackward0
------------
alpha: 1"]
	140490034467584 -> 140490034467344
	140490034467584 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140490034467488 -> 140490034467584
	140490034467488 [label="UnsafeViewBackward0
--------------------------
self_sym_sizes: (36, 3, 3)"]
	140490034468064 -> 140490034467488
	140490034468064 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140490034467872 -> 140490034468064
	140490034467872 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 3, 64)"]
	140490034468400 -> 140490034467872
	140490034468400 [label=CloneBackward0]
	140490034468544 -> 140490034468400
	140490034468544 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 3, 64)"]
	140490034468784 -> 140490034468544
	140490034468784 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140490034468976 -> 140490034468784
	140490034468976 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 3, 768)"]
	140490034469024 -> 140490034468976
	140490034469024 [label="ViewBackward0
------------------------
self_sym_sizes: (9, 768)"]
	140490034469408 -> 140490034469024
	140490034469408 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (9, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140490034469504 -> 140490034469408
	140490033695072 [label="q_encoder.encoder.information_exchanging_layer.8.attention.self.query.bias
 (768)" fillcolor=lightblue]
	140490033695072 -> 140490034469504
	140490034469504 [label=AccumulateGrad]
	140490034469600 -> 140490034469408
	140490034469600 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 3, 768)"]
	140490034464416 -> 140490034469600
	140490034464416 [label="CatBackward0
------------
dim: 1"]
	140490034470224 -> 140490034464416
	140490034470224 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 1, 768)
start         :                   0
step          :                   1"]
	140490034469696 -> 140490034470224
	140490034469696 [label="SliceBackward0
---------------------------
dim           :           1
end           :           1
self_sym_sizes: (3, 3, 768)
start         :           0
step          :           1"]
	140490034470848 -> 140490034469696
	140490034470848 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:         (3, 3, 768)
start         :                   0
step          :                   1"]
	140490034470464 -> 140490034470848
	140490034470464 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140490034471280 -> 140490034470464
	140490034471280 [label="AddBackward0
------------
alpha: 1"]
	140490034470512 -> 140490034471280
	140490034470512 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140490034471616 -> 140490034470512
	140490034471616 [label="ViewBackward0
------------------------
self_sym_sizes: (9, 768)"]
	140490034471904 -> 140490034471616
	140490034471904 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (9, 3072)
mat1_sym_strides:      (3072, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (3072, 768)
mat2_sym_strides:      (1, 3072)"]
	140490034471712 -> 140490034471904
	140490033694832 [label="q_encoder.encoder.information_exchanging_layer.7.output.dense.bias
 (768)" fillcolor=lightblue]
	140490033694832 -> 140490034471712
	140490034471712 [label=AccumulateGrad]
	140490034470704 -> 140490034471904
	140490034470704 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 3, 3072)"]
	140490034472048 -> 140490034470704
	140490034472048 [label="GeluBackward0
---------------------------
approximate:           none
self       : [saved tensor]"]
	140490034472144 -> 140490034472048
	140490034472144 [label="ViewBackward0
-------------------------
self_sym_sizes: (9, 3072)"]
	140490034472816 -> 140490034472144
	140490034472816 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (9, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (768, 3072)
mat2_sym_strides:       (1, 768)"]
	140490034472864 -> 140490034472816
	140490033694672 [label="q_encoder.encoder.information_exchanging_layer.7.intermediate.dense.bias
 (3072)" fillcolor=lightblue]
	140490033694672 -> 140490034472864
	140490034472864 [label=AccumulateGrad]
	140490034472912 -> 140490034472816
	140490034472912 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 3, 768)"]
	140490034470560 -> 140490034472912
	140490034470560 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140490034473488 -> 140490034470560
	140490034473488 [label="AddBackward0
------------
alpha: 1"]
	140490034473680 -> 140490034473488
	140490034473680 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140490034473920 -> 140490034473680
	140490034473920 [label="ViewBackward0
------------------------
self_sym_sizes: (9, 768)"]
	140490034474400 -> 140490034473920
	140490034474400 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (9, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140490034474016 -> 140490034474400
	140490033694272 [label="q_encoder.encoder.information_exchanging_layer.7.attention.output.dense.bias
 (768)" fillcolor=lightblue]
	140490033694272 -> 140490034474016
	140490034474016 [label=AccumulateGrad]
	140490034473008 -> 140490034474400
	140490034473008 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 3, 768)"]
	140490034474640 -> 140490034473008
	140490034474640 [label="ViewBackward0
------------------------------
self_sym_sizes: (3, 3, 12, 64)"]
	140490034474544 -> 140490034474640
	140490034474544 [label=CloneBackward0]
	140490034475120 -> 140490034474544
	140490034475120 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140490034475264 -> 140490034475120
	140490034475264 [label="UnsafeViewBackward0
---------------------------
self_sym_sizes: (36, 3, 64)"]
	140490034475312 -> 140490034475264
	140490034475312 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140490034475600 -> 140490034475312
	140490034475600 [label="ViewBackward0
-----------------------------
self_sym_sizes: (3, 12, 3, 3)"]
	140490034475888 -> 140490034475600
	140490034475888 [label="ExpandBackward0
-----------------------------
self_sym_sizes: (3, 12, 3, 3)"]
	140490034475936 -> 140490034475888
	140490034475936 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140490034476272 -> 140490034475936
	140490034476272 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140490034476512 -> 140490034476272
	140490034476512 [label="AddBackward0
------------
alpha: 1"]
	140490034476752 -> 140490034476512
	140490034476752 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140490034476704 -> 140490034476752
	140490034476704 [label="UnsafeViewBackward0
--------------------------
self_sym_sizes: (36, 3, 3)"]
	140490034477088 -> 140490034476704
	140490034477088 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140490034476944 -> 140490034477088
	140490034476944 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 3, 64)"]
	140490034477376 -> 140490034476944
	140490034477376 [label=CloneBackward0]
	140490034477520 -> 140490034477376
	140490034477520 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 3, 64)"]
	140490034477856 -> 140490034477520
	140490034477856 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140490034478144 -> 140490034477856
	140490034478144 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 3, 768)"]
	140490034478240 -> 140490034478144
	140490034478240 [label="ViewBackward0
------------------------
self_sym_sizes: (9, 768)"]
	140490034478672 -> 140490034478240
	140490034478672 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (9, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140490034478864 -> 140490034478672
	140490033693792 [label="q_encoder.encoder.information_exchanging_layer.7.attention.self.query.bias
 (768)" fillcolor=lightblue]
	140490033693792 -> 140490034478864
	140490034478864 [label=AccumulateGrad]
	140490034478912 -> 140490034478672
	140490034478912 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 3, 768)"]
	140490034473584 -> 140490034478912
	140490034473584 [label="CatBackward0
------------
dim: 1"]
	140490034413680 -> 140490034473584
	140490034413680 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 1, 768)
start         :                   0
step          :                   1"]
	140490034413968 -> 140490034413680
	140490034413968 [label="SliceBackward0
---------------------------
dim           :           1
end           :           1
self_sym_sizes: (3, 3, 768)
start         :           0
step          :           1"]
	140490034414688 -> 140490034413968
	140490034414688 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:         (3, 3, 768)
start         :                   0
step          :                   1"]
	140490034414208 -> 140490034414688
	140490034414208 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140490034415120 -> 140490034414208
	140490034415120 [label="AddBackward0
------------
alpha: 1"]
	140490034414256 -> 140490034415120
	140490034414256 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140490034415504 -> 140490034414256
	140490034415504 [label="ViewBackward0
------------------------
self_sym_sizes: (9, 768)"]
	140490034415984 -> 140490034415504
	140490034415984 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (9, 3072)
mat1_sym_strides:      (3072, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (3072, 768)
mat2_sym_strides:      (1, 3072)"]
	140490034415696 -> 140490034415984
	140490033693552 [label="q_encoder.encoder.information_exchanging_layer.6.output.dense.bias
 (768)" fillcolor=lightblue]
	140490033693552 -> 140490034415696
	140490034415696 [label=AccumulateGrad]
	140490034414544 -> 140490034415984
	140490034414544 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 3, 3072)"]
	140490034415792 -> 140490034414544
	140490034415792 [label="GeluBackward0
---------------------------
approximate:           none
self       : [saved tensor]"]
	140490034416032 -> 140490034415792
	140490034416032 [label="ViewBackward0
-------------------------
self_sym_sizes: (9, 3072)"]
	140490034416896 -> 140490034416032
	140490034416896 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (9, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (768, 3072)
mat2_sym_strides:       (1, 768)"]
	140490034416944 -> 140490034416896
	140490033693392 [label="q_encoder.encoder.information_exchanging_layer.6.intermediate.dense.bias
 (3072)" fillcolor=lightblue]
	140490033693392 -> 140490034416944
	140490034416944 [label=AccumulateGrad]
	140490034416992 -> 140490034416896
	140490034416992 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 3, 768)"]
	140490034414448 -> 140490034416992
	140490034414448 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140490034417904 -> 140490034414448
	140490034417904 [label="AddBackward0
------------
alpha: 1"]
	140490034418096 -> 140490034417904
	140490034418096 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140490034418384 -> 140490034418096
	140490034418384 [label="ViewBackward0
------------------------
self_sym_sizes: (9, 768)"]
	140490034418624 -> 140490034418384
	140490034418624 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (9, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140490034418528 -> 140490034418624
	140490033692992 [label="q_encoder.encoder.information_exchanging_layer.6.attention.output.dense.bias
 (768)" fillcolor=lightblue]
	140490033692992 -> 140490034418528
	140490034418528 [label=AccumulateGrad]
	140490034417088 -> 140490034418624
	140490034417088 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 3, 768)"]
	140490034418864 -> 140490034417088
	140490034418864 [label="ViewBackward0
------------------------------
self_sym_sizes: (3, 3, 12, 64)"]
	140490034418768 -> 140490034418864
	140490034418768 [label=CloneBackward0]
	140490034419488 -> 140490034418768
	140490034419488 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140490034419728 -> 140490034419488
	140490034419728 [label="UnsafeViewBackward0
---------------------------
self_sym_sizes: (36, 3, 64)"]
	140490034419776 -> 140490034419728
	140490034419776 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140490034420016 -> 140490034419776
	140490034420016 [label="ViewBackward0
-----------------------------
self_sym_sizes: (3, 12, 3, 3)"]
	140490034420304 -> 140490034420016
	140490034420304 [label="ExpandBackward0
-----------------------------
self_sym_sizes: (3, 12, 3, 3)"]
	140490034420400 -> 140490034420304
	140490034420400 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140490034420688 -> 140490034420400
	140490034420688 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140490034420928 -> 140490034420688
	140490034420928 [label="AddBackward0
------------
alpha: 1"]
	140490034421168 -> 140490034420928
	140490034421168 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140490034420976 -> 140490034421168
	140490034420976 [label="UnsafeViewBackward0
--------------------------
self_sym_sizes: (36, 3, 3)"]
	140490034421792 -> 140490034420976
	140490034421792 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140490034421744 -> 140490034421792
	140490034421744 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 3, 64)"]
	140490034422176 -> 140490034421744
	140490034422176 [label=CloneBackward0]
	140490034422272 -> 140490034422176
	140490034422272 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 3, 64)"]
	140490034422512 -> 140490034422272
	140490034422512 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140490034422704 -> 140490034422512
	140490034422704 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 3, 768)"]
	140490034422848 -> 140490034422704
	140490034422848 [label="ViewBackward0
------------------------
self_sym_sizes: (9, 768)"]
	140490034423184 -> 140490034422848
	140490034423184 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (9, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140490034423424 -> 140490034423184
	140490035216080 [label="q_encoder.encoder.information_exchanging_layer.6.attention.self.query.bias
 (768)" fillcolor=lightblue]
	140490035216080 -> 140490034423424
	140490034423424 [label=AccumulateGrad]
	140490034423520 -> 140490034423184
	140490034423520 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 3, 768)"]
	140490034418000 -> 140490034423520
	140490034418000 [label="CatBackward0
------------
dim: 1"]
	140490034424288 -> 140490034418000
	140490034424288 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 1, 768)
start         :                   0
step          :                   1"]
	140490034423568 -> 140490034424288
	140490034423568 [label="SliceBackward0
---------------------------
dim           :           1
end           :           1
self_sym_sizes: (3, 3, 768)
start         :           0
step          :           1"]
	140490034425008 -> 140490034423568
	140490034425008 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:         (3, 3, 768)
start         :                   0
step          :                   1"]
	140490034424480 -> 140490034425008
	140490034424480 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140490034425440 -> 140490034424480
	140490034425440 [label="AddBackward0
------------
alpha: 1"]
	140490034424576 -> 140490034425440
	140490034424576 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140490034425680 -> 140490034424576
	140490034425680 [label="ViewBackward0
------------------------
self_sym_sizes: (9, 768)"]
	140490034425920 -> 140490034425680
	140490034425920 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (9, 3072)
mat1_sym_strides:      (3072, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (3072, 768)
mat2_sym_strides:      (1, 3072)"]
	140490034425824 -> 140490034425920
	140490035215840 [label="q_encoder.encoder.information_exchanging_layer.5.output.dense.bias
 (768)" fillcolor=lightblue]
	140490035215840 -> 140490034425824
	140490034425824 [label=AccumulateGrad]
	140490034424816 -> 140490034425920
	140490034424816 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 3, 3072)"]
	140490034426112 -> 140490034424816
	140490034426112 [label="GeluBackward0
---------------------------
approximate:           none
self       : [saved tensor]"]
	140490034426016 -> 140490034426112
	140490034426016 [label="ViewBackward0
-------------------------
self_sym_sizes: (9, 3072)"]
	140490034426640 -> 140490034426016
	140490034426640 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (9, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (768, 3072)
mat2_sym_strides:       (1, 768)"]
	140490034426736 -> 140490034426640
	140490035215760 [label="q_encoder.encoder.information_exchanging_layer.5.intermediate.dense.bias
 (3072)" fillcolor=lightblue]
	140490035215760 -> 140490034426736
	140490034426736 [label=AccumulateGrad]
	140490034426832 -> 140490034426640
	140490034426832 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 3, 768)"]
	140490034424672 -> 140490034426832
	140490034424672 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140490034427456 -> 140490034424672
	140490034427456 [label="AddBackward0
------------
alpha: 1"]
	140490034427504 -> 140490034427456
	140490034427504 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140490034427888 -> 140490034427504
	140490034427888 [label="ViewBackward0
------------------------
self_sym_sizes: (9, 768)"]
	140490034428224 -> 140490034427888
	140490034428224 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (9, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140490034428128 -> 140490034428224
	140490035215360 [label="q_encoder.encoder.information_exchanging_layer.5.attention.output.dense.bias
 (768)" fillcolor=lightblue]
	140490035215360 -> 140490034428128
	140490034428128 [label=AccumulateGrad]
	140490034426976 -> 140490034428224
	140490034426976 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 3, 768)"]
	140490034428368 -> 140490034426976
	140490034428368 [label="ViewBackward0
------------------------------
self_sym_sizes: (3, 3, 12, 64)"]
	140490034428320 -> 140490034428368
	140490034428320 [label=CloneBackward0]
	140490034428800 -> 140490034428320
	140490034428800 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140490034429184 -> 140490034428800
	140490034429184 [label="UnsafeViewBackward0
---------------------------
self_sym_sizes: (36, 3, 64)"]
	140490034429088 -> 140490034429184
	140490034429088 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140490034429520 -> 140490034429088
	140490034429520 [label="ViewBackward0
-----------------------------
self_sym_sizes: (3, 12, 3, 3)"]
	140490034429904 -> 140490034429520
	140490034429904 [label="ExpandBackward0
-----------------------------
self_sym_sizes: (3, 12, 3, 3)"]
	140490034298944 -> 140490034429904
	140490034298944 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140490034299232 -> 140490034298944
	140490034299232 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140490034299376 -> 140490034299232
	140490034299376 [label="AddBackward0
------------
alpha: 1"]
	140490034299664 -> 140490034299376
	140490034299664 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140490034299520 -> 140490034299664
	140490034299520 [label="UnsafeViewBackward0
--------------------------
self_sym_sizes: (36, 3, 3)"]
	140490034300048 -> 140490034299520
	140490034300048 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140490034299904 -> 140490034300048
	140490034299904 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 3, 64)"]
	140490034300336 -> 140490034299904
	140490034300336 [label=CloneBackward0]
	140490034300384 -> 140490034300336
	140490034300384 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 3, 64)"]
	140490034300720 -> 140490034300384
	140490034300720 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140490034300912 -> 140490034300720
	140490034300912 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 3, 768)"]
	140490034300960 -> 140490034300912
	140490034300960 [label="ViewBackward0
------------------------
self_sym_sizes: (9, 768)"]
	140490034301344 -> 140490034300960
	140490034301344 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (9, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140490034301632 -> 140490034301344
	140490035214800 [label="q_encoder.encoder.information_exchanging_layer.5.attention.self.query.bias
 (768)" fillcolor=lightblue]
	140490035214800 -> 140490034301632
	140490034301632 [label=AccumulateGrad]
	140490034301680 -> 140490034301344
	140490034301680 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 3, 768)"]
	140490034427648 -> 140490034301680
	140490034427648 [label="CatBackward0
------------
dim: 1"]
	140490034302400 -> 140490034427648
	140490034302400 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 1, 768)
start         :                   0
step          :                   1"]
	140490034301776 -> 140490034302400
	140490034301776 [label="SliceBackward0
---------------------------
dim           :           1
end           :           1
self_sym_sizes: (3, 3, 768)
start         :           0
step          :           1"]
	140490034303120 -> 140490034301776
	140490034303120 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:         (3, 3, 768)
start         :                   0
step          :                   1"]
	140490034302640 -> 140490034303120
	140490034302640 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140490034303408 -> 140490034302640
	140490034303408 [label="AddBackward0
------------
alpha: 1"]
	140490034302736 -> 140490034303408
	140490034302736 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140490034303792 -> 140490034302736
	140490034303792 [label="ViewBackward0
------------------------
self_sym_sizes: (9, 768)"]
	140490034304128 -> 140490034303792
	140490034304128 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (9, 3072)
mat1_sym_strides:      (3072, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (3072, 768)
mat2_sym_strides:      (1, 3072)"]
	140490034303984 -> 140490034304128
	140490035214560 [label="q_encoder.encoder.information_exchanging_layer.4.output.dense.bias
 (768)" fillcolor=lightblue]
	140490035214560 -> 140490034303984
	140490034303984 [label=AccumulateGrad]
	140490034302976 -> 140490034304128
	140490034302976 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 3, 3072)"]
	140490034304416 -> 140490034302976
	140490034304416 [label="GeluBackward0
---------------------------
approximate:           none
self       : [saved tensor]"]
	140490034304320 -> 140490034304416
	140490034304320 [label="ViewBackward0
-------------------------
self_sym_sizes: (9, 3072)"]
	140490034305088 -> 140490034304320
	140490034305088 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (9, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (768, 3072)
mat2_sym_strides:       (1, 768)"]
	140490034305136 -> 140490034305088
	140490035214480 [label="q_encoder.encoder.information_exchanging_layer.4.intermediate.dense.bias
 (3072)" fillcolor=lightblue]
	140490035214480 -> 140490034305136
	140490034305136 [label=AccumulateGrad]
	140490034305280 -> 140490034305088
	140490034305280 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 3, 768)"]
	140490034302784 -> 140490034305280
	140490034302784 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140490034305856 -> 140490034302784
	140490034305856 [label="AddBackward0
------------
alpha: 1"]
	140490034306096 -> 140490034305856
	140490034306096 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140490034306336 -> 140490034306096
	140490034306336 [label="ViewBackward0
------------------------
self_sym_sizes: (9, 768)"]
	140490034306528 -> 140490034306336
	140490034306528 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (9, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140490034306432 -> 140490034306528
	140490035214080 [label="q_encoder.encoder.information_exchanging_layer.4.attention.output.dense.bias
 (768)" fillcolor=lightblue]
	140490035214080 -> 140490034306432
	140490034306432 [label=AccumulateGrad]
	140490034305376 -> 140490034306528
	140490034305376 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 3, 768)"]
	140490034306720 -> 140490034305376
	140490034306720 [label="ViewBackward0
------------------------------
self_sym_sizes: (3, 3, 12, 64)"]
	140490034306816 -> 140490034306720
	140490034306816 [label=CloneBackward0]
	140490034307344 -> 140490034306816
	140490034307344 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140490034307680 -> 140490034307344
	140490034307680 [label="UnsafeViewBackward0
---------------------------
self_sym_sizes: (36, 3, 64)"]
	140490034307536 -> 140490034307680
	140490034307536 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140490034308112 -> 140490034307536
	140490034308112 [label="ViewBackward0
-----------------------------
self_sym_sizes: (3, 12, 3, 3)"]
	140490034308544 -> 140490034308112
	140490034308544 [label="ExpandBackward0
-----------------------------
self_sym_sizes: (3, 12, 3, 3)"]
	140490034308640 -> 140490034308544
	140490034308640 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140490034308880 -> 140490034308640
	140490034308880 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140490034309120 -> 140490034308880
	140490034309120 [label="AddBackward0
------------
alpha: 1"]
	140490034309408 -> 140490034309120
	140490034309408 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140490034309264 -> 140490034309408
	140490034309264 [label="UnsafeViewBackward0
--------------------------
self_sym_sizes: (36, 3, 3)"]
	140490034309840 -> 140490034309264
	140490034309840 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140490034309648 -> 140490034309840
	140490034309648 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 3, 64)"]
	140490034310176 -> 140490034309648
	140490034310176 [label=CloneBackward0]
	140490034310224 -> 140490034310176
	140490034310224 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 3, 64)"]
	140490034310416 -> 140490034310224
	140490034310416 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140490034310512 -> 140490034310416
	140490034310512 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 3, 768)"]
	140490034310656 -> 140490034310512
	140490034310656 [label="ViewBackward0
------------------------
self_sym_sizes: (9, 768)"]
	140490034311040 -> 140490034310656
	140490034311040 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (9, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140490034311328 -> 140490034311040
	140490035213520 [label="q_encoder.encoder.information_exchanging_layer.4.attention.self.query.bias
 (768)" fillcolor=lightblue]
	140490035213520 -> 140490034311328
	140490034311328 [label=AccumulateGrad]
	140490034311376 -> 140490034311040
	140490034311376 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 3, 768)"]
	140490034306000 -> 140490034311376
	140490034306000 [label="CatBackward0
------------
dim: 1"]
	140490034312096 -> 140490034306000
	140490034312096 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 1, 768)
start         :                   0
step          :                   1"]
	140490034311616 -> 140490034312096
	140490034311616 [label="SliceBackward0
---------------------------
dim           :           1
end           :           1
self_sym_sizes: (3, 3, 768)
start         :           0
step          :           1"]
	140490034312864 -> 140490034311616
	140490034312864 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:         (3, 3, 768)
start         :                   0
step          :                   1"]
	140490034312288 -> 140490034312864
	140490034312288 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140490034313200 -> 140490034312288
	140490034313200 [label="AddBackward0
------------
alpha: 1"]
	140490034312336 -> 140490034313200
	140490034312336 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140490034313488 -> 140490034312336
	140490034313488 [label="ViewBackward0
------------------------
self_sym_sizes: (9, 768)"]
	140490034313728 -> 140490034313488
	140490034313728 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (9, 3072)
mat1_sym_strides:      (3072, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (3072, 768)
mat2_sym_strides:      (1, 3072)"]
	140490034313584 -> 140490034313728
	140490035213280 [label="q_encoder.encoder.information_exchanging_layer.3.output.dense.bias
 (768)" fillcolor=lightblue]
	140490035213280 -> 140490034313584
	140490034313584 [label=AccumulateGrad]
	140490034312624 -> 140490034313728
	140490034312624 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 3, 3072)"]
	140490034314016 -> 140490034312624
	140490034314016 [label="GeluBackward0
---------------------------
approximate:           none
self       : [saved tensor]"]
	140490034313920 -> 140490034314016
	140490034313920 [label="ViewBackward0
-------------------------
self_sym_sizes: (9, 3072)"]
	140490034314544 -> 140490034313920
	140490034314544 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (9, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (768, 3072)
mat2_sym_strides:       (1, 768)"]
	140490034314592 -> 140490034314544
	140490035213200 [label="q_encoder.encoder.information_exchanging_layer.3.intermediate.dense.bias
 (3072)" fillcolor=lightblue]
	140490035213200 -> 140490034314592
	140490034314592 [label=AccumulateGrad]
	140490034314640 -> 140490034314544
	140490034314640 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 3, 768)"]
	140490034312432 -> 140490034314640
	140490034312432 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140490034314784 -> 140490034312432
	140490034314784 [label="AddBackward0
------------
alpha: 1"]
	140490034053424 -> 140490034314784
	140490034053424 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140490034053664 -> 140490034053424
	140490034053664 [label="ViewBackward0
------------------------
self_sym_sizes: (9, 768)"]
	140490034054000 -> 140490034053664
	140490034054000 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (9, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140490034053808 -> 140490034054000
	140490035212800 [label="q_encoder.encoder.information_exchanging_layer.3.attention.output.dense.bias
 (768)" fillcolor=lightblue]
	140490035212800 -> 140490034053808
	140490034053808 [label=AccumulateGrad]
	140490034053184 -> 140490034054000
	140490034053184 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 3, 768)"]
	140490034054240 -> 140490034053184
	140490034054240 [label="ViewBackward0
------------------------------
self_sym_sizes: (3, 3, 12, 64)"]
	140490034054192 -> 140490034054240
	140490034054192 [label=CloneBackward0]
	140490034054768 -> 140490034054192
	140490034054768 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140490034055056 -> 140490034054768
	140490034055056 [label="UnsafeViewBackward0
---------------------------
self_sym_sizes: (36, 3, 64)"]
	140490034055104 -> 140490034055056
	140490034055104 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140490034055584 -> 140490034055104
	140490034055584 [label="ViewBackward0
-----------------------------
self_sym_sizes: (3, 12, 3, 3)"]
	140490034055968 -> 140490034055584
	140490034055968 [label="ExpandBackward0
-----------------------------
self_sym_sizes: (3, 12, 3, 3)"]
	140490034056016 -> 140490034055968
	140490034056016 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140490034056304 -> 140490034056016
	140490034056304 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140490034056544 -> 140490034056304
	140490034056544 [label="AddBackward0
------------
alpha: 1"]
	140490034056784 -> 140490034056544
	140490034056784 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140490034056688 -> 140490034056784
	140490034056688 [label="UnsafeViewBackward0
--------------------------
self_sym_sizes: (36, 3, 3)"]
	140490034057168 -> 140490034056688
	140490034057168 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140490034057072 -> 140490034057168
	140490034057072 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 3, 64)"]
	140490034057456 -> 140490034057072
	140490034057456 [label=CloneBackward0]
	140490034057648 -> 140490034057456
	140490034057648 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 3, 64)"]
	140490034057888 -> 140490034057648
	140490034057888 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140490034058128 -> 140490034057888
	140490034058128 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 3, 768)"]
	140490034058176 -> 140490034058128
	140490034058176 [label="ViewBackward0
------------------------
self_sym_sizes: (9, 768)"]
	140490034058512 -> 140490034058176
	140490034058512 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (9, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140490034058704 -> 140490034058512
	140490035212240 [label="q_encoder.encoder.information_exchanging_layer.3.attention.self.query.bias
 (768)" fillcolor=lightblue]
	140490035212240 -> 140490034058704
	140490034058704 [label=AccumulateGrad]
	140490034058752 -> 140490034058512
	140490034058752 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 3, 768)"]
	140490034053328 -> 140490034058752
	140490034053328 [label="CatBackward0
------------
dim: 1"]
	140490034059472 -> 140490034053328
	140490034059472 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 1, 768)
start         :                   0
step          :                   1"]
	140490034058896 -> 140490034059472
	140490034058896 [label="SliceBackward0
---------------------------
dim           :           1
end           :           1
self_sym_sizes: (3, 3, 768)
start         :           0
step          :           1"]
	140490034060144 -> 140490034058896
	140490034060144 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:         (3, 3, 768)
start         :                   0
step          :                   1"]
	140490034059664 -> 140490034060144
	140490034059664 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140490034060480 -> 140490034059664
	140490034060480 [label="AddBackward0
------------
alpha: 1"]
	140490034059760 -> 140490034060480
	140490034059760 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140490034060816 -> 140490034059760
	140490034060816 [label="ViewBackward0
------------------------
self_sym_sizes: (9, 768)"]
	140490034061056 -> 140490034060816
	140490034061056 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (9, 3072)
mat1_sym_strides:      (3072, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (3072, 768)
mat2_sym_strides:      (1, 3072)"]
	140490034060912 -> 140490034061056
	140490035212000 [label="q_encoder.encoder.information_exchanging_layer.2.output.dense.bias
 (768)" fillcolor=lightblue]
	140490035212000 -> 140490034060912
	140490034060912 [label=AccumulateGrad]
	140490034060000 -> 140490034061056
	140490034060000 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 3, 3072)"]
	140490034061296 -> 140490034060000
	140490034061296 [label="GeluBackward0
---------------------------
approximate:           none
self       : [saved tensor]"]
	140490034061200 -> 140490034061296
	140490034061200 [label="ViewBackward0
-------------------------
self_sym_sizes: (9, 3072)"]
	140490034062016 -> 140490034061200
	140490034062016 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (9, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (768, 3072)
mat2_sym_strides:       (1, 768)"]
	140490034062064 -> 140490034062016
	140490035211920 [label="q_encoder.encoder.information_exchanging_layer.2.intermediate.dense.bias
 (3072)" fillcolor=lightblue]
	140490035211920 -> 140490034062064
	140490034062064 [label=AccumulateGrad]
	140490034062160 -> 140490034062016
	140490034062160 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 3, 768)"]
	140490034059856 -> 140490034062160
	140490034059856 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140490034062832 -> 140490034059856
	140490034062832 [label="AddBackward0
------------
alpha: 1"]
	140490034063024 -> 140490034062832
	140490034063024 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140490034063264 -> 140490034063024
	140490034063264 [label="ViewBackward0
------------------------
self_sym_sizes: (9, 768)"]
	140490034063648 -> 140490034063264
	140490034063648 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (9, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140490034063408 -> 140490034063648
	140490035211520 [label="q_encoder.encoder.information_exchanging_layer.2.attention.output.dense.bias
 (768)" fillcolor=lightblue]
	140490035211520 -> 140490034063408
	140490034063408 [label=AccumulateGrad]
	140490034062208 -> 140490034063648
	140490034062208 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 3, 768)"]
	140490034063840 -> 140490034062208
	140490034063840 [label="ViewBackward0
------------------------------
self_sym_sizes: (3, 3, 12, 64)"]
	140490034064032 -> 140490034063840
	140490034064032 [label=CloneBackward0]
	140490034064752 -> 140490034064032
	140490034064752 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140490034064992 -> 140490034064752
	140490034064992 [label="UnsafeViewBackward0
---------------------------
self_sym_sizes: (36, 3, 64)"]
	140490034065088 -> 140490034064992
	140490034065088 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140490034065424 -> 140490034065088
	140490034065424 [label="ViewBackward0
-----------------------------
self_sym_sizes: (3, 12, 3, 3)"]
	140490034065712 -> 140490034065424
	140490034065712 [label="ExpandBackward0
-----------------------------
self_sym_sizes: (3, 12, 3, 3)"]
	140490034065808 -> 140490034065712
	140490034065808 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140490034066192 -> 140490034065808
	140490034066192 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140490034066528 -> 140490034066192
	140490034066528 [label="AddBackward0
------------
alpha: 1"]
	140490034066624 -> 140490034066528
	140490034066624 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140490034066576 -> 140490034066624
	140490034066576 [label="UnsafeViewBackward0
--------------------------
self_sym_sizes: (36, 3, 3)"]
	140490034067056 -> 140490034066576
	140490034067056 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140490034066960 -> 140490034067056
	140490034066960 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 3, 64)"]
	140490034067392 -> 140490034066960
	140490034067392 [label=CloneBackward0]
	140490034067536 -> 140490034067392
	140490034067536 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 3, 64)"]
	140490034067824 -> 140490034067536
	140490034067824 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140490034068064 -> 140490034067824
	140490034068064 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 3, 768)"]
	140490034068112 -> 140490034068064
	140490034068112 [label="ViewBackward0
------------------------
self_sym_sizes: (9, 768)"]
	140490034068352 -> 140490034068112
	140490034068352 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (9, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140490034068544 -> 140490034068352
	140490035210960 [label="q_encoder.encoder.information_exchanging_layer.2.attention.self.query.bias
 (768)" fillcolor=lightblue]
	140490035210960 -> 140490034068544
	140490034068544 [label=AccumulateGrad]
	140490034068592 -> 140490034068352
	140490034068592 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 3, 768)"]
	140490034062928 -> 140490034068592
	140490034062928 [label="CatBackward0
------------
dim: 1"]
	140490034069072 -> 140490034062928
	140490034069072 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 1, 768)
start         :                   0
step          :                   1"]
	140490034068688 -> 140490034069072
	140490034068688 [label="SliceBackward0
---------------------------
dim           :           1
end           :           1
self_sym_sizes: (3, 3, 768)
start         :           0
step          :           1"]
	140490034069456 -> 140490034068688
	140490034069456 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:         (3, 3, 768)
start         :                   0
step          :                   1"]
	140490034069360 -> 140490034069456
	140490034069360 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140490034020608 -> 140490034069360
	140490034020608 [label="AddBackward0
------------
alpha: 1"]
	140490034020752 -> 140490034020608
	140490034020752 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140490034021280 -> 140490034020752
	140490034021280 [label="ViewBackward0
------------------------
self_sym_sizes: (9, 768)"]
	140490034021520 -> 140490034021280
	140490034021520 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (9, 3072)
mat1_sym_strides:      (3072, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (3072, 768)
mat2_sym_strides:      (1, 3072)"]
	140490034021424 -> 140490034021520
	140490035210720 [label="q_encoder.encoder.information_exchanging_layer.1.output.dense.bias
 (768)" fillcolor=lightblue]
	140490035210720 -> 140490034021424
	140490034021424 [label=AccumulateGrad]
	140490034020512 -> 140490034021520
	140490034020512 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 3, 3072)"]
	140490034021712 -> 140490034020512
	140490034021712 [label="GeluBackward0
---------------------------
approximate:           none
self       : [saved tensor]"]
	140490034021664 -> 140490034021712
	140490034021664 [label="ViewBackward0
-------------------------
self_sym_sizes: (9, 3072)"]
	140490034022624 -> 140490034021664
	140490034022624 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (9, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (768, 3072)
mat2_sym_strides:       (1, 768)"]
	140490034022672 -> 140490034022624
	140490035210640 [label="q_encoder.encoder.information_exchanging_layer.1.intermediate.dense.bias
 (3072)" fillcolor=lightblue]
	140490035210640 -> 140490034022672
	140490034022672 [label=AccumulateGrad]
	140490034022720 -> 140490034022624
	140490034022720 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 3, 768)"]
	140490034020656 -> 140490034022720
	140490034020656 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140490034023344 -> 140490034020656
	140490034023344 [label="AddBackward0
------------
alpha: 1"]
	140490034023440 -> 140490034023344
	140490034023440 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140490034023824 -> 140490034023440
	140490034023824 [label="ViewBackward0
------------------------
self_sym_sizes: (9, 768)"]
	140490034024160 -> 140490034023824
	140490034024160 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (9, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140490034023920 -> 140490034024160
	140490035210240 [label="q_encoder.encoder.information_exchanging_layer.1.attention.output.dense.bias
 (768)" fillcolor=lightblue]
	140490035210240 -> 140490034023920
	140490034023920 [label=AccumulateGrad]
	140490034022864 -> 140490034024160
	140490034022864 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 3, 768)"]
	140490034024352 -> 140490034022864
	140490034024352 [label="ViewBackward0
------------------------------
self_sym_sizes: (3, 3, 12, 64)"]
	140490034024208 -> 140490034024352
	140490034024208 [label=CloneBackward0]
	140490034024928 -> 140490034024208
	140490034024928 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140490034025072 -> 140490034024928
	140490034025072 [label="UnsafeViewBackward0
---------------------------
self_sym_sizes: (36, 3, 64)"]
	140490034025216 -> 140490034025072
	140490034025216 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140490034025504 -> 140490034025216
	140490034025504 [label="ViewBackward0
-----------------------------
self_sym_sizes: (3, 12, 3, 3)"]
	140490034025936 -> 140490034025504
	140490034025936 [label="ExpandBackward0
-----------------------------
self_sym_sizes: (3, 12, 3, 3)"]
	140490034026080 -> 140490034025936
	140490034026080 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140490034026368 -> 140490034026080
	140490034026368 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140490034026704 -> 140490034026368
	140490034026704 [label="AddBackward0
------------
alpha: 1"]
	140490034026896 -> 140490034026704
	140490034026896 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140490034026800 -> 140490034026896
	140490034026800 [label="UnsafeViewBackward0
--------------------------
self_sym_sizes: (36, 3, 3)"]
	140490034027376 -> 140490034026800
	140490034027376 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140490034027232 -> 140490034027376
	140490034027232 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 3, 64)"]
	140490034027616 -> 140490034027232
	140490034027616 [label=CloneBackward0]
	140490034027808 -> 140490034027616
	140490034027808 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 3, 64)"]
	140490034028000 -> 140490034027808
	140490034028000 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140490034028192 -> 140490034028000
	140490034028192 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 3, 768)"]
	140490034028288 -> 140490034028192
	140490034028288 [label="ViewBackward0
------------------------
self_sym_sizes: (9, 768)"]
	140490034028672 -> 140490034028288
	140490034028672 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (9, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140490034028912 -> 140490034028672
	140490035209680 [label="q_encoder.encoder.information_exchanging_layer.1.attention.self.query.bias
 (768)" fillcolor=lightblue]
	140490035209680 -> 140490034028912
	140490034028912 [label=AccumulateGrad]
	140490034028960 -> 140490034028672
	140490034028960 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 3, 768)"]
	140490034023392 -> 140490034028960
	140490034023392 [label="CatBackward0
------------
dim: 1"]
	140490034029584 -> 140490034023392
	140490034029584 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 1, 768)
start         :                   0
step          :                   1"]
	140490034029056 -> 140490034029584
	140490034029056 [label="SliceBackward0
---------------------------
dim           :           1
end           :           1
self_sym_sizes: (3, 3, 768)
start         :           0
step          :           1"]
	140490034030208 -> 140490034029056
	140490034030208 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:         (3, 3, 768)
start         :                   0
step          :                   1"]
	140490034029824 -> 140490034030208
	140490034029824 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140490034030544 -> 140490034029824
	140490034030544 [label="AddBackward0
------------
alpha: 1"]
	140490034029872 -> 140490034030544
	140490034029872 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140490034030736 -> 140490034029872
	140490034030736 [label="ViewBackward0
------------------------
self_sym_sizes: (9, 768)"]
	140490034030976 -> 140490034030736
	140490034030976 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (9, 3072)
mat1_sym_strides:      (3072, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (3072, 768)
mat2_sym_strides:      (1, 3072)"]
	140490034030880 -> 140490034030976
	140490035209440 [label="q_encoder.encoder.information_exchanging_layer.0.output.dense.bias
 (768)" fillcolor=lightblue]
	140490035209440 -> 140490034030880
	140490034030880 [label=AccumulateGrad]
	140490034030112 -> 140490034030976
	140490034030112 [label="ViewBackward0
----------------------------
self_sym_sizes: (3, 3, 3072)"]
	140490034031264 -> 140490034030112
	140490034031264 [label="GeluBackward0
---------------------------
approximate:           none
self       : [saved tensor]"]
	140490034031168 -> 140490034031264
	140490034031168 [label="ViewBackward0
-------------------------
self_sym_sizes: (9, 3072)"]
	140490034031840 -> 140490034031168
	140490034031840 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (9, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (768, 3072)
mat2_sym_strides:       (1, 768)"]
	140490034031888 -> 140490034031840
	140490035209360 [label="q_encoder.encoder.information_exchanging_layer.0.intermediate.dense.bias
 (3072)" fillcolor=lightblue]
	140490035209360 -> 140490034031888
	140490034031888 [label=AccumulateGrad]
	140490034031936 -> 140490034031840
	140490034031936 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 3, 768)"]
	140490034029968 -> 140490034031936
	140490034029968 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140490034032608 -> 140490034029968
	140490034032608 [label="AddBackward0
------------
alpha: 1"]
	140490034032752 -> 140490034032608
	140490034032752 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140490034033088 -> 140490034032752
	140490034033088 [label="ViewBackward0
------------------------
self_sym_sizes: (9, 768)"]
	140490034033472 -> 140490034033088
	140490034033472 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (9, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140490034033184 -> 140490034033472
	140490035208960 [label="q_encoder.encoder.information_exchanging_layer.0.attention.output.dense.bias
 (768)" fillcolor=lightblue]
	140490035208960 -> 140490034033184
	140490034033184 [label=AccumulateGrad]
	140490034032080 -> 140490034033472
	140490034032080 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 3, 768)"]
	140490034033616 -> 140490034032080
	140490034033616 [label="ViewBackward0
------------------------------
self_sym_sizes: (3, 3, 12, 64)"]
	140490034033712 -> 140490034033616
	140490034033712 [label=CloneBackward0]
	140490034034192 -> 140490034033712
	140490034034192 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140490034034240 -> 140490034034192
	140490034034240 [label="UnsafeViewBackward0
---------------------------
self_sym_sizes: (36, 3, 64)"]
	140490034034480 -> 140490034034240
	140490034034480 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140490034034864 -> 140490034034480
	140490034034864 [label="ViewBackward0
-----------------------------
self_sym_sizes: (3, 12, 3, 3)"]
	140490034035200 -> 140490034034864
	140490034035200 [label="ExpandBackward0
-----------------------------
self_sym_sizes: (3, 12, 3, 3)"]
	140490034035248 -> 140490034035200
	140490034035248 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140490034035488 -> 140490034035248
	140490034035488 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140490034035728 -> 140490034035488
	140490034035728 [label="AddBackward0
------------
alpha: 1"]
	140490034035968 -> 140490034035728
	140490034035968 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140490034035920 -> 140490034035968
	140490034035920 [label="UnsafeViewBackward0
--------------------------
self_sym_sizes: (36, 3, 3)"]
	140490034036304 -> 140490034035920
	140490034036304 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140490034036160 -> 140490034036304
	140490034036160 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 3, 64)"]
	140490034036592 -> 140490034036160
	140490034036592 [label=CloneBackward0]
	140490034151536 -> 140490034036592
	140490034151536 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 3, 64)"]
	140490034151824 -> 140490034151536
	140490034151824 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140490034152064 -> 140490034151824
	140490034152064 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 3, 768)"]
	140490034152160 -> 140490034152064
	140490034152160 [label="ViewBackward0
------------------------
self_sym_sizes: (9, 768)"]
	140490034152400 -> 140490034152160
	140490034152400 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (9, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140490034152688 -> 140490034152400
	140490036498016 [label="q_encoder.encoder.information_exchanging_layer.0.attention.self.query.bias
 (768)" fillcolor=lightblue]
	140490036498016 -> 140490034152688
	140490034152688 [label=AccumulateGrad]
	140490034152736 -> 140490034152400
	140490034152736 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 3, 768)"]
	140490034032656 -> 140490034152736
	140490034032656 [label="CatBackward0
------------
dim: 1"]
	140490034153264 -> 140490034032656
	140490034153264 [label="RepeatBackward0
---------------------------
repeats       :   (3, 1, 1)
self_sym_sizes: (1, 1, 768)"]
	140490130024304 -> 140490034153264
	140490130024304 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (1, 1, 768)
start         :                   0
step          :                   1"]
	140490034153552 -> 140490130024304
	140490034153552 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:         (1, 1, 768)
start         :                   0
step          :                   1"]
	140490034153888 -> 140490034153552
	140490034153888 [label="UnsqueezeBackward0
------------------
dim: 0"]
	140490034152880 -> 140490034153888
	140490036436080 [label="q_encoder.doc_embeddings
 (1, 768)" fillcolor=lightblue]
	140490036436080 -> 140490034152880
	140490034152880 [label=AccumulateGrad]
	140490034153072 -> 140490034032656
	140490034153072 [label=CloneBackward0]
	140490034153312 -> 140490034153072
	140490034153312 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 2, 768)
start         :                   0
step          :                   1"]
	140490034154272 -> 140490034153312
	140490034154272 [label="SelectBackward0
--------------------------------
dim           :                2
index         :                1
self_sym_sizes: (3, 2, 502, 768)"]
	140490040889824 -> 140490034154272
	140490040889824 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:    (3, 2, 502, 768)
start         :                   0
step          :                   1"]
	140490034152832 -> 140490040889824
	140490034152832 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:    (3, 2, 502, 768)
start         :                   0
step          :                   1"]
	140490034154992 -> 140490034152832
	140490034154992 [label="ViewBackward0
-----------------------------
self_sym_sizes: (6, 502, 768)"]
	140490034154416 -> 140490034154992
	140490034154416 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140490034154464 -> 140490034154416
	140490034154464 [label="AddBackward0
------------
alpha: 1"]
	140490034153600 -> 140490034154464
	140490034153600 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140490034155568 -> 140490034153600
	140490034155568 [label="ViewBackward0
---------------------------
self_sym_sizes: (3012, 768)"]
	140490034155808 -> 140490034155568
	140490034155808 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :   (3012, 3072)
mat1_sym_strides:      (3072, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (3072, 768)
mat2_sym_strides:      (1, 3072)"]
	140490034155760 -> 140490034155808
	140490036503776 [label="q_encoder.encoder.text_encoding_layer.0.output.dense.bias
 (768)" fillcolor=lightblue]
	140490036503776 -> 140490034155760
	140490034155760 [label=AccumulateGrad]
	140490034154656 -> 140490034155808
	140490034154656 [label="ViewBackward0
------------------------------
self_sym_sizes: (6, 502, 3072)"]
	140490034156096 -> 140490034154656
	140490034156096 [label="GeluBackward0
---------------------------
approximate:           none
self       : [saved tensor]"]
	140490034156000 -> 140490034156096
	140490034156000 [label="ViewBackward0
----------------------------
self_sym_sizes: (3012, 3072)"]
	140490034156816 -> 140490034156000
	140490034156816 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :    (3012, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (768, 3072)
mat2_sym_strides:       (1, 768)"]
	140490034156912 -> 140490034156816
	140490034702336 [label="q_encoder.encoder.text_encoding_layer.0.intermediate.dense.bias
 (3072)" fillcolor=lightblue]
	140490034702336 -> 140490034156912
	140490034156912 [label=AccumulateGrad]
	140490034156864 -> 140490034156816
	140490034156864 [label="ViewBackward0
-----------------------------
self_sym_sizes: (6, 502, 768)"]
	140490034155280 -> 140490034156864
	140490034155280 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140490034157296 -> 140490034155280
	140490034157296 [label="AddBackward0
------------
alpha: 1"]
	140490034157488 -> 140490034157296
	140490034157488 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140490034157680 -> 140490034157488
	140490034157680 [label="ViewBackward0
---------------------------
self_sym_sizes: (3012, 768)"]
	140490034157968 -> 140490034157680
	140490034157968 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :    (3012, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140490034157872 -> 140490034157968
	140490036504416 [label="q_encoder.encoder.text_encoding_layer.0.attention.output.dense.bias
 (768)" fillcolor=lightblue]
	140490036504416 -> 140490034157872
	140490034157872 [label=AccumulateGrad]
	140490034157008 -> 140490034157968
	140490034157008 [label="ViewBackward0
-----------------------------
self_sym_sizes: (6, 502, 768)"]
	140490034158208 -> 140490034157008
	140490034158208 [label="ViewBackward0
--------------------------------
self_sym_sizes: (6, 502, 12, 64)"]
	140490034158160 -> 140490034158208
	140490034158160 [label=CloneBackward0]
	140490034158880 -> 140490034158160
	140490034158880 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140490034159024 -> 140490034158880
	140490034159024 [label="UnsafeViewBackward0
-----------------------------
self_sym_sizes: (72, 502, 64)"]
	140490034159120 -> 140490034159024
	140490034159120 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140490034159552 -> 140490034159120
	140490034159552 [label="ViewBackward0
---------------------------------
self_sym_sizes: (6, 12, 502, 502)"]
	140490034159840 -> 140490034159552
	140490034159840 [label="ExpandBackward0
---------------------------------
self_sym_sizes: (6, 12, 502, 502)"]
	140490034159888 -> 140490034159840
	140490034159888 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140490034160320 -> 140490034159888
	140490034160320 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140490034160608 -> 140490034160320
	140490034160608 [label="AddBackward0
------------
alpha: 1"]
	140490034160704 -> 140490034160608
	140490034160704 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140490034160656 -> 140490034160704
	140490034160656 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (72, 502, 502)"]
	140490034161040 -> 140490034160656
	140490034161040 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140490034160992 -> 140490034161040
	140490034160992 [label="UnsafeViewBackward0
--------------------------------
self_sym_sizes: (6, 12, 502, 64)"]
	140490034161520 -> 140490034160992
	140490034161520 [label=CloneBackward0]
	140490034161616 -> 140490034161520
	140490034161616 [label="ExpandBackward0
--------------------------------
self_sym_sizes: (6, 12, 502, 64)"]
	140490034161856 -> 140490034161616
	140490034161856 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140490034162048 -> 140490034161856
	140490034162048 [label="ViewBackward0
-----------------------------
self_sym_sizes: (6, 502, 768)"]
	140490034162336 -> 140490034162048
	140490034162336 [label="ViewBackward0
---------------------------
self_sym_sizes: (3012, 768)"]
	140490034162528 -> 140490034162336
	140490034162528 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :    (3012, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140490034162912 -> 140490034162528
	140490034249344 [label="q_encoder.encoder.text_encoding_layer.0.attention.self.query.bias
 (768)" fillcolor=lightblue]
	140490034249344 -> 140490034162912
	140490034162912 [label=AccumulateGrad]
	140490034162768 -> 140490034162528
	140490034162768 [label="ViewBackward0
-----------------------------
self_sym_sizes: (6, 502, 768)"]
	140490034157392 -> 140490034162768
	140490034157392 [label="CatBackward0
------------
dim: 1"]
	140490034163296 -> 140490034157392
	140490034163296 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140490034163440 -> 140490034163296
	140490034163440 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140490034163728 -> 140490034163440
	140490034163728 [label="AddBackward0
------------
alpha: 1"]
	140490034163968 -> 140490034163728
	140490034163968 [label="AddBackward0
------------
alpha: 1"]
	140490034163776 -> 140490034163968
	140490034163776 [label="EmbeddingBackward0
------------------------------------
indices             : [saved tensor]
padding_idx         :              0
scale_grad_by_freq  :          False
sparse              :          False
weight_sym_argsize_0:          30522"]
	140490034164400 -> 140490034163776
	140490034249024 [label="q_encoder.embeddings.word_embeddings.weight
 (30522, 768)" fillcolor=lightblue]
	140490034249024 -> 140490034164400
	140490034164400 [label=AccumulateGrad]
	140490034163488 -> 140490034163968
	140490034163488 [label="EmbeddingBackward0
------------------------------------------
indices             :       [saved tensor]
padding_idx         : 18446744073709551615
scale_grad_by_freq  :                False
sparse              :                False
weight_sym_argsize_0:                    2"]
	140490034164496 -> 140490034163488
	140490036438320 [label="q_encoder.embeddings.token_type_embeddings.weight
 (2, 768)" fillcolor=lightblue]
	140490036438320 -> 140490034164496
	140490034164496 [label=AccumulateGrad]
	140490034163872 -> 140490034163728
	140490034163872 [label="EmbeddingBackward0
------------------------------------------
indices             :       [saved tensor]
padding_idx         : 18446744073709551615
scale_grad_by_freq  :                False
sparse              :                False
weight_sym_argsize_0:                  512"]
	140490034164640 -> 140490034163872
	140490034247984 [label="q_encoder.embeddings.position_embeddings.weight
 (512, 768)" fillcolor=lightblue]
	140490034247984 -> 140490034164640
	140490034164640 [label=AccumulateGrad]
	140490034163680 -> 140490034163440
	140490034701776 [label="q_encoder.embeddings.LayerNorm.weight
 (768)" fillcolor=lightblue]
	140490034701776 -> 140490034163680
	140490034163680 [label=AccumulateGrad]
	140490034163632 -> 140490034163440
	140490034701696 [label="q_encoder.embeddings.LayerNorm.bias
 (768)" fillcolor=lightblue]
	140490034701696 -> 140490034163632
	140490034163632 [label=AccumulateGrad]
	140490034163344 -> 140490034162528
	140490034163344 [label=TBackward0]
	140490040874832 -> 140490034163344
	140490034248624 [label="q_encoder.encoder.text_encoding_layer.0.attention.self.query.weight
 (768, 768)" fillcolor=lightblue]
	140490034248624 -> 140490040874832
	140490040874832 [label=AccumulateGrad]
	140490034160896 -> 140490034161040
	140490034160896 [label="UnsafeViewBackward0
--------------------------------
self_sym_sizes: (6, 12, 64, 502)"]
	140490034161328 -> 140490034160896
	140490034161328 [label=CloneBackward0]
	140490034162144 -> 140490034161328
	140490034162144 [label="ExpandBackward0
--------------------------------
self_sym_sizes: (6, 12, 64, 502)"]
	140490034162720 -> 140490034162144
	140490034162720 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551615
dim1: 18446744073709551614"]
	140490034163008 -> 140490034162720
	140490034163008 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140490034164112 -> 140490034163008
	140490034164112 [label="ViewBackward0
-----------------------------
self_sym_sizes: (6, 502, 768)"]
	140490034163056 -> 140490034164112
	140490034163056 [label="ViewBackward0
---------------------------
self_sym_sizes: (3012, 768)"]
	140490034164304 -> 140490034163056
	140490034164304 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :    (3012, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140490034164544 -> 140490034164304
	140490034248224 [label="q_encoder.encoder.text_encoding_layer.0.attention.self.key.bias
 (768)" fillcolor=lightblue]
	140490034248224 -> 140490034164544
	140490034164544 [label=AccumulateGrad]
	140490034164592 -> 140490034164304
	140490034164592 [label="ViewBackward0
-----------------------------
self_sym_sizes: (6, 502, 768)"]
	140490034157392 -> 140490034164592
	140490034160848 -> 140490034164304
	140490034160848 [label=TBackward0]
	140490034165360 -> 140490034160848
	140490036501536 [label="q_encoder.encoder.text_encoding_layer.0.attention.self.key.weight
 (768, 768)" fillcolor=lightblue]
	140490036501536 -> 140490034165360
	140490034165360 [label=AccumulateGrad]
	140490034159504 -> 140490034159120
	140490034159504 [label="UnsafeViewBackward0
--------------------------------
self_sym_sizes: (6, 12, 502, 64)"]
	140490034159744 -> 140490034159504
	140490034159744 [label=CloneBackward0]
	140490034160800 -> 140490034159744
	140490034160800 [label="ExpandBackward0
--------------------------------
self_sym_sizes: (6, 12, 502, 64)"]
	140490034160944 -> 140490034160800
	140490034160944 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140490034161712 -> 140490034160944
	140490034161712 [label="ViewBackward0
-----------------------------
self_sym_sizes: (6, 502, 768)"]
	140490034163248 -> 140490034161712
	140490034163248 [label="ViewBackward0
---------------------------
self_sym_sizes: (3012, 768)"]
	140490034164208 -> 140490034163248
	140490034164208 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :    (3012, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140490034165024 -> 140490034164208
	140490036501296 [label="q_encoder.encoder.text_encoding_layer.0.attention.self.value.bias
 (768)" fillcolor=lightblue]
	140490036501296 -> 140490034165024
	140490034165024 [label=AccumulateGrad]
	140490034163104 -> 140490034164208
	140490034163104 [label="ViewBackward0
-----------------------------
self_sym_sizes: (6, 502, 768)"]
	140490034157392 -> 140490034163104
	140490034159648 -> 140490034164208
	140490034159648 [label=TBackward0]
	140490034165408 -> 140490034159648
	140490036503856 [label="q_encoder.encoder.text_encoding_layer.0.attention.self.value.weight
 (768, 768)" fillcolor=lightblue]
	140490036503856 -> 140490034165408
	140490034165408 [label=AccumulateGrad]
	140490034158592 -> 140490034157968
	140490034158592 [label=TBackward0]
	140490034158304 -> 140490034158592
	140490036504496 [label="q_encoder.encoder.text_encoding_layer.0.attention.output.dense.weight
 (768, 768)" fillcolor=lightblue]
	140490036504496 -> 140490034158304
	140490034158304 [label=AccumulateGrad]
	140490034157392 -> 140490034157296
	140490034157152 -> 140490034155280
	140490034702096 [label="q_encoder.encoder.text_encoding_layer.0.attention.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	140490034702096 -> 140490034157152
	140490034157152 [label=AccumulateGrad]
	140490034157632 -> 140490034155280
	140490036498336 [label="q_encoder.encoder.text_encoding_layer.0.attention.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	140490036498336 -> 140490034157632
	140490034157632 [label=AccumulateGrad]
	140490034157344 -> 140490034156816
	140490034157344 [label=TBackward0]
	140490034157584 -> 140490034157344
	140490034702256 [label="q_encoder.encoder.text_encoding_layer.0.intermediate.dense.weight
 (3072, 768)" fillcolor=lightblue]
	140490034702256 -> 140490034157584
	140490034157584 [label=AccumulateGrad]
	140490034156576 -> 140490034155808
	140490034156576 [label=TBackward0]
	140490034156336 -> 140490034156576
	140490034702416 [label="q_encoder.encoder.text_encoding_layer.0.output.dense.weight
 (768, 3072)" fillcolor=lightblue]
	140490034702416 -> 140490034156336
	140490034156336 [label=AccumulateGrad]
	140490034155280 -> 140490034154464
	140490034153792 -> 140490034154416
	140490034702656 [label="q_encoder.encoder.text_encoding_layer.0.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	140490034702656 -> 140490034153792
	140490034153792 [label=AccumulateGrad]
	140490034155520 -> 140490034154416
	140490034702496 [label="q_encoder.encoder.text_encoding_layer.0.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	140490034702496 -> 140490034155520
	140490034155520 [label=AccumulateGrad]
	140490034153216 -> 140490034152400
	140490034153216 [label=TBackward0]
	140490034153456 -> 140490034153216
	140490035208400 [label="q_encoder.encoder.information_exchanging_layer.0.attention.self.query.weight
 (768, 768)" fillcolor=lightblue]
	140490035208400 -> 140490034153456
	140490034153456 [label=AccumulateGrad]
	140490034036064 -> 140490034036304
	140490034036064 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 64, 3)"]
	140490034036016 -> 140490034036064
	140490034036016 [label=CloneBackward0]
	140490034152208 -> 140490034036016
	140490034152208 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 64, 3)"]
	140490034152496 -> 140490034152208
	140490034152496 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551615
dim1: 18446744073709551614"]
	140490034154128 -> 140490034152496
	140490034154128 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140490034154944 -> 140490034154128
	140490034154944 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 3, 768)"]
	140490034154704 -> 140490034154944
	140490034154704 [label="ViewBackward0
------------------------
self_sym_sizes: (9, 768)"]
	140490034155472 -> 140490034154704
	140490034155472 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (9, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140490034155616 -> 140490034155472
	140490035208640 [label="q_encoder.encoder.information_exchanging_layer.0.attention.self.key.bias
 (768)" fillcolor=lightblue]
	140490035208640 -> 140490034155616
	140490034155616 [label=AccumulateGrad]
	140490034155664 -> 140490034155472
	140490034155664 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 3, 768)"]
	140490034032656 -> 140490034155664
	140490034151632 -> 140490034155472
	140490034151632 [label=TBackward0]
	140490034157920 -> 140490034151632
	140490035208480 [label="q_encoder.encoder.information_exchanging_layer.0.attention.self.key.weight
 (768, 768)" fillcolor=lightblue]
	140490035208480 -> 140490034157920
	140490034157920 [label=AccumulateGrad]
	140490034034816 -> 140490034034480
	140490034034816 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 3, 64)"]
	140490034035104 -> 140490034034816
	140490034035104 [label=CloneBackward0]
	140490034035824 -> 140490034035104
	140490034035824 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 3, 64)"]
	140490034036112 -> 140490034035824
	140490034036112 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140490034034960 -> 140490034036112
	140490034034960 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 3, 768)"]
	140490034153168 -> 140490034034960
	140490034153168 [label="ViewBackward0
------------------------
self_sym_sizes: (9, 768)"]
	140490034154752 -> 140490034153168
	140490034154752 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (9, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140490034157536 -> 140490034154752
	140490035208800 [label="q_encoder.encoder.information_exchanging_layer.0.attention.self.value.bias
 (768)" fillcolor=lightblue]
	140490035208800 -> 140490034157536
	140490034157536 [label=AccumulateGrad]
	140490034152928 -> 140490034154752
	140490034152928 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 3, 768)"]
	140490034032656 -> 140490034152928
	140490034151680 -> 140490034154752
	140490034151680 [label=TBackward0]
	140490034157056 -> 140490034151680
	140490035208720 [label="q_encoder.encoder.information_exchanging_layer.0.attention.self.value.weight
 (768, 768)" fillcolor=lightblue]
	140490035208720 -> 140490034157056
	140490034157056 [label=AccumulateGrad]
	140490034034000 -> 140490034033472
	140490034034000 [label=TBackward0]
	140490034033760 -> 140490034034000
	140490035208880 [label="q_encoder.encoder.information_exchanging_layer.0.attention.output.dense.weight
 (768, 768)" fillcolor=lightblue]
	140490035208880 -> 140490034033760
	140490034033760 [label=AccumulateGrad]
	140490034032656 -> 140490034032608
	140490034032368 -> 140490034029968
	140490035209120 [label="q_encoder.encoder.information_exchanging_layer.0.attention.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	140490035209120 -> 140490034032368
	140490034032368 [label=AccumulateGrad]
	140490034032944 -> 140490034029968
	140490035209040 [label="q_encoder.encoder.information_exchanging_layer.0.attention.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	140490035209040 -> 140490034032944
	140490034032944 [label=AccumulateGrad]
	140490034032560 -> 140490034031840
	140490034032560 [label=TBackward0]
	140490034032896 -> 140490034032560
	140490035209280 [label="q_encoder.encoder.information_exchanging_layer.0.intermediate.dense.weight
 (3072, 768)" fillcolor=lightblue]
	140490035209280 -> 140490034032896
	140490034032896 [label=AccumulateGrad]
	140490034031600 -> 140490034030976
	140490034031600 [label=TBackward0]
	140490034031360 -> 140490034031600
	140490035208560 [label="q_encoder.encoder.information_exchanging_layer.0.output.dense.weight
 (768, 3072)" fillcolor=lightblue]
	140490035208560 -> 140490034031360
	140490034031360 [label=AccumulateGrad]
	140490034029968 -> 140490034030544
	140490034030592 -> 140490034029824
	140490035209600 [label="q_encoder.encoder.information_exchanging_layer.0.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	140490035209600 -> 140490034030592
	140490034030592 [label=AccumulateGrad]
	140490034030688 -> 140490034029824
	140490035209520 [label="q_encoder.encoder.information_exchanging_layer.0.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	140490035209520 -> 140490034030688
	140490034030688 [label=AccumulateGrad]
	140490034029296 -> 140490034023392
	140490034029296 [label=CloneBackward0]
	140490034029632 -> 140490034029296
	140490034029632 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 2, 768)
start         :                   0
step          :                   1"]
	140490034030352 -> 140490034029632
	140490034030352 [label="SelectBackward0
--------------------------------
dim           :                2
index         :                1
self_sym_sizes: (3, 2, 502, 768)"]
	140490034032800 -> 140490034030352
	140490034032800 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:    (3, 2, 502, 768)
start         :                   0
step          :                   1"]
	140490034030784 -> 140490034032800
	140490034030784 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:    (3, 2, 502, 768)
start         :                   0
step          :                   1"]
	140490034034144 -> 140490034030784
	140490034034144 [label="ViewBackward0
-----------------------------
self_sym_sizes: (6, 502, 768)"]
	140490034033808 -> 140490034034144
	140490034033808 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140490034032176 -> 140490034033808
	140490034032176 [label="AddBackward0
------------
alpha: 1"]
	140490034033280 -> 140490034032176
	140490034033280 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140490034030640 -> 140490034033280
	140490034030640 [label="ViewBackward0
---------------------------
self_sym_sizes: (3012, 768)"]
	140490034036352 -> 140490034030640
	140490034036352 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :   (3012, 3072)
mat1_sym_strides:      (3072, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (3072, 768)
mat2_sym_strides:      (1, 3072)"]
	140490034035632 -> 140490034036352
	140490034701616 [label="q_encoder.encoder.text_encoding_layer.1.output.dense.bias
 (768)" fillcolor=lightblue]
	140490034701616 -> 140490034035632
	140490034035632 [label=AccumulateGrad]
	140490034031408 -> 140490034036352
	140490034031408 [label="ViewBackward0
------------------------------
self_sym_sizes: (6, 502, 3072)"]
	140490034156240 -> 140490034031408
	140490034156240 [label="GeluBackward0
---------------------------
approximate:           none
self       : [saved tensor]"]
	140490034155328 -> 140490034156240
	140490034155328 [label="ViewBackward0
----------------------------
self_sym_sizes: (3012, 3072)"]
	140490034153648 -> 140490034155328
	140490034153648 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :    (3012, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (768, 3072)
mat2_sym_strides:       (1, 768)"]
	140490034158448 -> 140490034153648
	140490034703696 [label="q_encoder.encoder.text_encoding_layer.1.intermediate.dense.bias
 (3072)" fillcolor=lightblue]
	140490034703696 -> 140490034158448
	140490034158448 [label=AccumulateGrad]
	140490034158688 -> 140490034153648
	140490034158688 [label="ViewBackward0
-----------------------------
self_sym_sizes: (6, 502, 768)"]
	140490034034576 -> 140490034158688
	140490034034576 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140490034160272 -> 140490034034576
	140490034160272 [label="AddBackward0
------------
alpha: 1"]
	140490034162000 -> 140490034160272
	140490034162000 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140490034161184 -> 140490034162000
	140490034161184 [label="ViewBackward0
---------------------------
self_sym_sizes: (3012, 768)"]
	140490034164880 -> 140490034161184
	140490034164880 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :    (3012, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140490034165504 -> 140490034164880
	140490034703296 [label="q_encoder.encoder.text_encoding_layer.1.attention.output.dense.bias
 (768)" fillcolor=lightblue]
	140490034703296 -> 140490034165504
	140490034165504 [label=AccumulateGrad]
	140490034165216 -> 140490034164880
	140490034165216 [label="ViewBackward0
-----------------------------
self_sym_sizes: (6, 502, 768)"]
	140490034165840 -> 140490034165216
	140490034165840 [label="ViewBackward0
--------------------------------
self_sym_sizes: (6, 502, 12, 64)"]
	140490034165648 -> 140490034165840
	140490034165648 [label=CloneBackward0]
	140490034158928 -> 140490034165648
	140490034158928 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140490034166272 -> 140490034158928
	140490034166272 [label="UnsafeViewBackward0
-----------------------------
self_sym_sizes: (72, 502, 64)"]
	140490034166512 -> 140490034166272
	140490034166512 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140490034166704 -> 140490034166512
	140490034166704 [label="ViewBackward0
---------------------------------
self_sym_sizes: (6, 12, 502, 502)"]
	140490034166800 -> 140490034166704
	140490034166800 [label="ExpandBackward0
---------------------------------
self_sym_sizes: (6, 12, 502, 502)"]
	140490034167040 -> 140490034166800
	140490034167040 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140490034167280 -> 140490034167040
	140490034167280 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140490034167664 -> 140490034167280
	140490034167664 [label="AddBackward0
------------
alpha: 1"]
	140490034167760 -> 140490034167664
	140490034167760 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140490034166848 -> 140490034167760
	140490034166848 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (72, 502, 502)"]
	140490034069904 -> 140490034166848
	140490034069904 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140490034069952 -> 140490034069904
	140490034069952 [label="UnsafeViewBackward0
--------------------------------
self_sym_sizes: (6, 12, 502, 64)"]
	140490034070000 -> 140490034069952
	140490034070000 [label=CloneBackward0]
	140490034070240 -> 140490034070000
	140490034070240 [label="ExpandBackward0
--------------------------------
self_sym_sizes: (6, 12, 502, 64)"]
	140490034070336 -> 140490034070240
	140490034070336 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140490034070576 -> 140490034070336
	140490034070576 [label="ViewBackward0
-----------------------------
self_sym_sizes: (6, 502, 768)"]
	140490034070816 -> 140490034070576
	140490034070816 [label="ViewBackward0
---------------------------
self_sym_sizes: (3012, 768)"]
	140490034070912 -> 140490034070816
	140490034070912 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :    (3012, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140490034071104 -> 140490034070912
	140490034702816 [label="q_encoder.encoder.text_encoding_layer.1.attention.self.query.bias
 (768)" fillcolor=lightblue]
	140490034702816 -> 140490034071104
	140490034071104 [label=AccumulateGrad]
	140490034071008 -> 140490034070912
	140490034071008 [label="ViewBackward0
-----------------------------
self_sym_sizes: (6, 502, 768)"]
	140490034163200 -> 140490034071008
	140490034163200 [label="ViewBackward0
--------------------------------
self_sym_sizes: (3, 2, 502, 768)"]
	140490034071680 -> 140490034163200
	140490034071680 [label="AsStridedBackward0
----------------------------------------
size          :         (3, 2, 502, 768)
storage_offset:                        0
stride        : (771072, 385536, 768, 1)"]
	140490034071872 -> 140490034071680
	140490034071872 [label=CopySlices]
	140490034154416 -> 140490034071872
	140490034072208 -> 140490034071872
	140490034072208 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 2, 768)
start         :                   0
step          :                   1"]
	140490034071968 -> 140490034072208
	140490034071968 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:         (3, 3, 768)
start         :                   1
step          :                   1"]
	140490034072784 -> 140490034071968
	140490034072784 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:         (3, 3, 768)
start         :                   0
step          :                   1"]
	140490034029824 -> 140490034072784
	140490034070048 -> 140490034070912
	140490034070048 [label=TBackward0]
	140490034071248 -> 140490034070048
	140490034702736 [label="q_encoder.encoder.text_encoding_layer.1.attention.self.query.weight
 (768, 768)" fillcolor=lightblue]
	140490034702736 -> 140490034071248
	140490034071248 [label=AccumulateGrad]
	140490034069760 -> 140490034069904
	140490034069760 [label="UnsafeViewBackward0
--------------------------------
self_sym_sizes: (6, 12, 64, 502)"]
	140490034069616 -> 140490034069760
	140490034069616 [label=CloneBackward0]
	140490034070720 -> 140490034069616
	140490034070720 [label="ExpandBackward0
--------------------------------
self_sym_sizes: (6, 12, 64, 502)"]
	140490034070624 -> 140490034070720
	140490034070624 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551615
dim1: 18446744073709551614"]
	140490034072112 -> 140490034070624
	140490034072112 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140490034072976 -> 140490034072112
	140490034072976 [label="ViewBackward0
-----------------------------
self_sym_sizes: (6, 502, 768)"]
	140490034071200 -> 140490034072976
	140490034071200 [label="ViewBackward0
---------------------------
self_sym_sizes: (3012, 768)"]
	140490034071776 -> 140490034071200
	140490034071776 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :    (3012, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140490034072736 -> 140490034071776
	140490034702976 [label="q_encoder.encoder.text_encoding_layer.1.attention.self.key.bias
 (768)" fillcolor=lightblue]
	140490034702976 -> 140490034072736
	140490034072736 [label=AccumulateGrad]
	140490034073024 -> 140490034071776
	140490034073024 [label="ViewBackward0
-----------------------------
self_sym_sizes: (6, 502, 768)"]
	140490034163200 -> 140490034073024
	140490034070144 -> 140490034071776
	140490034070144 [label=TBackward0]
	140490034073264 -> 140490034070144
	140490034702896 [label="q_encoder.encoder.text_encoding_layer.1.attention.self.key.weight
 (768, 768)" fillcolor=lightblue]
	140490034702896 -> 140490034073264
	140490034073264 [label=AccumulateGrad]
	140490034166416 -> 140490034166512
	140490034166416 [label="UnsafeViewBackward0
--------------------------------
self_sym_sizes: (6, 12, 502, 64)"]
	140490034166752 -> 140490034166416
	140490034166752 [label=CloneBackward0]
	140490034167376 -> 140490034166752
	140490034167376 [label="ExpandBackward0
--------------------------------
self_sym_sizes: (6, 12, 502, 64)"]
	140490034166944 -> 140490034167376
	140490034166944 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140490034070432 -> 140490034166944
	140490034070432 [label="ViewBackward0
-----------------------------
self_sym_sizes: (6, 502, 768)"]
	140490034071920 -> 140490034070432
	140490034071920 [label="ViewBackward0
---------------------------
self_sym_sizes: (3012, 768)"]
	140490034072304 -> 140490034071920
	140490034072304 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :    (3012, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140490034072640 -> 140490034072304
	140490034703136 [label="q_encoder.encoder.text_encoding_layer.1.attention.self.value.bias
 (768)" fillcolor=lightblue]
	140490034703136 -> 140490034072640
	140490034072640 [label=AccumulateGrad]
	140490034071488 -> 140490034072304
	140490034071488 [label="ViewBackward0
-----------------------------
self_sym_sizes: (6, 502, 768)"]
	140490034163200 -> 140490034071488
	140490034069808 -> 140490034072304
	140490034069808 [label=TBackward0]
	140490034072832 -> 140490034069808
	140490034703056 [label="q_encoder.encoder.text_encoding_layer.1.attention.self.value.weight
 (768, 768)" fillcolor=lightblue]
	140490034703056 -> 140490034072832
	140490034072832 [label=AccumulateGrad]
	140490034165312 -> 140490034164880
	140490034165312 [label=TBackward0]
	140490034165696 -> 140490034165312
	140490034703216 [label="q_encoder.encoder.text_encoding_layer.1.attention.output.dense.weight
 (768, 768)" fillcolor=lightblue]
	140490034703216 -> 140490034165696
	140490034165696 [label=AccumulateGrad]
	140490034163200 -> 140490034160272
	140490034158352 -> 140490034034576
	140490034703456 [label="q_encoder.encoder.text_encoding_layer.1.attention.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	140490034703456 -> 140490034158352
	140490034158352 [label=AccumulateGrad]
	140490034160512 -> 140490034034576
	140490034703376 [label="q_encoder.encoder.text_encoding_layer.1.attention.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	140490034703376 -> 140490034160512
	140490034160512 [label=AccumulateGrad]
	140490034157248 -> 140490034153648
	140490034157248 [label=TBackward0]
	140490034165168 -> 140490034157248
	140490034703616 [label="q_encoder.encoder.text_encoding_layer.1.intermediate.dense.weight
 (3072, 768)" fillcolor=lightblue]
	140490034703616 -> 140490034165168
	140490034165168 [label=AccumulateGrad]
	140490034154080 -> 140490034036352
	140490034154080 [label=TBackward0]
	140490034156768 -> 140490034154080
	140490034703536 [label="q_encoder.encoder.text_encoding_layer.1.output.dense.weight
 (768, 3072)" fillcolor=lightblue]
	140490034703536 -> 140490034156768
	140490034156768 [label=AccumulateGrad]
	140490034034576 -> 140490034032176
	140490034034336 -> 140490034033808
	140490034703936 [label="q_encoder.encoder.text_encoding_layer.1.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	140490034703936 -> 140490034034336
	140490034034336 [label=AccumulateGrad]
	140490034029104 -> 140490034033808
	140490034703776 [label="q_encoder.encoder.text_encoding_layer.1.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	140490034703776 -> 140490034029104
	140490034029104 [label=AccumulateGrad]
	140490034029440 -> 140490034028672
	140490034029440 [label=TBackward0]
	140490034029680 -> 140490034029440
	140490035209200 [label="q_encoder.encoder.information_exchanging_layer.1.attention.self.query.weight
 (768, 768)" fillcolor=lightblue]
	140490035209200 -> 140490034029680
	140490034029680 [label=AccumulateGrad]
	140490034027088 -> 140490034027376
	140490034027088 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 64, 3)"]
	140490034027472 -> 140490034027088
	140490034027472 [label=CloneBackward0]
	140490034028336 -> 140490034027472
	140490034028336 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 64, 3)"]
	140490034028624 -> 140490034028336
	140490034028624 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551615
dim1: 18446744073709551614"]
	140490034030928 -> 140490034028624
	140490034030928 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140490034033904 -> 140490034030928
	140490034033904 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 3, 768)"]
	140490034032464 -> 140490034033904
	140490034032464 [label="ViewBackward0
------------------------
self_sym_sizes: (9, 768)"]
	140490034035440 -> 140490034032464
	140490034035440 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (9, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140490034031792 -> 140490034035440
	140490035209920 [label="q_encoder.encoder.information_exchanging_layer.1.attention.self.key.bias
 (768)" fillcolor=lightblue]
	140490035209920 -> 140490034031792
	140490034031792 [label=AccumulateGrad]
	140490034036208 -> 140490034035440
	140490034036208 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 3, 768)"]
	140490034023392 -> 140490034036208
	140490034026992 -> 140490034035440
	140490034026992 [label=TBackward0]
	140490034165600 -> 140490034026992
	140490035209760 [label="q_encoder.encoder.information_exchanging_layer.1.attention.self.key.weight
 (768, 768)" fillcolor=lightblue]
	140490035209760 -> 140490034165600
	140490034165600 [label=AccumulateGrad]
	140490034025408 -> 140490034025216
	140490034025408 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 3, 64)"]
	140490034025600 -> 140490034025408
	140490034025600 [label=CloneBackward0]
	140490034026752 -> 140490034025600
	140490034026752 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 3, 64)"]
	140490034027184 -> 140490034026752
	140490034027184 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140490034027904 -> 140490034027184
	140490034027904 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 3, 768)"]
	140490034029392 -> 140490034027904
	140490034029392 [label="ViewBackward0
------------------------
self_sym_sizes: (9, 768)"]
	140490034033376 -> 140490034029392
	140490034033376 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (9, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140490034035152 -> 140490034033376
	140490035210080 [label="q_encoder.encoder.information_exchanging_layer.1.attention.self.value.bias
 (768)" fillcolor=lightblue]
	140490035210080 -> 140490034035152
	140490034035152 [label=AccumulateGrad]
	140490034029152 -> 140490034033376
	140490034029152 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 3, 768)"]
	140490034023392 -> 140490034029152
	140490034025696 -> 140490034033376
	140490034025696 [label=TBackward0]
	140490034159792 -> 140490034025696
	140490035210000 [label="q_encoder.encoder.information_exchanging_layer.1.attention.self.value.weight
 (768, 768)" fillcolor=lightblue]
	140490035210000 -> 140490034159792
	140490034159792 [label=AccumulateGrad]
	140490034024688 -> 140490034024160
	140490034024688 [label=TBackward0]
	140490034024448 -> 140490034024688
	140490035210160 [label="q_encoder.encoder.information_exchanging_layer.1.attention.output.dense.weight
 (768, 768)" fillcolor=lightblue]
	140490035210160 -> 140490034024448
	140490034024448 [label=AccumulateGrad]
	140490034023392 -> 140490034023344
	140490034023152 -> 140490034020656
	140490035210400 [label="q_encoder.encoder.information_exchanging_layer.1.attention.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	140490035210400 -> 140490034023152
	140490034023152 [label=AccumulateGrad]
	140490034023728 -> 140490034020656
	140490035210320 [label="q_encoder.encoder.information_exchanging_layer.1.attention.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	140490035210320 -> 140490034023728
	140490034023728 [label=AccumulateGrad]
	140490034023296 -> 140490034022624
	140490034023296 [label=TBackward0]
	140490034023680 -> 140490034023296
	140490035210560 [label="q_encoder.encoder.information_exchanging_layer.1.intermediate.dense.weight
 (3072, 768)" fillcolor=lightblue]
	140490035210560 -> 140490034023680
	140490034023680 [label=AccumulateGrad]
	140490034022288 -> 140490034021520
	140490034022288 [label=TBackward0]
	140490034021808 -> 140490034022288
	140490035209840 [label="q_encoder.encoder.information_exchanging_layer.1.output.dense.weight
 (768, 3072)" fillcolor=lightblue]
	140490035209840 -> 140490034021808
	140490034021808 [label=AccumulateGrad]
	140490034020656 -> 140490034020608
	140490034020944 -> 140490034069360
	140490035210880 [label="q_encoder.encoder.information_exchanging_layer.1.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	140490035210880 -> 140490034020944
	140490034020944 [label=AccumulateGrad]
	140490034021136 -> 140490034069360
	140490035210800 [label="q_encoder.encoder.information_exchanging_layer.1.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	140490035210800 -> 140490034021136
	140490034021136 [label=AccumulateGrad]
	140490034068880 -> 140490034062928
	140490034068880 [label=CloneBackward0]
	140490034069120 -> 140490034068880
	140490034069120 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 2, 768)
start         :                   0
step          :                   1"]
	140490034068736 -> 140490034069120
	140490034068736 [label="SelectBackward0
--------------------------------
dim           :                2
index         :                1
self_sym_sizes: (3, 2, 502, 768)"]
	140490034023632 -> 140490034068736
	140490034023632 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:    (3, 2, 502, 768)
start         :                   0
step          :                   1"]
	140490034021376 -> 140490034023632
	140490034021376 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:    (3, 2, 502, 768)
start         :                   0
step          :                   1"]
	140490034024832 -> 140490034021376
	140490034024832 [label="ViewBackward0
-----------------------------
self_sym_sizes: (6, 502, 768)"]
	140490034024496 -> 140490034024832
	140490034024496 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140490034022912 -> 140490034024496
	140490034022912 [label="AddBackward0
------------
alpha: 1"]
	140490034023872 -> 140490034022912
	140490034023872 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140490034021040 -> 140490034023872
	140490034021040 [label="ViewBackward0
---------------------------
self_sym_sizes: (3012, 768)"]
	140490034030448 -> 140490034021040
	140490034030448 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :   (3012, 3072)
mat1_sym_strides:      (3072, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (3072, 768)
mat2_sym_strides:      (1, 3072)"]
	140490034028144 -> 140490034030448
	140490034702576 [label="q_encoder.encoder.text_encoding_layer.2.output.dense.bias
 (768)" fillcolor=lightblue]
	140490034702576 -> 140490034028144
	140490034028144 [label=AccumulateGrad]
	140490034027328 -> 140490034030448
	140490034027328 [label="ViewBackward0
------------------------------
self_sym_sizes: (6, 502, 3072)"]
	140490034021904 -> 140490034027328
	140490034021904 [label="GeluBackward0
---------------------------
approximate:           none
self       : [saved tensor]"]
	140490034151968 -> 140490034021904
	140490034151968 [label="ViewBackward0
----------------------------
self_sym_sizes: (3012, 3072)"]
	140490034159216 -> 140490034151968
	140490034159216 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :    (3012, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (768, 3072)
mat2_sym_strides:       (1, 768)"]
	140490034157776 -> 140490034159216
	140490034704976 [label="q_encoder.encoder.text_encoding_layer.2.intermediate.dense.bias
 (3072)" fillcolor=lightblue]
	140490034704976 -> 140490034157776
	140490034157776 [label=AccumulateGrad]
	140490034165936 -> 140490034159216
	140490034165936 [label="ViewBackward0
-----------------------------
self_sym_sizes: (6, 502, 768)"]
	140490034025264 -> 140490034165936
	140490034025264 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140490034167136 -> 140490034025264
	140490034167136 [label="AddBackward0
------------
alpha: 1"]
	140490034166560 -> 140490034167136
	140490034166560 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140490034071392 -> 140490034166560
	140490034071392 [label="ViewBackward0
---------------------------
self_sym_sizes: (3012, 768)"]
	140490034073120 -> 140490034071392
	140490034073120 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :    (3012, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140490034073312 -> 140490034073120
	140490034704576 [label="q_encoder.encoder.text_encoding_layer.2.attention.output.dense.bias
 (768)" fillcolor=lightblue]
	140490034704576 -> 140490034073312
	140490034073312 [label=AccumulateGrad]
	140490034072352 -> 140490034073120
	140490034072352 [label="ViewBackward0
-----------------------------
self_sym_sizes: (6, 502, 768)"]
	140490034073744 -> 140490034072352
	140490034073744 [label="ViewBackward0
--------------------------------
self_sym_sizes: (6, 502, 12, 64)"]
	140490034073504 -> 140490034073744
	140490034073504 [label=CloneBackward0]
	140490034070480 -> 140490034073504
	140490034070480 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140490034074128 -> 140490034070480
	140490034074128 [label="UnsafeViewBackward0
-----------------------------
self_sym_sizes: (72, 502, 64)"]
	140490034074320 -> 140490034074128
	140490034074320 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140490034074560 -> 140490034074320
	140490034074560 [label="ViewBackward0
---------------------------------
self_sym_sizes: (6, 12, 502, 502)"]
	140490034074608 -> 140490034074560
	140490034074608 [label="ExpandBackward0
---------------------------------
self_sym_sizes: (6, 12, 502, 502)"]
	140490034074896 -> 140490034074608
	140490034074896 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140490034074992 -> 140490034074896
	140490034074992 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140490034075184 -> 140490034074992
	140490034075184 [label="AddBackward0
------------
alpha: 1"]
	140490034075232 -> 140490034075184
	140490034075232 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140490034075280 -> 140490034075232
	140490034075280 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (72, 502, 502)"]
	140490034075568 -> 140490034075280
	140490034075568 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140490034075760 -> 140490034075568
	140490034075760 [label="UnsafeViewBackward0
--------------------------------
self_sym_sizes: (6, 12, 502, 64)"]
	140490034075856 -> 140490034075760
	140490034075856 [label=CloneBackward0]
	140490034076144 -> 140490034075856
	140490034076144 [label="ExpandBackward0
--------------------------------
self_sym_sizes: (6, 12, 502, 64)"]
	140490034076240 -> 140490034076144
	140490034076240 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140490034076336 -> 140490034076240
	140490034076336 [label="ViewBackward0
-----------------------------
self_sym_sizes: (6, 502, 768)"]
	140490034076672 -> 140490034076336
	140490034076672 [label="ViewBackward0
---------------------------
self_sym_sizes: (3012, 768)"]
	140490034076816 -> 140490034076672
	140490034076816 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :    (3012, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140490034077104 -> 140490034076816
	140490034704096 [label="q_encoder.encoder.text_encoding_layer.2.attention.self.query.bias
 (768)" fillcolor=lightblue]
	140490034704096 -> 140490034077104
	140490034077104 [label=AccumulateGrad]
	140490034076912 -> 140490034076816
	140490034076912 [label="ViewBackward0
-----------------------------
self_sym_sizes: (6, 502, 768)"]
	140490034073360 -> 140490034076912
	140490034073360 [label="ViewBackward0
--------------------------------
self_sym_sizes: (3, 2, 502, 768)"]
	140490034077392 -> 140490034073360
	140490034077392 [label="AsStridedBackward0
----------------------------------------
size          :         (3, 2, 502, 768)
storage_offset:                        0
stride        : (771072, 385536, 768, 1)"]
	140490034077488 -> 140490034077392
	140490034077488 [label=CopySlices]
	140490034033808 -> 140490034077488
	140490034077776 -> 140490034077488
	140490034077776 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 2, 768)
start         :                   0
step          :                   1"]
	140490034077536 -> 140490034077776
	140490034077536 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:         (3, 3, 768)
start         :                   1
step          :                   1"]
	140490034078496 -> 140490034077536
	140490034078496 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:         (3, 3, 768)
start         :                   0
step          :                   1"]
	140490034069360 -> 140490034078496
	140490034075904 -> 140490034076816
	140490034075904 [label=TBackward0]
	140490034077200 -> 140490034075904
	140490034704016 [label="q_encoder.encoder.text_encoding_layer.2.attention.self.query.weight
 (768, 768)" fillcolor=lightblue]
	140490034704016 -> 140490034077200
	140490034077200 [label=AccumulateGrad]
	140490034075376 -> 140490034075568
	140490034075376 [label="UnsafeViewBackward0
--------------------------------
self_sym_sizes: (6, 12, 64, 502)"]
	140490034075424 -> 140490034075376
	140490034075424 [label=CloneBackward0]
	140490034076768 -> 140490034075424
	140490034076768 [label="ExpandBackward0
--------------------------------
self_sym_sizes: (6, 12, 64, 502)"]
	140490034076576 -> 140490034076768
	140490034076576 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551615
dim1: 18446744073709551614"]
	140490034077680 -> 140490034076576
	140490034077680 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140490034078640 -> 140490034077680
	140490034078640 [label="ViewBackward0
-----------------------------
self_sym_sizes: (6, 502, 768)"]
	140490034077152 -> 140490034078640
	140490034077152 [label="ViewBackward0
---------------------------
self_sym_sizes: (3012, 768)"]
	140490034077440 -> 140490034077152
	140490034077440 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :    (3012, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140490034078352 -> 140490034077440
	140490034704256 [label="q_encoder.encoder.text_encoding_layer.2.attention.self.key.bias
 (768)" fillcolor=lightblue]
	140490034704256 -> 140490034078352
	140490034078352 [label=AccumulateGrad]
	140490034078928 -> 140490034077440
	140490034078928 [label="ViewBackward0
-----------------------------
self_sym_sizes: (6, 502, 768)"]
	140490034073360 -> 140490034078928
	140490034075952 -> 140490034077440
	140490034075952 [label=TBackward0]
	140490034078976 -> 140490034075952
	140490034704176 [label="q_encoder.encoder.text_encoding_layer.2.attention.self.key.weight
 (768, 768)" fillcolor=lightblue]
	140490034704176 -> 140490034078976
	140490034078976 [label=AccumulateGrad]
	140490034074224 -> 140490034074320
	140490034074224 [label="UnsafeViewBackward0
--------------------------------
self_sym_sizes: (6, 12, 502, 64)"]
	140490034074416 -> 140490034074224
	140490034074416 [label=CloneBackward0]
	140490034075040 -> 140490034074416
	140490034075040 [label="ExpandBackward0
--------------------------------
self_sym_sizes: (6, 12, 502, 64)"]
	140490034075088 -> 140490034075040
	140490034075088 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140490034076288 -> 140490034075088
	140490034076288 [label="ViewBackward0
-----------------------------
self_sym_sizes: (6, 502, 768)"]
	140490034077632 -> 140490034076288
	140490034077632 [label="ViewBackward0
---------------------------
self_sym_sizes: (3012, 768)"]
	140490034077872 -> 140490034077632
	140490034077872 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :    (3012, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140490034078400 -> 140490034077872
	140490034704416 [label="q_encoder.encoder.text_encoding_layer.2.attention.self.value.bias
 (768)" fillcolor=lightblue]
	140490034704416 -> 140490034078400
	140490034078400 [label=AccumulateGrad]
	140490034077344 -> 140490034077872
	140490034077344 [label="ViewBackward0
-----------------------------
self_sym_sizes: (6, 502, 768)"]
	140490034073360 -> 140490034077344
	140490034074800 -> 140490034077872
	140490034074800 [label=TBackward0]
	140490034078592 -> 140490034074800
	140490034704336 [label="q_encoder.encoder.text_encoding_layer.2.attention.self.value.weight
 (768, 768)" fillcolor=lightblue]
	140490034704336 -> 140490034078592
	140490034078592 [label=AccumulateGrad]
	140490034069712 -> 140490034073120
	140490034069712 [label=TBackward0]
	140490034073648 -> 140490034069712
	140490034704496 [label="q_encoder.encoder.text_encoding_layer.2.attention.output.dense.weight
 (768, 768)" fillcolor=lightblue]
	140490034704496 -> 140490034073648
	140490034073648 [label=AccumulateGrad]
	140490034073360 -> 140490034167136
	140490034166080 -> 140490034025264
	140490034704736 [label="q_encoder.encoder.text_encoding_layer.2.attention.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	140490034704736 -> 140490034166080
	140490034166080 [label=AccumulateGrad]
	140490034167616 -> 140490034025264
	140490034704656 [label="q_encoder.encoder.text_encoding_layer.2.attention.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	140490034704656 -> 140490034167616
	140490034167616 [label=AccumulateGrad]
	140490034156432 -> 140490034159216
	140490034156432 [label=TBackward0]
	140490034165888 -> 140490034156432
	140490034704896 [label="q_encoder.encoder.text_encoding_layer.2.intermediate.dense.weight
 (3072, 768)" fillcolor=lightblue]
	140490034704896 -> 140490034165888
	140490034165888 [label=AccumulateGrad]
	140490034026608 -> 140490034030448
	140490034026608 [label=TBackward0]
	140490034161376 -> 140490034026608
	140490034704816 [label="q_encoder.encoder.text_encoding_layer.2.output.dense.weight
 (768, 3072)" fillcolor=lightblue]
	140490034704816 -> 140490034161376
	140490034161376 [label=AccumulateGrad]
	140490034025264 -> 140490034022912
	140490034024976 -> 140490034024496
	140490034705216 [label="q_encoder.encoder.text_encoding_layer.2.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	140490034705216 -> 140490034024976
	140490034024976 [label=AccumulateGrad]
	140490034020848 -> 140490034024496
	140490034705056 [label="q_encoder.encoder.text_encoding_layer.2.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	140490034705056 -> 140490034020848
	140490034020848 [label=AccumulateGrad]
	140490034069024 -> 140490034068352
	140490034069024 [label=TBackward0]
	140490034069168 -> 140490034069024
	140490035210480 [label="q_encoder.encoder.information_exchanging_layer.2.attention.self.query.weight
 (768, 768)" fillcolor=lightblue]
	140490035210480 -> 140490034069168
	140490034069168 [label=AccumulateGrad]
	140490034066768 -> 140490034067056
	140490034066768 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 64, 3)"]
	140490034067200 -> 140490034066768
	140490034067200 [label=CloneBackward0]
	140490034068160 -> 140490034067200
	140490034068160 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 64, 3)"]
	140490034068496 -> 140490034068160
	140490034068496 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551615
dim1: 18446744073709551614"]
	140490034068784 -> 140490034068496
	140490034068784 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140490034066672 -> 140490034068784
	140490034066672 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 3, 768)"]
	140490034023200 -> 140490034066672
	140490034023200 [label="ViewBackward0
------------------------
self_sym_sizes: (9, 768)"]
	140490034026320 -> 140490034023200
	140490034026320 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (9, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140490034164928 -> 140490034026320
	140490035211200 [label="q_encoder.encoder.information_exchanging_layer.2.attention.self.key.bias
 (768)" fillcolor=lightblue]
	140490035211200 -> 140490034164928
	140490034164928 [label=AccumulateGrad]
	140490034166368 -> 140490034026320
	140490034166368 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 3, 768)"]
	140490034062928 -> 140490034166368
	140490034167520 -> 140490034026320
	140490034167520 [label=TBackward0]
	140490034027568 -> 140490034167520
	140490035211040 [label="q_encoder.encoder.information_exchanging_layer.2.attention.self.key.weight
 (768, 768)" fillcolor=lightblue]
	140490035211040 -> 140490034027568
	140490034027568 [label=AccumulateGrad]
	140490034065376 -> 140490034065088
	140490034065376 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 3, 64)"]
	140490034065616 -> 140490034065376
	140490034065616 [label=CloneBackward0]
	140490034066384 -> 140490034065616
	140490034066384 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 3, 64)"]
	140490034066912 -> 140490034066384
	140490034066912 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140490034067632 -> 140490034066912
	140490034067632 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 3, 768)"]
	140490034166176 -> 140490034067632
	140490034166176 [label="ViewBackward0
------------------------
self_sym_sizes: (9, 768)"]
	140490034065472 -> 140490034166176
	140490034065472 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (9, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140490034068928 -> 140490034065472
	140490035211360 [label="q_encoder.encoder.information_exchanging_layer.2.attention.self.value.bias
 (768)" fillcolor=lightblue]
	140490035211360 -> 140490034068928
	140490034068928 [label=AccumulateGrad]
	140490034021472 -> 140490034065472
	140490034021472 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 3, 768)"]
	140490034062928 -> 140490034021472
	140490034022144 -> 140490034065472
	140490034022144 [label=TBackward0]
	140490034024640 -> 140490034022144
	140490035211280 [label="q_encoder.encoder.information_exchanging_layer.2.attention.self.value.weight
 (768, 768)" fillcolor=lightblue]
	140490035211280 -> 140490034024640
	140490034024640 [label=AccumulateGrad]
	140490034064416 -> 140490034063648
	140490034064416 [label=TBackward0]
	140490034064080 -> 140490034064416
	140490035211440 [label="q_encoder.encoder.information_exchanging_layer.2.attention.output.dense.weight
 (768, 768)" fillcolor=lightblue]
	140490035211440 -> 140490034064080
	140490034064080 [label=AccumulateGrad]
	140490034062928 -> 140490034062832
	140490034062640 -> 140490034059856
	140490035211680 [label="q_encoder.encoder.information_exchanging_layer.2.attention.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	140490035211680 -> 140490034062640
	140490034062640 [label=AccumulateGrad]
	140490034063120 -> 140490034059856
	140490035211600 [label="q_encoder.encoder.information_exchanging_layer.2.attention.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	140490035211600 -> 140490034063120
	140490034063120 [label=AccumulateGrad]
	140490034062736 -> 140490034062016
	140490034062736 [label=TBackward0]
	140490034063216 -> 140490034062736
	140490035211840 [label="q_encoder.encoder.information_exchanging_layer.2.intermediate.dense.weight
 (3072, 768)" fillcolor=lightblue]
	140490035211840 -> 140490034063216
	140490034063216 [label=AccumulateGrad]
	140490034061776 -> 140490034061056
	140490034061776 [label=TBackward0]
	140490034061440 -> 140490034061776
	140490035211120 [label="q_encoder.encoder.information_exchanging_layer.2.output.dense.weight
 (768, 3072)" fillcolor=lightblue]
	140490035211120 -> 140490034061440
	140490034061440 [label=AccumulateGrad]
	140490034059856 -> 140490034060480
	140490034060672 -> 140490034059664
	140490035212160 [label="q_encoder.encoder.information_exchanging_layer.2.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	140490035212160 -> 140490034060672
	140490034060672 [label=AccumulateGrad]
	140490034060768 -> 140490034059664
	140490035212080 [label="q_encoder.encoder.information_exchanging_layer.2.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	140490035212080 -> 140490034060768
	140490034060768 [label=AccumulateGrad]
	140490034059280 -> 140490034053328
	140490034059280 [label=CloneBackward0]
	140490034059520 -> 140490034059280
	140490034059520 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 2, 768)
start         :                   0
step          :                   1"]
	140490034060240 -> 140490034059520
	140490034060240 [label="SelectBackward0
--------------------------------
dim           :                2
index         :                1
self_sym_sizes: (3, 2, 502, 768)"]
	140490034063072 -> 140490034060240
	140490034063072 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:    (3, 2, 502, 768)
start         :                   0
step          :                   1"]
	140490034060864 -> 140490034063072
	140490034060864 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:    (3, 2, 502, 768)
start         :                   0
step          :                   1"]
	140490034064560 -> 140490034060864
	140490034064560 [label="ViewBackward0
-----------------------------
self_sym_sizes: (6, 502, 768)"]
	140490034064224 -> 140490034064560
	140490034064224 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140490034062448 -> 140490034064224
	140490034062448 [label="AddBackward0
------------
alpha: 1"]
	140490034063360 -> 140490034062448
	140490034063360 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140490034060720 -> 140490034063360
	140490034060720 [label="ViewBackward0
---------------------------
self_sym_sizes: (3012, 768)"]
	140490034069216 -> 140490034060720
	140490034069216 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :   (3012, 3072)
mat1_sym_strides:      (3072, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (3072, 768)
mat2_sym_strides:      (1, 3072)"]
	140490034022528 -> 140490034069216
	140490034703856 [label="q_encoder.encoder.text_encoding_layer.3.output.dense.bias
 (768)" fillcolor=lightblue]
	140490034703856 -> 140490034022528
	140490034022528 [label=AccumulateGrad]
	140490034024016 -> 140490034069216
	140490034024016 [label="ViewBackward0
------------------------------
self_sym_sizes: (6, 502, 3072)"]
	140490034066336 -> 140490034024016
	140490034066336 [label="GeluBackward0
---------------------------
approximate:           none
self       : [saved tensor]"]
	140490034067968 -> 140490034066336
	140490034067968 [label="ViewBackward0
----------------------------
self_sym_sizes: (3012, 3072)"]
	140490034072400 -> 140490034067968
	140490034072400 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :    (3012, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (768, 3072)
mat2_sym_strides:       (1, 768)"]
	140490034072592 -> 140490034072400
	140490034706256 [label="q_encoder.encoder.text_encoding_layer.3.intermediate.dense.bias
 (3072)" fillcolor=lightblue]
	140490034706256 -> 140490034072592
	140490034072592 [label=AccumulateGrad]
	140490034073888 -> 140490034072400
	140490034073888 [label="ViewBackward0
-----------------------------
self_sym_sizes: (6, 502, 768)"]
	140490034065136 -> 140490034073888
	140490034065136 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140490034074944 -> 140490034065136
	140490034074944 [label="AddBackward0
------------
alpha: 1"]
	140490034076432 -> 140490034074944
	140490034076432 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140490034075520 -> 140490034076432
	140490034075520 [label="ViewBackward0
---------------------------
self_sym_sizes: (3012, 768)"]
	140490034078784 -> 140490034075520
	140490034078784 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :    (3012, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140490034079024 -> 140490034078784
	140490034705856 [label="q_encoder.encoder.text_encoding_layer.3.attention.output.dense.bias
 (768)" fillcolor=lightblue]
	140490034705856 -> 140490034079024
	140490034079024 [label=AccumulateGrad]
	140490034077920 -> 140490034078784
	140490034077920 [label="ViewBackward0
-----------------------------
self_sym_sizes: (6, 502, 768)"]
	140490034079264 -> 140490034077920
	140490034079264 [label="ViewBackward0
--------------------------------
self_sym_sizes: (6, 502, 12, 64)"]
	140490034079168 -> 140490034079264
	140490034079168 [label=CloneBackward0]
	140490034074176 -> 140490034079168
	140490034074176 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140490034079552 -> 140490034074176
	140490034079552 [label="UnsafeViewBackward0
-----------------------------
self_sym_sizes: (72, 502, 64)"]
	140490034079744 -> 140490034079552
	140490034079744 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140490034079936 -> 140490034079744
	140490034079936 [label="ViewBackward0
---------------------------------
self_sym_sizes: (6, 12, 502, 502)"]
	140490034080080 -> 140490034079936
	140490034080080 [label="ExpandBackward0
---------------------------------
self_sym_sizes: (6, 12, 502, 502)"]
	140490034080320 -> 140490034080080
	140490034080320 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140490034080464 -> 140490034080320
	140490034080464 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140490034080800 -> 140490034080464
	140490034080800 [label="AddBackward0
------------
alpha: 1"]
	140490034080848 -> 140490034080800
	140490034080848 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140490034080944 -> 140490034080848
	140490034080944 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (72, 502, 502)"]
	140490034081136 -> 140490034080944
	140490034081136 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140490034081280 -> 140490034081136
	140490034081280 [label="UnsafeViewBackward0
--------------------------------
self_sym_sizes: (6, 12, 502, 64)"]
	140490034081376 -> 140490034081280
	140490034081376 [label=CloneBackward0]
	140490034081616 -> 140490034081376
	140490034081616 [label="ExpandBackward0
--------------------------------
self_sym_sizes: (6, 12, 502, 64)"]
	140490034081760 -> 140490034081616
	140490034081760 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140490034082048 -> 140490034081760
	140490034082048 [label="ViewBackward0
-----------------------------
self_sym_sizes: (6, 502, 768)"]
	140490034082192 -> 140490034082048
	140490034082192 [label="ViewBackward0
---------------------------
self_sym_sizes: (3012, 768)"]
	140490034082288 -> 140490034082192
	140490034082288 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :    (3012, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140490034082480 -> 140490034082288
	140490034705376 [label="q_encoder.encoder.text_encoding_layer.3.attention.self.query.bias
 (768)" fillcolor=lightblue]
	140490034705376 -> 140490034082480
	140490034082480 [label=AccumulateGrad]
	140490034082432 -> 140490034082288
	140490034082432 [label="ViewBackward0
-----------------------------
self_sym_sizes: (6, 502, 768)"]
	140490034077296 -> 140490034082432
	140490034077296 [label="ViewBackward0
--------------------------------
self_sym_sizes: (3, 2, 502, 768)"]
	140490034082912 -> 140490034077296
	140490034082912 [label="AsStridedBackward0
----------------------------------------
size          :         (3, 2, 502, 768)
storage_offset:                        0
stride        : (771072, 385536, 768, 1)"]
	140490034083056 -> 140490034082912
	140490034083056 [label=CopySlices]
	140490034024496 -> 140490034083056
	140490034083296 -> 140490034083056
	140490034083296 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 2, 768)
start         :                   0
step          :                   1"]
	140490034083152 -> 140490034083296
	140490034083152 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:         (3, 3, 768)
start         :                   1
step          :                   1"]
	140490034083824 -> 140490034083152
	140490034083824 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:         (3, 3, 768)
start         :                   0
step          :                   1"]
	140490034059664 -> 140490034083824
	140490034081424 -> 140490034082288
	140490034081424 [label=TBackward0]
	140490034082672 -> 140490034081424
	140490034705296 [label="q_encoder.encoder.text_encoding_layer.3.attention.self.query.weight
 (768, 768)" fillcolor=lightblue]
	140490034705296 -> 140490034082672
	140490034082672 [label=AccumulateGrad]
	140490034080896 -> 140490034081136
	140490034080896 [label="UnsafeViewBackward0
--------------------------------
self_sym_sizes: (6, 12, 64, 502)"]
	140490034080992 -> 140490034080896
	140490034080992 [label=CloneBackward0]
	140490034082240 -> 140490034080992
	140490034082240 [label="ExpandBackward0
--------------------------------
self_sym_sizes: (6, 12, 64, 502)"]
	140490034082096 -> 140490034082240
	140490034082096 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551615
dim1: 18446744073709551614"]
	140490034083200 -> 140490034082096
	140490034083200 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140490034084064 -> 140490034083200
	140490034084064 [label="ViewBackward0
-----------------------------
self_sym_sizes: (6, 502, 768)"]
	140490034082576 -> 140490034084064
	140490034082576 [label="ViewBackward0
---------------------------
self_sym_sizes: (3012, 768)"]
	140490034082960 -> 140490034082576
	140490034082960 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :    (3012, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140490034083776 -> 140490034082960
	140490034705536 [label="q_encoder.encoder.text_encoding_layer.3.attention.self.key.bias
 (768)" fillcolor=lightblue]
	140490034705536 -> 140490034083776
	140490034083776 [label=AccumulateGrad]
	140490034084160 -> 140490034082960
	140490034084160 [label="ViewBackward0
-----------------------------
self_sym_sizes: (6, 502, 768)"]
	140490034077296 -> 140490034084160
	140490034081568 -> 140490034082960
	140490034081568 [label=TBackward0]
	140490034084352 -> 140490034081568
	140490034705456 [label="q_encoder.encoder.text_encoding_layer.3.attention.self.key.weight
 (768, 768)" fillcolor=lightblue]
	140490034705456 -> 140490034084352
	140490034084352 [label=AccumulateGrad]
	140490034079696 -> 140490034079744
	140490034079696 [label="UnsafeViewBackward0
--------------------------------
self_sym_sizes: (6, 12, 502, 64)"]
	140490034079984 -> 140490034079696
	140490034079984 [label=CloneBackward0]
	140490034080512 -> 140490034079984
	140490034080512 [label="ExpandBackward0
--------------------------------
self_sym_sizes: (6, 12, 502, 64)"]
	140490034080704 -> 140490034080512
	140490034080704 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140490034081808 -> 140490034080704
	140490034081808 [label="ViewBackward0
-----------------------------
self_sym_sizes: (6, 502, 768)"]
	140490034083104 -> 140490034081808
	140490034083104 [label="ViewBackward0
---------------------------
self_sym_sizes: (3012, 768)"]
	140490034083440 -> 140490034083104
	140490034083440 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :    (3012, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140490034083920 -> 140490034083440
	140490034705696 [label="q_encoder.encoder.text_encoding_layer.3.attention.self.value.bias
 (768)" fillcolor=lightblue]
	140490034705696 -> 140490034083920
	140490034083920 [label=AccumulateGrad]
	140490034082816 -> 140490034083440
	140490034082816 [label="ViewBackward0
-----------------------------
self_sym_sizes: (6, 502, 768)"]
	140490034077296 -> 140490034082816
	140490034080224 -> 140490034083440
	140490034080224 [label=TBackward0]
	140490034083968 -> 140490034080224
	140490034705616 [label="q_encoder.encoder.text_encoding_layer.3.attention.self.value.weight
 (768, 768)" fillcolor=lightblue]
	140490034705616 -> 140490034083968
	140490034083968 [label=AccumulateGrad]
	140490034078016 -> 140490034078784
	140490034078016 [label=TBackward0]
	140490034079216 -> 140490034078016
	140490034705776 [label="q_encoder.encoder.text_encoding_layer.3.attention.output.dense.weight
 (768, 768)" fillcolor=lightblue]
	140490034705776 -> 140490034079216
	140490034079216 [label=AccumulateGrad]
	140490034077296 -> 140490034074944
	140490034073840 -> 140490034065136
	140490034706016 [label="q_encoder.encoder.text_encoding_layer.3.attention.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	140490034706016 -> 140490034073840
	140490034073840 [label=AccumulateGrad]
	140490034075136 -> 140490034065136
	140490034705936 [label="q_encoder.encoder.text_encoding_layer.3.attention.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	140490034705936 -> 140490034075136
	140490034075136 [label=AccumulateGrad]
	140490034072496 -> 140490034072400
	140490034072496 [label=TBackward0]
	140490034078256 -> 140490034072496
	140490034706176 [label="q_encoder.encoder.text_encoding_layer.3.intermediate.dense.weight
 (3072, 768)" fillcolor=lightblue]
	140490034706176 -> 140490034078256
	140490034078256 [label=AccumulateGrad]
	140490034025840 -> 140490034069216
	140490034025840 [label=TBackward0]
	140490034067008 -> 140490034025840
	140490034706096 [label="q_encoder.encoder.text_encoding_layer.3.output.dense.weight
 (768, 3072)" fillcolor=lightblue]
	140490034706096 -> 140490034067008
	140490034067008 [label=AccumulateGrad]
	140490034065136 -> 140490034062448
	140490034064848 -> 140490034064224
	140490034706496 [label="q_encoder.encoder.text_encoding_layer.3.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	140490034706496 -> 140490034064848
	140490034064848 [label=AccumulateGrad]
	140490034058992 -> 140490034064224
	140490034706336 [label="q_encoder.encoder.text_encoding_layer.3.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	140490034706336 -> 140490034058992
	140490034058992 [label=AccumulateGrad]
	140490034059328 -> 140490034058512
	140490034059328 [label=TBackward0]
	140490034031504 -> 140490034059328
	140490035211760 [label="q_encoder.encoder.information_exchanging_layer.3.attention.self.query.weight
 (768, 768)" fillcolor=lightblue]
	140490035211760 -> 140490034031504
	140490034031504 [label=AccumulateGrad]
	140490034056928 -> 140490034057168
	140490034056928 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 64, 3)"]
	140490034057264 -> 140490034056928
	140490034057264 [label=CloneBackward0]
	140490034058224 -> 140490034057264
	140490034058224 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 64, 3)"]
	140490034058416 -> 140490034058224
	140490034058416 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551615
dim1: 18446744073709551614"]
	140490034060576 -> 140490034058416
	140490034060576 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140490034061584 -> 140490034060576
	140490034061584 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 3, 768)"]
	140490034064320 -> 140490034061584
	140490034064320 [label="ViewBackward0
------------------------
self_sym_sizes: (9, 768)"]
	140490034062496 -> 140490034064320
	140490034062496 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (9, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140490034061968 -> 140490034062496
	140490035212480 [label="q_encoder.encoder.information_exchanging_layer.3.attention.self.key.bias
 (768)" fillcolor=lightblue]
	140490035212480 -> 140490034061968
	140490034061968 [label=AccumulateGrad]
	140490034066096 -> 140490034062496
	140490034066096 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 3, 768)"]
	140490034053328 -> 140490034066096
	140490034056880 -> 140490034062496
	140490034056880 [label=TBackward0]
	140490034067296 -> 140490034056880
	140490035212320 [label="q_encoder.encoder.information_exchanging_layer.3.attention.self.key.weight
 (768, 768)" fillcolor=lightblue]
	140490035212320 -> 140490034067296
	140490034067296 [label=AccumulateGrad]
	140490034055488 -> 140490034055104
	140490034055488 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 3, 64)"]
	140490034055728 -> 140490034055488
	140490034055728 [label=CloneBackward0]
	140490034056592 -> 140490034055728
	140490034056592 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 3, 64)"]
	140490034057024 -> 140490034056592
	140490034057024 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140490034057840 -> 140490034057024
	140490034057840 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 3, 768)"]
	140490034059136 -> 140490034057840
	140490034059136 [label="ViewBackward0
------------------------
self_sym_sizes: (9, 768)"]
	140490034061008 -> 140490034059136
	140490034061008 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (9, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140490034061488 -> 140490034061008
	140490035212640 [label="q_encoder.encoder.information_exchanging_layer.3.attention.self.value.bias
 (768)" fillcolor=lightblue]
	140490035212640 -> 140490034061488
	140490034061488 [label=AccumulateGrad]
	140490034059040 -> 140490034061008
	140490034059040 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 3, 768)"]
	140490034053328 -> 140490034059040
	140490034055680 -> 140490034061008
	140490034055680 [label=TBackward0]
	140490034063504 -> 140490034055680
	140490035212560 [label="q_encoder.encoder.information_exchanging_layer.3.attention.self.value.weight
 (768, 768)" fillcolor=lightblue]
	140490035212560 -> 140490034063504
	140490034063504 [label=AccumulateGrad]
	140490034054576 -> 140490034054000
	140490034054576 [label=TBackward0]
	140490034054288 -> 140490034054576
	140490035212720 [label="q_encoder.encoder.information_exchanging_layer.3.attention.output.dense.weight
 (768, 768)" fillcolor=lightblue]
	140490035212720 -> 140490034054288
	140490034054288 [label=AccumulateGrad]
	140490034053328 -> 140490034314784
	140490034315072 -> 140490034312432
	140490035212960 [label="q_encoder.encoder.information_exchanging_layer.3.attention.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	140490035212960 -> 140490034315072
	140490034315072 [label=AccumulateGrad]
	140490034053520 -> 140490034312432
	140490035212880 [label="q_encoder.encoder.information_exchanging_layer.3.attention.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	140490035212880 -> 140490034053520
	140490034053520 [label=AccumulateGrad]
	140490034315168 -> 140490034314544
	140490034315168 [label=TBackward0]
	140490034314880 -> 140490034315168
	140490035213120 [label="q_encoder.encoder.information_exchanging_layer.3.intermediate.dense.weight
 (3072, 768)" fillcolor=lightblue]
	140490035213120 -> 140490034314880
	140490034314880 [label=AccumulateGrad]
	140490034314352 -> 140490034313728
	140490034314352 [label=TBackward0]
	140490034314112 -> 140490034314352
	140490035212400 [label="q_encoder.encoder.information_exchanging_layer.3.output.dense.weight
 (768, 3072)" fillcolor=lightblue]
	140490035212400 -> 140490034314112
	140490034314112 [label=AccumulateGrad]
	140490034312432 -> 140490034313200
	140490034313296 -> 140490034312288
	140490035213440 [label="q_encoder.encoder.information_exchanging_layer.3.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	140490035213440 -> 140490034313296
	140490034313296 [label=AccumulateGrad]
	140490034313392 -> 140490034312288
	140490035213360 [label="q_encoder.encoder.information_exchanging_layer.3.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	140490035213360 -> 140490034313392
	140490034313392 [label=AccumulateGrad]
	140490034312000 -> 140490034306000
	140490034312000 [label=CloneBackward0]
	140490034312240 -> 140490034312000
	140490034312240 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 2, 768)
start         :                   0
step          :                   1"]
	140490034313056 -> 140490034312240
	140490034313056 [label="SelectBackward0
--------------------------------
dim           :                2
index         :                1
self_sym_sizes: (3, 2, 502, 768)"]
	140490034315120 -> 140490034313056
	140490034315120 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:    (3, 2, 502, 768)
start         :                   0
step          :                   1"]
	140490034313344 -> 140490034315120
	140490034313344 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:    (3, 2, 502, 768)
start         :                   0
step          :                   1"]
	140490034314160 -> 140490034313344
	140490034314160 [label="ViewBackward0
-----------------------------
self_sym_sizes: (6, 502, 768)"]
	140490034311424 -> 140490034314160
	140490034311424 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140490034053952 -> 140490034311424
	140490034053952 [label="AddBackward0
------------
alpha: 1"]
	140490034054720 -> 140490034053952
	140490034054720 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140490034053760 -> 140490034054720
	140490034053760 [label="ViewBackward0
---------------------------
self_sym_sizes: (3012, 768)"]
	140490034059568 -> 140490034053760
	140490034059568 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :   (3012, 3072)
mat1_sym_strides:      (3072, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (3072, 768)
mat2_sym_strides:      (1, 3072)"]
	140490034058080 -> 140490034059568
	140490034705136 [label="q_encoder.encoder.text_encoding_layer.4.output.dense.bias
 (768)" fillcolor=lightblue]
	140490034705136 -> 140490034058080
	140490034058080 [label=AccumulateGrad]
	140490034057120 -> 140490034059568
	140490034057120 [label="ViewBackward0
------------------------------
self_sym_sizes: (6, 502, 3072)"]
	140490034054336 -> 140490034057120
	140490034054336 [label="GeluBackward0
---------------------------
approximate:           none
self       : [saved tensor]"]
	140490034079120 -> 140490034054336
	140490034079120 [label="ViewBackward0
----------------------------
self_sym_sizes: (3012, 3072)"]
	140490034074368 -> 140490034079120
	140490034074368 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :    (3012, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (768, 3072)
mat2_sym_strides:       (1, 768)"]
	140490034074704 -> 140490034074368
	140490034707536 [label="q_encoder.encoder.text_encoding_layer.4.intermediate.dense.bias
 (3072)" fillcolor=lightblue]
	140490034707536 -> 140490034074704
	140490034074704 [label=AccumulateGrad]
	140490034079456 -> 140490034074368
	140490034079456 [label="ViewBackward0
-----------------------------
self_sym_sizes: (6, 502, 768)"]
	140490034055296 -> 140490034079456
	140490034055296 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140490034080368 -> 140490034055296
	140490034080368 [label="AddBackward0
------------
alpha: 1"]
	140490034081952 -> 140490034080368
	140490034081952 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140490034081040 -> 140490034081952
	140490034081040 [label="ViewBackward0
---------------------------
self_sym_sizes: (3012, 768)"]
	140490034084112 -> 140490034081040
	140490034084112 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :    (3012, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140490034084448 -> 140490034084112
	140490034707136 [label="q_encoder.encoder.text_encoding_layer.4.attention.output.dense.bias
 (768)" fillcolor=lightblue]
	140490034707136 -> 140490034084448
	140490034084448 [label=AccumulateGrad]
	140490034083536 -> 140490034084112
	140490034083536 [label="ViewBackward0
-----------------------------
self_sym_sizes: (6, 502, 768)"]
	140490034084832 -> 140490034083536
	140490034084832 [label="ViewBackward0
--------------------------------
self_sym_sizes: (6, 502, 12, 64)"]
	140490034084640 -> 140490034084832
	140490034084640 [label=CloneBackward0]
	140490034079648 -> 140490034084640
	140490034079648 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140490034085168 -> 140490034079648
	140490034085168 [label="UnsafeViewBackward0
-----------------------------
self_sym_sizes: (72, 502, 64)"]
	140490034085456 -> 140490034085168
	140490034085456 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140490034085600 -> 140490034085456
	140490034085600 [label="ViewBackward0
---------------------------------
self_sym_sizes: (6, 12, 502, 502)"]
	140490034085744 -> 140490034085600
	140490034085744 [label="ExpandBackward0
---------------------------------
self_sym_sizes: (6, 12, 502, 502)"]
	140490034085840 -> 140490034085744
	140490034085840 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140490034282848 -> 140490034085840
	140490034282848 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140490034283088 -> 140490034282848
	140490034283088 [label="AddBackward0
------------
alpha: 1"]
	140490034283136 -> 140490034283088
	140490034283136 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140490034283280 -> 140490034283136
	140490034283280 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (72, 502, 502)"]
	140490034283520 -> 140490034283280
	140490034283520 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140490034283664 -> 140490034283520
	140490034283664 [label="UnsafeViewBackward0
--------------------------------
self_sym_sizes: (6, 12, 502, 64)"]
	140490034283856 -> 140490034283664
	140490034283856 [label=CloneBackward0]
	140490034284048 -> 140490034283856
	140490034284048 [label="ExpandBackward0
--------------------------------
self_sym_sizes: (6, 12, 502, 64)"]
	140490034284144 -> 140490034284048
	140490034284144 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140490034284528 -> 140490034284144
	140490034284528 [label="ViewBackward0
-----------------------------
self_sym_sizes: (6, 502, 768)"]
	140490034284624 -> 140490034284528
	140490034284624 [label="ViewBackward0
---------------------------
self_sym_sizes: (3012, 768)"]
	140490034284768 -> 140490034284624
	140490034284768 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :    (3012, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140490034284960 -> 140490034284768
	140490034706656 [label="q_encoder.encoder.text_encoding_layer.4.attention.self.query.bias
 (768)" fillcolor=lightblue]
	140490034706656 -> 140490034284960
	140490034284960 [label=AccumulateGrad]
	140490034284864 -> 140490034284768
	140490034284864 [label="ViewBackward0
-----------------------------
self_sym_sizes: (6, 502, 768)"]
	140490034082720 -> 140490034284864
	140490034082720 [label="ViewBackward0
--------------------------------
self_sym_sizes: (3, 2, 502, 768)"]
	140490034285344 -> 140490034082720
	140490034285344 [label="AsStridedBackward0
----------------------------------------
size          :         (3, 2, 502, 768)
storage_offset:                        0
stride        : (771072, 385536, 768, 1)"]
	140490034285488 -> 140490034285344
	140490034285488 [label=CopySlices]
	140490034064224 -> 140490034285488
	140490034285776 -> 140490034285488
	140490034285776 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 2, 768)
start         :                   0
step          :                   1"]
	140490034285584 -> 140490034285776
	140490034285584 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:         (3, 3, 768)
start         :                   1
step          :                   1"]
	140490034286688 -> 140490034285584
	140490034286688 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:         (3, 3, 768)
start         :                   0
step          :                   1"]
	140490034312288 -> 140490034286688
	140490034283904 -> 140490034284768
	140490034283904 [label=TBackward0]
	140490034285056 -> 140490034283904
	140490034706576 [label="q_encoder.encoder.text_encoding_layer.4.attention.self.query.weight
 (768, 768)" fillcolor=lightblue]
	140490034706576 -> 140490034285056
	140490034285056 [label=AccumulateGrad]
	140490034283232 -> 140490034283520
	140490034283232 [label="UnsafeViewBackward0
--------------------------------
self_sym_sizes: (6, 12, 64, 502)"]
	140490034283328 -> 140490034283232
	140490034283328 [label=CloneBackward0]
	140490034284672 -> 140490034283328
	140490034284672 [label="ExpandBackward0
--------------------------------
self_sym_sizes: (6, 12, 64, 502)"]
	140490034284384 -> 140490034284672
	140490034284384 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551615
dim1: 18446744073709551614"]
	140490034285680 -> 140490034284384
	140490034285680 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140490034286880 -> 140490034285680
	140490034286880 [label="ViewBackward0
-----------------------------
self_sym_sizes: (6, 502, 768)"]
	140490034285008 -> 140490034286880
	140490034285008 [label="ViewBackward0
---------------------------
self_sym_sizes: (3012, 768)"]
	140490034285392 -> 140490034285008
	140490034285392 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :    (3012, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140490034286544 -> 140490034285392
	140490034706816 [label="q_encoder.encoder.text_encoding_layer.4.attention.self.key.bias
 (768)" fillcolor=lightblue]
	140490034706816 -> 140490034286544
	140490034286544 [label=AccumulateGrad]
	140490034287024 -> 140490034285392
	140490034287024 [label="ViewBackward0
-----------------------------
self_sym_sizes: (6, 502, 768)"]
	140490034082720 -> 140490034287024
	140490034283952 -> 140490034285392
	140490034283952 [label=TBackward0]
	140490034287120 -> 140490034283952
	140490034706736 [label="q_encoder.encoder.text_encoding_layer.4.attention.self.key.weight
 (768, 768)" fillcolor=lightblue]
	140490034706736 -> 140490034287120
	140490034287120 [label=AccumulateGrad]
	140490034085312 -> 140490034085456
	140490034085312 [label="UnsafeViewBackward0
--------------------------------
self_sym_sizes: (6, 12, 502, 64)"]
	140490034085696 -> 140490034085312
	140490034085696 [label=CloneBackward0]
	140490034282896 -> 140490034085696
	140490034282896 [label="ExpandBackward0
--------------------------------
self_sym_sizes: (6, 12, 502, 64)"]
	140490034282944 -> 140490034282896
	140490034282944 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140490034284240 -> 140490034282944
	140490034284240 [label="ViewBackward0
-----------------------------
self_sym_sizes: (6, 502, 768)"]
	140490034285536 -> 140490034284240
	140490034285536 [label="ViewBackward0
---------------------------
self_sym_sizes: (3012, 768)"]
	140490034285968 -> 140490034285536
	140490034285968 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :    (3012, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140490034286592 -> 140490034285968
	140490034706976 [label="q_encoder.encoder.text_encoding_layer.4.attention.self.value.bias
 (768)" fillcolor=lightblue]
	140490034706976 -> 140490034286592
	140490034286592 [label=AccumulateGrad]
	140490034285296 -> 140490034285968
	140490034285296 [label="ViewBackward0
-----------------------------
self_sym_sizes: (6, 502, 768)"]
	140490034082720 -> 140490034285296
	140490034282608 -> 140490034285968
	140490034282608 [label=TBackward0]
	140490034286736 -> 140490034282608
	140490034706896 [label="q_encoder.encoder.text_encoding_layer.4.attention.self.value.weight
 (768, 768)" fillcolor=lightblue]
	140490034706896 -> 140490034286736
	140490034286736 [label=AccumulateGrad]
	140490034083584 -> 140490034084112
	140490034083584 [label=TBackward0]
	140490034084784 -> 140490034083584
	140490034707056 [label="q_encoder.encoder.text_encoding_layer.4.attention.output.dense.weight
 (768, 768)" fillcolor=lightblue]
	140490034707056 -> 140490034084784
	140490034084784 [label=AccumulateGrad]
	140490034082720 -> 140490034080368
	140490034079408 -> 140490034055296
	140490034707296 [label="q_encoder.encoder.text_encoding_layer.4.attention.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	140490034707296 -> 140490034079408
	140490034079408 [label=AccumulateGrad]
	140490034080608 -> 140490034055296
	140490034707216 [label="q_encoder.encoder.text_encoding_layer.4.attention.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	140490034707216 -> 140490034080608
	140490034080608 [label=AccumulateGrad]
	140490034078160 -> 140490034074368
	140490034078160 [label=TBackward0]
	140490034083632 -> 140490034078160
	140490034707456 [label="q_encoder.encoder.text_encoding_layer.4.intermediate.dense.weight
 (3072, 768)" fillcolor=lightblue]
	140490034707456 -> 140490034083632
	140490034083632 [label=AccumulateGrad]
	140490034056448 -> 140490034059568
	140490034056448 [label=TBackward0]
	140490034073792 -> 140490034056448
	140490034707376 [label="q_encoder.encoder.text_encoding_layer.4.output.dense.weight
 (768, 3072)" fillcolor=lightblue]
	140490034707376 -> 140490034073792
	140490034073792 [label=AccumulateGrad]
	140490034055296 -> 140490034053952
	140490034053472 -> 140490034311424
	140490034707776 [label="q_encoder.encoder.text_encoding_layer.4.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	140490034707776 -> 140490034053472
	140490034053472 [label=AccumulateGrad]
	140490034054432 -> 140490034311424
	140490034707616 [label="q_encoder.encoder.text_encoding_layer.4.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	140490034707616 -> 140490034054432
	140490034054432 [label=AccumulateGrad]
	140490034312048 -> 140490034311040
	140490034312048 [label=TBackward0]
	140490034312144 -> 140490034312048
	140490035213040 [label="q_encoder.encoder.information_exchanging_layer.4.attention.self.query.weight
 (768, 768)" fillcolor=lightblue]
	140490035213040 -> 140490034312144
	140490034312144 [label=AccumulateGrad]
	140490034309504 -> 140490034309840
	140490034309504 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 64, 3)"]
	140490034309984 -> 140490034309504
	140490034309984 [label=CloneBackward0]
	140490034310704 -> 140490034309984
	140490034310704 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 64, 3)"]
	140490034311136 -> 140490034310704
	140490034311136 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551615
dim1: 18446744073709551614"]
	140490034313632 -> 140490034311136
	140490034313632 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140490034314208 -> 140490034313632
	140490034314208 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 3, 768)"]
	140490034311664 -> 140490034314208
	140490034311664 [label="ViewBackward0
------------------------
self_sym_sizes: (9, 768)"]
	140490034309456 -> 140490034311664
	140490034309456 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (9, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140490034053616 -> 140490034309456
	140490035213760 [label="q_encoder.encoder.information_exchanging_layer.4.attention.self.key.bias
 (768)" fillcolor=lightblue]
	140490035213760 -> 140490034053616
	140490034053616 [label=AccumulateGrad]
	140490034057408 -> 140490034309456
	140490034057408 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 3, 768)"]
	140490034306000 -> 140490034057408
	140490034056208 -> 140490034309456
	140490034056208 [label=TBackward0]
	140490034055872 -> 140490034056208
	140490035213600 [label="q_encoder.encoder.information_exchanging_layer.4.attention.self.key.weight
 (768, 768)" fillcolor=lightblue]
	140490035213600 -> 140490034055872
	140490034055872 [label=AccumulateGrad]
	140490034308064 -> 140490034307536
	140490034308064 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 3, 64)"]
	140490034308352 -> 140490034308064
	140490034308352 [label=CloneBackward0]
	140490034309216 -> 140490034308352
	140490034309216 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 3, 64)"]
	140490034309600 -> 140490034309216
	140490034309600 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140490034310320 -> 140490034309600
	140490034310320 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 3, 768)"]
	140490034311904 -> 140490034310320
	140490034311904 [label="ViewBackward0
------------------------
self_sym_sizes: (9, 768)"]
	140490034314496 -> 140490034311904
	140490034314496 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (9, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140490034308160 -> 140490034314496
	140490035213920 [label="q_encoder.encoder.information_exchanging_layer.4.attention.self.value.bias
 (768)" fillcolor=lightblue]
	140490035213920 -> 140490034308160
	140490034308160 [label=AccumulateGrad]
	140490034313536 -> 140490034314496
	140490034313536 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 3, 768)"]
	140490034306000 -> 140490034313536
	140490034065664 -> 140490034314496
	140490034065664 [label=TBackward0]
	140490034079312 -> 140490034065664
	140490035213840 [label="q_encoder.encoder.information_exchanging_layer.4.attention.self.value.weight
 (768, 768)" fillcolor=lightblue]
	140490035213840 -> 140490034079312
	140490034079312 [label=AccumulateGrad]
	140490034307200 -> 140490034306528
	140490034307200 [label=TBackward0]
	140490034306912 -> 140490034307200
	140490035214000 [label="q_encoder.encoder.information_exchanging_layer.4.attention.output.dense.weight
 (768, 768)" fillcolor=lightblue]
	140490035214000 -> 140490034306912
	140490034306912 [label=AccumulateGrad]
	140490034306000 -> 140490034305856
	140490034305568 -> 140490034302784
	140490035214240 [label="q_encoder.encoder.information_exchanging_layer.4.attention.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	140490035214240 -> 140490034305568
	140490034305568 [label=AccumulateGrad]
	140490034306240 -> 140490034302784
	140490035214160 [label="q_encoder.encoder.information_exchanging_layer.4.attention.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	140490035214160 -> 140490034306240
	140490034306240 [label=AccumulateGrad]
	140490034305808 -> 140490034305088
	140490034305808 [label=TBackward0]
	140490034306192 -> 140490034305808
	140490035214400 [label="q_encoder.encoder.information_exchanging_layer.4.intermediate.dense.weight
 (3072, 768)" fillcolor=lightblue]
	140490035214400 -> 140490034306192
	140490034306192 [label=AccumulateGrad]
	140490034304704 -> 140490034304128
	140490034304704 [label=TBackward0]
	140490034304512 -> 140490034304704
	140490035213680 [label="q_encoder.encoder.information_exchanging_layer.4.output.dense.weight
 (768, 3072)" fillcolor=lightblue]
	140490035213680 -> 140490034304512
	140490034304512 [label=AccumulateGrad]
	140490034302784 -> 140490034303408
	140490034303456 -> 140490034302640
	140490035214720 [label="q_encoder.encoder.information_exchanging_layer.4.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	140490035214720 -> 140490034303456
	140490034303456 [label=AccumulateGrad]
	140490034303696 -> 140490034302640
	140490035214640 [label="q_encoder.encoder.information_exchanging_layer.4.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	140490035214640 -> 140490034303696
	140490034303696 [label=AccumulateGrad]
	140490034302160 -> 140490034427648
	140490034302160 [label=CloneBackward0]
	140490034302448 -> 140490034302160
	140490034302448 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 2, 768)
start         :                   0
step          :                   1"]
	140490034303264 -> 140490034302448
	140490034303264 [label="SelectBackward0
--------------------------------
dim           :                2
index         :                1
self_sym_sizes: (3, 2, 502, 768)"]
	140490034306144 -> 140490034303264
	140490034306144 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:    (3, 2, 502, 768)
start         :                   0
step          :                   1"]
	140490034303936 -> 140490034306144
	140490034303936 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:    (3, 2, 502, 768)
start         :                   0
step          :                   1"]
	140490034307296 -> 140490034303936
	140490034307296 [label="ViewBackward0
-----------------------------
self_sym_sizes: (6, 502, 768)"]
	140490034306960 -> 140490034307296
	140490034306960 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140490034305424 -> 140490034306960
	140490034305424 [label="AddBackward0
------------
alpha: 1"]
	140490034306384 -> 140490034305424
	140490034306384 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140490034303600 -> 140490034306384
	140490034303600 [label="ViewBackward0
---------------------------
self_sym_sizes: (3012, 768)"]
	140490034313152 -> 140490034303600
	140490034313152 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :   (3012, 3072)
mat1_sym_strides:      (3072, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (3072, 768)
mat2_sym_strides:      (1, 3072)"]
	140490034054912 -> 140490034313152
	140490035200080 [label="q_encoder.encoder.text_encoding_layer.5.output.dense.bias
 (768)" fillcolor=lightblue]
	140490035200080 -> 140490034054912
	140490034054912 [label=AccumulateGrad]
	140490034309744 -> 140490034313152
	140490034309744 [label="ViewBackward0
------------------------------
self_sym_sizes: (6, 502, 3072)"]
	140490034304560 -> 140490034309744
	140490034304560 [label="GeluBackward0
---------------------------
approximate:           none
self       : [saved tensor]"]
	140490034080176 -> 140490034304560
	140490034080176 [label="ViewBackward0
----------------------------
self_sym_sizes: (3012, 3072)"]
	140490034079840 -> 140490034080176
	140490034079840 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :    (3012, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (768, 3072)
mat2_sym_strides:       (1, 768)"]
	140490034079504 -> 140490034079840
	140490035200400 [label="q_encoder.encoder.text_encoding_layer.5.intermediate.dense.bias
 (3072)" fillcolor=lightblue]
	140490035200400 -> 140490034079504
	140490034079504 [label=AccumulateGrad]
	140490034085024 -> 140490034079840
	140490034085024 [label="ViewBackward0
-----------------------------
self_sym_sizes: (6, 502, 768)"]
	140490034307776 -> 140490034085024
	140490034307776 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140490034085552 -> 140490034307776
	140490034085552 [label="AddBackward0
------------
alpha: 1"]
	140490034284336 -> 140490034085552
	140490034284336 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140490034282752 -> 140490034284336
	140490034282752 [label="ViewBackward0
---------------------------
self_sym_sizes: (3012, 768)"]
	140490034286976 -> 140490034282752
	140490034286976 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :    (3012, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140490034287168 -> 140490034286976
	140490034708416 [label="q_encoder.encoder.text_encoding_layer.5.attention.output.dense.bias
 (768)" fillcolor=lightblue]
	140490034708416 -> 140490034287168
	140490034287168 [label=AccumulateGrad]
	140490034286064 -> 140490034286976
	140490034286064 [label="ViewBackward0
-----------------------------
self_sym_sizes: (6, 502, 768)"]
	140490034287504 -> 140490034286064
	140490034287504 [label="ViewBackward0
--------------------------------
self_sym_sizes: (6, 502, 12, 64)"]
	140490034287312 -> 140490034287504
	140490034287312 [label=CloneBackward0]
	140490034283424 -> 140490034287312
	140490034283424 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140490034287936 -> 140490034283424
	140490034287936 [label="UnsafeViewBackward0
-----------------------------
self_sym_sizes: (72, 502, 64)"]
	140490034288176 -> 140490034287936
	140490034288176 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140490034288320 -> 140490034288176
	140490034288320 [label="ViewBackward0
---------------------------------
self_sym_sizes: (6, 12, 502, 502)"]
	140490034288416 -> 140490034288320
	140490034288416 [label="ExpandBackward0
---------------------------------
self_sym_sizes: (6, 12, 502, 502)"]
	140490034288608 -> 140490034288416
	140490034288608 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140490034288704 -> 140490034288608
	140490034288704 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140490034289040 -> 140490034288704
	140490034289040 [label="AddBackward0
------------
alpha: 1"]
	140490034289136 -> 140490034289040
	140490034289136 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140490034289376 -> 140490034289136
	140490034289376 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (72, 502, 502)"]
	140490034289616 -> 140490034289376
	140490034289616 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140490034289664 -> 140490034289616
	140490034289664 [label="UnsafeViewBackward0
--------------------------------
self_sym_sizes: (6, 12, 502, 64)"]
	140490034289712 -> 140490034289664
	140490034289712 [label=CloneBackward0]
	140490034290000 -> 140490034289712
	140490034290000 [label="ExpandBackward0
--------------------------------
self_sym_sizes: (6, 12, 502, 64)"]
	140490034289904 -> 140490034290000
	140490034289904 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140490034290192 -> 140490034289904
	140490034290192 [label="ViewBackward0
-----------------------------
self_sym_sizes: (6, 502, 768)"]
	140490034290384 -> 140490034290192
	140490034290384 [label="ViewBackward0
---------------------------
self_sym_sizes: (3012, 768)"]
	140490034290480 -> 140490034290384
	140490034290480 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :    (3012, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140490034290624 -> 140490034290480
	140490034707936 [label="q_encoder.encoder.text_encoding_layer.5.attention.self.query.bias
 (768)" fillcolor=lightblue]
	140490034707936 -> 140490034290624
	140490034290624 [label=AccumulateGrad]
	140490034290528 -> 140490034290480
	140490034290528 [label="ViewBackward0
-----------------------------
self_sym_sizes: (6, 502, 768)"]
	140490034285104 -> 140490034290528
	140490034285104 [label="ViewBackward0
--------------------------------
self_sym_sizes: (3, 2, 502, 768)"]
	140490034291008 -> 140490034285104
	140490034291008 [label="AsStridedBackward0
----------------------------------------
size          :         (3, 2, 502, 768)
storage_offset:                        0
stride        : (771072, 385536, 768, 1)"]
	140490034291152 -> 140490034291008
	140490034291152 [label=CopySlices]
	140490034311424 -> 140490034291152
	140490034291440 -> 140490034291152
	140490034291440 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 2, 768)
start         :                   0
step          :                   1"]
	140490034291248 -> 140490034291440
	140490034291248 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:         (3, 3, 768)
start         :                   1
step          :                   1"]
	140490034292064 -> 140490034291248
	140490034292064 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:         (3, 3, 768)
start         :                   0
step          :                   1"]
	140490034302640 -> 140490034292064
	140490034289760 -> 140490034290480
	140490034289760 [label=TBackward0]
	140490034290672 -> 140490034289760
	140490034707856 [label="q_encoder.encoder.text_encoding_layer.5.attention.self.query.weight
 (768, 768)" fillcolor=lightblue]
	140490034707856 -> 140490034290672
	140490034290672 [label=AccumulateGrad]
	140490034289232 -> 140490034289616
	140490034289232 [label="UnsafeViewBackward0
--------------------------------
self_sym_sizes: (6, 12, 64, 502)"]
	140490034289472 -> 140490034289232
	140490034289472 [label=CloneBackward0]
	140490034290432 -> 140490034289472
	140490034290432 [label="ExpandBackward0
--------------------------------
self_sym_sizes: (6, 12, 64, 502)"]
	140490034290240 -> 140490034290432
	140490034290240 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551615
dim1: 18446744073709551614"]
	140490034291344 -> 140490034290240
	140490034291344 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140490034292208 -> 140490034291344
	140490034292208 [label="ViewBackward0
-----------------------------
self_sym_sizes: (6, 502, 768)"]
	140490034290816 -> 140490034292208
	140490034290816 [label="ViewBackward0
---------------------------
self_sym_sizes: (3012, 768)"]
	140490034291056 -> 140490034290816
	140490034291056 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :    (3012, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140490034291920 -> 140490034291056
	140490034708096 [label="q_encoder.encoder.text_encoding_layer.5.attention.self.key.bias
 (768)" fillcolor=lightblue]
	140490034708096 -> 140490034291920
	140490034291920 [label=AccumulateGrad]
	140490034292352 -> 140490034291056
	140490034292352 [label="ViewBackward0
-----------------------------
self_sym_sizes: (6, 502, 768)"]
	140490034285104 -> 140490034292352
	140490034289856 -> 140490034291056
	140490034289856 [label=TBackward0]
	140490034292400 -> 140490034289856
	140490034708016 [label="q_encoder.encoder.text_encoding_layer.5.attention.self.key.weight
 (768, 768)" fillcolor=lightblue]
	140490034708016 -> 140490034292400
	140490034292400 [label=AccumulateGrad]
	140490034288128 -> 140490034288176
	140490034288128 [label="UnsafeViewBackward0
--------------------------------
self_sym_sizes: (6, 12, 502, 64)"]
	140490034288368 -> 140490034288128
	140490034288368 [label=CloneBackward0]
	140490034288800 -> 140490034288368
	140490034288800 [label="ExpandBackward0
--------------------------------
self_sym_sizes: (6, 12, 502, 64)"]
	140490034288992 -> 140490034288800
	140490034288992 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140490034290048 -> 140490034288992
	140490034290048 [label="ViewBackward0
-----------------------------
self_sym_sizes: (6, 502, 768)"]
	140490034291200 -> 140490034290048
	140490034291200 [label="ViewBackward0
---------------------------
self_sym_sizes: (3012, 768)"]
	140490034291536 -> 140490034291200
	140490034291536 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :    (3012, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140490034291968 -> 140490034291536
	140490034708256 [label="q_encoder.encoder.text_encoding_layer.5.attention.self.value.bias
 (768)" fillcolor=lightblue]
	140490034708256 -> 140490034291968
	140490034291968 [label=AccumulateGrad]
	140490034290960 -> 140490034291536
	140490034290960 [label="ViewBackward0
-----------------------------
self_sym_sizes: (6, 502, 768)"]
	140490034285104 -> 140490034290960
	140490034288560 -> 140490034291536
	140490034288560 [label=TBackward0]
	140490034292112 -> 140490034288560
	140490034708176 [label="q_encoder.encoder.text_encoding_layer.5.attention.self.value.weight
 (768, 768)" fillcolor=lightblue]
	140490034708176 -> 140490034292112
	140490034292112 [label=AccumulateGrad]
	140490034286208 -> 140490034286976
	140490034286208 [label=TBackward0]
	140490034287408 -> 140490034286208
	140490034708336 [label="q_encoder.encoder.text_encoding_layer.5.attention.output.dense.weight
 (768, 768)" fillcolor=lightblue]
	140490034708336 -> 140490034287408
	140490034287408 [label=AccumulateGrad]
	140490034285104 -> 140490034085552
	140490034084976 -> 140490034307776
	140490035200160 [label="q_encoder.encoder.text_encoding_layer.5.attention.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	140490035200160 -> 140490034084976
	140490034084976 [label=AccumulateGrad]
	140490034283040 -> 140490034307776
	140490034706416 [label="q_encoder.encoder.text_encoding_layer.5.attention.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	140490034706416 -> 140490034283040
	140490034283040 [label=AccumulateGrad]
	140490034084592 -> 140490034079840
	140490034084592 [label=TBackward0]
	140490034085264 -> 140490034084592
	140490035200320 [label="q_encoder.encoder.text_encoding_layer.5.intermediate.dense.weight
 (3072, 768)" fillcolor=lightblue]
	140490035200320 -> 140490034085264
	140490034085264 [label=AccumulateGrad]
	140490034310464 -> 140490034313152
	140490034310464 [label=TBackward0]
	140490034074032 -> 140490034310464
	140490035200240 [label="q_encoder.encoder.text_encoding_layer.5.output.dense.weight
 (768, 3072)" fillcolor=lightblue]
	140490035200240 -> 140490034074032
	140490034074032 [label=AccumulateGrad]
	140490034307776 -> 140490034305424
	140490034307392 -> 140490034306960
	140490035200640 [label="q_encoder.encoder.text_encoding_layer.5.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	140490035200640 -> 140490034307392
	140490034307392 [label=AccumulateGrad]
	140490034301872 -> 140490034306960
	140490035200480 [label="q_encoder.encoder.text_encoding_layer.5.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	140490035200480 -> 140490034301872
	140490034301872 [label=AccumulateGrad]
	140490034302256 -> 140490034301344
	140490034302256 [label=TBackward0]
	140490034302496 -> 140490034302256
	140490035214320 [label="q_encoder.encoder.information_exchanging_layer.5.attention.self.query.weight
 (768, 768)" fillcolor=lightblue]
	140490035214320 -> 140490034302496
	140490034302496 [label=AccumulateGrad]
	140490034299808 -> 140490034300048
	140490034299808 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 64, 3)"]
	140490034300240 -> 140490034299808
	140490034300240 [label=CloneBackward0]
	140490034301008 -> 140490034300240
	140490034301008 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 64, 3)"]
	140490034301056 -> 140490034301008
	140490034301056 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551615
dim1: 18446744073709551614"]
	140490034304080 -> 140490034301056
	140490034304080 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140490034307104 -> 140490034304080
	140490034307104 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 3, 768)"]
	140490034305616 -> 140490034307104
	140490034305616 [label="ViewBackward0
------------------------
self_sym_sizes: (9, 768)"]
	140490034308784 -> 140490034305616
	140490034308784 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (9, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140490034083680 -> 140490034308784
	140490035215040 [label="q_encoder.encoder.information_exchanging_layer.5.attention.self.key.bias
 (768)" fillcolor=lightblue]
	140490035215040 -> 140490034083680
	140490034083680 [label=AccumulateGrad]
	140490034084880 -> 140490034308784
	140490034084880 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 3, 768)"]
	140490034427648 -> 140490034084880
	140490034085072 -> 140490034308784
	140490034085072 [label=TBackward0]
	140490034304944 -> 140490034085072
	140490035214880 [label="q_encoder.encoder.information_exchanging_layer.5.attention.self.key.weight
 (768, 768)" fillcolor=lightblue]
	140490035214880 -> 140490034304944
	140490034304944 [label=AccumulateGrad]
	140490034429472 -> 140490034429088
	140490034429472 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 3, 64)"]
	140490034429568 -> 140490034429472
	140490034429568 [label=CloneBackward0]
	140490034299472 -> 140490034429568
	140490034299472 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 3, 64)"]
	140490034299856 -> 140490034299472
	140490034299856 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140490034300480 -> 140490034299856
	140490034300480 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 3, 768)"]
	140490034302208 -> 140490034300480
	140490034302208 [label="ViewBackward0
------------------------
self_sym_sizes: (9, 768)"]
	140490034306480 -> 140490034302208
	140490034306480 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (9, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140490034310128 -> 140490034306480
	140490035215200 [label="q_encoder.encoder.information_exchanging_layer.5.attention.self.value.bias
 (768)" fillcolor=lightblue]
	140490035215200 -> 140490034310128
	140490034310128 [label=AccumulateGrad]
	140490034301968 -> 140490034306480
	140490034301968 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 3, 768)"]
	140490034427648 -> 140490034301968
	140490034299040 -> 140490034306480
	140490034299040 [label=TBackward0]
	140490034309024 -> 140490034299040
	140490035215120 [label="q_encoder.encoder.information_exchanging_layer.5.attention.self.value.weight
 (768, 768)" fillcolor=lightblue]
	140490035215120 -> 140490034309024
	140490034309024 [label=AccumulateGrad]
	140490034428608 -> 140490034428224
	140490034428608 [label=TBackward0]
	140490034428512 -> 140490034428608
	140490035215280 [label="q_encoder.encoder.information_exchanging_layer.5.attention.output.dense.weight
 (768, 768)" fillcolor=lightblue]
	140490035215280 -> 140490034428512
	140490034428512 [label=AccumulateGrad]
	140490034427648 -> 140490034427456
	140490034427312 -> 140490034424672
	140490035215520 [label="q_encoder.encoder.information_exchanging_layer.5.attention.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	140490035215520 -> 140490034427312
	140490034427312 [label=AccumulateGrad]
	140490034427840 -> 140490034424672
	140490035215440 [label="q_encoder.encoder.information_exchanging_layer.5.attention.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	140490035215440 -> 140490034427840
	140490034427840 [label=AccumulateGrad]
	140490034427408 -> 140490034426640
	140490034427408 [label=TBackward0]
	140490034427744 -> 140490034427408
	140490035215680 [label="q_encoder.encoder.information_exchanging_layer.5.intermediate.dense.weight
 (3072, 768)" fillcolor=lightblue]
	140490035215680 -> 140490034427744
	140490034427744 [label=AccumulateGrad]
	140490034426352 -> 140490034425920
	140490034426352 [label=TBackward0]
	140490034426160 -> 140490034426352
	140490035214960 [label="q_encoder.encoder.information_exchanging_layer.5.output.dense.weight
 (768, 3072)" fillcolor=lightblue]
	140490035214960 -> 140490034426160
	140490034426160 [label=AccumulateGrad]
	140490034424672 -> 140490034425440
	140490034425488 -> 140490034424480
	140490035216000 [label="q_encoder.encoder.information_exchanging_layer.5.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	140490035216000 -> 140490034425488
	140490034425488 [label=AccumulateGrad]
	140490034425632 -> 140490034424480
	140490035215920 [label="q_encoder.encoder.information_exchanging_layer.5.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	140490035215920 -> 140490034425632
	140490034425632 [label=AccumulateGrad]
	140490034424000 -> 140490034418000
	140490034424000 [label=CloneBackward0]
	140490034424384 -> 140490034424000
	140490034424384 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 2, 768)
start         :                   0
step          :                   1"]
	140490034425200 -> 140490034424384
	140490034425200 [label="SelectBackward0
--------------------------------
dim           :                2
index         :                1
self_sym_sizes: (3, 2, 502, 768)"]
	140490034427552 -> 140490034425200
	140490034427552 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:    (3, 2, 502, 768)
start         :                   0
step          :                   1"]
	140490034425776 -> 140490034427552
	140490034425776 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:    (3, 2, 502, 768)
start         :                   0
step          :                   1"]
	140490034428752 -> 140490034425776
	140490034428752 [label="ViewBackward0
-----------------------------
self_sym_sizes: (6, 502, 768)"]
	140490034428416 -> 140490034428752
	140490034428416 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140490034427072 -> 140490034428416
	140490034427072 [label="AddBackward0
------------
alpha: 1"]
	140490034429232 -> 140490034427072
	140490034429232 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140490034426592 -> 140490034429232
	140490034426592 [label="ViewBackward0
---------------------------
self_sym_sizes: (3012, 768)"]
	140490034303360 -> 140490034426592
	140490034303360 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :   (3012, 3072)
mat1_sym_strides:      (3072, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (3072, 768)
mat2_sym_strides:      (1, 3072)"]
	140490034300816 -> 140490034303360
	140490035200560 [label="q_encoder.encoder.text_encoding_layer.6.output.dense.bias
 (768)" fillcolor=lightblue]
	140490035200560 -> 140490034300816
	140490034300816 [label=AccumulateGrad]
	140490034299952 -> 140490034303360
	140490034299952 [label="ViewBackward0
------------------------------
self_sym_sizes: (6, 502, 3072)"]
	140490034299328 -> 140490034299952
	140490034299328 [label="GeluBackward0
---------------------------
approximate:           none
self       : [saved tensor]"]
	140490034308448 -> 140490034299328
	140490034308448 [label="ViewBackward0
----------------------------
self_sym_sizes: (3012, 3072)"]
	140490034282656 -> 140490034308448
	140490034282656 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :    (3012, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (768, 3072)
mat2_sym_strides:       (1, 768)"]
	140490034286352 -> 140490034282656
	140490035201680 [label="q_encoder.encoder.text_encoding_layer.6.intermediate.dense.bias
 (3072)" fillcolor=lightblue]
	140490035201680 -> 140490034286352
	140490034286352 [label=AccumulateGrad]
	140490034287744 -> 140490034282656
	140490034287744 [label="ViewBackward0
-----------------------------
self_sym_sizes: (6, 502, 768)"]
	140490034426208 -> 140490034287744
	140490034426208 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140490034288656 -> 140490034426208
	140490034288656 [label="AddBackward0
------------
alpha: 1"]
	140490034290096 -> 140490034288656
	140490034290096 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140490034289520 -> 140490034290096
	140490034289520 [label="ViewBackward0
---------------------------
self_sym_sizes: (3012, 768)"]
	140490034292256 -> 140490034289520
	140490034292256 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :    (3012, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140490034292496 -> 140490034292256
	140490035201280 [label="q_encoder.encoder.text_encoding_layer.6.attention.output.dense.bias
 (768)" fillcolor=lightblue]
	140490035201280 -> 140490034292496
	140490034292496 [label=AccumulateGrad]
	140490034291632 -> 140490034292256
	140490034291632 [label="ViewBackward0
-----------------------------
self_sym_sizes: (6, 502, 768)"]
	140490034292832 -> 140490034291632
	140490034292832 [label="ViewBackward0
--------------------------------
self_sym_sizes: (6, 502, 12, 64)"]
	140490034292736 -> 140490034292832
	140490034292736 [label=CloneBackward0]
	140490034288032 -> 140490034292736
	140490034288032 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140490034293168 -> 140490034288032
	140490034293168 [label="UnsafeViewBackward0
-----------------------------
self_sym_sizes: (72, 502, 64)"]
	140490034293504 -> 140490034293168
	140490034293504 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140490034293744 -> 140490034293504
	140490034293744 [label="ViewBackward0
---------------------------------
self_sym_sizes: (6, 12, 502, 502)"]
	140490034293888 -> 140490034293744
	140490034293888 [label="ExpandBackward0
---------------------------------
self_sym_sizes: (6, 12, 502, 502)"]
	140490034294320 -> 140490034293888
	140490034294320 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140490034294560 -> 140490034294320
	140490034294560 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140490034294896 -> 140490034294560
	140490034294896 [label="AddBackward0
------------
alpha: 1"]
	140490034295136 -> 140490034294896
	140490034295136 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140490034295184 -> 140490034295136
	140490034295184 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (72, 502, 502)"]
	140490034295424 -> 140490034295184
	140490034295424 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140490034295616 -> 140490034295424
	140490034295616 [label="UnsafeViewBackward0
--------------------------------
self_sym_sizes: (6, 12, 502, 64)"]
	140490034295760 -> 140490034295616
	140490034295760 [label=CloneBackward0]
	140490034295952 -> 140490034295760
	140490034295952 [label="ExpandBackward0
--------------------------------
self_sym_sizes: (6, 12, 502, 64)"]
	140490034296000 -> 140490034295952
	140490034296000 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140490034296336 -> 140490034296000
	140490034296336 [label="ViewBackward0
-----------------------------
self_sym_sizes: (6, 502, 768)"]
	140490034296480 -> 140490034296336
	140490034296480 [label="ViewBackward0
---------------------------
self_sym_sizes: (3012, 768)"]
	140490034296576 -> 140490034296480
	140490034296576 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :    (3012, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140490034296864 -> 140490034296576
	140490035200800 [label="q_encoder.encoder.text_encoding_layer.6.attention.self.query.bias
 (768)" fillcolor=lightblue]
	140490035200800 -> 140490034296864
	140490034296864 [label=AccumulateGrad]
	140490034296768 -> 140490034296576
	140490034296768 [label="ViewBackward0
-----------------------------
self_sym_sizes: (6, 502, 768)"]
	140490034290912 -> 140490034296768
	140490034290912 [label="ViewBackward0
--------------------------------
self_sym_sizes: (3, 2, 502, 768)"]
	140490034297296 -> 140490034290912
	140490034297296 [label="AsStridedBackward0
----------------------------------------
size          :         (3, 2, 502, 768)
storage_offset:                        0
stride        : (771072, 385536, 768, 1)"]
	140490034297392 -> 140490034297296
	140490034297392 [label=CopySlices]
	140490034306960 -> 140490034297392
	140490034297728 -> 140490034297392
	140490034297728 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 2, 768)
start         :                   0
step          :                   1"]
	140490034297584 -> 140490034297728
	140490034297584 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:         (3, 3, 768)
start         :                   1
step          :                   1"]
	140490034298400 -> 140490034297584
	140490034298400 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:         (3, 3, 768)
start         :                   0
step          :                   1"]
	140490034424480 -> 140490034298400
	140490034295472 -> 140490034296576
	140490034295472 [label=TBackward0]
	140490034297008 -> 140490034295472
	140490035200720 [label="q_encoder.encoder.text_encoding_layer.6.attention.self.query.weight
 (768, 768)" fillcolor=lightblue]
	140490035200720 -> 140490034297008
	140490034297008 [label=AccumulateGrad]
	140490034294944 -> 140490034295424
	140490034294944 [label="UnsafeViewBackward0
--------------------------------
self_sym_sizes: (6, 12, 64, 502)"]
	140490034295232 -> 140490034294944
	140490034295232 [label=CloneBackward0]
	140490034296528 -> 140490034295232
	140490034296528 [label="ExpandBackward0
--------------------------------
self_sym_sizes: (6, 12, 64, 502)"]
	140490034296384 -> 140490034296528
	140490034296384 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551615
dim1: 18446744073709551614"]
	140490034297680 -> 140490034296384
	140490034297680 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140490034298592 -> 140490034297680
	140490034298592 [label="ViewBackward0
-----------------------------
self_sym_sizes: (6, 502, 768)"]
	140490034296960 -> 140490034298592
	140490034296960 [label="ViewBackward0
---------------------------
self_sym_sizes: (3012, 768)"]
	140490034297344 -> 140490034296960
	140490034297344 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :    (3012, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140490034298208 -> 140490034297344
	140490035200960 [label="q_encoder.encoder.text_encoding_layer.6.attention.self.key.bias
 (768)" fillcolor=lightblue]
	140490035200960 -> 140490034298208
	140490034298208 [label=AccumulateGrad]
	140490034298736 -> 140490034297344
	140490034298736 [label="ViewBackward0
-----------------------------
self_sym_sizes: (6, 502, 768)"]
	140490034290912 -> 140490034298736
	140490034295856 -> 140490034297344
	140490034295856 [label=TBackward0]
	140490034298832 -> 140490034295856
	140490035200880 [label="q_encoder.encoder.text_encoding_layer.6.attention.self.key.weight
 (768, 768)" fillcolor=lightblue]
	140490035200880 -> 140490034298832
	140490034298832 [label=AccumulateGrad]
	140490034293408 -> 140490034293504
	140490034293408 [label="UnsafeViewBackward0
--------------------------------
self_sym_sizes: (6, 12, 502, 64)"]
	140490034293840 -> 140490034293408
	140490034293840 [label=CloneBackward0]
	140490034294704 -> 140490034293840
	140490034294704 [label="ExpandBackward0
--------------------------------
self_sym_sizes: (6, 12, 502, 64)"]
	140490034294800 -> 140490034294704
	140490034294800 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140490034296144 -> 140490034294800
	140490034296144 [label="ViewBackward0
-----------------------------
self_sym_sizes: (6, 502, 768)"]
	140490034297440 -> 140490034296144
	140490034297440 [label="ViewBackward0
---------------------------
self_sym_sizes: (3012, 768)"]
	140490034297824 -> 140490034297440
	140490034297824 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :    (3012, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140490034298352 -> 140490034297824
	140490035201120 [label="q_encoder.encoder.text_encoding_layer.6.attention.self.value.bias
 (768)" fillcolor=lightblue]
	140490035201120 -> 140490034298352
	140490034298352 [label=AccumulateGrad]
	140490034297248 -> 140490034297824
	140490034297248 [label="ViewBackward0
-----------------------------
self_sym_sizes: (6, 502, 768)"]
	140490034290912 -> 140490034297248
	140490034294176 -> 140490034297824
	140490034294176 [label=TBackward0]
	140490034298016 -> 140490034294176
	140490035201040 [label="q_encoder.encoder.text_encoding_layer.6.attention.self.value.weight
 (768, 768)" fillcolor=lightblue]
	140490035201040 -> 140490034298016
	140490034298016 [label=AccumulateGrad]
	140490034291680 -> 140490034292256
	140490034291680 [label=TBackward0]
	140490034292640 -> 140490034291680
	140490035201200 [label="q_encoder.encoder.text_encoding_layer.6.attention.output.dense.weight
 (768, 768)" fillcolor=lightblue]
	140490035201200 -> 140490034292640
	140490034292640 [label=AccumulateGrad]
	140490034290912 -> 140490034288656
	140490034287648 -> 140490034426208
	140490035201440 [label="q_encoder.encoder.text_encoding_layer.6.attention.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	140490035201440 -> 140490034287648
	140490034287648 [label=AccumulateGrad]
	140490034288896 -> 140490034426208
	140490035201360 [label="q_encoder.encoder.text_encoding_layer.6.attention.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	140490035201360 -> 140490034288896
	140490034288896 [label=AccumulateGrad]
	140490034286448 -> 140490034282656
	140490034286448 [label=TBackward0]
	140490034291728 -> 140490034286448
	140490035201600 [label="q_encoder.encoder.text_encoding_layer.6.intermediate.dense.weight
 (3072, 768)" fillcolor=lightblue]
	140490035201600 -> 140490034291728
	140490034291728 [label=AccumulateGrad]
	140490034299184 -> 140490034303360
	140490034299184 [label=TBackward0]
	140490034299760 -> 140490034299184
	140490035201520 [label="q_encoder.encoder.text_encoding_layer.6.output.dense.weight
 (768, 3072)" fillcolor=lightblue]
	140490035201520 -> 140490034299760
	140490034299760 [label=AccumulateGrad]
	140490034426208 -> 140490034427072
	140490034428896 -> 140490034428416
	140490035201920 [label="q_encoder.encoder.text_encoding_layer.6.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	140490035201920 -> 140490034428896
	140490034428896 [label=AccumulateGrad]
	140490034423664 -> 140490034428416
	140490035201760 [label="q_encoder.encoder.text_encoding_layer.6.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	140490035201760 -> 140490034423664
	140490034423664 [label=AccumulateGrad]
	140490034424144 -> 140490034423184
	140490034424144 [label=TBackward0]
	140490034424432 -> 140490034424144
	140490035215600 [label="q_encoder.encoder.information_exchanging_layer.6.attention.self.query.weight
 (768, 768)" fillcolor=lightblue]
	140490035215600 -> 140490034424432
	140490034424432 [label=AccumulateGrad]
	140490034421408 -> 140490034421792
	140490034421408 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 64, 3)"]
	140490034422032 -> 140490034421408
	140490034422032 [label=CloneBackward0]
	140490034422944 -> 140490034422032
	140490034422944 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 64, 3)"]
	140490034423136 -> 140490034422944
	140490034423136 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551615
dim1: 18446744073709551614"]
	140490034425872 -> 140490034423136
	140490034425872 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140490034428560 -> 140490034425872
	140490034428560 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 3, 768)"]
	140490034427360 -> 140490034428560
	140490034427360 [label="ViewBackward0
------------------------
self_sym_sizes: (9, 768)"]
	140490034429712 -> 140490034427360
	140490034429712 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (9, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140490034425584 -> 140490034429712
	140490035216320 [label="q_encoder.encoder.information_exchanging_layer.6.attention.self.key.bias
 (768)" fillcolor=lightblue]
	140490035216320 -> 140490034425584
	140490034425584 [label=AccumulateGrad]
	140490034428032 -> 140490034429712
	140490034428032 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 3, 768)"]
	140490034418000 -> 140490034428032
	140490034421360 -> 140490034429712
	140490034421360 [label=TBackward0]
	140490034300288 -> 140490034421360
	140490035216160 [label="q_encoder.encoder.information_exchanging_layer.6.attention.self.key.weight
 (768, 768)" fillcolor=lightblue]
	140490035216160 -> 140490034300288
	140490034300288 [label=AccumulateGrad]
	140490034419968 -> 140490034419776
	140490034419968 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 3, 64)"]
	140490034420208 -> 140490034419968
	140490034420208 [label=CloneBackward0]
	140490034421072 -> 140490034420208
	140490034421072 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 3, 64)"]
	140490034421552 -> 140490034421072
	140490034421552 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140490034422464 -> 140490034421552
	140490034422464 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 3, 768)"]
	140490034424096 -> 140490034422464
	140490034424096 [label="ViewBackward0
------------------------
self_sym_sizes: (9, 768)"]
	140490034304656 -> 140490034424096
	140490034304656 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (9, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140490034420160 -> 140490034304656
	140490033692832 [label="q_encoder.encoder.information_exchanging_layer.6.attention.self.value.bias
 (768)" fillcolor=lightblue]
	140490033692832 -> 140490034420160
	140490034420160 [label=AccumulateGrad]
	140490034423856 -> 140490034304656
	140490034423856 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 3, 768)"]
	140490034418000 -> 140490034423856
	140490034428176 -> 140490034304656
	140490034428176 [label=TBackward0]
	140490034287552 -> 140490034428176
	140490033692752 [label="q_encoder.encoder.information_exchanging_layer.6.attention.self.value.weight
 (768, 768)" fillcolor=lightblue]
	140490033692752 -> 140490034287552
	140490034287552 [label=AccumulateGrad]
	140490034419296 -> 140490034418624
	140490034419296 [label=TBackward0]
	140490034419008 -> 140490034419296
	140490033692912 [label="q_encoder.encoder.information_exchanging_layer.6.attention.output.dense.weight
 (768, 768)" fillcolor=lightblue]
	140490033692912 -> 140490034419008
	140490034419008 [label=AccumulateGrad]
	140490034418000 -> 140490034417904
	140490034417760 -> 140490034414448
	140490033693152 [label="q_encoder.encoder.information_exchanging_layer.6.attention.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	140490033693152 -> 140490034417760
	140490034417760 [label=AccumulateGrad]
	140490034418288 -> 140490034414448
	140490033693072 [label="q_encoder.encoder.information_exchanging_layer.6.attention.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	140490033693072 -> 140490034418288
	140490034418288 [label=AccumulateGrad]
	140490034417472 -> 140490034416896
	140490034417472 [label=TBackward0]
	140490034418240 -> 140490034417472
	140490033693312 [label="q_encoder.encoder.information_exchanging_layer.6.intermediate.dense.weight
 (3072, 768)" fillcolor=lightblue]
	140490033693312 -> 140490034418240
	140490034418240 [label=AccumulateGrad]
	140490034416656 -> 140490034415984
	140490034416656 [label=TBackward0]
	140490034416272 -> 140490034416656
	140490033693472 [label="q_encoder.encoder.information_exchanging_layer.6.output.dense.weight
 (768, 3072)" fillcolor=lightblue]
	140490033693472 -> 140490034416272
	140490034416272 [label=AccumulateGrad]
	140490034414448 -> 140490034415120
	140490034415264 -> 140490034414208
	140490033693712 [label="q_encoder.encoder.information_exchanging_layer.6.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	140490033693712 -> 140490034415264
	140490034415264 [label=AccumulateGrad]
	140490034415456 -> 140490034414208
	140490033693632 [label="q_encoder.encoder.information_exchanging_layer.6.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	140490033693632 -> 140490034415456
	140490034415456 [label=AccumulateGrad]
	140490034414064 -> 140490034473584
	140490034414064 [label=CloneBackward0]
	140490034414160 -> 140490034414064
	140490034414160 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 2, 768)
start         :                   0
step          :                   1"]
	140490034414784 -> 140490034414160
	140490034414784 [label="SelectBackward0
--------------------------------
dim           :                2
index         :                1
self_sym_sizes: (3, 2, 502, 768)"]
	140490034418192 -> 140490034414784
	140490034418192 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:    (3, 2, 502, 768)
start         :                   0
step          :                   1"]
	140490034415648 -> 140490034418192
	140490034415648 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:    (3, 2, 502, 768)
start         :                   0
step          :                   1"]
	140490034419440 -> 140490034415648
	140490034419440 [label="ViewBackward0
-----------------------------
self_sym_sizes: (6, 502, 768)"]
	140490034419104 -> 140490034419440
	140490034419104 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140490034417376 -> 140490034419104
	140490034417376 [label="AddBackward0
------------
alpha: 1"]
	140490034418480 -> 140490034417376
	140490034418480 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140490034415360 -> 140490034418480
	140490034415360 [label="ViewBackward0
---------------------------
self_sym_sizes: (3012, 768)"]
	140490034425344 -> 140490034415360
	140490034425344 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :   (3012, 3072)
mat1_sym_strides:      (3072, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (3072, 768)
mat2_sym_strides:      (1, 3072)"]
	140490034422656 -> 140490034425344
	140490035201840 [label="q_encoder.encoder.text_encoding_layer.7.output.dense.bias
 (768)" fillcolor=lightblue]
	140490035201840 -> 140490034422656
	140490034422656 [label=AccumulateGrad]
	140490034421600 -> 140490034425344
	140490034421600 [label="ViewBackward0
------------------------------
self_sym_sizes: (6, 502, 3072)"]
	140490034416416 -> 140490034421600
	140490034416416 [label="GeluBackward0
---------------------------
approximate:           none
self       : [saved tensor]"]
	140490034288464 -> 140490034416416
	140490034288464 [label="ViewBackward0
----------------------------
self_sym_sizes: (3012, 3072)"]
	140490034288272 -> 140490034288464
	140490034288272 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :    (3012, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (768, 3072)
mat2_sym_strides:       (1, 768)"]
	140490034287792 -> 140490034288272
	140490035202960 [label="q_encoder.encoder.text_encoding_layer.7.intermediate.dense.bias
 (3072)" fillcolor=lightblue]
	140490035202960 -> 140490034287792
	140490034287792 [label=AccumulateGrad]
	140490034293024 -> 140490034288272
	140490034293024 [label="ViewBackward0
-----------------------------
self_sym_sizes: (6, 502, 768)"]
	140490034419824 -> 140490034293024
	140490034419824 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140490034294464 -> 140490034419824
	140490034294464 [label="AddBackward0
------------
alpha: 1"]
	140490034296240 -> 140490034294464
	140490034296240 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140490034295280 -> 140490034296240
	140490034295280 [label="ViewBackward0
---------------------------
self_sym_sizes: (3012, 768)"]
	140490034298160 -> 140490034295280
	140490034298160 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :    (3012, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140490034298544 -> 140490034298160
	140490035202560 [label="q_encoder.encoder.text_encoding_layer.7.attention.output.dense.bias
 (768)" fillcolor=lightblue]
	140490035202560 -> 140490034298544
	140490034298544 [label=AccumulateGrad]
	140490034293312 -> 140490034298160
	140490034293312 [label="ViewBackward0
-----------------------------
self_sym_sizes: (6, 502, 768)"]
	140488973631840 -> 140490034293312
	140488973631840 [label="ViewBackward0
--------------------------------
self_sym_sizes: (6, 502, 12, 64)"]
	140488973631744 -> 140488973631840
	140488973631744 [label=CloneBackward0]
	140488973631552 -> 140488973631744
	140488973631552 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140488973632176 -> 140488973631552
	140488973632176 [label="UnsafeViewBackward0
-----------------------------
self_sym_sizes: (72, 502, 64)"]
	140488973632320 -> 140488973632176
	140488973632320 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140488973632416 -> 140488973632320
	140488973632416 [label="ViewBackward0
---------------------------------
self_sym_sizes: (6, 12, 502, 502)"]
	140488973632512 -> 140488973632416
	140488973632512 [label="ExpandBackward0
---------------------------------
self_sym_sizes: (6, 12, 502, 502)"]
	140488973632704 -> 140488973632512
	140488973632704 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140488973632848 -> 140488973632704
	140488973632848 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140488973633184 -> 140488973632848
	140488973633184 [label="AddBackward0
------------
alpha: 1"]
	140488973633232 -> 140488973633184
	140488973633232 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140488973633376 -> 140488973633232
	140488973633376 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (72, 502, 502)"]
	140488973633664 -> 140488973633376
	140488973633664 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140488973633808 -> 140488973633664
	140488973633808 [label="UnsafeViewBackward0
--------------------------------
self_sym_sizes: (6, 12, 502, 64)"]
	140488973633856 -> 140488973633808
	140488973633856 [label=CloneBackward0]
	140488973634048 -> 140488973633856
	140488973634048 [label="ExpandBackward0
--------------------------------
self_sym_sizes: (6, 12, 502, 64)"]
	140488973634144 -> 140488973634048
	140488973634144 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140488973634384 -> 140488973634144
	140488973634384 [label="ViewBackward0
-----------------------------
self_sym_sizes: (6, 502, 768)"]
	140488973634576 -> 140488973634384
	140488973634576 [label="ViewBackward0
---------------------------
self_sym_sizes: (3012, 768)"]
	140488973634768 -> 140488973634576
	140488973634768 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :    (3012, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140488973634912 -> 140488973634768
	140490035202080 [label="q_encoder.encoder.text_encoding_layer.7.attention.self.query.bias
 (768)" fillcolor=lightblue]
	140490035202080 -> 140488973634912
	140488973634912 [label=AccumulateGrad]
	140488973634816 -> 140488973634768
	140488973634816 [label="ViewBackward0
-----------------------------
self_sym_sizes: (6, 502, 768)"]
	140490034297152 -> 140488973634816
	140490034297152 [label="ViewBackward0
--------------------------------
self_sym_sizes: (3, 2, 502, 768)"]
	140488973635344 -> 140490034297152
	140488973635344 [label="AsStridedBackward0
----------------------------------------
size          :         (3, 2, 502, 768)
storage_offset:                        0
stride        : (771072, 385536, 768, 1)"]
	140488973635488 -> 140488973635344
	140488973635488 [label=CopySlices]
	140490034428416 -> 140488973635488
	140488973635680 -> 140488973635488
	140488973635680 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 2, 768)
start         :                   0
step          :                   1"]
	140488973635584 -> 140488973635680
	140488973635584 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:         (3, 3, 768)
start         :                   1
step          :                   1"]
	140488973636256 -> 140488973635584
	140488973636256 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:         (3, 3, 768)
start         :                   0
step          :                   1"]
	140490034414208 -> 140488973636256
	140488973634000 -> 140488973634768
	140488973634000 [label=TBackward0]
	140488973635104 -> 140488973634000
	140490035202000 [label="q_encoder.encoder.text_encoding_layer.7.attention.self.query.weight
 (768, 768)" fillcolor=lightblue]
	140490035202000 -> 140488973635104
	140488973635104 [label=AccumulateGrad]
	140488973633280 -> 140488973633664
	140488973633280 [label="UnsafeViewBackward0
--------------------------------
self_sym_sizes: (6, 12, 64, 502)"]
	140488973633472 -> 140488973633280
	140488973633472 [label=CloneBackward0]
	140488973634720 -> 140488973633472
	140488973634720 [label="ExpandBackward0
--------------------------------
self_sym_sizes: (6, 12, 64, 502)"]
	140488973634528 -> 140488973634720
	140488973634528 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551615
dim1: 18446744073709551614"]
	140488973635632 -> 140488973634528
	140488973635632 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140488973636400 -> 140488973635632
	140488973636400 [label="ViewBackward0
-----------------------------
self_sym_sizes: (6, 502, 768)"]
	140488973635008 -> 140488973636400
	140488973635008 [label="ViewBackward0
---------------------------
self_sym_sizes: (3012, 768)"]
	140488973635392 -> 140488973635008
	140488973635392 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :    (3012, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140488973636016 -> 140488973635392
	140490035202240 [label="q_encoder.encoder.text_encoding_layer.7.attention.self.key.bias
 (768)" fillcolor=lightblue]
	140490035202240 -> 140488973636016
	140488973636016 [label=AccumulateGrad]
	140488973636544 -> 140488973635392
	140488973636544 [label="ViewBackward0
-----------------------------
self_sym_sizes: (6, 502, 768)"]
	140490034297152 -> 140488973636544
	140488973633904 -> 140488973635392
	140488973633904 [label=TBackward0]
	140488973636592 -> 140488973633904
	140490035202160 [label="q_encoder.encoder.text_encoding_layer.7.attention.self.key.weight
 (768, 768)" fillcolor=lightblue]
	140490035202160 -> 140488973636592
	140488973636592 [label=AccumulateGrad]
	140488973632272 -> 140488973632320
	140488973632272 [label="UnsafeViewBackward0
--------------------------------
self_sym_sizes: (6, 12, 502, 64)"]
	140488973632464 -> 140488973632272
	140488973632464 [label=CloneBackward0]
	140488973632896 -> 140488973632464
	140488973632896 [label="ExpandBackward0
--------------------------------
self_sym_sizes: (6, 12, 502, 64)"]
	140488973632944 -> 140488973632896
	140488973632944 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140488973634192 -> 140488973632944
	140488973634192 [label="ViewBackward0
-----------------------------
self_sym_sizes: (6, 502, 768)"]
	140488973635536 -> 140488973634192
	140488973635536 [label="ViewBackward0
---------------------------
self_sym_sizes: (3012, 768)"]
	140488973635728 -> 140488973635536
	140488973635728 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :    (3012, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140488973636160 -> 140488973635728
	140490035202400 [label="q_encoder.encoder.text_encoding_layer.7.attention.self.value.bias
 (768)" fillcolor=lightblue]
	140490035202400 -> 140488973636160
	140488973636160 [label=AccumulateGrad]
	140488973635248 -> 140488973635728
	140488973635248 [label="ViewBackward0
-----------------------------
self_sym_sizes: (6, 502, 768)"]
	140490034297152 -> 140488973635248
	140488973632656 -> 140488973635728
	140488973632656 [label=TBackward0]
	140488973636352 -> 140488973632656
	140490035202320 [label="q_encoder.encoder.text_encoding_layer.7.attention.self.value.weight
 (768, 768)" fillcolor=lightblue]
	140490035202320 -> 140488973636352
	140488973636352 [label=AccumulateGrad]
	140488973631648 -> 140490034298160
	140488973631648 [label=TBackward0]
	140488973631792 -> 140488973631648
	140490035202480 [label="q_encoder.encoder.text_encoding_layer.7.attention.output.dense.weight
 (768, 768)" fillcolor=lightblue]
	140490035202480 -> 140488973631792
	140488973631792 [label=AccumulateGrad]
	140490034297152 -> 140490034294464
	140490034292976 -> 140490034419824
	140490035202720 [label="q_encoder.encoder.text_encoding_layer.7.attention.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	140490035202720 -> 140490034292976
	140490034292976 [label=AccumulateGrad]
	140490034294848 -> 140490034419824
	140490035202640 [label="q_encoder.encoder.text_encoding_layer.7.attention.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	140490035202640 -> 140490034294848
	140490034294848 [label=AccumulateGrad]
	140490034292592 -> 140490034288272
	140490034292592 [label=TBackward0]
	140490034297920 -> 140490034292592
	140490035202880 [label="q_encoder.encoder.text_encoding_layer.7.intermediate.dense.weight
 (3072, 768)" fillcolor=lightblue]
	140490035202880 -> 140490034297920
	140490034297920 [label=AccumulateGrad]
	140490034420880 -> 140490034425344
	140490034420880 [label=TBackward0]
	140490034287216 -> 140490034420880
	140490035202800 [label="q_encoder.encoder.text_encoding_layer.7.output.dense.weight
 (768, 3072)" fillcolor=lightblue]
	140490035202800 -> 140490034287216
	140490034287216 [label=AccumulateGrad]
	140490034419824 -> 140490034417376
	140490034419584 -> 140490034419104
	140490035203200 [label="q_encoder.encoder.text_encoding_layer.7.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	140490035203200 -> 140490034419584
	140490034419584 [label=AccumulateGrad]
	140490034413824 -> 140490034419104
	140490035203040 [label="q_encoder.encoder.text_encoding_layer.7.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	140490035203040 -> 140490034413824
	140490034413824 [label=AccumulateGrad]
	140490034479056 -> 140490034478672
	140490034479056 [label=TBackward0]
	140490034414352 -> 140490034479056
	140490033693232 [label="q_encoder.encoder.information_exchanging_layer.7.attention.self.query.weight
 (768, 768)" fillcolor=lightblue]
	140490033693232 -> 140490034414352
	140490034414352 [label=AccumulateGrad]
	140490034476848 -> 140490034477088
	140490034476848 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 64, 3)"]
	140490034477328 -> 140490034476848
	140490034477328 [label=CloneBackward0]
	140490034478288 -> 140490034477328
	140490034478288 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 64, 3)"]
	140490034478576 -> 140490034478288
	140490034478576 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551615
dim1: 18446744073709551614"]
	140490034476800 -> 140490034478576
	140490034476800 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140490034419200 -> 140490034476800
	140490034419200 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 3, 768)"]
	140490034417520 -> 140490034419200
	140490034417520 [label="ViewBackward0
------------------------
self_sym_sizes: (9, 768)"]
	140490034420592 -> 140490034417520
	140490034420592 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (9, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140490034416800 -> 140490034420592
	140490033693952 [label="q_encoder.encoder.information_exchanging_layer.7.attention.self.key.bias
 (768)" fillcolor=lightblue]
	140490033693952 -> 140490034416800
	140490034416800 [label=AccumulateGrad]
	140490034422128 -> 140490034420592
	140490034422128 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 3, 768)"]
	140490034473584 -> 140490034422128
	140490034415888 -> 140490034420592
	140490034415888 [label=TBackward0]
	140490034420256 -> 140490034415888
	140490033693872 [label="q_encoder.encoder.information_exchanging_layer.7.attention.self.key.weight
 (768, 768)" fillcolor=lightblue]
	140490033693872 -> 140490034420256
	140490034420256 [label=AccumulateGrad]
	140490034475552 -> 140490034475312
	140490034475552 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 3, 64)"]
	140490034475696 -> 140490034475552
	140490034475696 [label=CloneBackward0]
	140490034476608 -> 140490034475696
	140490034476608 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 3, 64)"]
	140490034476896 -> 140490034476608
	140490034476896 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140490034477664 -> 140490034476896
	140490034477664 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 3, 768)"]
	140490034479008 -> 140490034477664
	140490034479008 [label="ViewBackward0
------------------------
self_sym_sizes: (9, 768)"]
	140490034475648 -> 140490034479008
	140490034475648 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (9, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140490034293072 -> 140490034475648
	140490033694112 [label="q_encoder.encoder.information_exchanging_layer.7.attention.self.value.bias
 (768)" fillcolor=lightblue]
	140490033694112 -> 140490034293072
	140490034293072 [label=AccumulateGrad]
	140490034292928 -> 140490034475648
	140490034292928 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 3, 768)"]
	140490034473584 -> 140490034292928
	140490034291824 -> 140490034475648
	140490034291824 [label=TBackward0]
	140490034294032 -> 140490034291824
	140490033694032 [label="q_encoder.encoder.information_exchanging_layer.7.attention.self.value.weight
 (768, 768)" fillcolor=lightblue]
	140490033694032 -> 140490034294032
	140490034294032 [label=AccumulateGrad]
	140490034474928 -> 140490034474400
	140490034474928 [label=TBackward0]
	140490034474688 -> 140490034474928
	140490033694192 [label="q_encoder.encoder.information_exchanging_layer.7.attention.output.dense.weight
 (768, 768)" fillcolor=lightblue]
	140490033694192 -> 140490034474688
	140490034474688 [label=AccumulateGrad]
	140490034473584 -> 140490034473488
	140490034473248 -> 140490034470560
	140490033694432 [label="q_encoder.encoder.information_exchanging_layer.7.attention.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	140490033694432 -> 140490034473248
	140490034473248 [label=AccumulateGrad]
	140490034473872 -> 140490034470560
	140490033694352 [label="q_encoder.encoder.information_exchanging_layer.7.attention.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	140490033694352 -> 140490034473872
	140490034473872 [label=AccumulateGrad]
	140490034473392 -> 140490034472816
	140490034473392 [label=TBackward0]
	140490034473824 -> 140490034473392
	140490033694592 [label="q_encoder.encoder.information_exchanging_layer.7.intermediate.dense.weight
 (3072, 768)" fillcolor=lightblue]
	140490033694592 -> 140490034473824
	140490034473824 [label=AccumulateGrad]
	140490034472576 -> 140490034471904
	140490034472576 [label=TBackward0]
	140490034472336 -> 140490034472576
	140490033694752 [label="q_encoder.encoder.information_exchanging_layer.7.output.dense.weight
 (768, 3072)" fillcolor=lightblue]
	140490033694752 -> 140490034472336
	140490034472336 [label=AccumulateGrad]
	140490034470560 -> 140490034471280
	140490034471376 -> 140490034470464
	140490033694992 [label="q_encoder.encoder.information_exchanging_layer.7.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	140490033694992 -> 140490034471376
	140490034471376 [label=AccumulateGrad]
	140490034471568 -> 140490034470464
	140490033694912 [label="q_encoder.encoder.information_exchanging_layer.7.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	140490033694912 -> 140490034471568
	140490034471568 [label=AccumulateGrad]
	140490034470080 -> 140490034464416
	140490034470080 [label=CloneBackward0]
	140490034470272 -> 140490034470080
	140490034470272 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 2, 768)
start         :                   0
step          :                   1"]
	140490034470992 -> 140490034470272
	140490034470992 [label="SelectBackward0
--------------------------------
dim           :                2
index         :                1
self_sym_sizes: (3, 2, 502, 768)"]
	140490034473728 -> 140490034470992
	140490034473728 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:    (3, 2, 502, 768)
start         :                   0
step          :                   1"]
	140490034471808 -> 140490034473728
	140490034471808 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:    (3, 2, 502, 768)
start         :                   0
step          :                   1"]
	140490034475072 -> 140490034471808
	140490034475072 [label="ViewBackward0
-----------------------------
self_sym_sizes: (6, 502, 768)"]
	140490034474832 -> 140490034475072
	140490034474832 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140490034473152 -> 140490034474832
	140490034473152 [label="AddBackward0
------------
alpha: 1"]
	140490034473968 -> 140490034473152
	140490034473968 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140490034471472 -> 140490034473968
	140490034471472 [label="ViewBackward0
---------------------------
self_sym_sizes: (3012, 768)"]
	140490034478048 -> 140490034471472
	140490034478048 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :   (3012, 3072)
mat1_sym_strides:      (3072, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (3072, 768)
mat2_sym_strides:      (1, 3072)"]
	140490034472192 -> 140490034478048
	140490035203120 [label="q_encoder.encoder.text_encoding_layer.8.output.dense.bias
 (768)" fillcolor=lightblue]
	140490035203120 -> 140490034472192
	140490034472192 [label=AccumulateGrad]
	140490034477040 -> 140490034478048
	140490034477040 [label="ViewBackward0
------------------------------
self_sym_sizes: (6, 502, 3072)"]
	140490034415072 -> 140490034477040
	140490034415072 [label="GeluBackward0
---------------------------
approximate:           none
self       : [saved tensor]"]
	140490034413872 -> 140490034415072
	140490034413872 [label="ViewBackward0
----------------------------
self_sym_sizes: (3012, 3072)"]
	140490034298688 -> 140490034413872
	140490034298688 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :    (3012, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (768, 3072)
mat2_sym_strides:       (1, 768)"]
	140490034293648 -> 140490034298688
	140490035204240 [label="q_encoder.encoder.text_encoding_layer.8.intermediate.dense.bias
 (3072)" fillcolor=lightblue]
	140490035204240 -> 140490034293648
	140490034293648 [label=AccumulateGrad]
	140488973632128 -> 140490034298688
	140488973632128 [label="ViewBackward0
-----------------------------
self_sym_sizes: (6, 502, 768)"]
	140490034475360 -> 140488973632128
	140490034475360 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140488973632800 -> 140490034475360
	140488973632800 [label="AddBackward0
------------
alpha: 1"]
	140488973634288 -> 140488973632800
	140488973634288 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140488973633520 -> 140488973634288
	140488973633520 [label="ViewBackward0
---------------------------
self_sym_sizes: (3012, 768)"]
	140488973636448 -> 140488973633520
	140488973636448 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :    (3012, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140488973636688 -> 140488973636448
	140490035203840 [label="q_encoder.encoder.text_encoding_layer.8.attention.output.dense.bias
 (768)" fillcolor=lightblue]
	140490035203840 -> 140488973636688
	140488973636688 [label=AccumulateGrad]
	140488973635776 -> 140488973636448
	140488973635776 [label="ViewBackward0
-----------------------------
self_sym_sizes: (6, 502, 768)"]
	140488973636976 -> 140488973635776
	140488973636976 [label="ViewBackward0
--------------------------------
self_sym_sizes: (6, 502, 12, 64)"]
	140488973636784 -> 140488973636976
	140488973636784 [label=CloneBackward0]
	140488973632224 -> 140488973636784
	140488973632224 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140488973637408 -> 140488973632224
	140488973637408 [label="UnsafeViewBackward0
-----------------------------
self_sym_sizes: (72, 502, 64)"]
	140488973637600 -> 140488973637408
	140488973637600 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140488973637648 -> 140488973637600
	140488973637648 [label="ViewBackward0
---------------------------------
self_sym_sizes: (6, 12, 502, 502)"]
	140488973637936 -> 140488973637648
	140488973637936 [label="ExpandBackward0
---------------------------------
self_sym_sizes: (6, 12, 502, 502)"]
	140488973638176 -> 140488973637936
	140488973638176 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140488973638416 -> 140488973638176
	140488973638416 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140488973638608 -> 140488973638416
	140488973638608 [label="AddBackward0
------------
alpha: 1"]
	140488973638704 -> 140488973638608
	140488973638704 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140488973638752 -> 140488973638704
	140488973638752 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (72, 502, 502)"]
	140488973639040 -> 140488973638752
	140488973639040 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140488973639088 -> 140488973639040
	140488973639088 [label="UnsafeViewBackward0
--------------------------------
self_sym_sizes: (6, 12, 502, 64)"]
	140488973639232 -> 140488973639088
	140488973639232 [label=CloneBackward0]
	140488973639568 -> 140488973639232
	140488973639568 [label="ExpandBackward0
--------------------------------
self_sym_sizes: (6, 12, 502, 64)"]
	140488973639616 -> 140488973639568
	140488973639616 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140488973639808 -> 140488973639616
	140488973639808 [label="ViewBackward0
-----------------------------
self_sym_sizes: (6, 502, 768)"]
	140488973639952 -> 140488973639808
	140488973639952 [label="ViewBackward0
---------------------------
self_sym_sizes: (3012, 768)"]
	140488973640144 -> 140488973639952
	140488973640144 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :    (3012, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140488973640240 -> 140488973640144
	140490035203360 [label="q_encoder.encoder.text_encoding_layer.8.attention.self.query.bias
 (768)" fillcolor=lightblue]
	140490035203360 -> 140488973640240
	140488973640240 [label=AccumulateGrad]
	140488973640192 -> 140488973640144
	140488973640192 [label="ViewBackward0
-----------------------------
self_sym_sizes: (6, 502, 768)"]
	140488973635152 -> 140488973640192
	140488973635152 [label="ViewBackward0
--------------------------------
self_sym_sizes: (3, 2, 502, 768)"]
	140488973640576 -> 140488973635152
	140488973640576 [label="AsStridedBackward0
----------------------------------------
size          :         (3, 2, 502, 768)
storage_offset:                        0
stride        : (771072, 385536, 768, 1)"]
	140488973640720 -> 140488973640576
	140488973640720 [label=CopySlices]
	140490034419104 -> 140488973640720
	140488973641152 -> 140488973640720
	140488973641152 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 2, 768)
start         :                   0
step          :                   1"]
	140488973640864 -> 140488973641152
	140488973640864 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:         (3, 3, 768)
start         :                   1
step          :                   1"]
	140488973641824 -> 140488973640864
	140488973641824 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:         (3, 3, 768)
start         :                   0
step          :                   1"]
	140490034470464 -> 140488973641824
	140488973639280 -> 140488973640144
	140488973639280 [label=TBackward0]
	140488973640336 -> 140488973639280
	140490035203280 [label="q_encoder.encoder.text_encoding_layer.8.attention.self.query.weight
 (768, 768)" fillcolor=lightblue]
	140490035203280 -> 140488973640336
	140488973640336 [label=AccumulateGrad]
	140488973638848 -> 140488973639040
	140488973638848 [label="UnsafeViewBackward0
--------------------------------
self_sym_sizes: (6, 12, 64, 502)"]
	140488973638944 -> 140488973638848
	140488973638944 [label=CloneBackward0]
	140488973640048 -> 140488973638944
	140488973640048 [label="ExpandBackward0
--------------------------------
self_sym_sizes: (6, 12, 64, 502)"]
	140488973639856 -> 140488973640048
	140488973639856 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551615
dim1: 18446744073709551614"]
	140488973640960 -> 140488973639856
	140488973640960 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140488973641920 -> 140488973640960
	140488973641920 [label="ViewBackward0
-----------------------------
self_sym_sizes: (6, 502, 768)"]
	140488973640288 -> 140488973641920
	140488973640288 [label="ViewBackward0
---------------------------
self_sym_sizes: (3012, 768)"]
	140488973640624 -> 140488973640288
	140488973640624 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :    (3012, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140488973641632 -> 140488973640624
	140490035203520 [label="q_encoder.encoder.text_encoding_layer.8.attention.self.key.bias
 (768)" fillcolor=lightblue]
	140490035203520 -> 140488973641632
	140488973641632 [label=AccumulateGrad]
	140488973642016 -> 140488973640624
	140488973642016 [label="ViewBackward0
-----------------------------
self_sym_sizes: (6, 502, 768)"]
	140488973635152 -> 140488973642016
	140488973639424 -> 140488973640624
	140488973639424 [label=TBackward0]
	140488973642064 -> 140488973639424
	140490035203440 [label="q_encoder.encoder.text_encoding_layer.8.attention.self.key.weight
 (768, 768)" fillcolor=lightblue]
	140490035203440 -> 140488973642064
	140488973642064 [label=AccumulateGrad]
	140488973637552 -> 140488973637600
	140488973637552 [label="UnsafeViewBackward0
--------------------------------
self_sym_sizes: (6, 12, 502, 64)"]
	140488973637792 -> 140488973637552
	140488973637792 [label=CloneBackward0]
	140488973638464 -> 140488973637792
	140488973638464 [label="ExpandBackward0
--------------------------------
self_sym_sizes: (6, 12, 502, 64)"]
	140488973638512 -> 140488973638464
	140488973638512 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140488973639664 -> 140488973638512
	140488973639664 [label="ViewBackward0
-----------------------------
self_sym_sizes: (6, 502, 768)"]
	140488973640768 -> 140488973639664
	140488973640768 [label="ViewBackward0
---------------------------
self_sym_sizes: (3012, 768)"]
	140488973641248 -> 140488973640768
	140488973641248 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :    (3012, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140488973641728 -> 140488973641248
	140490035203680 [label="q_encoder.encoder.text_encoding_layer.8.attention.self.value.bias
 (768)" fillcolor=lightblue]
	140490035203680 -> 140488973641728
	140488973641728 [label=AccumulateGrad]
	140488973640480 -> 140488973641248
	140488973640480 [label="ViewBackward0
-----------------------------
self_sym_sizes: (6, 502, 768)"]
	140488973635152 -> 140488973640480
	140488973638080 -> 140488973641248
	140488973638080 [label=TBackward0]
	140488973641872 -> 140488973638080
	140490035203600 [label="q_encoder.encoder.text_encoding_layer.8.attention.self.value.weight
 (768, 768)" fillcolor=lightblue]
	140490035203600 -> 140488973641872
	140488973641872 [label=AccumulateGrad]
	140488973635872 -> 140488973636448
	140488973635872 [label=TBackward0]
	140488973636928 -> 140488973635872
	140490035203760 [label="q_encoder.encoder.text_encoding_layer.8.attention.output.dense.weight
 (768, 768)" fillcolor=lightblue]
	140490035203760 -> 140488973636928
	140488973636928 [label=AccumulateGrad]
	140488973635152 -> 140488973632800
	140488973631984 -> 140490034475360
	140490035204000 [label="q_encoder.encoder.text_encoding_layer.8.attention.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	140490035204000 -> 140488973631984
	140488973631984 [label=AccumulateGrad]
	140488973633136 -> 140490034475360
	140490035203920 [label="q_encoder.encoder.text_encoding_layer.8.attention.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	140490035203920 -> 140488973633136
	140488973633136 [label=AccumulateGrad]
	140488973632080 -> 140490034298688
	140488973632080 [label=TBackward0]
	140488973635920 -> 140488973632080
	140490035204160 [label="q_encoder.encoder.text_encoding_layer.8.intermediate.dense.weight
 (3072, 768)" fillcolor=lightblue]
	140490035204160 -> 140488973635920
	140488973635920 [label=AccumulateGrad]
	140490034476416 -> 140490034478048
	140490034476416 [label=TBackward0]
	140490034297968 -> 140490034476416
	140490035204080 [label="q_encoder.encoder.text_encoding_layer.8.output.dense.weight
 (768, 3072)" fillcolor=lightblue]
	140490035204080 -> 140490034297968
	140490034297968 [label=AccumulateGrad]
	140490034475360 -> 140490034473152
	140490034475168 -> 140490034474832
	140490035204480 [label="q_encoder.encoder.text_encoding_layer.8.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	140490035204480 -> 140490034475168
	140490034475168 [label=AccumulateGrad]
	140490034469744 -> 140490034474832
	140490035204320 [label="q_encoder.encoder.text_encoding_layer.8.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	140490035204320 -> 140490034469744
	140490034469744 [label=AccumulateGrad]
	140490034470176 -> 140490034469408
	140490034470176 [label=TBackward0]
	140490034470368 -> 140490034470176
	140490033694512 [label="q_encoder.encoder.information_exchanging_layer.8.attention.self.query.weight
 (768, 768)" fillcolor=lightblue]
	140490033694512 -> 140490034470368
	140490034470368 [label=AccumulateGrad]
	140490034467632 -> 140490034468064
	140490034467632 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 64, 3)"]
	140490034468208 -> 140490034467632
	140490034468208 [label=CloneBackward0]
	140490034469120 -> 140490034468208
	140490034469120 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 64, 3)"]
	140490034469360 -> 140490034469120
	140490034469360 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551615
dim1: 18446744073709551614"]
	140490034471856 -> 140490034469360
	140490034471856 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140490034474880 -> 140490034471856
	140490034474880 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 3, 768)"]
	140490034473344 -> 140490034474880
	140490034473344 [label="ViewBackward0
------------------------
self_sym_sizes: (9, 768)"]
	140490034476128 -> 140490034473344
	140490034476128 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (9, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140490034416464 -> 140490034476128
	140490033695232 [label="q_encoder.encoder.information_exchanging_layer.8.attention.self.key.bias
 (768)" fillcolor=lightblue]
	140490033695232 -> 140490034416464
	140490034416464 [label=AccumulateGrad]
	140490034426304 -> 140490034476128
	140490034426304 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 3, 768)"]
	140490034464416 -> 140490034426304
	140490034418576 -> 140490034476128
	140490034418576 [label=TBackward0]
	140490034472768 -> 140490034418576
	140490033695152 [label="q_encoder.encoder.information_exchanging_layer.8.attention.self.key.weight
 (768, 768)" fillcolor=lightblue]
	140490033695152 -> 140490034472768
	140490034472768 [label=AccumulateGrad]
	140490034466480 -> 140490034466192
	140490034466480 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 3, 64)"]
	140490034466672 -> 140490034466480
	140490034466672 [label=CloneBackward0]
	140490034467392 -> 140490034466672
	140490034467392 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 3, 64)"]
	140490034467824 -> 140490034467392
	140490034467824 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140490034468592 -> 140490034467824
	140490034468592 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 3, 768)"]
	140490034469984 -> 140490034468592
	140490034469984 [label="ViewBackward0
------------------------
self_sym_sizes: (9, 768)"]
	140490034474112 -> 140490034469984
	140490034474112 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (9, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140490034477472 -> 140490034474112
	140490033695392 [label="q_encoder.encoder.information_exchanging_layer.8.attention.self.value.bias
 (768)" fillcolor=lightblue]
	140490033695392 -> 140490034477472
	140490034477472 [label=AccumulateGrad]
	140490034469840 -> 140490034474112
	140490034469840 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 3, 768)"]
	140490034464416 -> 140490034469840
	140490034466624 -> 140490034474112
	140490034466624 [label=TBackward0]
	140490034467728 -> 140490034466624
	140490033695312 [label="q_encoder.encoder.information_exchanging_layer.8.attention.self.value.weight
 (768, 768)" fillcolor=lightblue]
	140490033695312 -> 140490034467728
	140490034467728 [label=AccumulateGrad]
	140490034465760 -> 140490034465184
	140490034465760 [label=TBackward0]
	140490034465568 -> 140490034465760
	140490033695472 [label="q_encoder.encoder.information_exchanging_layer.8.attention.output.dense.weight
 (768, 768)" fillcolor=lightblue]
	140490033695472 -> 140490034465568
	140490034465568 [label=AccumulateGrad]
	140490034464416 -> 140490034464320
	140490034464080 -> 140490034510192
	140490033695712 [label="q_encoder.encoder.information_exchanging_layer.8.attention.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	140490033695712 -> 140490034464080
	140490034464080 [label=AccumulateGrad]
	140490034464704 -> 140490034510192
	140490033695632 [label="q_encoder.encoder.information_exchanging_layer.8.attention.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	140490033695632 -> 140490034464704
	140490034464704 [label=AccumulateGrad]
	140490034464224 -> 140490034463552
	140490034464224 [label=TBackward0]
	140490034464608 -> 140490034464224
	140490033695872 [label="q_encoder.encoder.information_exchanging_layer.8.intermediate.dense.weight
 (3072, 768)" fillcolor=lightblue]
	140490033695872 -> 140490034464608
	140490034464608 [label=AccumulateGrad]
	140490034511776 -> 140490034511632
	140490034511776 [label=TBackward0]
	140490034463072 -> 140490034511776
	140490033696032 [label="q_encoder.encoder.information_exchanging_layer.8.output.dense.weight
 (768, 3072)" fillcolor=lightblue]
	140490033696032 -> 140490034463072
	140490034463072 [label=AccumulateGrad]
	140490034510192 -> 140490034511008
	140490034511056 -> 140490034510000
	140490033696272 [label="q_encoder.encoder.information_exchanging_layer.8.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	140490033696272 -> 140490034511056
	140490034511056 [label=AccumulateGrad]
	140490034511200 -> 140490034510000
	140490033696192 [label="q_encoder.encoder.information_exchanging_layer.8.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	140490033696192 -> 140490034511200
	140490034511200 [label=AccumulateGrad]
	140490034509664 -> 140490034503712
	140490034509664 [label=CloneBackward0]
	140490034509856 -> 140490034509664
	140490034509856 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 2, 768)
start         :                   0
step          :                   1"]
	140490034510672 -> 140490034509856
	140490034510672 [label="SelectBackward0
--------------------------------
dim           :                2
index         :                1
self_sym_sizes: (3, 2, 502, 768)"]
	140490034511344 -> 140490034510672
	140490034511344 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:    (3, 2, 502, 768)
start         :                   0
step          :                   1"]
	140490034509472 -> 140490034511344
	140490034509472 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:    (3, 2, 502, 768)
start         :                   0
step          :                   1"]
	140490034465904 -> 140490034509472
	140490034465904 [label="ViewBackward0
-----------------------------
self_sym_sizes: (6, 502, 768)"]
	140490034465616 -> 140490034465904
	140490034465616 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140490034463264 -> 140490034465616
	140490034463264 [label="AddBackward0
------------
alpha: 1"]
	140490034464896 -> 140490034463264
	140490034464896 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140490034463888 -> 140490034464896
	140490034463888 [label="ViewBackward0
---------------------------
self_sym_sizes: (3012, 768)"]
	140490034471184 -> 140490034463888
	140490034471184 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :   (3012, 3072)
mat1_sym_strides:      (3072, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (3072, 768)
mat2_sym_strides:      (1, 3072)"]
	140490034468928 -> 140490034471184
	140490035204400 [label="q_encoder.encoder.text_encoding_layer.9.output.dense.bias
 (768)" fillcolor=lightblue]
	140490035204400 -> 140490034468928
	140490034468928 [label=AccumulateGrad]
	140490034467968 -> 140490034471184
	140490034467968 [label="ViewBackward0
------------------------------
self_sym_sizes: (6, 502, 3072)"]
	140490034475792 -> 140490034467968
	140490034475792 [label="GeluBackward0
---------------------------
approximate:           none
self       : [saved tensor]"]
	140488973631936 -> 140490034475792
	140488973631936 [label="ViewBackward0
----------------------------
self_sym_sizes: (3012, 3072)"]
	140488973632368 -> 140488973631936
	140488973632368 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :    (3012, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (768, 3072)
mat2_sym_strides:       (1, 768)"]
	140488973632608 -> 140488973632368
	140490035205520 [label="q_encoder.encoder.text_encoding_layer.9.intermediate.dense.bias
 (3072)" fillcolor=lightblue]
	140490035205520 -> 140488973632608
	140488973632608 [label=AccumulateGrad]
	140488973637216 -> 140488973632368
	140488973637216 [label="ViewBackward0
-----------------------------
self_sym_sizes: (6, 502, 768)"]
	140490034466288 -> 140488973637216
	140490034466288 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140488973638272 -> 140490034466288
	140488973638272 [label="AddBackward0
------------
alpha: 1"]
	140488973639712 -> 140488973638272
	140488973639712 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140488973638992 -> 140488973639712
	140488973638992 [label="ViewBackward0
---------------------------
self_sym_sizes: (3012, 768)"]
	140488973641968 -> 140488973638992
	140488973641968 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :    (3012, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140488973642160 -> 140488973641968
	140490035205120 [label="q_encoder.encoder.text_encoding_layer.9.attention.output.dense.bias
 (768)" fillcolor=lightblue]
	140490035205120 -> 140488973642160
	140488973642160 [label=AccumulateGrad]
	140488973641296 -> 140488973641968
	140488973641296 [label="ViewBackward0
-----------------------------
self_sym_sizes: (6, 502, 768)"]
	140488973642544 -> 140488973641296
	140488973642544 [label="ViewBackward0
--------------------------------
self_sym_sizes: (6, 502, 12, 64)"]
	140488973642256 -> 140488973642544
	140488973642256 [label=CloneBackward0]
	140488973637504 -> 140488973642256
	140488973637504 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140488973642976 -> 140488973637504
	140488973642976 [label="UnsafeViewBackward0
-----------------------------
self_sym_sizes: (72, 502, 64)"]
	140488973643168 -> 140488973642976
	140488973643168 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140488973643456 -> 140488973643168
	140488973643456 [label="ViewBackward0
---------------------------------
self_sym_sizes: (6, 12, 502, 502)"]
	140488973643600 -> 140488973643456
	140488973643600 [label="ExpandBackward0
---------------------------------
self_sym_sizes: (6, 12, 502, 502)"]
	140488973643792 -> 140488973643600
	140488973643792 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140488973644032 -> 140488973643792
	140488973644032 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140488973644320 -> 140488973644032
	140488973644320 [label="AddBackward0
------------
alpha: 1"]
	140488973644368 -> 140488973644320
	140488973644368 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140488973644560 -> 140488973644368
	140488973644560 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (72, 502, 502)"]
	140488973644752 -> 140488973644560
	140488973644752 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140488973644800 -> 140488973644752
	140488973644800 [label="UnsafeViewBackward0
--------------------------------
self_sym_sizes: (6, 12, 502, 64)"]
	140488973644896 -> 140488973644800
	140488973644896 [label=CloneBackward0]
	140488973645088 -> 140488973644896
	140488973645088 [label="ExpandBackward0
--------------------------------
self_sym_sizes: (6, 12, 502, 64)"]
	140488973645136 -> 140488973645088
	140488973645136 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140488973645376 -> 140488973645136
	140488973645376 [label="ViewBackward0
-----------------------------
self_sym_sizes: (6, 502, 768)"]
	140488973645424 -> 140488973645376
	140488973645424 [label="ViewBackward0
---------------------------
self_sym_sizes: (3012, 768)"]
	140488973645712 -> 140488973645424
	140488973645712 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :    (3012, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140488973645808 -> 140488973645712
	140490035204640 [label="q_encoder.encoder.text_encoding_layer.9.attention.self.query.bias
 (768)" fillcolor=lightblue]
	140490035204640 -> 140488973645808
	140488973645808 [label=AccumulateGrad]
	140488973645760 -> 140488973645712
	140488973645760 [label="ViewBackward0
-----------------------------
self_sym_sizes: (6, 502, 768)"]
	140488973640384 -> 140488973645760
	140488973640384 [label="ViewBackward0
--------------------------------
self_sym_sizes: (3, 2, 502, 768)"]
	140488973646240 -> 140488973640384
	140488973646240 [label="AsStridedBackward0
----------------------------------------
size          :         (3, 2, 502, 768)
storage_offset:                        0
stride        : (771072, 385536, 768, 1)"]
	140488973646384 -> 140488973646240
	140488973646384 [label=CopySlices]
	140490034474832 -> 140488973646384
	140488973646624 -> 140488973646384
	140488973646624 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 2, 768)
start         :                   0
step          :                   1"]
	140488973646528 -> 140488973646624
	140488973646528 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:         (3, 3, 768)
start         :                   1
step          :                   1"]
	140488973647200 -> 140488973646528
	140488973647200 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:         (3, 3, 768)
start         :                   0
step          :                   1"]
	140490034510000 -> 140488973647200
	140488973645040 -> 140488973645712
	140488973645040 [label=TBackward0]
	140488973646048 -> 140488973645040
	140490035204560 [label="q_encoder.encoder.text_encoding_layer.9.attention.self.query.weight
 (768, 768)" fillcolor=lightblue]
	140490035204560 -> 140488973646048
	140488973646048 [label=AccumulateGrad]
	140488973644464 -> 140488973644752
	140488973644464 [label="UnsafeViewBackward0
--------------------------------
self_sym_sizes: (6, 12, 64, 502)"]
	140488973644656 -> 140488973644464
	140488973644656 [label=CloneBackward0]
	140488973645616 -> 140488973644656
	140488973645616 [label="ExpandBackward0
--------------------------------
self_sym_sizes: (6, 12, 64, 502)"]
	140488973645520 -> 140488973645616
	140488973645520 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551615
dim1: 18446744073709551614"]
	140488973646576 -> 140488973645520
	140488973646576 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140488973647392 -> 140488973646576
	140488973647392 [label="ViewBackward0
-----------------------------
self_sym_sizes: (6, 502, 768)"]
	140488973645904 -> 140488973647392
	140488973645904 [label="ViewBackward0
---------------------------
self_sym_sizes: (3012, 768)"]
	140488973646336 -> 140488973645904
	140488973646336 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :    (3012, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140488973647056 -> 140488973646336
	140490035204800 [label="q_encoder.encoder.text_encoding_layer.9.attention.self.key.bias
 (768)" fillcolor=lightblue]
	140490035204800 -> 140488973647056
	140488973647056 [label=AccumulateGrad]
	140488973647536 -> 140488973646336
	140488973647536 [label="ViewBackward0
-----------------------------
self_sym_sizes: (6, 502, 768)"]
	140488973640384 -> 140488973647536
	140488973644944 -> 140488973646336
	140488973644944 [label=TBackward0]
	140488973647632 -> 140488973644944
	140490035204720 [label="q_encoder.encoder.text_encoding_layer.9.attention.self.key.weight
 (768, 768)" fillcolor=lightblue]
	140490035204720 -> 140488973647632
	140488973647632 [label=AccumulateGrad]
	140488973643120 -> 140488973643168
	140488973643120 [label="UnsafeViewBackward0
--------------------------------
self_sym_sizes: (6, 12, 502, 64)"]
	140488973643552 -> 140488973643120
	140488973643552 [label=CloneBackward0]
	140488973644128 -> 140488973643552
	140488973644128 [label="ExpandBackward0
--------------------------------
self_sym_sizes: (6, 12, 502, 64)"]
	140488973644224 -> 140488973644128
	140488973644224 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140488973645232 -> 140488973644224
	140488973645232 [label="ViewBackward0
-----------------------------
self_sym_sizes: (6, 502, 768)"]
	140488973646432 -> 140488973645232
	140488973646432 [label="ViewBackward0
---------------------------
self_sym_sizes: (3012, 768)"]
	140488973646720 -> 140488973646432
	140488973646720 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :    (3012, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140488973647152 -> 140488973646720
	140490035204960 [label="q_encoder.encoder.text_encoding_layer.9.attention.self.value.bias
 (768)" fillcolor=lightblue]
	140490035204960 -> 140488973647152
	140488973647152 [label=AccumulateGrad]
	140488973646144 -> 140488973646720
	140488973646144 [label="ViewBackward0
-----------------------------
self_sym_sizes: (6, 502, 768)"]
	140488973640384 -> 140488973646144
	140488973643744 -> 140488973646720
	140488973643744 [label=TBackward0]
	140488973647296 -> 140488973643744
	140490035204880 [label="q_encoder.encoder.text_encoding_layer.9.attention.self.value.weight
 (768, 768)" fillcolor=lightblue]
	140490035204880 -> 140488973647296
	140488973647296 [label=AccumulateGrad]
	140488973641392 -> 140488973641968
	140488973641392 [label=TBackward0]
	140488973642352 -> 140488973641392
	140490035205040 [label="q_encoder.encoder.text_encoding_layer.9.attention.output.dense.weight
 (768, 768)" fillcolor=lightblue]
	140490035205040 -> 140488973642352
	140488973642352 [label=AccumulateGrad]
	140488973640384 -> 140488973638272
	140488973637120 -> 140490034466288
	140490035205280 [label="q_encoder.encoder.text_encoding_layer.9.attention.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	140490035205280 -> 140488973637120
	140488973637120 [label=AccumulateGrad]
	140488973638560 -> 140490034466288
	140490035205200 [label="q_encoder.encoder.text_encoding_layer.9.attention.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	140490035205200 -> 140488973638560
	140488973638560 [label=AccumulateGrad]
	140488973636736 -> 140488973632368
	140488973636736 [label=TBackward0]
	140488973641488 -> 140488973636736
	140490035205440 [label="q_encoder.encoder.text_encoding_layer.9.intermediate.dense.weight
 (3072, 768)" fillcolor=lightblue]
	140490035205440 -> 140488973641488
	140488973641488 [label=AccumulateGrad]
	140490034467296 -> 140490034471184
	140490034467296 [label=TBackward0]
	140490034464512 -> 140490034467296
	140490035205360 [label="q_encoder.encoder.text_encoding_layer.9.output.dense.weight
 (768, 3072)" fillcolor=lightblue]
	140490035205360 -> 140490034464512
	140490034464512 [label=AccumulateGrad]
	140490034466288 -> 140490034463264
	140490034466048 -> 140490034465616
	140490035205760 [label="q_encoder.encoder.text_encoding_layer.9.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	140490035205760 -> 140490034466048
	140490034466048 [label=AccumulateGrad]
	140490034464128 -> 140490034465616
	140490035205600 [label="q_encoder.encoder.text_encoding_layer.9.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	140490035205600 -> 140490034464128
	140490034464128 [label=AccumulateGrad]
	140490034509760 -> 140490034509040
	140490034509760 [label=TBackward0]
	140490034509904 -> 140490034509760
	140490033695792 [label="q_encoder.encoder.information_exchanging_layer.9.attention.self.query.weight
 (768, 768)" fillcolor=lightblue]
	140490033695792 -> 140490034509904
	140490034509904 [label=AccumulateGrad]
	140490034507504 -> 140490034507744
	140490034507504 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 64, 3)"]
	140490034507840 -> 140490034507504
	140490034507840 [label=CloneBackward0]
	140490034508752 -> 140490034507840
	140490034508752 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 64, 3)"]
	140490034508944 -> 140490034508752
	140490034508944 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551615
dim1: 18446744073709551614"]
	140490034511584 -> 140490034508944
	140490034511584 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140490034511104 -> 140490034511584
	140490034511104 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 3, 768)"]
	140490034507456 -> 140490034511104
	140490034507456 [label="ViewBackward0
------------------------
self_sym_sizes: (9, 768)"]
	140490034467152 -> 140490034507456
	140490034467152 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (9, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140490034465088 -> 140490034467152
	140490033696512 [label="q_encoder.encoder.information_exchanging_layer.9.attention.self.key.bias
 (768)" fillcolor=lightblue]
	140490033696512 -> 140490034465088
	140490034465088 [label=AccumulateGrad]
	140490034468304 -> 140490034467152
	140490034468304 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 3, 768)"]
	140490034503712 -> 140490034468304
	140490034463408 -> 140490034467152
	140490034463408 [label=TBackward0]
	140490034466720 -> 140490034463408
	140490033696432 [label="q_encoder.encoder.information_exchanging_layer.9.attention.self.key.weight
 (768, 768)" fillcolor=lightblue]
	140490033696432 -> 140490034466720
	140490034466720 [label=AccumulateGrad]
	140490034505872 -> 140490034505536
	140490034505872 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 3, 64)"]
	140490034506208 -> 140490034505872
	140490034506208 [label=CloneBackward0]
	140490034507216 -> 140490034506208
	140490034507216 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 3, 64)"]
	140490034507552 -> 140490034507216
	140490034507552 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140490034508224 -> 140490034507552
	140490034508224 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 3, 768)"]
	140490034509712 -> 140490034508224
	140490034509712 [label="ViewBackward0
------------------------
self_sym_sizes: (9, 768)"]
	140490034509568 -> 140490034509712
	140490034509568 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (9, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140490034506064 -> 140490034509568
	140490033696672 [label="q_encoder.encoder.information_exchanging_layer.9.attention.self.value.bias
 (768)" fillcolor=lightblue]
	140490033696672 -> 140490034506064
	140490034506064 [label=AccumulateGrad]
	140490034463168 -> 140490034509568
	140490034463168 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 3, 768)"]
	140490034503712 -> 140490034463168
	140490034472432 -> 140490034509568
	140490034472432 [label=TBackward0]
	140488973637024 -> 140490034472432
	140490033696592 [label="q_encoder.encoder.information_exchanging_layer.9.attention.self.value.weight
 (768, 768)" fillcolor=lightblue]
	140490033696592 -> 140488973637024
	140488973637024 [label=AccumulateGrad]
	140490034505008 -> 140490034504432
	140490034505008 [label=TBackward0]
	140490034504816 -> 140490034505008
	140490033696752 [label="q_encoder.encoder.information_exchanging_layer.9.attention.output.dense.weight
 (768, 768)" fillcolor=lightblue]
	140490033696752 -> 140490034504816
	140490034504816 [label=AccumulateGrad]
	140490034503712 -> 140490034503664
	140490034503424 -> 140490034500880
	140490033696992 [label="q_encoder.encoder.information_exchanging_layer.9.attention.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	140490033696992 -> 140490034503424
	140490034503424 [label=AccumulateGrad]
	140490034504048 -> 140490034500880
	140490033696912 [label="q_encoder.encoder.information_exchanging_layer.9.attention.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	140490033696912 -> 140490034504048
	140490034504048 [label=AccumulateGrad]
	140490034503520 -> 140490034503040
	140490034503520 [label=TBackward0]
	140490034504000 -> 140490034503520
	140490033697152 [label="q_encoder.encoder.information_exchanging_layer.9.intermediate.dense.weight
 (3072, 768)" fillcolor=lightblue]
	140490033697152 -> 140490034504000
	140490034504000 [label=AccumulateGrad]
	140490034502896 -> 140490034502272
	140490034502896 [label=TBackward0]
	140490034502512 -> 140490034502896
	140490033697312 [label="q_encoder.encoder.information_exchanging_layer.9.output.dense.weight
 (768, 3072)" fillcolor=lightblue]
	140490033697312 -> 140490034502512
	140490034502512 [label=AccumulateGrad]
	140490034500880 -> 140490034501648
	140490034501696 -> 140490034500736
	140490033697552 [label="q_encoder.encoder.information_exchanging_layer.9.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	140490033697552 -> 140490034501696
	140490034501696 [label=AccumulateGrad]
	140490034501888 -> 140490034500736
	140490033697472 [label="q_encoder.encoder.information_exchanging_layer.9.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	140490033697472 -> 140490034501888
	140490034501888 [label=AccumulateGrad]
	140490034500256 -> 140490034904032
	140490034500256 [label=CloneBackward0]
	140490034500640 -> 140490034500256
	140490034500640 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 2, 768)
start         :                   0
step          :                   1"]
	140490034501360 -> 140490034500640
	140490034501360 [label="SelectBackward0
--------------------------------
dim           :                2
index         :                1
self_sym_sizes: (3, 2, 502, 768)"]
	140490034503904 -> 140490034501360
	140490034503904 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:    (3, 2, 502, 768)
start         :                   0
step          :                   1"]
	140490034502080 -> 140490034503904
	140490034502080 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:    (3, 2, 502, 768)
start         :                   0
step          :                   1"]
	140490034505200 -> 140490034502080
	140490034505200 [label="ViewBackward0
-----------------------------
self_sym_sizes: (6, 502, 768)"]
	140490034504960 -> 140490034505200
	140490034504960 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140490034503328 -> 140490034504960
	140490034503328 [label="AddBackward0
------------
alpha: 1"]
	140490034504240 -> 140490034503328
	140490034504240 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140490034501792 -> 140490034504240
	140490034501792 [label="ViewBackward0
---------------------------
self_sym_sizes: (3012, 768)"]
	140490034510864 -> 140490034501792
	140490034510864 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :   (3012, 3072)
mat1_sym_strides:      (3072, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (3072, 768)
mat2_sym_strides:      (1, 3072)"]
	140490034465664 -> 140490034510864
	140490035205680 [label="q_encoder.encoder.text_encoding_layer.10.output.dense.bias
 (768)" fillcolor=lightblue]
	140490035205680 -> 140490034465664
	140490034465664 [label=AccumulateGrad]
	140490034507696 -> 140490034510864
	140490034507696 [label="ViewBackward0
------------------------------
self_sym_sizes: (6, 502, 3072)"]
	140490034502560 -> 140490034507696
	140490034502560 [label="GeluBackward0
---------------------------
approximate:           none
self       : [saved tensor]"]
	140488973637984 -> 140490034502560
	140488973637984 [label="ViewBackward0
----------------------------
self_sym_sizes: (3012, 3072)"]
	140488973637744 -> 140488973637984
	140488973637744 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :    (3012, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (768, 3072)
mat2_sym_strides:       (1, 768)"]
	140488973637264 -> 140488973637744
	140490035206800 [label="q_encoder.encoder.text_encoding_layer.10.intermediate.dense.bias
 (3072)" fillcolor=lightblue]
	140490035206800 -> 140488973637264
	140488973637264 [label=AccumulateGrad]
	140488973642832 -> 140488973637744
	140488973642832 [label="ViewBackward0
-----------------------------
self_sym_sizes: (6, 502, 768)"]
	140490034505632 -> 140488973642832
	140490034505632 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140488973643888 -> 140490034505632
	140488973643888 [label="AddBackward0
------------
alpha: 1"]
	140488973645280 -> 140488973643888
	140488973645280 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140488973644704 -> 140488973645280
	140488973644704 [label="ViewBackward0
---------------------------
self_sym_sizes: (3012, 768)"]
	140488973647488 -> 140488973644704
	140488973647488 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :    (3012, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140488973647776 -> 140488973647488
	140490035206400 [label="q_encoder.encoder.text_encoding_layer.10.attention.output.dense.bias
 (768)" fillcolor=lightblue]
	140490035206400 -> 140488973647776
	140488973647776 [label=AccumulateGrad]
	140488973646768 -> 140488973647488
	140488973646768 [label="ViewBackward0
-----------------------------
self_sym_sizes: (6, 502, 768)"]
	140488973533536 -> 140488973646768
	140488973533536 [label="ViewBackward0
--------------------------------
self_sym_sizes: (6, 502, 12, 64)"]
	140488973533344 -> 140488973533536
	140488973533344 [label=CloneBackward0]
	140488973533584 -> 140488973533344
	140488973533584 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140488973533824 -> 140488973533584
	140488973533824 [label="UnsafeViewBackward0
-----------------------------
self_sym_sizes: (72, 502, 64)"]
	140488973534016 -> 140488973533824
	140488973534016 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140488973534160 -> 140488973534016
	140488973534160 [label="ViewBackward0
---------------------------------
self_sym_sizes: (6, 12, 502, 502)"]
	140488973534256 -> 140488973534160
	140488973534256 [label="ExpandBackward0
---------------------------------
self_sym_sizes: (6, 12, 502, 502)"]
	140488973534592 -> 140488973534256
	140488973534592 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140488973534784 -> 140488973534592
	140488973534784 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140488973535072 -> 140488973534784
	140488973535072 [label="AddBackward0
------------
alpha: 1"]
	140488973535216 -> 140488973535072
	140488973535216 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140488973535360 -> 140488973535216
	140488973535360 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (72, 502, 502)"]
	140488973535648 -> 140488973535360
	140488973535648 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140488973535792 -> 140488973535648
	140488973535792 [label="UnsafeViewBackward0
--------------------------------
self_sym_sizes: (6, 12, 502, 64)"]
	140488973535696 -> 140488973535792
	140488973535696 [label=CloneBackward0]
	140488973536128 -> 140488973535696
	140488973536128 [label="ExpandBackward0
--------------------------------
self_sym_sizes: (6, 12, 502, 64)"]
	140488973536176 -> 140488973536128
	140488973536176 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140488973536464 -> 140488973536176
	140488973536464 [label="ViewBackward0
-----------------------------
self_sym_sizes: (6, 502, 768)"]
	140488973536656 -> 140488973536464
	140488973536656 [label="ViewBackward0
---------------------------
self_sym_sizes: (3012, 768)"]
	140488973536800 -> 140488973536656
	140488973536800 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :    (3012, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140488973536944 -> 140488973536800
	140490035205920 [label="q_encoder.encoder.text_encoding_layer.10.attention.self.query.bias
 (768)" fillcolor=lightblue]
	140490035205920 -> 140488973536944
	140488973536944 [label=AccumulateGrad]
	140488973536848 -> 140488973536800
	140488973536848 [label="ViewBackward0
-----------------------------
self_sym_sizes: (6, 502, 768)"]
	140488973646096 -> 140488973536848
	140488973646096 [label="ViewBackward0
--------------------------------
self_sym_sizes: (3, 2, 502, 768)"]
	140488973537424 -> 140488973646096
	140488973537424 [label="AsStridedBackward0
----------------------------------------
size          :         (3, 2, 502, 768)
storage_offset:                        0
stride        : (771072, 385536, 768, 1)"]
	140488973537568 -> 140488973537424
	140488973537568 [label=CopySlices]
	140490034465616 -> 140488973537568
	140488973537904 -> 140488973537568
	140488973537904 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 2, 768)
start         :                   0
step          :                   1"]
	140488973537760 -> 140488973537904
	140488973537760 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:         (3, 3, 768)
start         :                   1
step          :                   1"]
	140488973538576 -> 140488973537760
	140488973538576 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:         (3, 3, 768)
start         :                   0
step          :                   1"]
	140490034500736 -> 140488973538576
	140488973535984 -> 140488973536800
	140488973535984 [label=TBackward0]
	140488973537232 -> 140488973535984
	140490035205840 [label="q_encoder.encoder.text_encoding_layer.10.attention.self.query.weight
 (768, 768)" fillcolor=lightblue]
	140490035205840 -> 140488973537232
	140488973537232 [label=AccumulateGrad]
	140488973535264 -> 140488973535648
	140488973535264 [label="UnsafeViewBackward0
--------------------------------
self_sym_sizes: (6, 12, 64, 502)"]
	140488973535456 -> 140488973535264
	140488973535456 [label=CloneBackward0]
	140488973536704 -> 140488973535456
	140488973536704 [label="ExpandBackward0
--------------------------------
self_sym_sizes: (6, 12, 64, 502)"]
	140488973536512 -> 140488973536704
	140488973536512 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551615
dim1: 18446744073709551614"]
	140488973537856 -> 140488973536512
	140488973537856 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140488973538672 -> 140488973537856
	140488973538672 [label="ViewBackward0
-----------------------------
self_sym_sizes: (6, 502, 768)"]
	140488973537040 -> 140488973538672
	140488973537040 [label="ViewBackward0
---------------------------
self_sym_sizes: (3012, 768)"]
	140488973537520 -> 140488973537040
	140488973537520 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :    (3012, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140488973538432 -> 140488973537520
	140490035206080 [label="q_encoder.encoder.text_encoding_layer.10.attention.self.key.bias
 (768)" fillcolor=lightblue]
	140490035206080 -> 140488973538432
	140488973538432 [label=AccumulateGrad]
	140488973538768 -> 140488973537520
	140488973538768 [label="ViewBackward0
-----------------------------
self_sym_sizes: (6, 502, 768)"]
	140488973646096 -> 140488973538768
	140488973535840 -> 140488973537520
	140488973535840 [label=TBackward0]
	140488973538912 -> 140488973535840
	140490035206000 [label="q_encoder.encoder.text_encoding_layer.10.attention.self.key.weight
 (768, 768)" fillcolor=lightblue]
	140490035206000 -> 140488973538912
	140488973538912 [label=AccumulateGrad]
	140488973533968 -> 140488973534016
	140488973533968 [label="UnsafeViewBackward0
--------------------------------
self_sym_sizes: (6, 12, 502, 64)"]
	140488973534208 -> 140488973533968
	140488973534208 [label=CloneBackward0]
	140488973534832 -> 140488973534208
	140488973534832 [label="ExpandBackward0
--------------------------------
self_sym_sizes: (6, 12, 502, 64)"]
	140488973534928 -> 140488973534832
	140488973534928 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140488973536272 -> 140488973534928
	140488973536272 [label="ViewBackward0
-----------------------------
self_sym_sizes: (6, 502, 768)"]
	140488973537616 -> 140488973536272
	140488973537616 [label="ViewBackward0
---------------------------
self_sym_sizes: (3012, 768)"]
	140488973538000 -> 140488973537616
	140488973538000 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :    (3012, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140488973538480 -> 140488973538000
	140490035206240 [label="q_encoder.encoder.text_encoding_layer.10.attention.self.value.bias
 (768)" fillcolor=lightblue]
	140490035206240 -> 140488973538480
	140488973538480 [label=AccumulateGrad]
	140488973537376 -> 140488973538000
	140488973537376 [label="ViewBackward0
-----------------------------
self_sym_sizes: (6, 502, 768)"]
	140488973646096 -> 140488973537376
	140488973534448 -> 140488973538000
	140488973534448 [label=TBackward0]
	140488973538624 -> 140488973534448
	140490035206160 [label="q_encoder.encoder.text_encoding_layer.10.attention.self.value.weight
 (768, 768)" fillcolor=lightblue]
	140490035206160 -> 140488973538624
	140488973538624 [label=AccumulateGrad]
	140488973646816 -> 140488973647488
	140488973646816 [label=TBackward0]
	140488973533440 -> 140488973646816
	140490035206320 [label="q_encoder.encoder.text_encoding_layer.10.attention.output.dense.weight
 (768, 768)" fillcolor=lightblue]
	140490035206320 -> 140488973533440
	140488973533440 [label=AccumulateGrad]
	140488973646096 -> 140488973643888
	140488973642688 -> 140490034505632
	140490035206560 [label="q_encoder.encoder.text_encoding_layer.10.attention.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	140490035206560 -> 140488973642688
	140488973642688 [label=AccumulateGrad]
	140488973644272 -> 140490034505632
	140490035206480 [label="q_encoder.encoder.text_encoding_layer.10.attention.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	140490035206480 -> 140488973644272
	140488973644272 [label=AccumulateGrad]
	140488973642208 -> 140488973637744
	140488973642208 [label=TBackward0]
	140488973646864 -> 140488973642208
	140490035206720 [label="q_encoder.encoder.text_encoding_layer.10.intermediate.dense.weight
 (3072, 768)" fillcolor=lightblue]
	140490035206720 -> 140488973646864
	140488973646864 [label=AccumulateGrad]
	140490034508464 -> 140490034510864
	140490034508464 [label=TBackward0]
	140488973635968 -> 140490034508464
	140490035206640 [label="q_encoder.encoder.text_encoding_layer.10.output.dense.weight
 (768, 3072)" fillcolor=lightblue]
	140490035206640 -> 140488973635968
	140488973635968 [label=AccumulateGrad]
	140490034505632 -> 140490034503328
	140490034505344 -> 140490034504960
	140490035207040 [label="q_encoder.encoder.text_encoding_layer.10.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	140490035207040 -> 140490034505344
	140490034505344 [label=AccumulateGrad]
	140490034499872 -> 140490034504960
	140490035206880 [label="q_encoder.encoder.text_encoding_layer.10.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	140490035206880 -> 140490034499872
	140490034499872 [label=AccumulateGrad]
	140490034500400 -> 140490034499488
	140490034500400 [label=TBackward0]
	140490034500544 -> 140490034500400
	140490033697632 [label="q_encoder.encoder.information_exchanging_layer.10.attention.self.query.weight
 (768, 768)" fillcolor=lightblue]
	140490033697632 -> 140490034500544
	140490034500544 [label=AccumulateGrad]
	140490034497760 -> 140490034498192
	140490034497760 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 64, 3)"]
	140490034498288 -> 140490034497760
	140490034498288 [label=CloneBackward0]
	140490034499056 -> 140490034498288
	140490034499056 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 64, 3)"]
	140490034499440 -> 140490034499056
	140490034499440 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551615
dim1: 18446744073709551614"]
	140490034502224 -> 140490034499440
	140490034502224 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140490034504864 -> 140490034502224
	140490034504864 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 3, 768)"]
	140490034503472 -> 140490034504864
	140490034503472 [label="ViewBackward0
------------------------
self_sym_sizes: (9, 768)"]
	140490034506592 -> 140490034503472
	140490034506592 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (9, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140490034502800 -> 140490034506592
	140490033697872 [label="q_encoder.encoder.information_exchanging_layer.10.attention.self.key.bias
 (768)" fillcolor=lightblue]
	140490033697872 -> 140490034502800
	140490034502800 [label=AccumulateGrad]
	140490034507936 -> 140490034506592
	140490034507936 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 3, 768)"]
	140490034904032 -> 140490034507936
	140490034497712 -> 140490034506592
	140490034497712 [label=TBackward0]
	140490034506256 -> 140490034497712
	140490033697792 [label="q_encoder.encoder.information_exchanging_layer.10.attention.self.key.weight
 (768, 768)" fillcolor=lightblue]
	140490033697792 -> 140490034506256
	140490034506256 [label=AccumulateGrad]
	140490034496464 -> 140490034496224
	140490034496464 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 3, 64)"]
	140490034496896 -> 140490034496464
	140490034496896 [label=CloneBackward0]
	140490034497568 -> 140490034496896
	140490034497568 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 3, 64)"]
	140490034497808 -> 140490034497568
	140490034497808 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140490034498672 -> 140490034497808
	140490034498672 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 3, 768)"]
	140490034500352 -> 140490034498672
	140490034500352 [label="ViewBackward0
------------------------
self_sym_sizes: (9, 768)"]
	140490034504384 -> 140490034500352
	140490034504384 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (9, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140490034507024 -> 140490034504384
	140490033698032 [label="q_encoder.encoder.information_exchanging_layer.10.attention.self.value.bias
 (768)" fillcolor=lightblue]
	140490033698032 -> 140490034507024
	140490034507024 [label=AccumulateGrad]
	140490034500016 -> 140490034504384
	140490034500016 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 3, 768)"]
	140490034904032 -> 140490034500016
	140490034496800 -> 140490034504384
	140490034496800 [label=TBackward0]
	140488973642640 -> 140490034496800
	140490033697952 [label="q_encoder.encoder.information_exchanging_layer.10.attention.self.value.weight
 (768, 768)" fillcolor=lightblue]
	140490033697952 -> 140488973642640
	140488973642640 [label=AccumulateGrad]
	140490034905040 -> 140490034904704
	140490034905040 [label=TBackward0]
	140490034904848 -> 140490034905040
	140490033698112 [label="q_encoder.encoder.information_exchanging_layer.10.attention.output.dense.weight
 (768, 768)" fillcolor=lightblue]
	140490033698112 -> 140490034904848
	140490034904848 [label=AccumulateGrad]
	140490034904032 -> 140490034903984
	140490034903792 -> 140490034900912
	140490033698352 [label="q_encoder.encoder.information_exchanging_layer.10.attention.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	140490033698352 -> 140490034903792
	140490034903792 [label=AccumulateGrad]
	140490034904416 -> 140490034900912
	140490033698272 [label="q_encoder.encoder.information_exchanging_layer.10.attention.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	140490033698272 -> 140490034904416
	140490034904416 [label=AccumulateGrad]
	140490034903936 -> 140490034903120
	140490034903936 [label=TBackward0]
	140490034904320 -> 140490034903936
	140490035216240 [label="q_encoder.encoder.information_exchanging_layer.10.intermediate.dense.weight
 (3072, 768)" fillcolor=lightblue]
	140490035216240 -> 140490034904320
	140490034904320 [label=AccumulateGrad]
	140490034902928 -> 140490034902304
	140490034902928 [label=TBackward0]
	140490034902592 -> 140490034902928
	140490033698592 [label="q_encoder.encoder.information_exchanging_layer.10.output.dense.weight
 (768, 3072)" fillcolor=lightblue]
	140490033698592 -> 140490034902592
	140490034902592 [label=AccumulateGrad]
	140490034900912 -> 140490034901680
	140490034901728 -> 140490034900768
	140490033698832 [label="q_encoder.encoder.information_exchanging_layer.10.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	140490033698832 -> 140490034901728
	140490034901728 [label=AccumulateGrad]
	140490034901968 -> 140490034900768
	140490033698752 [label="q_encoder.encoder.information_exchanging_layer.10.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	140490033698752 -> 140490034901968
	140490034901968 [label=AccumulateGrad]
	140490034900336 -> 140490034894384
	140490034900336 [label=CloneBackward0]
	140490034900672 -> 140490034900336
	140490034900672 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 2, 768)
start         :                   0
step          :                   1"]
	140490034901440 -> 140490034900672
	140490034901440 [label="SelectBackward0
--------------------------------
dim           :                2
index         :                1
self_sym_sizes: (3, 2, 502, 768)"]
	140490034904224 -> 140490034901440
	140490034904224 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:    (3, 2, 502, 768)
start         :                   0
step          :                   1"]
	140490034902064 -> 140490034904224
	140490034902064 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:    (3, 2, 502, 768)
start         :                   0
step          :                   1"]
	140490034904944 -> 140490034902064
	140490034904944 [label="ViewBackward0
-----------------------------
self_sym_sizes: (6, 502, 768)"]
	140490034903072 -> 140490034904944
	140490034903072 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140490034901776 -> 140490034903072
	140490034901776 [label="AddBackward0
------------
alpha: 1"]
	140490034496944 -> 140490034901776
	140490034496944 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140490034495984 -> 140490034496944
	140490034495984 [label="ViewBackward0
---------------------------
self_sym_sizes: (3012, 768)"]
	140490034501552 -> 140490034495984
	140490034501552 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :   (3012, 3072)
mat1_sym_strides:      (3072, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (3072, 768)
mat2_sym_strides:      (1, 3072)"]
	140490034498960 -> 140490034501552
	140490035206960 [label="q_encoder.encoder.text_encoding_layer.11.output.dense.bias
 (768)" fillcolor=lightblue]
	140490035206960 -> 140490034498960
	140490034498960 [label=AccumulateGrad]
	140490034498048 -> 140490034501552
	140490034498048 [label="ViewBackward0
------------------------------
self_sym_sizes: (6, 502, 3072)"]
	140490034495888 -> 140490034498048
	140490034495888 [label="GeluBackward0
---------------------------
approximate:           none
self       : [saved tensor]"]
	140488973643696 -> 140490034495888
	140488973643696 [label="ViewBackward0
----------------------------
self_sym_sizes: (3012, 3072)"]
	140488973643312 -> 140488973643696
	140488973643312 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :    (3012, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (768, 3072)
mat2_sym_strides:       (1, 768)"]
	140488973647824 -> 140488973643312
	140490035208080 [label="q_encoder.encoder.text_encoding_layer.11.intermediate.dense.bias
 (3072)" fillcolor=lightblue]
	140490035208080 -> 140488973647824
	140488973647824 [label=AccumulateGrad]
	140488973642880 -> 140488973643312
	140488973642880 [label="ViewBackward0
-----------------------------
self_sym_sizes: (6, 502, 768)"]
	140490034496320 -> 140488973642880
	140490034496320 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (768,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140488973534640 -> 140490034496320
	140488973534640 [label="AddBackward0
------------
alpha: 1"]
	140488973536320 -> 140488973534640
	140488973536320 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140488973535600 -> 140488973536320
	140488973535600 [label="ViewBackward0
---------------------------
self_sym_sizes: (3012, 768)"]
	140488973538720 -> 140488973535600
	140488973538720 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :    (3012, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140488973538960 -> 140488973538720
	140490035207680 [label="q_encoder.encoder.text_encoding_layer.11.attention.output.dense.bias
 (768)" fillcolor=lightblue]
	140490035207680 -> 140488973538960
	140488973538960 [label=AccumulateGrad]
	140488973538192 -> 140488973538720
	140488973538192 [label="ViewBackward0
-----------------------------
self_sym_sizes: (6, 502, 768)"]
	140488973539296 -> 140488973538192
	140488973539296 [label="ViewBackward0
--------------------------------
self_sym_sizes: (6, 502, 12, 64)"]
	140488973539152 -> 140488973539296
	140488973539152 [label=CloneBackward0]
	140488973533872 -> 140488973539152
	140488973533872 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140488973539680 -> 140488973533872
	140488973539680 [label="UnsafeViewBackward0
-----------------------------
self_sym_sizes: (72, 502, 64)"]
	140488973539872 -> 140488973539680
	140488973539872 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140488973540064 -> 140488973539872
	140488973540064 [label="ViewBackward0
---------------------------------
self_sym_sizes: (6, 12, 502, 502)"]
	140488973540256 -> 140488973540064
	140488973540256 [label="ExpandBackward0
---------------------------------
self_sym_sizes: (6, 12, 502, 502)"]
	140488973540400 -> 140488973540256
	140488973540400 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140488973540640 -> 140488973540400
	140488973540640 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140488973540976 -> 140488973540640
	140488973540976 [label="AddBackward0
------------
alpha: 1"]
	140488973541024 -> 140488973540976
	140488973541024 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140488973541408 -> 140488973541024
	140488973541408 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (72, 502, 502)"]
	140488973541504 -> 140488973541408
	140488973541504 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140488973541600 -> 140488973541504
	140488973541600 [label="UnsafeViewBackward0
--------------------------------
self_sym_sizes: (6, 12, 502, 64)"]
	140488973541648 -> 140488973541600
	140488973541648 [label=CloneBackward0]
	140488973541936 -> 140488973541648
	140488973541936 [label="ExpandBackward0
--------------------------------
self_sym_sizes: (6, 12, 502, 64)"]
	140488973542032 -> 140488973541936
	140488973542032 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140488973542272 -> 140488973542032
	140488973542272 [label="ViewBackward0
-----------------------------
self_sym_sizes: (6, 502, 768)"]
	140488973542416 -> 140488973542272
	140488973542416 [label="ViewBackward0
---------------------------
self_sym_sizes: (3012, 768)"]
	140488973542560 -> 140488973542416
	140488973542560 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :    (3012, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140488973542752 -> 140488973542560
	140490035207200 [label="q_encoder.encoder.text_encoding_layer.11.attention.self.query.bias
 (768)" fillcolor=lightblue]
	140490035207200 -> 140488973542752
	140488973542752 [label=AccumulateGrad]
	140488973542656 -> 140488973542560
	140488973542656 [label="ViewBackward0
-----------------------------
self_sym_sizes: (6, 502, 768)"]
	140488973537328 -> 140488973542656
	140488973537328 [label="ViewBackward0
--------------------------------
self_sym_sizes: (3, 2, 502, 768)"]
	140488973543088 -> 140488973537328
	140488973543088 [label="AsStridedBackward0
----------------------------------------
size          :         (3, 2, 502, 768)
storage_offset:                        0
stride        : (771072, 385536, 768, 1)"]
	140488973543280 -> 140488973543088
	140488973543280 [label=CopySlices]
	140490034504960 -> 140488973543280
	140488973543568 -> 140488973543280
	140488973543568 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (3, 2, 768)
start         :                   0
step          :                   1"]
	140488973543376 -> 140488973543568
	140488973543376 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:         (3, 3, 768)
start         :                   1
step          :                   1"]
	140488973544000 -> 140488973543376
	140488973544000 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:         (3, 3, 768)
start         :                   0
step          :                   1"]
	140490034900768 -> 140488973544000
	140488973541792 -> 140488973542560
	140488973541792 [label=TBackward0]
	140488973542944 -> 140488973541792
	140490035207120 [label="q_encoder.encoder.text_encoding_layer.11.attention.self.query.weight
 (768, 768)" fillcolor=lightblue]
	140490035207120 -> 140488973542944
	140488973542944 [label=AccumulateGrad]
	140488973541216 -> 140488973541504
	140488973541216 [label="UnsafeViewBackward0
--------------------------------
self_sym_sizes: (6, 12, 64, 502)"]
	140488973541264 -> 140488973541216
	140488973541264 [label=CloneBackward0]
	140488973542464 -> 140488973541264
	140488973542464 [label="ExpandBackward0
--------------------------------
self_sym_sizes: (6, 12, 64, 502)"]
	140488973542368 -> 140488973542464
	140488973542368 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551615
dim1: 18446744073709551614"]
	140488973543424 -> 140488973542368
	140488973543424 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140488973544144 -> 140488973543424
	140488973544144 [label="ViewBackward0
-----------------------------
self_sym_sizes: (6, 502, 768)"]
	140488973542848 -> 140488973544144
	140488973542848 [label="ViewBackward0
---------------------------
self_sym_sizes: (3012, 768)"]
	140488973543184 -> 140488973542848
	140488973543184 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :    (3012, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140488973543904 -> 140488973543184
	140490035207360 [label="q_encoder.encoder.text_encoding_layer.11.attention.self.key.bias
 (768)" fillcolor=lightblue]
	140490035207360 -> 140488973543904
	140488973543904 [label=AccumulateGrad]
	140488973544288 -> 140488973543184
	140488973544288 [label="ViewBackward0
-----------------------------
self_sym_sizes: (6, 502, 768)"]
	140488973537328 -> 140488973544288
	140488973541696 -> 140488973543184
	140488973541696 [label=TBackward0]
	140488973544384 -> 140488973541696
	140490035207280 [label="q_encoder.encoder.text_encoding_layer.11.attention.self.key.weight
 (768, 768)" fillcolor=lightblue]
	140490035207280 -> 140488973544384
	140488973544384 [label=AccumulateGrad]
	140488973539776 -> 140488973539872
	140488973539776 [label="UnsafeViewBackward0
--------------------------------
self_sym_sizes: (6, 12, 502, 64)"]
	140488973540160 -> 140488973539776
	140488973540160 [label=CloneBackward0]
	140488973540544 -> 140488973540160
	140488973540544 [label="ExpandBackward0
--------------------------------
self_sym_sizes: (6, 12, 502, 64)"]
	140488973540784 -> 140488973540544
	140488973540784 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140488973542128 -> 140488973540784
	140488973542128 [label="ViewBackward0
-----------------------------
self_sym_sizes: (6, 502, 768)"]
	140488973543328 -> 140488973542128
	140488973543328 [label="ViewBackward0
---------------------------
self_sym_sizes: (3012, 768)"]
	140488973543616 -> 140488973543328
	140488973543616 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :    (3012, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140488973543952 -> 140488973543616
	140490035207520 [label="q_encoder.encoder.text_encoding_layer.11.attention.self.value.bias
 (768)" fillcolor=lightblue]
	140490035207520 -> 140488973543952
	140488973543952 [label=AccumulateGrad]
	140488973543040 -> 140488973543616
	140488973543040 [label="ViewBackward0
-----------------------------
self_sym_sizes: (6, 502, 768)"]
	140488973537328 -> 140488973543040
	140488973540352 -> 140488973543616
	140488973540352 [label=TBackward0]
	140488973544048 -> 140488973540352
	140490035207440 [label="q_encoder.encoder.text_encoding_layer.11.attention.self.value.weight
 (768, 768)" fillcolor=lightblue]
	140490035207440 -> 140488973544048
	140488973544048 [label=AccumulateGrad]
	140488973538288 -> 140488973538720
	140488973538288 [label=TBackward0]
	140488973539248 -> 140488973538288
	140490035207600 [label="q_encoder.encoder.text_encoding_layer.11.attention.output.dense.weight
 (768, 768)" fillcolor=lightblue]
	140490035207600 -> 140488973539248
	140488973539248 [label=AccumulateGrad]
	140488973537328 -> 140488973534640
	140488973533776 -> 140490034496320
	140490035207840 [label="q_encoder.encoder.text_encoding_layer.11.attention.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	140490035207840 -> 140488973533776
	140488973533776 [label=AccumulateGrad]
	140488973534976 -> 140490034496320
	140490035207760 [label="q_encoder.encoder.text_encoding_layer.11.attention.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	140490035207760 -> 140488973534976
	140488973534976 [label=AccumulateGrad]
	140488973533296 -> 140488973643312
	140488973533296 [label=TBackward0]
	140488973538336 -> 140488973533296
	140490035208000 [label="q_encoder.encoder.text_encoding_layer.11.intermediate.dense.weight
 (3072, 768)" fillcolor=lightblue]
	140490035208000 -> 140488973538336
	140488973538336 [label=AccumulateGrad]
	140490034495552 -> 140490034501552
	140490034495552 [label=TBackward0]
	140488973641536 -> 140490034495552
	140490035207920 [label="q_encoder.encoder.text_encoding_layer.11.output.dense.weight
 (768, 3072)" fillcolor=lightblue]
	140490035207920 -> 140488973641536
	140488973641536 [label=AccumulateGrad]
	140490034496320 -> 140490034901776
	140490034903696 -> 140490034903072
	140490035208320 [label="q_encoder.encoder.text_encoding_layer.11.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	140490035208320 -> 140490034903696
	140490034903696 [label=AccumulateGrad]
	140490034900144 -> 140490034903072
	140490035208160 [label="q_encoder.encoder.text_encoding_layer.11.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	140490035208160 -> 140490034900144
	140490034900144 [label=AccumulateGrad]
	140490034900480 -> 140490034899808
	140490034900480 [label=TBackward0]
	140490034900720 -> 140490034900480
	140490033698432 [label="q_encoder.encoder.information_exchanging_layer.11.attention.self.query.weight
 (768, 768)" fillcolor=lightblue]
	140490033698432 -> 140490034900720
	140490034900720 [label=AccumulateGrad]
	140490034897984 -> 140490034898368
	140490034897984 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 64, 3)"]
	140490034898416 -> 140490034897984
	140490034898416 [label=CloneBackward0]
	140490034899472 -> 140490034898416
	140490034899472 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 64, 3)"]
	140490034899712 -> 140490034899472
	140490034899712 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551615
dim1: 18446744073709551614"]
	140490034902256 -> 140490034899712
	140490034902256 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140490034904512 -> 140490034902256
	140490034904512 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 3, 768)"]
	140490034903840 -> 140490034904512
	140490034903840 [label="ViewBackward0
------------------------
self_sym_sizes: (9, 768)"]
	140490034902640 -> 140490034903840
	140490034902640 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (9, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140490034898176 -> 140490034902640
	140490033699072 [label="q_encoder.encoder.information_exchanging_layer.11.attention.self.key.bias
 (768)" fillcolor=lightblue]
	140490033699072 -> 140490034898176
	140490034898176 [label=AccumulateGrad]
	140488973647008 -> 140490034902640
	140488973647008 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 3, 768)"]
	140490034894384 -> 140488973647008
	140488973643024 -> 140490034902640
	140488973643024 [label=TBackward0]
	140490034497424 -> 140488973643024
	140490033698992 [label="q_encoder.encoder.information_exchanging_layer.11.attention.self.key.weight
 (768, 768)" fillcolor=lightblue]
	140490033698992 -> 140490034497424
	140490034497424 [label=AccumulateGrad]
	140490034896880 -> 140490034896448
	140490034896880 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (3, 12, 3, 64)"]
	140490034897024 -> 140490034896880
	140490034897024 [label=CloneBackward0]
	140490034897840 -> 140490034897024
	140490034897840 [label="ExpandBackward0
------------------------------
self_sym_sizes: (3, 12, 3, 64)"]
	140490034897936 -> 140490034897840
	140490034897936 [label="PermuteBackward0
------------------
dims: (0, 2, 1, 3)"]
	140490034898944 -> 140490034897936
	140490034898944 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 3, 768)"]
	140490034900384 -> 140490034898944
	140490034900384 [label="ViewBackward0
------------------------
self_sym_sizes: (9, 768)"]
	140490034904608 -> 140490034900384
	140490034904608 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (9, 768)
mat1_sym_strides:       (768, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (768, 768)
mat2_sym_strides:       (1, 768)"]
	140490034902736 -> 140490034904608
	140490033699232 [label="q_encoder.encoder.information_exchanging_layer.11.attention.self.value.bias
 (768)" fillcolor=lightblue]
	140490033699232 -> 140490034902736
	140490034902736 [label=AccumulateGrad]
	140490034900192 -> 140490034904608
	140490034900192 [label="ViewBackward0
---------------------------
self_sym_sizes: (3, 3, 768)"]
	140490034894384 -> 140490034900192
	140490034896928 -> 140490034904608
	140490034896928 [label=TBackward0]
	140490034502656 -> 140490034896928
	140490033699152 [label="q_encoder.encoder.information_exchanging_layer.11.attention.self.value.weight
 (768, 768)" fillcolor=lightblue]
	140490033699152 -> 140490034502656
	140490034502656 [label=AccumulateGrad]
	140490034895392 -> 140490034894816
	140490034895392 [label=TBackward0]
	140490034895056 -> 140490034895392
	140490033699312 [label="q_encoder.encoder.information_exchanging_layer.11.attention.output.dense.weight
 (768, 768)" fillcolor=lightblue]
	140490033699312 -> 140490034895056
	140490034895056 [label=AccumulateGrad]
	140490034894384 -> 140490034894336
	140490034893904 -> 140490034891360
	140490033699552 [label="q_encoder.encoder.information_exchanging_layer.11.attention.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	140490033699552 -> 140490034893904
	140490034893904 [label=AccumulateGrad]
	140490034894576 -> 140490034891360
	140490033699472 [label="q_encoder.encoder.information_exchanging_layer.11.attention.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	140490033699472 -> 140490034894576
	140490034894576 [label=AccumulateGrad]
	140490034895440 -> 140490034893568
	140490034895440 [label=TBackward0]
	140490034894432 -> 140490034895440
	140490033699712 [label="q_encoder.encoder.information_exchanging_layer.11.intermediate.dense.weight
 (3072, 768)" fillcolor=lightblue]
	140490033699712 -> 140490034894432
	140490034894432 [label=AccumulateGrad]
	140490034892656 -> 140490034893280
	140490034892656 [label=TBackward0]
	140490034892560 -> 140490034892656
	140490033699872 [label="q_encoder.encoder.information_exchanging_layer.11.output.dense.weight
 (768, 3072)" fillcolor=lightblue]
	140490033699872 -> 140490034892560
	140490034892560 [label=AccumulateGrad]
	140490034891360 -> 140490034891072
	140490034893328 -> 140490034890928
	140490033700112 [label="q_encoder.encoder.information_exchanging_layer.11.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	140490033700112 -> 140490034893328
	140490034893328 [label=AccumulateGrad]
	140490034896208 -> 140490034890928
	140490033700032 [label="q_encoder.encoder.information_exchanging_layer.11.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	140490033700032 -> 140490034896208
	140490034896208 [label=AccumulateGrad]
	140490044785424 -> 140490044736608
}

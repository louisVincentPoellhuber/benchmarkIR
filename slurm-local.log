2025-01-29 11:33 - WARNING - ============ Finetuning glue_template. ============
2025-01-29 11:33 - INFO - Model Configuration: {'config': {'max_position_embeddings': 514, 'hidden_size': 768, 'num_attention_heads': 12, 'num_hidden_layers': 6, 'type_vocab_size': 1, 'attn_mechanism': 'eager', 'num_labels': 2, 'position_embedding_type': 'relative_key'}, 'settings': {'model': 'FacebookAI/roberta-base', 'save_path': 'STORAGE_DIR/models/finetune_roberta/roberta', 'task': 'glue', 'exp_name': 'glue_template', 'train': True, 'validate': False, 'evaluate': False, 'logging': False, 'epochs': 10, 'batch_size': 16, 'lr': 0.0001}}
2025-01-29 11:33 - WARNING - Processing cola.
2025-01-29 11:34 - INFO - Model Configuration: {'config': {'max_position_embeddings': 514, 'hidden_size': 768, 'num_attention_heads': 12, 'num_hidden_layers': 6, 'type_vocab_size': 1, 'attn_mechanism': 'eager', 'num_labels': 2, 'position_embedding_type': 'relative_key'}, 'settings': {'model': 'FacebookAI/roberta-base', 'save_path': 'STORAGE_DIR/models/finetune_roberta/roberta', 'task': 'glue', 'exp_name': 'glue_template', 'train': True, 'validate': False, 'evaluate': False, 'logging': False, 'epochs': 10, 'batch_size': 16, 'lr': 0.0001}}
2025-01-29 11:34 - WARNING - Processing cola.
2025-01-29 11:35 - INFO - Model Configuration: {'config': {'max_position_embeddings': 514, 'hidden_size': 768, 'num_attention_heads': 12, 'num_hidden_layers': 6, 'type_vocab_size': 1, 'attn_mechanism': 'eager', 'num_labels': 2, 'position_embedding_type': 'relative_key'}, 'settings': {'model': 'FacebookAI/roberta-base', 'save_path': 'STORAGE_DIR/models/finetune_roberta/roberta', 'task': 'glue', 'exp_name': 'glue_template', 'train': True, 'validate': False, 'evaluate': False, 'logging': False, 'epochs': 10, 'batch_size': 16, 'lr': 0.0001}}
2025-01-29 11:35 - WARNING - Processing cola.
2025-01-29 11:36 - INFO - Model Configuration: {'config': {'max_position_embeddings': 514, 'hidden_size': 768, 'num_attention_heads': 12, 'num_hidden_layers': 6, 'type_vocab_size': 1, 'attn_mechanism': 'eager', 'num_labels': 2, 'position_embedding_type': 'relative_key'}, 'settings': {'model': 'FacebookAI/roberta-base', 'save_path': '/home/lvpoellhuber/storage/models/finetune_roberta/roberta', 'task': 'glue', 'exp_name': 'glue_template', 'train': True, 'validate': False, 'evaluate': False, 'logging': False, 'epochs': 10, 'batch_size': 16, 'lr': 0.0001, 'dataset_dir': '/home/lvpoellhuber/storage/datasets', 'tokenizer': 'FacebookAI/roberta-base', 'eval_strategy': 'no', 'accelerate': True, 'weight_decay': 0, 'adam_beta1': 0.9, 'adam_beta2': 0.98, 'lr_scheduler_type': 'linear', 'warmup_steps': None, 'warmup_ratio': 0.6, 'save_strategy': 'epoch', 'save_total_limit': 1, 'optim': 'adamw_torch', 'auto_find_batch_size': False, 'resume_from_checkpoint': None}}
2025-01-29 11:36 - WARNING - Processing cola.
2025-01-29 11:37 - INFO - Model Configuration: {'config': {'max_position_embeddings': 514, 'hidden_size': 768, 'num_attention_heads': 12, 'num_hidden_layers': 6, 'type_vocab_size': 1, 'attn_mechanism': 'eager', 'num_labels': 2, 'position_embedding_type': 'relative_key'}, 'settings': {'model': 'FacebookAI/roberta-base', 'save_path': '/home/lvpoellhuber/storage/models/finetune_roberta/roberta', 'task': 'glue', 'exp_name': 'glue_template', 'train': True, 'validate': False, 'evaluate': False, 'logging': False, 'epochs': 10, 'batch_size': 16, 'lr': 0.0001, 'dataset_dir': '/home/lvpoellhuber/storage/datasets', 'tokenizer': 'FacebookAI/roberta-base', 'eval_strategy': 'no', 'accelerate': True, 'weight_decay': 0, 'adam_beta1': 0.9, 'adam_beta2': 0.98, 'lr_scheduler_type': 'linear', 'warmup_steps': None, 'warmup_ratio': 0.6, 'save_strategy': 'epoch', 'save_total_limit': 1, 'optim': 'adamw_torch', 'auto_find_batch_size': False, 'resume_from_checkpoint': None}}
2025-01-29 11:37 - WARNING - Processing cola.
2025-01-29 11:42 - INFO - Model Configuration: {'config': {'max_position_embeddings': 514, 'hidden_size': 768, 'num_attention_heads': 12, 'num_hidden_layers': 6, 'type_vocab_size': 1, 'attn_mechanism': 'eager', 'num_labels': 2, 'position_embedding_type': 'relative_key'}, 'settings': {'model': 'FacebookAI/roberta-base', 'save_path': '/home/lvpoellhuber/storage/models/finetune_roberta/roberta', 'task': 'glue', 'exp_name': 'glue_template', 'train': True, 'validate': False, 'evaluate': False, 'logging': False, 'epochs': 10, 'batch_size': 16, 'lr': 0.0001, 'dataset_dir': '/home/lvpoellhuber/storage/datasets', 'tokenizer': 'FacebookAI/roberta-base', 'eval_strategy': 'no', 'accelerate': True, 'weight_decay': 0, 'adam_beta1': 0.9, 'adam_beta2': 0.98, 'lr_scheduler_type': 'linear', 'warmup_steps': None, 'warmup_ratio': 0.6, 'save_strategy': 'epoch', 'save_total_limit': 1, 'optim': 'adamw_torch', 'auto_find_batch_size': False, 'resume_from_checkpoint': None}}
2025-01-29 11:51 - INFO - Model Configuration: {'config': {'max_position_embeddings': 514, 'hidden_size': 768, 'num_attention_heads': 12, 'num_hidden_layers': 6, 'type_vocab_size': 1, 'attn_mechanism': 'eager', 'num_labels': 2, 'position_embedding_type': 'relative_key'}, 'settings': {'model': 'FacebookAI/roberta-base', 'save_path': '/home/lvpoellhuber/storage/models/finetune_roberta/roberta', 'task': 'glue', 'exp_name': 'glue_template', 'train': True, 'validate': False, 'evaluate': False, 'logging': False, 'epochs': 10, 'batch_size': 16, 'lr': 0.0001, 'dataset_dir': '/home/lvpoellhuber/storage/datasets', 'tokenizer': 'FacebookAI/roberta-base', 'eval_strategy': 'no', 'accelerate': True, 'weight_decay': 0, 'adam_beta1': 0.9, 'adam_beta2': 0.98, 'lr_scheduler_type': 'linear', 'warmup_steps': None, 'warmup_ratio': 0.6, 'save_strategy': 'epoch', 'save_total_limit': 1, 'optim': 'adamw_torch', 'auto_find_batch_size': False, 'resume_from_checkpoint': None}}
2025-01-29 12:03 - INFO - Model Configuration: {'config': {'max_position_embeddings': 514, 'hidden_size': 768, 'num_attention_heads': 12, 'num_hidden_layers': 6, 'type_vocab_size': 1, 'attn_mechanism': 'eager', 'num_labels': 2, 'position_embedding_type': 'relative_key'}, 'settings': {'model': 'FacebookAI/roberta-base', 'save_path': '/home/lvpoellhuber/storage/models/finetune_roberta/roberta', 'task': 'glue', 'exp_name': 'glue_template', 'train': True, 'validate': False, 'evaluate': False, 'logging': False, 'epochs': 10, 'batch_size': 16, 'lr': 0.0001, 'dataset_dir': '/home/lvpoellhuber/storage/datasets', 'tokenizer': 'FacebookAI/roberta-base', 'eval_strategy': 'no', 'accelerate': True, 'weight_decay': 0, 'adam_beta1': 0.9, 'adam_beta2': 0.98, 'lr_scheduler_type': 'linear', 'warmup_steps': 0.0, 'warmup_ratio': 0.6, 'save_strategy': 'epoch', 'save_total_limit': 1, 'optim': 'adamw_torch', 'auto_find_batch_size': False, 'resume_from_checkpoint': None}}
2025-01-29 12:04 - INFO - Model Configuration: {'config': {'max_position_embeddings': 514, 'hidden_size': 768, 'num_attention_heads': 12, 'num_hidden_layers': 6, 'type_vocab_size': 1, 'attn_mechanism': 'eager', 'num_labels': 2, 'position_embedding_type': 'relative_key'}, 'settings': {'model': 'FacebookAI/roberta-base', 'save_path': '/home/lvpoellhuber/storage/models/finetune_roberta/roberta', 'task': 'glue', 'exp_name': 'glue_template', 'train': True, 'validate': False, 'evaluate': False, 'logging': False, 'epochs': 10, 'batch_size': 16, 'lr': 0.0001, 'dataset_dir': '/home/lvpoellhuber/storage/datasets', 'tokenizer': 'FacebookAI/roberta-base', 'eval_strategy': 'no', 'accelerate': True, 'weight_decay': 0, 'adam_beta1': 0.9, 'adam_beta2': 0.98, 'lr_scheduler_type': 'linear', 'warmup_steps': 0, 'warmup_ratio': 0.6, 'save_strategy': 'epoch', 'save_total_limit': 1, 'optim': 'adamw_torch', 'auto_find_batch_size': False, 'resume_from_checkpoint': None}}
2025-01-29 12:05 - INFO - Model Configuration: {'config': {'max_position_embeddings': 514, 'hidden_size': 768, 'num_attention_heads': 12, 'num_hidden_layers': 6, 'type_vocab_size': 1, 'attn_mechanism': 'eager', 'num_labels': 2, 'position_embedding_type': 'relative_key'}, 'settings': {'model': 'FacebookAI/roberta-base', 'save_path': '/home/lvpoellhuber/storage/models/finetune_roberta/roberta', 'task': 'glue', 'exp_name': 'glue_template', 'train': True, 'validate': False, 'evaluate': False, 'logging': False, 'epochs': 10, 'batch_size': 16, 'lr': 0.0001, 'dataset_dir': '/home/lvpoellhuber/storage/datasets', 'tokenizer': 'FacebookAI/roberta-base', 'eval_strategy': 'no', 'accelerate': True, 'weight_decay': 0, 'adam_beta1': 0.9, 'adam_beta2': 0.98, 'lr_scheduler_type': 'linear', 'warmup_steps': 0, 'warmup_ratio': 0.6, 'save_strategy': 'epoch', 'save_total_limit': 1, 'optim': 'adamw_torch', 'auto_find_batch_size': False, 'resume_from_checkpoint': None}}
2025-01-29 12:06 - INFO - Model Configuration: {'config': {'max_position_embeddings': 514, 'hidden_size': 768, 'num_attention_heads': 12, 'num_hidden_layers': 6, 'type_vocab_size': 1, 'attn_mechanism': 'eager', 'num_labels': 2, 'position_embedding_type': 'relative_key'}, 'settings': {'model': 'FacebookAI/roberta-base', 'save_path': '/home/lvpoellhuber/storage/models/finetune_roberta/roberta', 'task': 'glue', 'exp_name': 'glue_template', 'train': True, 'validate': False, 'evaluate': False, 'logging': False, 'epochs': 10, 'batch_size': 16, 'lr': 0.0001, 'dataset_dir': '/home/lvpoellhuber/storage/datasets', 'tokenizer': 'FacebookAI/roberta-base', 'eval_strategy': 'no', 'accelerate': True, 'weight_decay': 0, 'adam_beta1': 0.9, 'adam_beta2': 0.98, 'lr_scheduler_type': 'linear', 'warmup_steps': 0, 'warmup_ratio': 0.6, 'save_strategy': 'epoch', 'save_total_limit': 1, 'optim': 'adamw_torch', 'auto_find_batch_size': False, 'resume_from_checkpoint': None}}
2025-01-29 12:07 - INFO - config set model/num_parameters = 82514306 - None
2025-01-29 12:07 - INFO - Model Configuration: {'config': {'max_position_embeddings': 514, 'hidden_size': 768, 'num_attention_heads': 12, 'num_hidden_layers': 6, 'type_vocab_size': 1, 'attn_mechanism': 'eager', 'num_labels': 2, 'position_embedding_type': 'relative_key'}, 'settings': {'model': 'FacebookAI/roberta-base', 'save_path': '/home/lvpoellhuber/storage/models/finetune_roberta/roberta', 'task': 'glue', 'exp_name': 'glue_template', 'train': True, 'validate': False, 'evaluate': False, 'logging': False, 'epochs': 10, 'batch_size': 16, 'lr': 0.0001, 'dataset_dir': '/home/lvpoellhuber/storage/datasets', 'tokenizer': 'FacebookAI/roberta-base', 'eval_strategy': 'no', 'accelerate': True, 'weight_decay': 0, 'adam_beta1': 0.9, 'adam_beta2': 0.98, 'lr_scheduler_type': 'linear', 'warmup_steps': 0, 'warmup_ratio': 0.6, 'save_strategy': 'epoch', 'save_total_limit': 1, 'optim': 'adamw_torch', 'auto_find_batch_size': False, 'resume_from_checkpoint': None}}
2025-01-29 12:09 - INFO - Model Configuration: {'config': {'max_position_embeddings': 514, 'hidden_size': 768, 'num_attention_heads': 12, 'num_hidden_layers': 6, 'type_vocab_size': 1, 'attn_mechanism': 'eager', 'num_labels': 2, 'position_embedding_type': 'relative_key'}, 'settings': {'model': 'FacebookAI/roberta-base', 'save_path': '/home/lvpoellhuber/storage/models/finetune_roberta/roberta', 'task': 'glue', 'exp_name': 'glue_template', 'train': True, 'validate': False, 'evaluate': False, 'logging': False, 'epochs': 10, 'batch_size': 16, 'lr': 0.0001, 'dataset_dir': '/home/lvpoellhuber/storage/datasets', 'tokenizer': 'FacebookAI/roberta-base', 'eval_strategy': 'no', 'accelerate': True, 'weight_decay': 0, 'adam_beta1': 0.9, 'adam_beta2': 0.98, 'lr_scheduler_type': 'linear', 'warmup_steps': 0, 'warmup_ratio': 0.6, 'save_strategy': 'epoch', 'save_total_limit': 1, 'optim': 'adamw_torch', 'auto_find_batch_size': False, 'resume_from_checkpoint': None}}
2025-01-29 12:09 - INFO - Model Configuration: {'config': {'max_position_embeddings': 514, 'hidden_size': 768, 'num_attention_heads': 12, 'num_hidden_layers': 6, 'type_vocab_size': 1, 'attn_mechanism': 'eager', 'num_labels': 2, 'position_embedding_type': 'relative_key'}, 'settings': {'task': 'glue', 'exp_name': 'glue_template', 'save_path': '/home/lvpoellhuber/storage/models/finetune_roberta/roberta/glue_template', 'model': 'FacebookAI/roberta-base', 'train': True, 'validate': False, 'evaluate': False, 'logging': False, 'epochs': 10, 'batch_size': 16, 'lr': 0.0001, 'dataset_dir': '/home/lvpoellhuber/storage/datasets', 'tokenizer': 'FacebookAI/roberta-base', 'eval_strategy': 'no', 'accelerate': True, 'weight_decay': 0, 'adam_beta1': 0.9, 'adam_beta2': 0.98, 'lr_scheduler_type': 'linear', 'warmup_steps': 0, 'warmup_ratio': 0.6, 'save_strategy': 'epoch', 'save_total_limit': 1, 'optim': 'adamw_torch', 'auto_find_batch_size': False, 'resume_from_checkpoint': None}}
2025-01-29 12:17 - INFO - Model Configuration: {'config': {'max_position_embeddings': 514, 'hidden_size': 768, 'num_attention_heads': 12, 'num_hidden_layers': 6, 'type_vocab_size': 1, 'attn_mechanism': 'eager', 'num_labels': 2, 'position_embedding_type': 'relative_key'}, 'settings': {'task': 'glue', 'exp_name': 'glue_template', 'save_path': '/home/lvpoellhuber/storage/models/finetune_roberta/roberta/glue_template', 'model': 'FacebookAI/roberta-base', 'train': True, 'validate': False, 'evaluate': False, 'logging': False, 'epochs': 10, 'batch_size': 16, 'lr': 0.0001, 'dataset_dir': '/home/lvpoellhuber/storage/datasets', 'tokenizer': 'FacebookAI/roberta-base', 'eval_strategy': 'no', 'accelerate': True, 'weight_decay': 0, 'adam_beta1': 0.9, 'adam_beta2': 0.98, 'lr_scheduler_type': 'linear', 'warmup_steps': 0, 'warmup_ratio': 0.6, 'save_strategy': 'epoch', 'save_total_limit': 1, 'optim': 'adamw_torch', 'auto_find_batch_size': False, 'resume_from_checkpoint': None}}
2025-01-29 12:17 - INFO - Model Configuration: {'config': {'max_position_embeddings': 514, 'hidden_size': 768, 'num_attention_heads': 12, 'num_hidden_layers': 6, 'type_vocab_size': 1, 'attn_mechanism': 'eager', 'num_labels': 2, 'position_embedding_type': 'relative_key'}, 'settings': {'task': 'glue', 'exp_name': 'glue_template', 'save_path': '/home/lvpoellhuber/storage/models/finetune_roberta/roberta/glue_template', 'model': 'FacebookAI/roberta-base', 'train': True, 'validate': False, 'evaluate': False, 'logging': False, 'epochs': 10, 'batch_size': 16, 'lr': 0.0001, 'dataset_dir': '/home/lvpoellhuber/storage/datasets', 'tokenizer': 'FacebookAI/roberta-base', 'eval_strategy': 'no', 'accelerate': True, 'weight_decay': 0, 'adam_beta1': 0.9, 'adam_beta2': 0.98, 'lr_scheduler_type': 'linear', 'warmup_steps': 0, 'warmup_ratio': 0.6, 'save_strategy': 'epoch', 'save_total_limit': 1, 'optim': 'adamw_torch', 'auto_find_batch_size': False, 'resume_from_checkpoint': None}}
2025-01-29 12:17 - INFO - config set model/num_parameters = 82514306 - None
2025-01-29 12:19 - INFO - Model Configuration: {'config': {'max_position_embeddings': 514, 'hidden_size': 768, 'num_attention_heads': 12, 'num_hidden_layers': 6, 'type_vocab_size': 1, 'attn_mechanism': 'eager', 'num_labels': 2, 'position_embedding_type': 'relative_key'}, 'settings': {'task': 'glue', 'exp_name': 'glue_template', 'save_path': '/home/lvpoellhuber/storage/models/finetune_roberta/roberta/glue_template', 'model': 'FacebookAI/roberta-base', 'train': True, 'validate': False, 'evaluate': False, 'logging': False, 'epochs': 10, 'batch_size': 16, 'lr': 0.0001, 'dataset_dir': '/home/lvpoellhuber/storage/datasets', 'tokenizer': 'FacebookAI/roberta-base', 'eval_strategy': 'no', 'accelerate': True, 'weight_decay': 0, 'adam_beta1': 0.9, 'adam_beta2': 0.98, 'lr_scheduler_type': 'linear', 'warmup_steps': 0, 'warmup_ratio': 0.6, 'save_strategy': 'epoch', 'save_total_limit': 1, 'optim': 'adamw_torch', 'auto_find_batch_size': False, 'resume_from_checkpoint': None}}
2025-01-29 12:19 - INFO - config set model/num_parameters = 82514306 - None
2025-01-29 12:33 - INFO - Model Configuration: {'config': {'max_position_embeddings': 514, 'hidden_size': 768, 'num_attention_heads': 12, 'num_hidden_layers': 6, 'type_vocab_size': 1, 'attn_mechanism': 'eager', 'num_labels': 2, 'position_embedding_type': 'relative_key'}, 'settings': {'task': 'glue', 'exp_name': 'glue_template', 'save_path': '/home/lvpoellhuber/storage/models/finetune_roberta/roberta/roberta_1', 'model': 'FacebookAI/roberta-base', 'train': False, 'validate': False, 'evaluate': True, 'logging': False, 'epochs': 10, 'batch_size': 16, 'lr': 0.0001, 'dataset_dir': '/home/lvpoellhuber/storage/datasets', 'tokenizer': 'FacebookAI/roberta-base', 'eval_strategy': 'no', 'accelerate': True, 'weight_decay': 0, 'adam_beta1': 0.9, 'adam_beta2': 0.98, 'lr_scheduler_type': 'linear', 'warmup_steps': 0, 'warmup_ratio': 0.6, 'save_strategy': 'epoch', 'save_total_limit': 1, 'optim': 'adamw_torch', 'auto_find_batch_size': False, 'resume_from_checkpoint': None}}
2025-01-29 12:34 - INFO - Model Configuration: {'config': {'max_position_embeddings': 514, 'hidden_size': 768, 'num_attention_heads': 12, 'num_hidden_layers': 6, 'type_vocab_size': 1, 'attn_mechanism': 'eager', 'num_labels': 2, 'position_embedding_type': 'relative_key'}, 'settings': {'task': 'glue', 'exp_name': 'glue_template', 'save_path': '/home/lvpoellhuber/storage/models/finetune_roberta/roberta/roberta_1', 'model': 'FacebookAI/roberta-base', 'train': False, 'validate': False, 'evaluate': True, 'logging': False, 'epochs': 10, 'batch_size': 16, 'lr': 0.0001, 'dataset_dir': '/home/lvpoellhuber/storage/datasets', 'tokenizer': 'FacebookAI/roberta-base', 'eval_strategy': 'no', 'accelerate': True, 'weight_decay': 0, 'adam_beta1': 0.9, 'adam_beta2': 0.98, 'lr_scheduler_type': 'linear', 'warmup_steps': 0, 'warmup_ratio': 0.6, 'save_strategy': 'epoch', 'save_total_limit': 1, 'optim': 'adamw_torch', 'auto_find_batch_size': False, 'resume_from_checkpoint': None}}
2025-01-29 12:34 - INFO - Model Configuration: {'config': {'max_position_embeddings': 514, 'hidden_size': 768, 'num_attention_heads': 12, 'num_hidden_layers': 6, 'type_vocab_size': 1, 'attn_mechanism': 'eager', 'num_labels': 2, 'position_embedding_type': 'relative_key'}, 'settings': {'task': 'glue', 'exp_name': 'glue_template', 'save_path': '/home/lvpoellhuber/storage/models/finetune_roberta/roberta/roberta_1', 'model': 'FacebookAI/roberta-base', 'train': False, 'validate': False, 'evaluate': True, 'logging': False, 'epochs': 10, 'batch_size': 16, 'lr': 0.0001, 'dataset_dir': '/home/lvpoellhuber/storage/datasets', 'tokenizer': 'FacebookAI/roberta-base', 'eval_strategy': 'no', 'accelerate': True, 'weight_decay': 0, 'adam_beta1': 0.9, 'adam_beta2': 0.98, 'lr_scheduler_type': 'linear', 'warmup_steps': 0, 'warmup_ratio': 0.6, 'save_strategy': 'epoch', 'save_total_limit': 1, 'optim': 'adamw_torch', 'auto_find_batch_size': False, 'resume_from_checkpoint': None}}
2025-01-29 12:35 - INFO - config set model/num_parameters = 82514306 - None
2025-01-29 12:41 - INFO - Model Configuration: {'config': {'max_position_embeddings': 514, 'hidden_size': 768, 'num_attention_heads': 12, 'num_hidden_layers': 6, 'type_vocab_size': 1, 'attn_mechanism': 'eager', 'num_labels': 2, 'position_embedding_type': 'relative_key'}, 'settings': {'task': 'glue', 'exp_name': 'glue_template', 'save_path': '/home/lvpoellhuber/storage/models/finetune_roberta/roberta/roberta_1', 'model': 'FacebookAI/roberta-base', 'train': False, 'validate': False, 'evaluate': True, 'logging': False, 'epochs': 10, 'batch_size': 16, 'lr': 0.0001, 'dataset_dir': '/home/lvpoellhuber/storage/datasets', 'tokenizer': 'FacebookAI/roberta-base', 'eval_strategy': 'no', 'accelerate': True, 'weight_decay': 0, 'adam_beta1': 0.9, 'adam_beta2': 0.98, 'lr_scheduler_type': 'linear', 'warmup_steps': 0, 'warmup_ratio': 0.6, 'save_strategy': 'epoch', 'save_total_limit': 1, 'optim': 'adamw_torch', 'auto_find_batch_size': False, 'resume_from_checkpoint': None}}
2025-01-29 12:42 - INFO - Model Configuration: {'config': {'max_position_embeddings': 514, 'hidden_size': 768, 'num_attention_heads': 12, 'num_hidden_layers': 6, 'type_vocab_size': 1, 'attn_mechanism': 'eager', 'num_labels': 2, 'position_embedding_type': 'relative_key'}, 'settings': {'task': 'glue', 'exp_name': 'glue_template', 'save_path': '/home/lvpoellhuber/storage/models/finetune_roberta/roberta/roberta_1', 'model': 'FacebookAI/roberta-base', 'train': False, 'validate': False, 'evaluate': True, 'logging': False, 'epochs': 10, 'batch_size': 16, 'lr': 0.0001, 'dataset_dir': '/home/lvpoellhuber/storage/datasets', 'tokenizer': 'FacebookAI/roberta-base', 'eval_strategy': 'no', 'accelerate': True, 'weight_decay': 0, 'adam_beta1': 0.9, 'adam_beta2': 0.98, 'lr_scheduler_type': 'linear', 'warmup_steps': 0, 'warmup_ratio': 0.6, 'save_strategy': 'epoch', 'save_total_limit': 1, 'optim': 'adamw_torch', 'auto_find_batch_size': False, 'resume_from_checkpoint': None}}
2025-01-29 12:44 - INFO - Model Configuration: {'config': {'max_position_embeddings': 514, 'hidden_size': 768, 'num_attention_heads': 12, 'num_hidden_layers': 6, 'type_vocab_size': 1, 'attn_mechanism': 'eager', 'num_labels': 2, 'position_embedding_type': 'relative_key'}, 'settings': {'task': 'glue', 'exp_name': 'glue_template', 'save_path': '/home/lvpoellhuber/storage/models/finetune_roberta/roberta/roberta_1', 'model': 'FacebookAI/roberta-base', 'train': False, 'validate': False, 'evaluate': True, 'logging': False, 'epochs': 10, 'batch_size': 16, 'lr': 0.0001, 'dataset_dir': '/home/lvpoellhuber/storage/datasets', 'tokenizer': 'FacebookAI/roberta-base', 'eval_strategy': 'no', 'accelerate': True, 'weight_decay': 0, 'adam_beta1': 0.9, 'adam_beta2': 0.98, 'lr_scheduler_type': 'linear', 'warmup_steps': 0, 'warmup_ratio': 0.6, 'save_strategy': 'epoch', 'save_total_limit': 1, 'optim': 'adamw_torch', 'auto_find_batch_size': False, 'resume_from_checkpoint': None}}
2025-01-29 12:49 - INFO - Model Configuration: {'config': {'max_position_embeddings': 514, 'hidden_size': 768, 'num_attention_heads': 12, 'num_hidden_layers': 6, 'type_vocab_size': 1, 'attn_mechanism': 'eager', 'num_labels': 2, 'position_embedding_type': 'relative_key'}, 'settings': {'task': 'glue', 'exp_name': 'glue_template', 'save_path': '/home/lvpoellhuber/storage/models/finetune_roberta/roberta/roberta_1', 'model': 'FacebookAI/roberta-base', 'train': False, 'validate': False, 'evaluate': True, 'logging': False, 'epochs': 10, 'batch_size': 16, 'lr': 0.0001, 'dataset_dir': '/home/lvpoellhuber/storage/datasets', 'tokenizer': 'FacebookAI/roberta-base', 'eval_strategy': 'no', 'accelerate': True, 'weight_decay': 0, 'adam_beta1': 0.9, 'adam_beta2': 0.98, 'lr_scheduler_type': 'linear', 'warmup_steps': 0, 'warmup_ratio': 0.6, 'save_strategy': 'epoch', 'save_total_limit': 1, 'optim': 'adamw_torch', 'auto_find_batch_size': False, 'resume_from_checkpoint': None}}
2025-01-29 12:50 - INFO - Model Configuration: {'config': {'max_position_embeddings': 514, 'hidden_size': 768, 'num_attention_heads': 12, 'num_hidden_layers': 6, 'type_vocab_size': 1, 'attn_mechanism': 'eager', 'num_labels': 2, 'position_embedding_type': 'relative_key'}, 'settings': {'task': 'glue', 'exp_name': 'glue_template', 'save_path': '/home/lvpoellhuber/storage/models/finetune_roberta/roberta/roberta_1', 'model': 'FacebookAI/roberta-base', 'train': False, 'validate': False, 'evaluate': True, 'logging': False, 'epochs': 10, 'batch_size': 16, 'lr': 0.0001, 'dataset_dir': '/home/lvpoellhuber/storage/datasets', 'tokenizer': 'FacebookAI/roberta-base', 'eval_strategy': 'no', 'accelerate': True, 'weight_decay': 0, 'adam_beta1': 0.9, 'adam_beta2': 0.98, 'lr_scheduler_type': 'linear', 'warmup_steps': 0, 'warmup_ratio': 0.6, 'save_strategy': 'epoch', 'save_total_limit': 1, 'optim': 'adamw_torch', 'auto_find_batch_size': False, 'resume_from_checkpoint': None}}
2025-01-29 12:52 - INFO - Model Configuration: {'config': {'max_position_embeddings': 514, 'hidden_size': 768, 'num_attention_heads': 12, 'num_hidden_layers': 6, 'type_vocab_size': 1, 'attn_mechanism': 'eager', 'num_labels': 2, 'position_embedding_type': 'relative_key'}, 'settings': {'task': 'glue', 'exp_name': 'glue_template', 'save_path': '/home/lvpoellhuber/storage/models/finetune_roberta/roberta/roberta_1', 'model': 'FacebookAI/roberta-base', 'train': False, 'validate': False, 'evaluate': True, 'logging': False, 'epochs': 10, 'batch_size': 16, 'lr': 0.0001, 'dataset_dir': '/home/lvpoellhuber/storage/datasets', 'tokenizer': 'FacebookAI/roberta-base', 'eval_strategy': 'no', 'accelerate': True, 'weight_decay': 0, 'adam_beta1': 0.9, 'adam_beta2': 0.98, 'lr_scheduler_type': 'linear', 'warmup_steps': 0, 'warmup_ratio': 0.6, 'save_strategy': 'epoch', 'save_total_limit': 1, 'optim': 'adamw_torch', 'auto_find_batch_size': False, 'resume_from_checkpoint': None}}
2025-01-29 12:53 - INFO - config set model/num_parameters = 82514306 - None
2025-01-29 13:09 - INFO - Model Configuration: {'config': {'max_position_embeddings': 514, 'hidden_size': 768, 'num_attention_heads': 12, 'num_hidden_layers': 6, 'type_vocab_size': 1, 'attn_mechanism': 'eager', 'num_labels': 2, 'position_embedding_type': 'relative_key'}, 'settings': {'task': 'wnli', 'exp_name': 'glue_template', 'save_path': '/home/lvpoellhuber/storage/models/finetune_roberta/roberta/glue_template', 'model': 'FacebookAI/roberta-base', 'train': False, 'validate': False, 'evaluate': True, 'logging': False, 'epochs': 10, 'batch_size': 16, 'lr': 0.0001, 'dataset_dir': '/home/lvpoellhuber/storage/datasets', 'tokenizer': 'FacebookAI/roberta-base', 'eval_strategy': 'no', 'accelerate': True, 'weight_decay': 0, 'adam_beta1': 0.9, 'adam_beta2': 0.98, 'lr_scheduler_type': 'linear', 'warmup_steps': 0, 'warmup_ratio': 0.6, 'save_strategy': 'epoch', 'save_total_limit': 1, 'optim': 'adamw_torch', 'auto_find_batch_size': False, 'resume_from_checkpoint': None}}
2025-01-29 13:11 - INFO - Model Configuration: {'config': {'max_position_embeddings': 514, 'hidden_size': 768, 'num_attention_heads': 12, 'num_hidden_layers': 6, 'type_vocab_size': 1, 'attn_mechanism': 'eager', 'num_labels': 2, 'position_embedding_type': 'relative_key'}, 'settings': {'task': 'wnli', 'exp_name': 'glue_template', 'save_path': '/home/lvpoellhuber/storage/models/finetune_roberta/roberta/glue_template', 'model': 'FacebookAI/roberta-base', 'train': False, 'validate': False, 'evaluate': True, 'logging': False, 'epochs': 10, 'batch_size': 16, 'lr': 0.0001, 'dataset_dir': '/home/lvpoellhuber/storage/datasets', 'tokenizer': 'FacebookAI/roberta-base', 'eval_strategy': 'no', 'accelerate': True, 'weight_decay': 0, 'adam_beta1': 0.9, 'adam_beta2': 0.98, 'lr_scheduler_type': 'linear', 'warmup_steps': 0, 'warmup_ratio': 0.6, 'save_strategy': 'epoch', 'save_total_limit': 1, 'optim': 'adamw_torch', 'auto_find_batch_size': False, 'resume_from_checkpoint': None}}
2025-01-29 14:05 - INFO - Model Configuration: {'config': {'max_position_embeddings': 514, 'hidden_size': 768, 'num_attention_heads': 12, 'num_hidden_layers': 6, 'type_vocab_size': 1, 'attn_mechanism': 'eager', 'num_labels': 2, 'position_embedding_type': 'relative_key'}, 'settings': {'task': 'wnli', 'exp_name': 'glue_template', 'save_path': '/home/lvpoellhuber/storage/models/finetune_roberta/roberta/glue_template', 'model': 'FacebookAI/roberta-base', 'train': False, 'validate': False, 'evaluate': True, 'logging': False, 'epochs': 10, 'batch_size': 16, 'lr': 0.0001, 'dataset_dir': '/home/lvpoellhuber/storage/datasets', 'tokenizer': 'FacebookAI/roberta-base', 'eval_strategy': 'no', 'accelerate': True, 'weight_decay': 0, 'adam_beta1': 0.9, 'adam_beta2': 0.98, 'lr_scheduler_type': 'linear', 'warmup_steps': 0, 'warmup_ratio': 0.6, 'save_strategy': 'epoch', 'save_total_limit': 1, 'optim': 'adamw_torch', 'auto_find_batch_size': False, 'resume_from_checkpoint': None}}
2025-01-29 14:06 - INFO - Model Configuration: {'config': {'max_position_embeddings': 514, 'hidden_size': 768, 'num_attention_heads': 12, 'num_hidden_layers': 6, 'type_vocab_size': 1, 'attn_mechanism': 'eager', 'num_labels': 2, 'position_embedding_type': 'relative_key'}, 'settings': {'task': 'glue', 'exp_name': 'glue_template', 'save_path': '/home/lvpoellhuber/storage/models/finetune_roberta/roberta/glue_template', 'model': 'FacebookAI/roberta-base', 'train': False, 'validate': False, 'evaluate': True, 'logging': False, 'epochs': 10, 'batch_size': 16, 'lr': 0.0001, 'dataset_dir': '/home/lvpoellhuber/storage/datasets', 'tokenizer': 'FacebookAI/roberta-base', 'eval_strategy': 'no', 'accelerate': True, 'weight_decay': 0, 'adam_beta1': 0.9, 'adam_beta2': 0.98, 'lr_scheduler_type': 'linear', 'warmup_steps': 0, 'warmup_ratio': 0.6, 'save_strategy': 'epoch', 'save_total_limit': 1, 'optim': 'adamw_torch', 'auto_find_batch_size': False, 'resume_from_checkpoint': None}}
2025-01-29 14:12 - INFO - Model Configuration: {'config': {'max_position_embeddings': 514, 'hidden_size': 768, 'num_attention_heads': 12, 'num_hidden_layers': 6, 'type_vocab_size': 1, 'attn_mechanism': 'eager', 'num_labels': 2, 'position_embedding_type': 'relative_key'}, 'settings': {'task': 'glue', 'exp_name': 'glue_template', 'save_path': '/home/lvpoellhuber/storage/models/finetune_roberta/roberta/roberta_1', 'model': 'FacebookAI/roberta-base', 'train': False, 'validate': False, 'evaluate': True, 'logging': False, 'epochs': 10, 'batch_size': 16, 'lr': 0.0001, 'dataset_dir': '/home/lvpoellhuber/storage/datasets', 'tokenizer': 'FacebookAI/roberta-base', 'eval_strategy': 'no', 'accelerate': True, 'weight_decay': 0, 'adam_beta1': 0.9, 'adam_beta2': 0.98, 'lr_scheduler_type': 'linear', 'warmup_steps': 0, 'warmup_ratio': 0.6, 'save_strategy': 'epoch', 'save_total_limit': 1, 'optim': 'adamw_torch', 'auto_find_batch_size': False, 'resume_from_checkpoint': None}}
2025-01-29 14:13 - INFO - config set model/num_parameters = 82514306 - None
2025-01-29 15:16 - INFO - Model Configuration: {'config': {'max_position_embeddings': 514, 'hidden_size': 768, 'num_attention_heads': 12, 'num_hidden_layers': 6, 'type_vocab_size': 1, 'attn_mechanism': 'eager', 'num_labels': 2, 'position_embedding_type': 'relative_key'}, 'settings': {'task': 'glue', 'exp_name': 'glue_template', 'save_path': '/home/lvpoellhuber/storage/models/finetune_roberta/roberta/roberta_1', 'model': 'FacebookAI/roberta-base', 'train': False, 'validate': False, 'evaluate': True, 'logging': False, 'epochs': 10, 'batch_size': 16, 'lr': 0.0001, 'dataset_dir': '/home/lvpoellhuber/storage/datasets', 'tokenizer': 'FacebookAI/roberta-base', 'eval_strategy': 'no', 'accelerate': True, 'weight_decay': 0, 'adam_beta1': 0.9, 'adam_beta2': 0.98, 'lr_scheduler_type': 'linear', 'warmup_steps': 0, 'warmup_ratio': 0.6, 'save_strategy': 'epoch', 'save_total_limit': 1, 'optim': 'adamw_torch', 'auto_find_batch_size': False, 'resume_from_checkpoint': None}}
2025-01-29 15:16 - INFO - config set model/num_parameters = 82514306 - None
2025-01-30 10:33 - INFO - Model Configuration: {'config': {'max_position_embeddings': 514, 'hidden_size': 768, 'num_attention_heads': 12, 'num_hidden_layers': 6, 'type_vocab_size': 1, 'attn_mechanism': 'eager', 'num_labels': 2, 'position_embedding_type': 'relative_key'}, 'settings': {'task': 'glue', 'exp_name': 'glue_template', 'save_path': '/home/lvpoellhuber/storage/models/finetune_roberta/roberta/roberta_1', 'model': 'FacebookAI/roberta-base', 'train': False, 'validate': False, 'evaluate': True, 'logging': False, 'epochs': 10, 'batch_size': 16, 'lr': 0.0001, 'dataset_dir': '/home/lvpoellhuber/storage/datasets', 'tokenizer': 'FacebookAI/roberta-base', 'eval_strategy': 'no', 'accelerate': True, 'weight_decay': 0, 'adam_beta1': 0.9, 'adam_beta2': 0.98, 'lr_scheduler_type': 'linear', 'warmup_steps': 0, 'warmup_ratio': 0.6, 'save_strategy': 'epoch', 'save_total_limit': 1, 'optim': 'adamw_torch', 'auto_find_batch_size': False, 'resume_from_checkpoint': None}}
2025-01-30 10:34 - WARNING - Using the latest cached version of the module from /home/lvpoellhuber/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--glue/05234ba7acc44554edcca0978db5fa3bc600eeee66229abe79ff9887eacaf3ed (last modified on Wed Jan 29 12:35:48 2025) since it couldn't be found locally at evaluate-metric--glue, or remotely on the Hugging Face Hub.
2025-01-30 10:34 - WARNING - Using the latest cached version of the module from /home/lvpoellhuber/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--accuracy/f887c0aab52c2d38e1f8a215681126379eca617f96c447638f751434e8e65b14 (last modified on Wed Jan 29 12:50:09 2025) since it couldn't be found locally at evaluate-metric--accuracy, or remotely on the Hugging Face Hub.
2025-01-30 10:34 - INFO - config set model/num_parameters = 82514306 - None
2025-01-30 10:53 - INFO - Model Configuration: {'config': {'max_position_embeddings': 514, 'hidden_size': 768, 'num_attention_heads': 12, 'num_hidden_layers': 6, 'type_vocab_size': 1, 'attn_mechanism': 'eager', 'num_labels': 2, 'position_embedding_type': 'relative_key'}, 'settings': {'task': 'glue', 'exp_name': 'glue_template', 'save_path': '/home/lvpoellhuber/storage/models/finetune_roberta/roberta/roberta_1', 'model': 'FacebookAI/roberta-base', 'train': False, 'validate': False, 'evaluate': True, 'logging': False, 'epochs': 10, 'batch_size': 16, 'lr': 0.0001, 'dataset_dir': '/home/lvpoellhuber/storage/datasets', 'tokenizer': 'FacebookAI/roberta-base', 'eval_strategy': 'no', 'accelerate': True, 'weight_decay': 0, 'adam_beta1': 0.9, 'adam_beta2': 0.98, 'lr_scheduler_type': 'linear', 'warmup_steps': 0, 'warmup_ratio': 0.6, 'save_strategy': 'epoch', 'save_total_limit': 1, 'optim': 'adamw_torch', 'auto_find_batch_size': False, 'resume_from_checkpoint': None}}
2025-01-30 10:53 - WARNING - Using the latest cached version of the module from /home/lvpoellhuber/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--glue/05234ba7acc44554edcca0978db5fa3bc600eeee66229abe79ff9887eacaf3ed (last modified on Wed Jan 29 12:35:48 2025) since it couldn't be found locally at evaluate-metric--glue, or remotely on the Hugging Face Hub.
2025-01-30 10:53 - WARNING - Using the latest cached version of the module from /home/lvpoellhuber/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--accuracy/f887c0aab52c2d38e1f8a215681126379eca617f96c447638f751434e8e65b14 (last modified on Wed Jan 29 12:50:09 2025) since it couldn't be found locally at evaluate-metric--accuracy, or remotely on the Hugging Face Hub.
2025-01-30 10:54 - INFO - config set model/num_parameters = 82514306 - None
2025-01-30 10:58 - INFO - Model Configuration: {'config': {'max_position_embeddings': 514, 'hidden_size': 768, 'num_attention_heads': 12, 'num_hidden_layers': 6, 'type_vocab_size': 1, 'attn_mechanism': 'eager', 'num_labels': 2, 'position_embedding_type': 'relative_key'}, 'settings': {'task': 'glue', 'exp_name': 'glue_template', 'save_path': '/home/lvpoellhuber/storage/models/finetune_roberta/roberta/roberta_1', 'model': 'FacebookAI/roberta-base', 'train': False, 'validate': False, 'evaluate': True, 'logging': False, 'epochs': 10, 'batch_size': 16, 'lr': 0.0001, 'dataset_dir': '/home/lvpoellhuber/storage/datasets', 'tokenizer': 'FacebookAI/roberta-base', 'eval_strategy': 'no', 'accelerate': True, 'weight_decay': 0, 'adam_beta1': 0.9, 'adam_beta2': 0.98, 'lr_scheduler_type': 'linear', 'warmup_steps': 0, 'warmup_ratio': 0.6, 'save_strategy': 'epoch', 'save_total_limit': 1, 'optim': 'adamw_torch', 'auto_find_batch_size': False, 'resume_from_checkpoint': None}}
2025-01-30 10:59 - WARNING - Using the latest cached version of the module from /home/lvpoellhuber/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--glue/05234ba7acc44554edcca0978db5fa3bc600eeee66229abe79ff9887eacaf3ed (last modified on Wed Jan 29 12:35:48 2025) since it couldn't be found locally at evaluate-metric--glue, or remotely on the Hugging Face Hub.
2025-01-30 10:59 - WARNING - Using the latest cached version of the module from /home/lvpoellhuber/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--accuracy/f887c0aab52c2d38e1f8a215681126379eca617f96c447638f751434e8e65b14 (last modified on Wed Jan 29 12:50:09 2025) since it couldn't be found locally at evaluate-metric--accuracy, or remotely on the Hugging Face Hub.
2025-01-30 10:59 - INFO - config set model/num_parameters = 82514306 - None
2025-01-30 11:04 - INFO - Model Configuration: {'config': {'max_position_embeddings': 514, 'hidden_size': 768, 'num_attention_heads': 12, 'num_hidden_layers': 6, 'type_vocab_size': 1, 'attn_mechanism': 'eager', 'num_labels': 2, 'position_embedding_type': 'relative_key'}, 'settings': {'task': 'glue', 'exp_name': 'glue_template', 'save_path': '/home/lvpoellhuber/storage/models/finetune_roberta/roberta/roberta_1', 'model': 'FacebookAI/roberta-base', 'train': False, 'validate': False, 'evaluate': True, 'logging': False, 'epochs': 10, 'batch_size': 16, 'lr': 0.0001, 'dataset_dir': '/home/lvpoellhuber/storage/datasets', 'tokenizer': 'FacebookAI/roberta-base', 'eval_strategy': 'no', 'accelerate': True, 'weight_decay': 0, 'adam_beta1': 0.9, 'adam_beta2': 0.98, 'lr_scheduler_type': 'linear', 'warmup_steps': 0, 'warmup_ratio': 0.6, 'save_strategy': 'epoch', 'save_total_limit': 1, 'optim': 'adamw_torch', 'auto_find_batch_size': False, 'resume_from_checkpoint': None}}
2025-01-30 11:05 - WARNING - Using the latest cached version of the module from /home/lvpoellhuber/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--glue/05234ba7acc44554edcca0978db5fa3bc600eeee66229abe79ff9887eacaf3ed (last modified on Wed Jan 29 12:35:48 2025) since it couldn't be found locally at evaluate-metric--glue, or remotely on the Hugging Face Hub.
2025-01-30 11:05 - WARNING - Using the latest cached version of the module from /home/lvpoellhuber/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--accuracy/f887c0aab52c2d38e1f8a215681126379eca617f96c447638f751434e8e65b14 (last modified on Wed Jan 29 12:50:09 2025) since it couldn't be found locally at evaluate-metric--accuracy, or remotely on the Hugging Face Hub.
2025-01-30 11:05 - INFO - config set model/num_parameters = 82514306 - None
2025-01-30 11:06 - INFO - Model Configuration: {'config': {'max_position_embeddings': 514, 'hidden_size': 768, 'num_attention_heads': 12, 'num_hidden_layers': 6, 'type_vocab_size': 1, 'attn_mechanism': 'eager', 'num_labels': 2, 'position_embedding_type': 'relative_key'}, 'settings': {'task': 'glue', 'exp_name': 'glue_template', 'save_path': '/home/lvpoellhuber/storage/models/finetune_roberta/roberta/roberta_1', 'model': 'FacebookAI/roberta-base', 'train': False, 'validate': False, 'evaluate': True, 'logging': False, 'epochs': 10, 'batch_size': 16, 'lr': 0.0001, 'dataset_dir': '/home/lvpoellhuber/storage/datasets', 'tokenizer': 'FacebookAI/roberta-base', 'eval_strategy': 'no', 'accelerate': True, 'weight_decay': 0, 'adam_beta1': 0.9, 'adam_beta2': 0.98, 'lr_scheduler_type': 'linear', 'warmup_steps': 0, 'warmup_ratio': 0.6, 'save_strategy': 'epoch', 'save_total_limit': 1, 'optim': 'adamw_torch', 'auto_find_batch_size': False, 'resume_from_checkpoint': None}}
2025-01-30 11:07 - WARNING - Using the latest cached version of the module from /home/lvpoellhuber/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--glue/05234ba7acc44554edcca0978db5fa3bc600eeee66229abe79ff9887eacaf3ed (last modified on Wed Jan 29 12:35:48 2025) since it couldn't be found locally at evaluate-metric--glue, or remotely on the Hugging Face Hub.
2025-01-30 11:07 - WARNING - Using the latest cached version of the module from /home/lvpoellhuber/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--accuracy/f887c0aab52c2d38e1f8a215681126379eca617f96c447638f751434e8e65b14 (last modified on Wed Jan 29 12:50:09 2025) since it couldn't be found locally at evaluate-metric--accuracy, or remotely on the Hugging Face Hub.
2025-01-30 11:07 - INFO - config set model/num_parameters = 82514306 - None
2025-01-30 11:09 - INFO - Model Configuration: {'config': {'max_position_embeddings': 514, 'hidden_size': 768, 'num_attention_heads': 12, 'num_hidden_layers': 6, 'type_vocab_size': 1, 'attn_mechanism': 'eager', 'num_labels': 2, 'position_embedding_type': 'relative_key'}, 'settings': {'task': 'glue', 'exp_name': 'glue_template', 'save_path': '/home/lvpoellhuber/storage/models/finetune_roberta/roberta/roberta_1', 'model': 'FacebookAI/roberta-base', 'train': False, 'validate': False, 'evaluate': True, 'logging': False, 'epochs': 10, 'batch_size': 16, 'lr': 0.0001, 'dataset_dir': '/home/lvpoellhuber/storage/datasets', 'tokenizer': 'FacebookAI/roberta-base', 'eval_strategy': 'no', 'accelerate': True, 'weight_decay': 0, 'adam_beta1': 0.9, 'adam_beta2': 0.98, 'lr_scheduler_type': 'linear', 'warmup_steps': 0, 'warmup_ratio': 0.6, 'save_strategy': 'epoch', 'save_total_limit': 1, 'optim': 'adamw_torch', 'auto_find_batch_size': False, 'resume_from_checkpoint': None}}
2025-01-30 11:10 - WARNING - Using the latest cached version of the module from /home/lvpoellhuber/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--glue/05234ba7acc44554edcca0978db5fa3bc600eeee66229abe79ff9887eacaf3ed (last modified on Wed Jan 29 12:35:48 2025) since it couldn't be found locally at evaluate-metric--glue, or remotely on the Hugging Face Hub.
2025-01-30 11:10 - WARNING - Using the latest cached version of the module from /home/lvpoellhuber/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--accuracy/f887c0aab52c2d38e1f8a215681126379eca617f96c447638f751434e8e65b14 (last modified on Wed Jan 29 12:50:09 2025) since it couldn't be found locally at evaluate-metric--accuracy, or remotely on the Hugging Face Hub.
2025-01-30 11:10 - INFO - config set model/num_parameters = 82514306 - None
2025-01-30 11:11 - INFO - Model Configuration: {'config': {'max_position_embeddings': 514, 'hidden_size': 768, 'num_attention_heads': 12, 'num_hidden_layers': 6, 'type_vocab_size': 1, 'attn_mechanism': 'eager', 'num_labels': 2, 'position_embedding_type': 'relative_key'}, 'settings': {'task': 'glue', 'exp_name': 'glue_template', 'save_path': '/home/lvpoellhuber/storage/models/finetune_roberta/roberta/roberta_1', 'model': 'FacebookAI/roberta-base', 'train': False, 'validate': False, 'evaluate': True, 'logging': False, 'epochs': 10, 'batch_size': 16, 'lr': 0.0001, 'dataset_dir': '/home/lvpoellhuber/storage/datasets', 'tokenizer': 'FacebookAI/roberta-base', 'eval_strategy': 'no', 'accelerate': True, 'weight_decay': 0, 'adam_beta1': 0.9, 'adam_beta2': 0.98, 'lr_scheduler_type': 'linear', 'warmup_steps': 0, 'warmup_ratio': 0.6, 'save_strategy': 'epoch', 'save_total_limit': 1, 'optim': 'adamw_torch', 'auto_find_batch_size': False, 'resume_from_checkpoint': None}}
2025-01-30 11:11 - WARNING - Using the latest cached version of the module from /home/lvpoellhuber/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--glue/05234ba7acc44554edcca0978db5fa3bc600eeee66229abe79ff9887eacaf3ed (last modified on Wed Jan 29 12:35:48 2025) since it couldn't be found locally at evaluate-metric--glue, or remotely on the Hugging Face Hub.
2025-01-30 11:12 - WARNING - Using the latest cached version of the module from /home/lvpoellhuber/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--accuracy/f887c0aab52c2d38e1f8a215681126379eca617f96c447638f751434e8e65b14 (last modified on Wed Jan 29 12:50:09 2025) since it couldn't be found locally at evaluate-metric--accuracy, or remotely on the Hugging Face Hub.
2025-01-30 11:12 - INFO - config set model/num_parameters = 82514306 - None
2025-01-30 11:12 - INFO - Model Configuration: {'config': {'max_position_embeddings': 514, 'hidden_size': 768, 'num_attention_heads': 12, 'num_hidden_layers': 6, 'type_vocab_size': 1, 'attn_mechanism': 'eager', 'num_labels': 2, 'position_embedding_type': 'relative_key'}, 'settings': {'task': 'glue', 'exp_name': 'glue_template', 'save_path': '/home/lvpoellhuber/storage/models/finetune_roberta/roberta/roberta_1', 'model': 'FacebookAI/roberta-base', 'train': False, 'validate': False, 'evaluate': True, 'logging': False, 'epochs': 10, 'batch_size': 16, 'lr': 0.0001, 'dataset_dir': '/home/lvpoellhuber/storage/datasets', 'tokenizer': 'FacebookAI/roberta-base', 'eval_strategy': 'no', 'accelerate': True, 'weight_decay': 0, 'adam_beta1': 0.9, 'adam_beta2': 0.98, 'lr_scheduler_type': 'linear', 'warmup_steps': 0, 'warmup_ratio': 0.6, 'save_strategy': 'epoch', 'save_total_limit': 1, 'optim': 'adamw_torch', 'auto_find_batch_size': False, 'resume_from_checkpoint': None}}
2025-01-30 11:13 - WARNING - Using the latest cached version of the module from /home/lvpoellhuber/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--glue/05234ba7acc44554edcca0978db5fa3bc600eeee66229abe79ff9887eacaf3ed (last modified on Wed Jan 29 12:35:48 2025) since it couldn't be found locally at evaluate-metric--glue, or remotely on the Hugging Face Hub.
2025-01-30 11:13 - WARNING - Using the latest cached version of the module from /home/lvpoellhuber/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--accuracy/f887c0aab52c2d38e1f8a215681126379eca617f96c447638f751434e8e65b14 (last modified on Wed Jan 29 12:50:09 2025) since it couldn't be found locally at evaluate-metric--accuracy, or remotely on the Hugging Face Hub.
2025-01-30 11:13 - INFO - config set model/num_parameters = 82514306 - None
2025-01-30 11:13 - INFO - Model Configuration: {'config': {'max_position_embeddings': 514, 'hidden_size': 768, 'num_attention_heads': 12, 'num_hidden_layers': 6, 'type_vocab_size': 1, 'attn_mechanism': 'eager', 'num_labels': 2, 'position_embedding_type': 'relative_key'}, 'settings': {'task': 'glue', 'exp_name': 'glue_template', 'save_path': '/home/lvpoellhuber/storage/models/finetune_roberta/roberta/roberta_1', 'model': 'FacebookAI/roberta-base', 'train': False, 'validate': False, 'evaluate': True, 'logging': False, 'epochs': 10, 'batch_size': 16, 'lr': 0.0001, 'dataset_dir': '/home/lvpoellhuber/storage/datasets', 'tokenizer': 'FacebookAI/roberta-base', 'eval_strategy': 'no', 'accelerate': True, 'weight_decay': 0, 'adam_beta1': 0.9, 'adam_beta2': 0.98, 'lr_scheduler_type': 'linear', 'warmup_steps': 0, 'warmup_ratio': 0.6, 'save_strategy': 'epoch', 'save_total_limit': 1, 'optim': 'adamw_torch', 'auto_find_batch_size': False, 'resume_from_checkpoint': None}}
2025-01-30 11:14 - WARNING - Using the latest cached version of the module from /home/lvpoellhuber/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--glue/05234ba7acc44554edcca0978db5fa3bc600eeee66229abe79ff9887eacaf3ed (last modified on Wed Jan 29 12:35:48 2025) since it couldn't be found locally at evaluate-metric--glue, or remotely on the Hugging Face Hub.
2025-01-30 11:14 - WARNING - Using the latest cached version of the module from /home/lvpoellhuber/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--accuracy/f887c0aab52c2d38e1f8a215681126379eca617f96c447638f751434e8e65b14 (last modified on Wed Jan 29 12:50:09 2025) since it couldn't be found locally at evaluate-metric--accuracy, or remotely on the Hugging Face Hub.
2025-01-30 11:14 - INFO - config set model/num_parameters = 82514306 - None
2025-01-30 11:14 - INFO - Model Configuration: {'config': {'max_position_embeddings': 514, 'hidden_size': 768, 'num_attention_heads': 12, 'num_hidden_layers': 6, 'type_vocab_size': 1, 'attn_mechanism': 'eager', 'num_labels': 2, 'position_embedding_type': 'relative_key'}, 'settings': {'task': 'glue', 'exp_name': 'glue_template', 'save_path': '/home/lvpoellhuber/storage/models/finetune_roberta/roberta/roberta_1', 'model': 'FacebookAI/roberta-base', 'train': False, 'validate': False, 'evaluate': True, 'logging': False, 'epochs': 10, 'batch_size': 16, 'lr': 0.0001, 'dataset_dir': '/home/lvpoellhuber/storage/datasets', 'tokenizer': 'FacebookAI/roberta-base', 'eval_strategy': 'no', 'accelerate': True, 'weight_decay': 0, 'adam_beta1': 0.9, 'adam_beta2': 0.98, 'lr_scheduler_type': 'linear', 'warmup_steps': 0, 'warmup_ratio': 0.6, 'save_strategy': 'epoch', 'save_total_limit': 1, 'optim': 'adamw_torch', 'auto_find_batch_size': False, 'resume_from_checkpoint': None}}
2025-01-30 11:15 - WARNING - Using the latest cached version of the module from /home/lvpoellhuber/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--glue/05234ba7acc44554edcca0978db5fa3bc600eeee66229abe79ff9887eacaf3ed (last modified on Wed Jan 29 12:35:48 2025) since it couldn't be found locally at evaluate-metric--glue, or remotely on the Hugging Face Hub.
2025-01-30 11:15 - WARNING - Using the latest cached version of the module from /home/lvpoellhuber/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--accuracy/f887c0aab52c2d38e1f8a215681126379eca617f96c447638f751434e8e65b14 (last modified on Wed Jan 29 12:50:09 2025) since it couldn't be found locally at evaluate-metric--accuracy, or remotely on the Hugging Face Hub.
2025-01-30 11:15 - INFO - config set model/num_parameters = 82514306 - None
2025-01-30 11:18 - INFO - Model Configuration: {'config': {'max_position_embeddings': 514, 'hidden_size': 768, 'num_attention_heads': 12, 'num_hidden_layers': 6, 'type_vocab_size': 1, 'attn_mechanism': 'eager', 'num_labels': 2, 'position_embedding_type': 'relative_key'}, 'settings': {'task': 'glue', 'exp_name': 'glue_template', 'save_path': '/home/lvpoellhuber/storage/models/finetune_roberta/roberta/roberta_1', 'model': 'FacebookAI/roberta-base', 'train': False, 'validate': False, 'evaluate': True, 'logging': False, 'epochs': 10, 'batch_size': 16, 'lr': 0.0001, 'dataset_dir': '/home/lvpoellhuber/storage/datasets', 'tokenizer': 'FacebookAI/roberta-base', 'eval_strategy': 'no', 'accelerate': True, 'weight_decay': 0, 'adam_beta1': 0.9, 'adam_beta2': 0.98, 'lr_scheduler_type': 'linear', 'warmup_steps': 0, 'warmup_ratio': 0.6, 'save_strategy': 'epoch', 'save_total_limit': 1, 'optim': 'adamw_torch', 'auto_find_batch_size': False, 'resume_from_checkpoint': None}}
2025-01-30 11:18 - WARNING - Using the latest cached version of the module from /home/lvpoellhuber/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--glue/05234ba7acc44554edcca0978db5fa3bc600eeee66229abe79ff9887eacaf3ed (last modified on Wed Jan 29 12:35:48 2025) since it couldn't be found locally at evaluate-metric--glue, or remotely on the Hugging Face Hub.
2025-01-30 11:18 - WARNING - Using the latest cached version of the module from /home/lvpoellhuber/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--accuracy/f887c0aab52c2d38e1f8a215681126379eca617f96c447638f751434e8e65b14 (last modified on Wed Jan 29 12:50:09 2025) since it couldn't be found locally at evaluate-metric--accuracy, or remotely on the Hugging Face Hub.
2025-01-30 11:18 - INFO - config set model/num_parameters = 82514306 - None
2025-01-30 11:25 - INFO - Model Configuration: {'config': {'max_position_embeddings': 514, 'hidden_size': 768, 'num_attention_heads': 12, 'num_hidden_layers': 6, 'type_vocab_size': 1, 'attn_mechanism': 'eager', 'num_labels': 2, 'position_embedding_type': 'relative_key'}, 'settings': {'task': 'glue', 'exp_name': 'glue_template', 'save_path': '/home/lvpoellhuber/storage/models/finetune_roberta/roberta/roberta_1', 'model': 'FacebookAI/roberta-base', 'train': False, 'validate': False, 'evaluate': True, 'logging': False, 'epochs': 10, 'batch_size': 16, 'lr': 0.0001, 'dataset_dir': '/home/lvpoellhuber/storage/datasets', 'tokenizer': 'FacebookAI/roberta-base', 'eval_strategy': 'no', 'accelerate': True, 'weight_decay': 0, 'adam_beta1': 0.9, 'adam_beta2': 0.98, 'lr_scheduler_type': 'linear', 'warmup_steps': 0, 'warmup_ratio': 0.6, 'save_strategy': 'epoch', 'save_total_limit': 1, 'optim': 'adamw_torch', 'auto_find_batch_size': False, 'resume_from_checkpoint': None}}
2025-01-30 11:25 - WARNING - Using the latest cached version of the module from /home/lvpoellhuber/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--glue/05234ba7acc44554edcca0978db5fa3bc600eeee66229abe79ff9887eacaf3ed (last modified on Wed Jan 29 12:35:48 2025) since it couldn't be found locally at evaluate-metric--glue, or remotely on the Hugging Face Hub.
2025-01-30 11:25 - WARNING - Using the latest cached version of the module from /home/lvpoellhuber/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--accuracy/f887c0aab52c2d38e1f8a215681126379eca617f96c447638f751434e8e65b14 (last modified on Wed Jan 29 12:50:09 2025) since it couldn't be found locally at evaluate-metric--accuracy, or remotely on the Hugging Face Hub.
2025-01-30 11:25 - INFO - config set model/num_parameters = 82514306 - None
2025-01-30 11:27 - WARNING - Using the latest cached version of the module from /home/lvpoellhuber/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--glue/05234ba7acc44554edcca0978db5fa3bc600eeee66229abe79ff9887eacaf3ed (last modified on Wed Jan 29 12:35:48 2025) since it couldn't be found locally at evaluate-metric--glue, or remotely on the Hugging Face Hub.
2025-01-30 11:27 - WARNING - Using the latest cached version of the module from /home/lvpoellhuber/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--accuracy/f887c0aab52c2d38e1f8a215681126379eca617f96c447638f751434e8e65b14 (last modified on Wed Jan 29 12:50:09 2025) since it couldn't be found locally at evaluate-metric--accuracy, or remotely on the Hugging Face Hub.
2025-01-30 11:27 - INFO - config set model/num_parameters = 82514306 - None
2025-01-30 11:30 - WARNING - Using the latest cached version of the module from /home/lvpoellhuber/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--glue/05234ba7acc44554edcca0978db5fa3bc600eeee66229abe79ff9887eacaf3ed (last modified on Wed Jan 29 12:35:48 2025) since it couldn't be found locally at evaluate-metric--glue, or remotely on the Hugging Face Hub.
2025-01-30 11:30 - WARNING - Using the latest cached version of the module from /home/lvpoellhuber/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--accuracy/f887c0aab52c2d38e1f8a215681126379eca617f96c447638f751434e8e65b14 (last modified on Wed Jan 29 12:50:09 2025) since it couldn't be found locally at evaluate-metric--accuracy, or remotely on the Hugging Face Hub.
2025-01-30 11:30 - INFO - config set model/num_parameters = 82514306 - None
2025-01-30 11:31 - INFO - Model Configuration: {'config': {'max_position_embeddings': 514, 'hidden_size': 768, 'num_attention_heads': 12, 'num_hidden_layers': 6, 'type_vocab_size': 1, 'attn_mechanism': 'eager', 'num_labels': 2, 'position_embedding_type': 'relative_key'}, 'settings': {'task': 'glue', 'exp_name': 'test', 'save_path': '/home/lvpoellhuber/storage/models/finetune_roberta/roberta/roberta_1', 'model': 'FacebookAI/roberta-base', 'train': False, 'validate': False, 'evaluate': True, 'logging': False, 'epochs': 10, 'batch_size': 16, 'lr': 0.0001, 'dataset_dir': '/home/lvpoellhuber/storage/datasets', 'tokenizer': 'FacebookAI/roberta-base', 'eval_strategy': 'no', 'accelerate': True, 'weight_decay': 0, 'adam_beta1': 0.9, 'adam_beta2': 0.98, 'lr_scheduler_type': 'linear', 'warmup_steps': 0, 'warmup_ratio': 0.6, 'save_strategy': 'epoch', 'save_total_limit': 1, 'optim': 'adamw_torch', 'auto_find_batch_size': False, 'resume_from_checkpoint': None}}
2025-01-30 11:31 - INFO - Model Configuration: {'config': {'max_position_embeddings': 514, 'hidden_size': 768, 'num_attention_heads': 12, 'num_hidden_layers': 6, 'type_vocab_size': 1, 'attn_mechanism': 'eager', 'num_labels': 2, 'position_embedding_type': 'relative_key'}, 'settings': {'task': 'glue', 'exp_name': 'glue_template', 'save_path': '/home/lvpoellhuber/storage/models/finetune_roberta/roberta/roberta_1', 'model': 'FacebookAI/roberta-base', 'train': False, 'validate': False, 'evaluate': True, 'logging': False, 'epochs': 10, 'batch_size': 16, 'lr': 0.0001, 'dataset_dir': '/home/lvpoellhuber/storage/datasets', 'tokenizer': 'FacebookAI/roberta-base', 'eval_strategy': 'no', 'accelerate': True, 'weight_decay': 0, 'adam_beta1': 0.9, 'adam_beta2': 0.98, 'lr_scheduler_type': 'linear', 'warmup_steps': 0, 'warmup_ratio': 0.6, 'save_strategy': 'epoch', 'save_total_limit': 1, 'optim': 'adamw_torch', 'auto_find_batch_size': False, 'resume_from_checkpoint': None}}
2025-01-30 11:32 - WARNING - Using the latest cached version of the module from /home/lvpoellhuber/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--glue/05234ba7acc44554edcca0978db5fa3bc600eeee66229abe79ff9887eacaf3ed (last modified on Wed Jan 29 12:35:48 2025) since it couldn't be found locally at evaluate-metric--glue, or remotely on the Hugging Face Hub.
2025-01-30 11:32 - WARNING - Using the latest cached version of the module from /home/lvpoellhuber/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--accuracy/f887c0aab52c2d38e1f8a215681126379eca617f96c447638f751434e8e65b14 (last modified on Wed Jan 29 12:50:09 2025) since it couldn't be found locally at evaluate-metric--accuracy, or remotely on the Hugging Face Hub.
2025-01-30 11:32 - INFO - config set model/num_parameters = 82514306 - None
2025-01-30 11:32 - INFO - Model Configuration: {'config': {'max_position_embeddings': 514, 'hidden_size': 768, 'num_attention_heads': 12, 'num_hidden_layers': 6, 'type_vocab_size': 1, 'attn_mechanism': 'eager', 'num_labels': 2, 'position_embedding_type': 'relative_key'}, 'settings': {'task': 'glue', 'exp_name': 'glue_template', 'save_path': '/home/lvpoellhuber/storage/models/finetune_roberta/roberta/roberta_1', 'model': 'FacebookAI/roberta-base', 'train': False, 'validate': False, 'evaluate': True, 'logging': False, 'epochs': 10, 'batch_size': 16, 'lr': 0.0001, 'dataset_dir': '/home/lvpoellhuber/storage/datasets', 'tokenizer': 'FacebookAI/roberta-base', 'eval_strategy': 'no', 'accelerate': True, 'weight_decay': 0, 'adam_beta1': 0.9, 'adam_beta2': 0.98, 'lr_scheduler_type': 'linear', 'warmup_steps': 0, 'warmup_ratio': 0.6, 'save_strategy': 'epoch', 'save_total_limit': 1, 'optim': 'adamw_torch', 'auto_find_batch_size': False, 'resume_from_checkpoint': None}}
2025-01-30 11:33 - WARNING - Using the latest cached version of the module from /home/lvpoellhuber/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--glue/05234ba7acc44554edcca0978db5fa3bc600eeee66229abe79ff9887eacaf3ed (last modified on Wed Jan 29 12:35:48 2025) since it couldn't be found locally at evaluate-metric--glue, or remotely on the Hugging Face Hub.
2025-01-30 11:33 - WARNING - Using the latest cached version of the module from /home/lvpoellhuber/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--accuracy/f887c0aab52c2d38e1f8a215681126379eca617f96c447638f751434e8e65b14 (last modified on Wed Jan 29 12:50:09 2025) since it couldn't be found locally at evaluate-metric--accuracy, or remotely on the Hugging Face Hub.
2025-01-30 11:33 - INFO - config set model/num_parameters = 82514306 - None
2025-01-30 11:35 - INFO - Model Configuration: {'config': {'max_position_embeddings': 514, 'hidden_size': 768, 'num_attention_heads': 12, 'num_hidden_layers': 6, 'type_vocab_size': 1, 'attn_mechanism': 'eager', 'num_labels': 2, 'position_embedding_type': 'relative_key'}, 'settings': {'task': 'glue', 'exp_name': 'glue_template', 'save_path': '/home/lvpoellhuber/storage/models/finetune_roberta/roberta/roberta_1', 'model': 'FacebookAI/roberta-base', 'train': False, 'validate': False, 'evaluate': True, 'logging': False, 'epochs': 10, 'batch_size': 16, 'lr': 0.0001, 'dataset_dir': '/home/lvpoellhuber/storage/datasets', 'tokenizer': 'FacebookAI/roberta-base', 'eval_strategy': 'no', 'accelerate': True, 'weight_decay': 0, 'adam_beta1': 0.9, 'adam_beta2': 0.98, 'lr_scheduler_type': 'linear', 'warmup_steps': 0, 'warmup_ratio': 0.6, 'save_strategy': 'epoch', 'save_total_limit': 1, 'optim': 'adamw_torch', 'auto_find_batch_size': False, 'resume_from_checkpoint': None}}
2025-01-30 11:36 - WARNING - Using the latest cached version of the module from /home/lvpoellhuber/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--glue/05234ba7acc44554edcca0978db5fa3bc600eeee66229abe79ff9887eacaf3ed (last modified on Wed Jan 29 12:35:48 2025) since it couldn't be found locally at evaluate-metric--glue, or remotely on the Hugging Face Hub.
2025-01-30 11:36 - WARNING - Using the latest cached version of the module from /home/lvpoellhuber/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--accuracy/f887c0aab52c2d38e1f8a215681126379eca617f96c447638f751434e8e65b14 (last modified on Wed Jan 29 12:50:09 2025) since it couldn't be found locally at evaluate-metric--accuracy, or remotely on the Hugging Face Hub.
2025-01-30 11:36 - INFO - config set model/num_parameters = 82514306 - None
2025-01-30 11:37 - INFO - Model Configuration: {'config': {'max_position_embeddings': 514, 'hidden_size': 768, 'num_attention_heads': 12, 'num_hidden_layers': 6, 'type_vocab_size': 1, 'attn_mechanism': 'eager', 'num_labels': 2, 'position_embedding_type': 'relative_key'}, 'settings': {'task': 'glue', 'exp_name': 'glue_template', 'save_path': '/home/lvpoellhuber/storage/models/finetune_roberta/roberta/roberta_1', 'model': 'FacebookAI/roberta-base', 'train': False, 'validate': False, 'evaluate': True, 'logging': False, 'epochs': 10, 'batch_size': 16, 'lr': 0.0001, 'dataset_dir': '/home/lvpoellhuber/storage/datasets', 'tokenizer': 'FacebookAI/roberta-base', 'eval_strategy': 'no', 'accelerate': True, 'weight_decay': 0, 'adam_beta1': 0.9, 'adam_beta2': 0.98, 'lr_scheduler_type': 'linear', 'warmup_steps': 0, 'warmup_ratio': 0.6, 'save_strategy': 'epoch', 'save_total_limit': 1, 'optim': 'adamw_torch', 'auto_find_batch_size': False, 'resume_from_checkpoint': None}}
2025-01-30 11:37 - WARNING - Using the latest cached version of the module from /home/lvpoellhuber/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--glue/05234ba7acc44554edcca0978db5fa3bc600eeee66229abe79ff9887eacaf3ed (last modified on Wed Jan 29 12:35:48 2025) since it couldn't be found locally at evaluate-metric--glue, or remotely on the Hugging Face Hub.
2025-01-30 11:37 - WARNING - Using the latest cached version of the module from /home/lvpoellhuber/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--accuracy/f887c0aab52c2d38e1f8a215681126379eca617f96c447638f751434e8e65b14 (last modified on Wed Jan 29 12:50:09 2025) since it couldn't be found locally at evaluate-metric--accuracy, or remotely on the Hugging Face Hub.
2025-01-30 11:38 - INFO - config set model/num_parameters = 82514306 - None
2025-01-30 11:41 - INFO - Model Configuration: {'config': {'max_position_embeddings': 514, 'hidden_size': 768, 'num_attention_heads': 12, 'num_hidden_layers': 6, 'type_vocab_size': 1, 'attn_mechanism': 'eager', 'num_labels': 2, 'position_embedding_type': 'relative_key'}, 'settings': {'task': 'glue', 'exp_name': 'glue_template', 'save_path': '/home/lvpoellhuber/storage/models/finetune_roberta/roberta/roberta_1', 'model': 'FacebookAI/roberta-base', 'train': False, 'validate': False, 'evaluate': True, 'logging': False, 'epochs': 10, 'batch_size': 16, 'lr': 0.0001, 'dataset_dir': '/home/lvpoellhuber/storage/datasets', 'tokenizer': 'FacebookAI/roberta-base', 'eval_strategy': 'no', 'accelerate': True, 'weight_decay': 0, 'adam_beta1': 0.9, 'adam_beta2': 0.98, 'lr_scheduler_type': 'linear', 'warmup_steps': 0, 'warmup_ratio': 0.6, 'save_strategy': 'epoch', 'save_total_limit': 1, 'optim': 'adamw_torch', 'auto_find_batch_size': False, 'resume_from_checkpoint': None}}
2025-01-30 11:41 - WARNING - Using the latest cached version of the module from /home/lvpoellhuber/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--glue/05234ba7acc44554edcca0978db5fa3bc600eeee66229abe79ff9887eacaf3ed (last modified on Wed Jan 29 12:35:48 2025) since it couldn't be found locally at evaluate-metric--glue, or remotely on the Hugging Face Hub.
2025-01-30 11:41 - WARNING - Using the latest cached version of the module from /home/lvpoellhuber/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--accuracy/f887c0aab52c2d38e1f8a215681126379eca617f96c447638f751434e8e65b14 (last modified on Wed Jan 29 12:50:09 2025) since it couldn't be found locally at evaluate-metric--accuracy, or remotely on the Hugging Face Hub.
2025-01-30 11:41 - INFO - config set model/num_parameters = 82514306 - None
2025-01-30 11:43 - INFO - Model Configuration: {'config': {'max_position_embeddings': 514, 'hidden_size': 768, 'num_attention_heads': 12, 'num_hidden_layers': 6, 'type_vocab_size': 1, 'attn_mechanism': 'eager', 'num_labels': 2, 'position_embedding_type': 'relative_key'}, 'settings': {'task': 'glue', 'exp_name': 'glue_template', 'save_path': '/home/lvpoellhuber/storage/models/finetune_roberta/roberta/roberta_1', 'model': 'FacebookAI/roberta-base', 'train': False, 'validate': False, 'evaluate': True, 'logging': False, 'epochs': 10, 'batch_size': 16, 'lr': 0.0001, 'dataset_dir': '/home/lvpoellhuber/storage/datasets', 'tokenizer': 'FacebookAI/roberta-base', 'eval_strategy': 'no', 'accelerate': True, 'weight_decay': 0, 'adam_beta1': 0.9, 'adam_beta2': 0.98, 'lr_scheduler_type': 'linear', 'warmup_steps': 0, 'warmup_ratio': 0.6, 'save_strategy': 'epoch', 'save_total_limit': 1, 'optim': 'adamw_torch', 'auto_find_batch_size': False, 'resume_from_checkpoint': None}}
2025-01-30 11:44 - WARNING - Using the latest cached version of the module from /home/lvpoellhuber/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--glue/05234ba7acc44554edcca0978db5fa3bc600eeee66229abe79ff9887eacaf3ed (last modified on Wed Jan 29 12:35:48 2025) since it couldn't be found locally at evaluate-metric--glue, or remotely on the Hugging Face Hub.
2025-01-30 11:44 - WARNING - Using the latest cached version of the module from /home/lvpoellhuber/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--accuracy/f887c0aab52c2d38e1f8a215681126379eca617f96c447638f751434e8e65b14 (last modified on Wed Jan 29 12:50:09 2025) since it couldn't be found locally at evaluate-metric--accuracy, or remotely on the Hugging Face Hub.
2025-01-30 11:44 - INFO - config set model/num_parameters = 82514306 - None
2025-01-30 11:50 - INFO - Model Configuration: {'config': {'max_position_embeddings': 514, 'hidden_size': 768, 'num_attention_heads': 12, 'num_hidden_layers': 6, 'type_vocab_size': 1, 'attn_mechanism': 'eager', 'num_labels': 2, 'position_embedding_type': 'relative_key'}, 'settings': {'task': 'glue', 'exp_name': 'glue_template', 'save_path': '/home/lvpoellhuber/storage/models/finetune_roberta/roberta/roberta_1', 'model': 'FacebookAI/roberta-base', 'train': False, 'validate': False, 'evaluate': True, 'logging': False, 'epochs': 10, 'batch_size': 16, 'lr': 0.0001, 'dataset_dir': '/home/lvpoellhuber/storage/datasets', 'tokenizer': 'FacebookAI/roberta-base', 'eval_strategy': 'no', 'accelerate': True, 'weight_decay': 0, 'adam_beta1': 0.9, 'adam_beta2': 0.98, 'lr_scheduler_type': 'linear', 'warmup_steps': 0, 'warmup_ratio': 0.6, 'save_strategy': 'epoch', 'save_total_limit': 1, 'optim': 'adamw_torch', 'auto_find_batch_size': False, 'resume_from_checkpoint': None}}
2025-01-30 11:50 - WARNING - Using the latest cached version of the module from /home/lvpoellhuber/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--glue/05234ba7acc44554edcca0978db5fa3bc600eeee66229abe79ff9887eacaf3ed (last modified on Wed Jan 29 12:35:48 2025) since it couldn't be found locally at evaluate-metric--glue, or remotely on the Hugging Face Hub.
2025-01-30 11:50 - WARNING - Using the latest cached version of the module from /home/lvpoellhuber/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--accuracy/f887c0aab52c2d38e1f8a215681126379eca617f96c447638f751434e8e65b14 (last modified on Wed Jan 29 12:50:09 2025) since it couldn't be found locally at evaluate-metric--accuracy, or remotely on the Hugging Face Hub.
2025-01-30 11:50 - INFO - config set model/num_parameters = 82514306 - None
2025-01-30 11:52 - WARNING - Using the latest cached version of the module from /home/lvpoellhuber/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--glue/05234ba7acc44554edcca0978db5fa3bc600eeee66229abe79ff9887eacaf3ed (last modified on Wed Jan 29 12:35:48 2025) since it couldn't be found locally at evaluate-metric--glue, or remotely on the Hugging Face Hub.
2025-01-30 11:52 - WARNING - Using the latest cached version of the module from /home/lvpoellhuber/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--accuracy/f887c0aab52c2d38e1f8a215681126379eca617f96c447638f751434e8e65b14 (last modified on Wed Jan 29 12:50:09 2025) since it couldn't be found locally at evaluate-metric--accuracy, or remotely on the Hugging Face Hub.
2025-01-30 11:52 - INFO - config set model/num_parameters = 82514306 - None
2025-01-30 11:53 - INFO - Model Configuration: {'config': {'max_position_embeddings': 514, 'hidden_size': 768, 'num_attention_heads': 12, 'num_hidden_layers': 6, 'type_vocab_size': 1, 'attn_mechanism': 'eager', 'num_labels': 2, 'position_embedding_type': 'relative_key'}, 'settings': {'task': 'glue', 'exp_name': 'glue_template', 'save_path': '/home/lvpoellhuber/storage/models/finetune_roberta/roberta/roberta_1', 'model': 'FacebookAI/roberta-base', 'train': False, 'validate': False, 'evaluate': True, 'logging': False, 'epochs': 10, 'batch_size': 16, 'lr': 0.0001, 'dataset_dir': '/home/lvpoellhuber/storage/datasets', 'tokenizer': 'FacebookAI/roberta-base', 'eval_strategy': 'no', 'accelerate': True, 'weight_decay': 0, 'adam_beta1': 0.9, 'adam_beta2': 0.98, 'lr_scheduler_type': 'linear', 'warmup_steps': 0, 'warmup_ratio': 0.6, 'save_strategy': 'epoch', 'save_total_limit': 1, 'optim': 'adamw_torch', 'auto_find_batch_size': False, 'resume_from_checkpoint': None}}
2025-01-30 11:53 - WARNING - Using the latest cached version of the module from /home/lvpoellhuber/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--glue/05234ba7acc44554edcca0978db5fa3bc600eeee66229abe79ff9887eacaf3ed (last modified on Wed Jan 29 12:35:48 2025) since it couldn't be found locally at evaluate-metric--glue, or remotely on the Hugging Face Hub.
2025-01-30 11:53 - WARNING - Using the latest cached version of the module from /home/lvpoellhuber/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--accuracy/f887c0aab52c2d38e1f8a215681126379eca617f96c447638f751434e8e65b14 (last modified on Wed Jan 29 12:50:09 2025) since it couldn't be found locally at evaluate-metric--accuracy, or remotely on the Hugging Face Hub.
2025-01-30 11:53 - INFO - config set model/num_parameters = 82514306 - None
2025-01-30 11:55 - INFO - Model Configuration: {'config': {'max_position_embeddings': 514, 'hidden_size': 768, 'num_attention_heads': 12, 'num_hidden_layers': 6, 'type_vocab_size': 1, 'attn_mechanism': 'eager', 'num_labels': 2, 'position_embedding_type': 'relative_key'}, 'settings': {'task': 'glue', 'exp_name': 'test', 'save_path': '/home/lvpoellhuber/storage/models/finetune_roberta/roberta/roberta_1', 'model': 'FacebookAI/roberta-base', 'train': False, 'validate': False, 'evaluate': True, 'logging': False, 'epochs': 10, 'batch_size': 16, 'lr': 0.0001, 'dataset_dir': '/home/lvpoellhuber/storage/datasets', 'tokenizer': 'FacebookAI/roberta-base', 'eval_strategy': 'no', 'accelerate': True, 'weight_decay': 0, 'adam_beta1': 0.9, 'adam_beta2': 0.98, 'lr_scheduler_type': 'linear', 'warmup_steps': 0, 'warmup_ratio': 0.6, 'save_strategy': 'epoch', 'save_total_limit': 1, 'optim': 'adamw_torch', 'auto_find_batch_size': False, 'resume_from_checkpoint': None}}
2025-01-30 11:55 - WARNING - Using the latest cached version of the module from /home/lvpoellhuber/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--glue/05234ba7acc44554edcca0978db5fa3bc600eeee66229abe79ff9887eacaf3ed (last modified on Wed Jan 29 12:35:48 2025) since it couldn't be found locally at evaluate-metric--glue, or remotely on the Hugging Face Hub.
2025-01-30 11:55 - WARNING - Using the latest cached version of the module from /home/lvpoellhuber/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--accuracy/f887c0aab52c2d38e1f8a215681126379eca617f96c447638f751434e8e65b14 (last modified on Wed Jan 29 12:50:09 2025) since it couldn't be found locally at evaluate-metric--accuracy, or remotely on the Hugging Face Hub.
2025-01-30 11:55 - INFO - config set model/num_parameters = 82514306 - None
2025-01-30 11:56 - INFO - Model Configuration: {'config': {'max_position_embeddings': 514, 'hidden_size': 768, 'num_attention_heads': 12, 'num_hidden_layers': 6, 'type_vocab_size': 1, 'attn_mechanism': 'eager', 'num_labels': 2, 'position_embedding_type': 'relative_key'}, 'settings': {'task': 'glue', 'exp_name': 'glue_template', 'save_path': '/home/lvpoellhuber/storage/models/finetune_roberta/roberta/roberta_1', 'model': 'FacebookAI/roberta-base', 'train': False, 'validate': False, 'evaluate': True, 'logging': False, 'epochs': 10, 'batch_size': 16, 'lr': 0.0001, 'dataset_dir': '/home/lvpoellhuber/storage/datasets', 'tokenizer': 'FacebookAI/roberta-base', 'eval_strategy': 'no', 'accelerate': True, 'weight_decay': 0, 'adam_beta1': 0.9, 'adam_beta2': 0.98, 'lr_scheduler_type': 'linear', 'warmup_steps': 0, 'warmup_ratio': 0.6, 'save_strategy': 'epoch', 'save_total_limit': 1, 'optim': 'adamw_torch', 'auto_find_batch_size': False, 'resume_from_checkpoint': None}}
2025-01-30 11:57 - WARNING - Using the latest cached version of the module from /home/lvpoellhuber/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--glue/05234ba7acc44554edcca0978db5fa3bc600eeee66229abe79ff9887eacaf3ed (last modified on Wed Jan 29 12:35:48 2025) since it couldn't be found locally at evaluate-metric--glue, or remotely on the Hugging Face Hub.
2025-01-30 11:57 - WARNING - Using the latest cached version of the module from /home/lvpoellhuber/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--accuracy/f887c0aab52c2d38e1f8a215681126379eca617f96c447638f751434e8e65b14 (last modified on Wed Jan 29 12:50:09 2025) since it couldn't be found locally at evaluate-metric--accuracy, or remotely on the Hugging Face Hub.
2025-01-30 11:57 - INFO - config set model/num_parameters = 82514306 - None
2025-01-30 11:58 - INFO - Model Configuration: {'config': {'max_position_embeddings': 514, 'hidden_size': 768, 'num_attention_heads': 12, 'num_hidden_layers': 6, 'type_vocab_size': 1, 'attn_mechanism': 'eager', 'num_labels': 2, 'position_embedding_type': 'relative_key'}, 'settings': {'task': 'wnli', 'exp_name': 'glue_template', 'save_path': '/home/lvpoellhuber/storage/models/finetune_roberta/roberta/roberta_1', 'model': 'FacebookAI/roberta-base', 'train': True, 'validate': False, 'evaluate': True, 'logging': False, 'epochs': 10, 'batch_size': 16, 'lr': 0.0001, 'dataset_dir': '/home/lvpoellhuber/storage/datasets', 'tokenizer': 'FacebookAI/roberta-base', 'eval_strategy': 'no', 'accelerate': True, 'weight_decay': 0, 'adam_beta1': 0.9, 'adam_beta2': 0.98, 'lr_scheduler_type': 'linear', 'warmup_steps': 0, 'warmup_ratio': 0.6, 'save_strategy': 'epoch', 'save_total_limit': 1, 'optim': 'adamw_torch', 'auto_find_batch_size': False, 'resume_from_checkpoint': None}}
2025-01-30 11:58 - INFO - config set model/num_parameters = 82514306 - None
2025-01-30 11:59 - INFO - Model Configuration: {'config': {'max_position_embeddings': 514, 'hidden_size': 768, 'num_attention_heads': 12, 'num_hidden_layers': 6, 'type_vocab_size': 1, 'attn_mechanism': 'eager', 'num_labels': 2, 'position_embedding_type': 'relative_key'}, 'settings': {'task': 'wnli', 'exp_name': 'test', 'save_path': '/home/lvpoellhuber/storage/models/new-attention/roberta/test', 'model': 'FacebookAI/roberta-base', 'train': True, 'validate': False, 'evaluate': True, 'logging': False, 'epochs': 10, 'batch_size': 16, 'lr': 0.0001, 'dataset_dir': '/home/lvpoellhuber/storage/datasets', 'tokenizer': 'FacebookAI/roberta-base', 'eval_strategy': 'no', 'accelerate': True, 'weight_decay': 0, 'adam_beta1': 0.9, 'adam_beta2': 0.98, 'lr_scheduler_type': 'linear', 'warmup_steps': 0, 'warmup_ratio': 0.6, 'save_strategy': 'epoch', 'save_total_limit': 1, 'optim': 'adamw_torch', 'auto_find_batch_size': False, 'resume_from_checkpoint': None}}
2025-01-30 11:59 - INFO - config set model/num_parameters = 82514306 - None
2025-01-30 12:02 - INFO - Model Configuration: {'config': {'max_position_embeddings': 514, 'hidden_size': 768, 'num_attention_heads': 12, 'num_hidden_layers': 6, 'type_vocab_size': 1, 'attn_mechanism': 'eager', 'num_labels': 2, 'position_embedding_type': 'relative_key'}, 'settings': {'task': 'wnli', 'exp_name': 'test', 'save_path': '/home/lvpoellhuber/storage/models/finetune_roberta/roberta/roberta_1', 'model': 'FacebookAI/roberta-base', 'train': False, 'validate': False, 'evaluate': True, 'logging': False, 'epochs': 10, 'batch_size': 16, 'lr': 0.0001, 'dataset_dir': '/home/lvpoellhuber/storage/datasets', 'tokenizer': 'FacebookAI/roberta-base', 'eval_strategy': 'no', 'accelerate': True, 'weight_decay': 0, 'adam_beta1': 0.9, 'adam_beta2': 0.98, 'lr_scheduler_type': 'linear', 'warmup_steps': 0, 'warmup_ratio': 0.6, 'save_strategy': 'epoch', 'save_total_limit': 1, 'optim': 'adamw_torch', 'auto_find_batch_size': False, 'resume_from_checkpoint': None}}
2025-01-30 12:03 - INFO - Model Configuration: {'config': {'max_position_embeddings': 514, 'hidden_size': 768, 'num_attention_heads': 12, 'num_hidden_layers': 6, 'type_vocab_size': 1, 'attn_mechanism': 'eager', 'num_labels': 2, 'position_embedding_type': 'relative_key'}, 'settings': {'task': 'wnli', 'exp_name': 'test', 'save_path': '/home/lvpoellhuber/storage/models/finetune_roberta/roberta/roberta_1', 'model': 'FacebookAI/roberta-base', 'train': False, 'validate': False, 'evaluate': True, 'logging': False, 'epochs': 10, 'batch_size': 16, 'lr': 0.0001, 'dataset_dir': '/home/lvpoellhuber/storage/datasets', 'tokenizer': 'FacebookAI/roberta-base', 'eval_strategy': 'no', 'accelerate': True, 'weight_decay': 0, 'adam_beta1': 0.9, 'adam_beta2': 0.98, 'lr_scheduler_type': 'linear', 'warmup_steps': 0, 'warmup_ratio': 0.6, 'save_strategy': 'epoch', 'save_total_limit': 1, 'optim': 'adamw_torch', 'auto_find_batch_size': False, 'resume_from_checkpoint': None}}
2025-01-30 12:03 - WARNING - Using the latest cached version of the module from /home/lvpoellhuber/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--glue/05234ba7acc44554edcca0978db5fa3bc600eeee66229abe79ff9887eacaf3ed (last modified on Wed Jan 29 12:35:48 2025) since it couldn't be found locally at evaluate-metric--glue, or remotely on the Hugging Face Hub.
2025-01-30 12:03 - WARNING - Using the latest cached version of the module from /home/lvpoellhuber/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--accuracy/f887c0aab52c2d38e1f8a215681126379eca617f96c447638f751434e8e65b14 (last modified on Wed Jan 29 12:50:09 2025) since it couldn't be found locally at evaluate-metric--accuracy, or remotely on the Hugging Face Hub.
2025-01-30 12:03 - INFO - config set model/num_parameters = 82514306 - None
2025-01-30 12:05 - INFO - Model Configuration: {'config': {'max_position_embeddings': 514, 'hidden_size': 768, 'num_attention_heads': 12, 'num_hidden_layers': 6, 'type_vocab_size': 1, 'attn_mechanism': 'eager', 'num_labels': 2, 'position_embedding_type': 'relative_key'}, 'settings': {'task': 'wnli', 'exp_name': 'test', 'save_path': '/home/lvpoellhuber/storage/models/finetune_roberta/roberta/roberta_1', 'model': 'FacebookAI/roberta-base', 'train': False, 'validate': False, 'evaluate': True, 'logging': False, 'epochs': 10, 'batch_size': 16, 'lr': 0.0001, 'dataset_dir': '/home/lvpoellhuber/storage/datasets', 'tokenizer': 'FacebookAI/roberta-base', 'eval_strategy': 'no', 'accelerate': True, 'weight_decay': 0, 'adam_beta1': 0.9, 'adam_beta2': 0.98, 'lr_scheduler_type': 'linear', 'warmup_steps': 0, 'warmup_ratio': 0.6, 'save_strategy': 'epoch', 'save_total_limit': 1, 'optim': 'adamw_torch', 'auto_find_batch_size': False, 'resume_from_checkpoint': None}}
2025-01-30 12:06 - INFO - Model Configuration: {'config': {'max_position_embeddings': 514, 'hidden_size': 768, 'num_attention_heads': 12, 'num_hidden_layers': 6, 'type_vocab_size': 1, 'attn_mechanism': 'eager', 'num_labels': 2, 'position_embedding_type': 'relative_key'}, 'settings': {'task': 'wnli', 'exp_name': 'test', 'save_path': '/home/lvpoellhuber/storage/models/finetune_roberta/roberta/roberta_1', 'model': 'FacebookAI/roberta-base', 'train': False, 'validate': False, 'evaluate': True, 'logging': False, 'epochs': 10, 'batch_size': 16, 'lr': 0.0001, 'dataset_dir': '/home/lvpoellhuber/storage/datasets', 'tokenizer': 'FacebookAI/roberta-base', 'eval_strategy': 'no', 'accelerate': True, 'weight_decay': 0, 'adam_beta1': 0.9, 'adam_beta2': 0.98, 'lr_scheduler_type': 'linear', 'warmup_steps': 0, 'warmup_ratio': 0.6, 'save_strategy': 'epoch', 'save_total_limit': 1, 'optim': 'adamw_torch', 'auto_find_batch_size': False, 'resume_from_checkpoint': None}}
2025-01-30 12:06 - INFO - Model Configuration: {'config': {'max_position_embeddings': 514, 'hidden_size': 768, 'num_attention_heads': 12, 'num_hidden_layers': 6, 'type_vocab_size': 1, 'attn_mechanism': 'eager', 'num_labels': 2, 'position_embedding_type': 'relative_key'}, 'settings': {'task': 'wnli', 'exp_name': 'test', 'save_path': '/home/lvpoellhuber/storage/models/finetune_roberta/roberta/roberta_1', 'model': 'FacebookAI/roberta-base', 'train': False, 'validate': False, 'evaluate': True, 'logging': False, 'epochs': 10, 'batch_size': 16, 'lr': 0.0001, 'dataset_dir': '/home/lvpoellhuber/storage/datasets', 'tokenizer': 'FacebookAI/roberta-base', 'eval_strategy': 'no', 'accelerate': True, 'weight_decay': 0, 'adam_beta1': 0.9, 'adam_beta2': 0.98, 'lr_scheduler_type': 'linear', 'warmup_steps': 0, 'warmup_ratio': 0.6, 'save_strategy': 'epoch', 'save_total_limit': 1, 'optim': 'adamw_torch', 'auto_find_batch_size': False, 'resume_from_checkpoint': None}}
2025-01-30 12:07 - INFO - Model Configuration: {'config': {'max_position_embeddings': 514, 'hidden_size': 768, 'num_attention_heads': 12, 'num_hidden_layers': 6, 'type_vocab_size': 1, 'attn_mechanism': 'eager', 'num_labels': 2, 'position_embedding_type': 'relative_key'}, 'settings': {'task': 'wnli', 'exp_name': 'test', 'save_path': '/home/lvpoellhuber/storage/models/finetune_roberta/roberta/roberta_1', 'model': 'FacebookAI/roberta-base', 'train': False, 'validate': False, 'evaluate': True, 'logging': False, 'epochs': 10, 'batch_size': 16, 'lr': 0.0001, 'dataset_dir': '/home/lvpoellhuber/storage/datasets', 'tokenizer': 'FacebookAI/roberta-base', 'eval_strategy': 'no', 'accelerate': True, 'weight_decay': 0, 'adam_beta1': 0.9, 'adam_beta2': 0.98, 'lr_scheduler_type': 'linear', 'warmup_steps': 0, 'warmup_ratio': 0.6, 'save_strategy': 'epoch', 'save_total_limit': 1, 'optim': 'adamw_torch', 'auto_find_batch_size': False, 'resume_from_checkpoint': None}}
2025-01-30 12:07 - INFO - Model Configuration: {'config': {'max_position_embeddings': 514, 'hidden_size': 768, 'num_attention_heads': 12, 'num_hidden_layers': 6, 'type_vocab_size': 1, 'attn_mechanism': 'eager', 'num_labels': 2, 'position_embedding_type': 'relative_key'}, 'settings': {'task': 'wnli', 'exp_name': 'test', 'save_path': '/home/lvpoellhuber/storage/models/finetune_roberta/roberta/roberta_1', 'model': 'FacebookAI/roberta-base', 'train': False, 'validate': False, 'evaluate': True, 'logging': False, 'epochs': 10, 'batch_size': 16, 'lr': 0.0001, 'dataset_dir': '/home/lvpoellhuber/storage/datasets', 'tokenizer': 'FacebookAI/roberta-base', 'eval_strategy': 'no', 'accelerate': True, 'weight_decay': 0, 'adam_beta1': 0.9, 'adam_beta2': 0.98, 'lr_scheduler_type': 'linear', 'warmup_steps': 0, 'warmup_ratio': 0.6, 'save_strategy': 'epoch', 'save_total_limit': 1, 'optim': 'adamw_torch', 'auto_find_batch_size': False, 'resume_from_checkpoint': None}}
2025-01-30 12:08 - INFO - Model Configuration: {'config': {'max_position_embeddings': 514, 'hidden_size': 768, 'num_attention_heads': 12, 'num_hidden_layers': 6, 'type_vocab_size': 1, 'attn_mechanism': 'eager', 'num_labels': 2, 'position_embedding_type': 'relative_key'}, 'settings': {'task': 'wnli', 'exp_name': 'test', 'save_path': '/home/lvpoellhuber/storage/models/finetune_roberta/roberta/roberta_1', 'model': 'FacebookAI/roberta-base', 'train': False, 'validate': False, 'evaluate': True, 'logging': False, 'epochs': 10, 'batch_size': 16, 'lr': 0.0001, 'dataset_dir': '/home/lvpoellhuber/storage/datasets', 'tokenizer': 'FacebookAI/roberta-base', 'eval_strategy': 'no', 'accelerate': True, 'weight_decay': 0, 'adam_beta1': 0.9, 'adam_beta2': 0.98, 'lr_scheduler_type': 'linear', 'warmup_steps': 0, 'warmup_ratio': 0.6, 'save_strategy': 'epoch', 'save_total_limit': 1, 'optim': 'adamw_torch', 'auto_find_batch_size': False, 'resume_from_checkpoint': None}}
2025-01-30 12:08 - WARNING - Using the latest cached version of the module from /home/lvpoellhuber/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--glue/05234ba7acc44554edcca0978db5fa3bc600eeee66229abe79ff9887eacaf3ed (last modified on Wed Jan 29 12:35:48 2025) since it couldn't be found locally at evaluate-metric--glue, or remotely on the Hugging Face Hub.
2025-01-30 12:08 - WARNING - Using the latest cached version of the module from /home/lvpoellhuber/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--accuracy/f887c0aab52c2d38e1f8a215681126379eca617f96c447638f751434e8e65b14 (last modified on Wed Jan 29 12:50:09 2025) since it couldn't be found locally at evaluate-metric--accuracy, or remotely on the Hugging Face Hub.
2025-01-30 12:09 - INFO - Model Configuration: {'config': {'max_position_embeddings': 514, 'hidden_size': 768, 'num_attention_heads': 12, 'num_hidden_layers': 6, 'type_vocab_size': 1, 'attn_mechanism': 'eager', 'num_labels': 2, 'position_embedding_type': 'relative_key'}, 'settings': {'task': 'wnli', 'exp_name': 'test', 'save_path': '/home/lvpoellhuber/storage/models/finetune_roberta/roberta/roberta_1', 'model': 'FacebookAI/roberta-base', 'train': False, 'validate': False, 'evaluate': True, 'logging': False, 'epochs': 10, 'batch_size': 16, 'lr': 0.0001, 'dataset_dir': '/home/lvpoellhuber/storage/datasets', 'tokenizer': 'FacebookAI/roberta-base', 'eval_strategy': 'no', 'accelerate': True, 'weight_decay': 0, 'adam_beta1': 0.9, 'adam_beta2': 0.98, 'lr_scheduler_type': 'linear', 'warmup_steps': 0, 'warmup_ratio': 0.6, 'save_strategy': 'epoch', 'save_total_limit': 1, 'optim': 'adamw_torch', 'auto_find_batch_size': False, 'resume_from_checkpoint': None}}
2025-01-30 12:09 - WARNING - Using the latest cached version of the module from /home/lvpoellhuber/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--glue/05234ba7acc44554edcca0978db5fa3bc600eeee66229abe79ff9887eacaf3ed (last modified on Wed Jan 29 12:35:48 2025) since it couldn't be found locally at evaluate-metric--glue, or remotely on the Hugging Face Hub.
2025-01-30 12:09 - WARNING - Using the latest cached version of the module from /home/lvpoellhuber/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--accuracy/f887c0aab52c2d38e1f8a215681126379eca617f96c447638f751434e8e65b14 (last modified on Wed Jan 29 12:50:09 2025) since it couldn't be found locally at evaluate-metric--accuracy, or remotely on the Hugging Face Hub.
2025-01-30 17:07 - INFO - Model Configuration: {'config': {'max_position_embeddings': 514, 'hidden_size': 768, 'num_attention_heads': 12, 'num_hidden_layers': 6, 'type_vocab_size': 1, 'attn_mechanism': 'eager', 'num_labels': 2, 'position_embedding_type': 'relative_key'}, 'settings': {'task': 'wnli', 'exp_name': 'test', 'save_path': '/home/lvpoellhuber/storage/models/finetune_roberta/roberta/roberta_1', 'model': 'FacebookAI/roberta-base', 'train': False, 'validate': False, 'evaluate': True, 'logging': False, 'epochs': 10, 'batch_size': 16, 'lr': 0.0001, 'dataset_dir': '/home/lvpoellhuber/storage/datasets', 'tokenizer': 'FacebookAI/roberta-base', 'eval_strategy': 'no', 'accelerate': True, 'weight_decay': 0.01, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon ': 1e-08, 'lr_scheduler_type': 'linear', 'warmup_steps': 0, 'warmup_ratio': 0.06, 'save_strategy': 'epoch', 'save_total_limit': 1, 'optim': 'adamw_torch', 'auto_find_batch_size': False, 'resume_from_checkpoint': None}}
2025-01-30 17:08 - INFO - Model Configuration: {'config': {'max_position_embeddings': 514, 'hidden_size': 768, 'num_attention_heads': 12, 'num_hidden_layers': 6, 'type_vocab_size': 1, 'attn_mechanism': 'eager', 'num_labels': 2, 'position_embedding_type': 'relative_key'}, 'settings': {'task': 'glue', 'exp_name': 'test', 'save_path': '/home/lvpoellhuber/storage/models/finetune_roberta/roberta/roberta_1', 'model': 'FacebookAI/roberta-base', 'train': False, 'validate': False, 'evaluate': True, 'logging': False, 'epochs': 10, 'batch_size': 16, 'lr': 0.0001, 'dataset_dir': '/home/lvpoellhuber/storage/datasets', 'tokenizer': 'FacebookAI/roberta-base', 'eval_strategy': 'no', 'accelerate': True, 'weight_decay': 0.01, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon ': 1e-08, 'lr_scheduler_type': 'linear', 'warmup_steps': 0, 'warmup_ratio': 0.06, 'save_strategy': 'epoch', 'save_total_limit': 1, 'optim': 'adamw_torch', 'auto_find_batch_size': False, 'resume_from_checkpoint': None}}
2025-01-30 17:09 - INFO - Model Configuration: {'config': {'max_position_embeddings': 514, 'hidden_size': 768, 'num_attention_heads': 12, 'num_hidden_layers': 6, 'type_vocab_size': 1, 'attn_mechanism': 'eager', 'num_labels': 2, 'position_embedding_type': 'relative_key'}, 'settings': {'task': 'glue', 'exp_name': 'test', 'save_path': '/home/lvpoellhuber/storage/models/new-attention/roberta/roberta_1', 'model': 'FacebookAI/roberta-base', 'train': False, 'validate': False, 'evaluate': True, 'logging': False, 'epochs': 10, 'batch_size': 16, 'lr': 0.0001, 'dataset_dir': '/home/lvpoellhuber/storage/datasets', 'tokenizer': 'FacebookAI/roberta-base', 'eval_strategy': 'no', 'accelerate': True, 'weight_decay': 0.01, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon ': 1e-08, 'lr_scheduler_type': 'linear', 'warmup_steps': 0, 'warmup_ratio': 0.06, 'save_strategy': 'epoch', 'save_total_limit': 1, 'optim': 'adamw_torch', 'auto_find_batch_size': False, 'resume_from_checkpoint': None}}
2025-01-30 17:10 - INFO - Model Configuration: {'config': {'max_position_embeddings': 514, 'hidden_size': 768, 'num_attention_heads': 12, 'num_hidden_layers': 6, 'type_vocab_size': 1, 'attn_mechanism': 'eager', 'num_labels': 2, 'position_embedding_type': 'relative_key'}, 'settings': {'task': 'glue', 'exp_name': 'test', 'save_path': '/home/lvpoellhuber/storage/models/new-attention/roberta/roberta_1', 'model': 'FacebookAI/roberta-base', 'train': True, 'validate': False, 'evaluate': False, 'logging': False, 'epochs': 10, 'batch_size': 16, 'lr': 0.0001, 'dataset_dir': '/home/lvpoellhuber/storage/datasets', 'tokenizer': 'FacebookAI/roberta-base', 'eval_strategy': 'no', 'accelerate': True, 'weight_decay': 0.01, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon ': 1e-08, 'lr_scheduler_type': 'linear', 'warmup_steps': 0, 'warmup_ratio': 0.06, 'save_strategy': 'epoch', 'save_total_limit': 1, 'optim': 'adamw_torch', 'auto_find_batch_size': False, 'resume_from_checkpoint': None}}
2025-01-30 17:11 - INFO - Model Configuration: {'config': {'max_position_embeddings': 514, 'hidden_size': 768, 'num_attention_heads': 12, 'num_hidden_layers': 6, 'type_vocab_size': 1, 'attn_mechanism': 'eager', 'num_labels': 2, 'position_embedding_type': 'relative_key'}, 'settings': {'task': 'glue', 'exp_name': 'test', 'save_path': '/home/lvpoellhuber/storage/models/new-attention/roberta/roberta_1', 'model': 'FacebookAI/roberta-base', 'train': True, 'validate': False, 'evaluate': False, 'logging': False, 'epochs': 10, 'batch_size': 16, 'lr': 0.0001, 'dataset_dir': '/home/lvpoellhuber/storage/datasets', 'tokenizer': 'FacebookAI/roberta-base', 'eval_strategy': 'no', 'accelerate': True, 'weight_decay': 0.01, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon ': 1e-08, 'lr_scheduler_type': 'linear', 'warmup_steps': 0, 'warmup_ratio': 0.06, 'save_strategy': 'epoch', 'save_total_limit': 1, 'optim': 'adamw_torch', 'auto_find_batch_size': False, 'resume_from_checkpoint': None}}
2025-01-30 17:11 - INFO - Model Configuration: {'config': {'max_position_embeddings': 514, 'hidden_size': 768, 'num_attention_heads': 12, 'num_hidden_layers': 6, 'type_vocab_size': 1, 'attn_mechanism': 'eager', 'num_labels': 2, 'position_embedding_type': 'relative_key'}, 'settings': {'task': 'glue', 'exp_name': 'test', 'save_path': '/home/lvpoellhuber/storage/models/new-attention/roberta/roberta_1', 'model': 'FacebookAI/roberta-base', 'train': True, 'validate': False, 'evaluate': False, 'logging': False, 'epochs': 10, 'batch_size': 16, 'lr': 0.0001, 'dataset_dir': '/home/lvpoellhuber/storage/datasets', 'tokenizer': 'FacebookAI/roberta-base', 'eval_strategy': 'no', 'accelerate': True, 'weight_decay': 0.01, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon ': 1e-08, 'lr_scheduler_type': 'linear', 'warmup_steps': 0, 'warmup_ratio': 0.06, 'save_strategy': 'epoch', 'save_total_limit': 1, 'optim': 'adamw_torch', 'auto_find_batch_size': False, 'resume_from_checkpoint': None}}
2025-01-30 17:12 - INFO - Model Configuration: {'config': {'max_position_embeddings': 514, 'hidden_size': 768, 'num_attention_heads': 12, 'num_hidden_layers': 6, 'type_vocab_size': 1, 'attn_mechanism': 'eager', 'num_labels': 2, 'position_embedding_type': 'relative_key'}, 'settings': {'task': 'glue', 'exp_name': 'test', 'save_path': '/home/lvpoellhuber/storage/models/new-attention/roberta/roberta_1', 'model': 'FacebookAI/roberta-base', 'train': True, 'validate': False, 'evaluate': False, 'logging': False, 'epochs': 10, 'batch_size': 16, 'lr': 0.0001, 'dataset_dir': '/home/lvpoellhuber/storage/datasets', 'tokenizer': 'FacebookAI/roberta-base', 'eval_strategy': 'no', 'accelerate': True, 'weight_decay': 0.01, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon ': 1e-08, 'lr_scheduler_type': 'linear', 'warmup_steps': 0, 'warmup_ratio': 0.06, 'save_strategy': 'epoch', 'save_total_limit': 1, 'optim': 'adamw_torch', 'auto_find_batch_size': False, 'resume_from_checkpoint': None}}
2025-01-30 17:14 - INFO - Model Configuration: {'config': {'max_position_embeddings': 514, 'hidden_size': 768, 'num_attention_heads': 12, 'num_hidden_layers': 6, 'type_vocab_size': 1, 'attn_mechanism': 'eager', 'num_labels': 2, 'position_embedding_type': 'relative_key'}, 'settings': {'task': 'glue', 'exp_name': 'test', 'save_path': '/home/lvpoellhuber/storage/models/new-attention/roberta/roberta_1', 'model': 'FacebookAI/roberta-base', 'train': True, 'validate': False, 'evaluate': False, 'logging': False, 'epochs': 10, 'batch_size': 16, 'lr': 0.0001, 'dataset_dir': '/home/lvpoellhuber/storage/datasets', 'tokenizer': 'FacebookAI/roberta-base', 'eval_strategy': 'no', 'accelerate': True, 'weight_decay': 0.01, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon ': 1e-08, 'lr_scheduler_type': 'linear', 'warmup_steps': 0, 'warmup_ratio': 0.06, 'save_strategy': 'epoch', 'save_total_limit': 1, 'optim': 'adamw_torch', 'auto_find_batch_size': False, 'resume_from_checkpoint': None}}
2025-01-30 17:16 - INFO - Model Configuration: {'config': {'max_position_embeddings': 514, 'hidden_size': 768, 'num_attention_heads': 12, 'num_hidden_layers': 6, 'type_vocab_size': 1, 'attn_mechanism': 'eager', 'num_labels': 2, 'position_embedding_type': 'relative_key'}, 'settings': {'task': 'glue', 'exp_name': 'test', 'save_path': '/home/lvpoellhuber/storage/models/new-attention/roberta/roberta_1', 'model': 'FacebookAI/roberta-base', 'train': True, 'validate': False, 'evaluate': False, 'logging': False, 'epochs': 10, 'batch_size': 16, 'lr': 0.0001, 'dataset_dir': '/home/lvpoellhuber/storage/datasets', 'tokenizer': 'FacebookAI/roberta-base', 'eval_strategy': 'no', 'accelerate': True, 'weight_decay': 0.01, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon ': 1e-08, 'lr_scheduler_type': 'linear', 'warmup_steps': 0, 'warmup_ratio': 0.06, 'save_strategy': 'epoch', 'save_total_limit': 1, 'optim': 'adamw_torch', 'auto_find_batch_size': False, 'resume_from_checkpoint': None}}
2025-01-30 17:47 - INFO - Model Configuration: {'config': {'max_position_embeddings': 514, 'hidden_size': 768, 'num_attention_heads': 12, 'num_hidden_layers': 6, 'type_vocab_size': 1, 'attn_mechanism': 'eager', 'num_labels': 2, 'position_embedding_type': 'relative_key'}, 'settings': {'task': 'glue', 'exp_name': 'test', 'save_path': '/home/lvpoellhuber/storage/models/new-attention/roberta/roberta_1', 'model': 'FacebookAI/roberta-base', 'train': True, 'validate': False, 'evaluate': False, 'logging': False, 'epochs': 10, 'batch_size': 16, 'lr': 0.0001, 'dataset_dir': '/home/lvpoellhuber/storage/datasets', 'tokenizer': 'FacebookAI/roberta-base', 'eval_strategy': 'no', 'accelerate': True, 'weight_decay': 0.01, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon ': 1e-08, 'lr_scheduler_type': 'linear', 'warmup_steps': 0, 'warmup_ratio': 0.06, 'save_strategy': 'epoch', 'save_total_limit': 1, 'optim': 'adamw_torch', 'auto_find_batch_size': False, 'resume_from_checkpoint': None}}
2025-01-30 17:53 - INFO - Model Configuration: {'config': {'max_position_embeddings': 514, 'hidden_size': 768, 'num_attention_heads': 12, 'num_hidden_layers': 6, 'type_vocab_size': 1, 'attn_mechanism': 'eager', 'num_labels': 2, 'position_embedding_type': 'relative_key'}, 'settings': {'task': 'glue', 'exp_name': 'test', 'save_path': '/home/lvpoellhuber/storage/models/new-attention/roberta/roberta_1', 'model': 'FacebookAI/roberta-base', 'train': True, 'validate': False, 'evaluate': False, 'logging': False, 'epochs': 10, 'batch_size': 16, 'lr': 0.0001, 'dataset_dir': '/home/lvpoellhuber/storage/datasets', 'tokenizer': 'FacebookAI/roberta-base', 'eval_strategy': 'no', 'accelerate': True, 'weight_decay': 0.01, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'lr_scheduler_type': 'linear', 'warmup_steps': 0, 'warmup_ratio': 0.06, 'save_strategy': 'epoch', 'save_total_limit': 1, 'optim': 'adamw_torch', 'auto_find_batch_size': False, 'resume_from_checkpoint': None}}
2025-01-30 17:55 - WARNING - ============ Finetuning test_rpeK. ============
2025-01-30 17:55 - INFO - Model Configuration: {'settings': {'model': 'FacebookAI/roberta-base', 'save_path': '/home/lvpoellhuber/storage/models/finetune_roberta/roberta', 'tokenizer': 'FacebookAI/roberta-base', 'dataset': '/home/lvpoellhuber/storage/datasets', 'task': 'glue', 'accelerate': True, 'logging': False, 'exp_name': 'test_rpeK', 'epochs': 10, 'batch_size': 16, 'lr': 0.0001}, 'config': {'max_position_embeddings': 514, 'hidden_size': 768, 'num_attention_heads': 12, 'num_hidden_layers': 6, 'type_vocab_size': 1, 'attn_mechanism': 'eager', 'num_labels': 2, 'position_embedding_type': 'relative_key'}}
2025-01-30 17:55 - WARNING - Processing cola.
2025-01-30 17:55 - WARNING - Initializing training.
2025-01-30 17:57 - WARNING - ============ Finetuning test_rpeK. ============
2025-01-30 17:57 - INFO - Model Configuration: {'settings': {'model': 'FacebookAI/roberta-base', 'save_path': '/home/lvpoellhuber/storage/models/finetune_roberta/roberta', 'tokenizer': 'FacebookAI/roberta-base', 'dataset': '/home/lvpoellhuber/storage/datasets', 'task': 'glue', 'accelerate': True, 'logging': False, 'exp_name': 'test_rpeK', 'epochs': 10, 'batch_size': 16, 'lr': 0.0001}, 'config': {'max_position_embeddings': 514, 'hidden_size': 768, 'num_attention_heads': 12, 'num_hidden_layers': 6, 'type_vocab_size': 1, 'attn_mechanism': 'eager', 'num_labels': 2, 'position_embedding_type': 'relative_key'}}
2025-01-30 17:57 - WARNING - Processing cola.
2025-01-30 17:57 - WARNING - ============ Finetuning test_rpeK. ============
2025-01-30 17:57 - INFO - Model Configuration: {'settings': {'model': 'FacebookAI/roberta-base', 'save_path': '/home/lvpoellhuber/storage/models/finetune_roberta/roberta', 'tokenizer': 'FacebookAI/roberta-base', 'dataset': '/home/lvpoellhuber/storage/datasets', 'task': 'glue', 'accelerate': True, 'logging': False, 'exp_name': 'test_rpeK', 'epochs': 10, 'batch_size': 32, 'lr': 0.0001}, 'config': {'max_position_embeddings': 514, 'hidden_size': 768, 'num_attention_heads': 12, 'num_hidden_layers': 6, 'type_vocab_size': 1, 'attn_mechanism': 'eager', 'num_labels': 2, 'position_embedding_type': 'relative_key'}}
2025-01-30 17:57 - WARNING - Processing cola.
2025-01-30 17:57 - WARNING - Initializing training.
2025-01-30 17:58 - INFO - Model Configuration: {'config': {'max_position_embeddings': 514, 'hidden_size': 768, 'num_attention_heads': 12, 'num_hidden_layers': 6, 'type_vocab_size': 1, 'attn_mechanism': 'eager', 'num_labels': 2, 'position_embedding_type': 'relative_key'}, 'settings': {'task': 'glue', 'exp_name': 'test', 'save_path': '/home/lvpoellhuber/storage/models/new-attention/roberta/roberta_1', 'model': 'FacebookAI/roberta-base', 'train': True, 'validate': False, 'evaluate': False, 'logging': False, 'epochs': 10, 'batch_size': 16, 'lr': 0.0001, 'dataset_dir': '/home/lvpoellhuber/storage/datasets', 'tokenizer': 'FacebookAI/roberta-base', 'eval_strategy': 'no', 'accelerate': True, 'weight_decay': 0.01, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'lr_scheduler_type': 'linear', 'warmup_steps': 0, 'warmup_ratio': 0.06, 'save_strategy': 'epoch', 'save_total_limit': 1, 'optim': 'adamw_torch', 'auto_find_batch_size': False, 'resume_from_checkpoint': None}}
2025-01-30 17:59 - WARNING - ============ Finetuning test_rpeK. ============
2025-01-30 17:59 - INFO - Model Configuration: {'settings': {'model': 'FacebookAI/roberta-base', 'save_path': '/home/lvpoellhuber/storage/models/finetune_roberta/roberta', 'tokenizer': 'FacebookAI/roberta-base', 'dataset': '/home/lvpoellhuber/storage/datasets', 'task': 'glue', 'accelerate': True, 'logging': False, 'exp_name': 'test_rpeK', 'epochs': 10, 'batch_size': 16, 'lr': 0.0001}, 'config': {'max_position_embeddings': 514, 'hidden_size': 768, 'num_attention_heads': 12, 'num_hidden_layers': 6, 'type_vocab_size': 1, 'attn_mechanism': 'eager', 'num_labels': 2, 'position_embedding_type': 'relative_key'}}
2025-01-30 17:59 - WARNING - Processing cola.
2025-01-30 17:59 - WARNING - Initializing training.
2025-01-30 17:59 - INFO - Model Configuration: {'config': {'max_position_embeddings': 514, 'hidden_size': 768, 'num_attention_heads': 12, 'num_hidden_layers': 6, 'type_vocab_size': 1, 'attn_mechanism': 'eager', 'num_labels': 2, 'position_embedding_type': 'relative_key'}, 'settings': {'task': 'glue', 'exp_name': 'test', 'save_path': '/home/lvpoellhuber/storage/models/new-attention/roberta/roberta_1', 'model': 'FacebookAI/roberta-base', 'train': True, 'validate': False, 'evaluate': False, 'logging': False, 'epochs': 10, 'batch_size': 16, 'lr': 0.0001, 'dataset_dir': '/home/lvpoellhuber/storage/datasets', 'tokenizer': 'FacebookAI/roberta-base', 'eval_strategy': 'no', 'accelerate': True, 'weight_decay': 0.01, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'lr_scheduler_type': 'linear', 'warmup_steps': 0, 'warmup_ratio': 0.06, 'save_strategy': 'epoch', 'save_total_limit': 1, 'optim': 'adamw_torch', 'auto_find_batch_size': False, 'resume_from_checkpoint': None}}
2025-01-30 18:12 - INFO - Model Configuration: {'config': {'max_position_embeddings': 514, 'hidden_size': 768, 'num_attention_heads': 12, 'num_hidden_layers': 6, 'type_vocab_size': 1, 'attn_mechanism': 'eager', 'num_labels': 2, 'position_embedding_type': 'relative_key'}, 'settings': {'task': 'glue', 'exp_name': 'test', 'save_path': '/home/lvpoellhuber/storage/models/new-attention/roberta/roberta_1', 'model': 'FacebookAI/roberta-base', 'train': True, 'validate': False, 'evaluate': False, 'logging': False, 'epochs': 10, 'batch_size': 16, 'lr': 0.0001, 'dataset_dir': '/home/lvpoellhuber/storage/datasets', 'tokenizer': 'FacebookAI/roberta-base', 'eval_strategy': 'no', 'accelerate': True, 'weight_decay': 0.01, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'lr_scheduler_type': 'linear', 'warmup_steps': 0, 'warmup_ratio': 0.06, 'save_strategy': 'epoch', 'save_total_limit': 1, 'optim': 'adamw_torch', 'auto_find_batch_size': False, 'resume_from_checkpoint': None}}
2025-01-30 18:13 - WARNING - ============ Finetuning test_rpeK. ============
2025-01-30 18:13 - INFO - Model Configuration: {'settings': {'model': 'FacebookAI/roberta-base', 'save_path': '/home/lvpoellhuber/storage/models/finetune_roberta/roberta', 'tokenizer': 'FacebookAI/roberta-base', 'dataset': '/home/lvpoellhuber/storage/datasets', 'task': 'glue', 'accelerate': True, 'logging': False, 'exp_name': 'test_rpeK', 'epochs': 10, 'batch_size': 16, 'lr': 0.0001}, 'config': {'max_position_embeddings': 514, 'hidden_size': 768, 'num_attention_heads': 12, 'num_hidden_layers': 6, 'type_vocab_size': 1, 'attn_mechanism': 'eager', 'num_labels': 2, 'position_embedding_type': 'relative_key'}}
2025-01-30 18:13 - WARNING - Processing cola.
2025-01-30 18:13 - WARNING - Initializing training.
2025-01-30 18:21 - WARNING - ============ Finetuning test_rpeK. ============
2025-01-30 18:21 - INFO - Model Configuration: {'settings': {'model': 'FacebookAI/roberta-base', 'save_path': '/home/lvpoellhuber/storage/models/finetune_roberta/roberta', 'tokenizer': 'FacebookAI/roberta-base', 'dataset': '/home/lvpoellhuber/storage/datasets', 'task': 'glue', 'accelerate': True, 'logging': False, 'exp_name': 'test_rpeK', 'epochs': 10, 'batch_size': 16, 'lr': 0.0001}, 'config': {'max_position_embeddings': 514, 'hidden_size': 768, 'num_attention_heads': 12, 'num_hidden_layers': 6, 'type_vocab_size': 1, 'attn_mechanism': 'eager', 'num_labels': 2, 'position_embedding_type': 'relative_key'}}
2025-01-30 18:21 - WARNING - Processing cola.
2025-01-30 18:21 - WARNING - Initializing training.
2025-01-30 18:25 - WARNING - ============ Finetuning test_rpeK. ============
2025-01-30 18:25 - INFO - Model Configuration: {'settings': {'model': 'FacebookAI/roberta-base', 'save_path': '/home/lvpoellhuber/storage/models/finetune_roberta/roberta', 'tokenizer': 'FacebookAI/roberta-base', 'dataset': '/home/lvpoellhuber/storage/datasets', 'task': 'glue', 'accelerate': True, 'logging': False, 'exp_name': 'test_rpeK', 'epochs': 10, 'batch_size': 16, 'lr': 0.0001}, 'config': {'max_position_embeddings': 514, 'hidden_size': 768, 'num_attention_heads': 12, 'num_hidden_layers': 6, 'type_vocab_size': 1, 'attn_mechanism': 'eager', 'num_labels': 2, 'position_embedding_type': 'relative_key'}}
2025-01-30 18:25 - WARNING - Processing cola.
2025-01-30 18:25 - WARNING - Initializing training.
2025-01-30 18:26 - WARNING - ============ Finetuning test_rpeK. ============
2025-01-30 18:26 - INFO - Model Configuration: {'settings': {'model': 'FacebookAI/roberta-base', 'save_path': '/home/lvpoellhuber/storage/models/finetune_roberta/roberta', 'tokenizer': 'FacebookAI/roberta-base', 'dataset': '/home/lvpoellhuber/storage/datasets', 'task': 'glue', 'accelerate': True, 'logging': False, 'exp_name': 'test_rpeK', 'epochs': 10, 'batch_size': 16, 'lr': 0.0001}, 'config': {'max_position_embeddings': 514, 'hidden_size': 768, 'num_attention_heads': 12, 'num_hidden_layers': 6, 'type_vocab_size': 1, 'attn_mechanism': 'eager', 'num_labels': 2, 'position_embedding_type': 'relative_key'}}
2025-01-30 18:26 - WARNING - Processing cola.
2025-01-30 18:26 - WARNING - Initializing training.
2025-01-30 18:27 - WARNING - ============ Finetuning test_rpeK. ============
2025-01-30 18:27 - INFO - Model Configuration: {'settings': {'model': 'FacebookAI/roberta-base', 'save_path': '/home/lvpoellhuber/storage/models/finetune_roberta/roberta', 'tokenizer': 'FacebookAI/roberta-base', 'dataset': '/home/lvpoellhuber/storage/datasets', 'task': 'glue', 'accelerate': True, 'logging': False, 'exp_name': 'test_rpeK', 'epochs': 10, 'batch_size': 16, 'lr': 0.0001}, 'config': {'max_position_embeddings': 514, 'hidden_size': 768, 'num_attention_heads': 12, 'num_hidden_layers': 6, 'type_vocab_size': 1, 'attn_mechanism': 'eager', 'num_labels': 2, 'position_embedding_type': 'relative_key'}}
2025-01-30 18:27 - WARNING - Processing cola.
2025-01-30 18:27 - WARNING - Initializing training.
2025-01-30 18:27 - WARNING - ============ Finetuning test_rpeK. ============
2025-01-30 18:27 - INFO - Model Configuration: {'settings': {'model': 'FacebookAI/roberta-base', 'save_path': '/home/lvpoellhuber/storage/models/finetune_roberta/roberta', 'tokenizer': 'FacebookAI/roberta-base', 'dataset': '/home/lvpoellhuber/storage/datasets', 'task': 'glue', 'accelerate': True, 'logging': False, 'exp_name': 'test_rpeK', 'epochs': 10, 'batch_size': 16, 'lr': 0.0001}, 'config': {'max_position_embeddings': 514, 'hidden_size': 768, 'num_attention_heads': 12, 'num_hidden_layers': 6, 'type_vocab_size': 1, 'attn_mechanism': 'eager', 'num_labels': 2, 'position_embedding_type': 'relative_key'}}
2025-01-30 18:27 - WARNING - Processing cola.
2025-01-30 18:27 - WARNING - Initializing training.
2025-01-31 10:35 - INFO - Model Configuration: {'config': {'max_position_embeddings': 514, 'hidden_size': 768, 'num_attention_heads': 12, 'num_hidden_layers': 6, 'type_vocab_size': 1, 'attn_mechanism': 'eager', 'num_labels': 2, 'position_embedding_type': 'relative_key'}, 'settings': {'task': 'wnli', 'exp_name': 'test', 'save_path': '/home/lvpoellhuber/storage/models/new-attention/roberta/roberta_1', 'model': 'FacebookAI/roberta-base', 'train': True, 'validate': False, 'evaluate': False, 'logging': False, 'epochs': 1, 'batch_size': 16, 'lr': 0.0001, 'dataset_dir': '/home/lvpoellhuber/storage/datasets', 'tokenizer': 'FacebookAI/roberta-base', 'eval_strategy': 'no', 'accelerate': True, 'weight_decay': 0.01, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'lr_scheduler_type': 'linear', 'warmup_steps': 0, 'warmup_ratio': 0.06, 'save_strategy': 'epoch', 'save_total_limit': 1, 'optim': 'adamw_torch', 'auto_find_batch_size': False, 'resume_from_checkpoint': None}}
2025-01-31 10:39 - INFO - Model Configuration: {'config': {'max_position_embeddings': 514, 'hidden_size': 768, 'num_attention_heads': 12, 'num_hidden_layers': 6, 'type_vocab_size': 1, 'attn_mechanism': 'eager', 'num_labels': 2, 'position_embedding_type': 'relative_key'}, 'settings': {'task': 'wnli', 'exp_name': 'test', 'save_path': '/home/lvpoellhuber/storage/models/new-attention/roberta/roberta_1', 'model': 'FacebookAI/roberta-base', 'train': True, 'validate': False, 'evaluate': False, 'logging': False, 'epochs': 1, 'batch_size': 16, 'lr': 0.0001, 'dataset_dir': '/home/lvpoellhuber/storage/datasets', 'tokenizer': 'FacebookAI/roberta-base', 'eval_strategy': 'no', 'accelerate': True, 'weight_decay': 0.01, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'lr_scheduler_type': 'linear', 'warmup_steps': 0, 'warmup_ratio': 0.06, 'save_strategy': 'epoch', 'save_total_limit': 1, 'optim': 'adamw_torch', 'auto_find_batch_size': False, 'resume_from_checkpoint': None}}
2025-01-31 11:04 - WARNING - ============ Finetuning test_rpeK. ============
2025-01-31 11:04 - INFO - Model Configuration: {'settings': {'model': 'FacebookAI/roberta-base', 'save_path': '/home/lvpoellhuber/storage/models/finetune_roberta/roberta', 'tokenizer': 'FacebookAI/roberta-base', 'dataset': '/home/lvpoellhuber/storage/datasets', 'task': 'wnli', 'accelerate': True, 'logging': False, 'exp_name': 'test_rpeK', 'epochs': 10, 'batch_size': 16, 'lr': 0.0001}, 'config': {'max_position_embeddings': 514, 'hidden_size': 768, 'num_attention_heads': 12, 'num_hidden_layers': 6, 'type_vocab_size': 1, 'attn_mechanism': 'eager', 'num_labels': 2, 'position_embedding_type': 'relative_key'}}
2025-01-31 11:04 - WARNING - Initializing training.
2025-01-31 11:04 - WARNING - ============ Finetuning test_rpeK. ============
2025-01-31 11:04 - INFO - Model Configuration: {'settings': {'model': 'FacebookAI/roberta-base', 'save_path': '/home/lvpoellhuber/storage/models/finetune_roberta/roberta', 'tokenizer': 'FacebookAI/roberta-base', 'dataset': '/home/lvpoellhuber/storage/datasets/qnli/qnli_train.pt', 'task': 'wnli', 'accelerate': True, 'logging': False, 'exp_name': 'test_rpeK', 'epochs': 10, 'batch_size': 16, 'lr': 0.0001}, 'config': {'max_position_embeddings': 514, 'hidden_size': 768, 'num_attention_heads': 12, 'num_hidden_layers': 6, 'type_vocab_size': 1, 'attn_mechanism': 'eager', 'num_labels': 2, 'position_embedding_type': 'relative_key'}}
2025-01-31 11:04 - WARNING - Initializing training.
2025-01-31 11:05 - WARNING - ============ Finetuning test_rpeK. ============
2025-01-31 11:05 - INFO - Model Configuration: {'settings': {'model': 'FacebookAI/roberta-base', 'save_path': '/home/lvpoellhuber/storage/models/finetune_roberta/roberta', 'tokenizer': 'FacebookAI/roberta-base', 'dataset': '/home/lvpoellhuber/storage/datasets/wnli/wnli_train.pt', 'task': 'wnli', 'accelerate': True, 'logging': False, 'exp_name': 'test_rpeK', 'epochs': 10, 'batch_size': 16, 'lr': 0.0001}, 'config': {'max_position_embeddings': 514, 'hidden_size': 768, 'num_attention_heads': 12, 'num_hidden_layers': 6, 'type_vocab_size': 1, 'attn_mechanism': 'eager', 'num_labels': 2, 'position_embedding_type': 'relative_key'}}
2025-01-31 11:05 - WARNING - Initializing training.
2025-01-31 11:06 - WARNING - Beginning training process.
2025-01-31 11:09 - WARNING - ============ Finetuning test_rpeK. ============
2025-01-31 11:09 - INFO - Model Configuration: {'settings': {'model': 'FacebookAI/roberta-base', 'save_path': '/home/lvpoellhuber/storage/models/finetune_roberta/roberta', 'tokenizer': 'FacebookAI/roberta-base', 'dataset': '/home/lvpoellhuber/storage/datasets/wnli/wnli_train.pt', 'task': 'wnli', 'accelerate': True, 'logging': False, 'exp_name': 'test_rpeK', 'epochs': 1, 'batch_size': 16, 'lr': 0.0001}, 'config': {'max_position_embeddings': 514, 'hidden_size': 768, 'num_attention_heads': 12, 'num_hidden_layers': 6, 'type_vocab_size': 1, 'attn_mechanism': 'eager', 'num_labels': 2, 'position_embedding_type': 'relative_key'}}
2025-01-31 11:09 - WARNING - Initializing training.
2025-01-31 11:09 - WARNING - Beginning training process.
2025-01-31 11:11 - WARNING - Training done. Saving model.
2025-01-31 11:13 - WARNING - ============ Finetuning test_rpeK. ============
2025-01-31 11:13 - INFO - Model Configuration: {'settings': {'model': 'FacebookAI/roberta-base', 'save_path': '/home/lvpoellhuber/storage/models/finetune_roberta/roberta', 'tokenizer': 'FacebookAI/roberta-base', 'dataset': '/home/lvpoellhuber/storage/datasets', 'task': 'glue', 'accelerate': True, 'logging': False, 'exp_name': 'test_rpeK', 'epochs': 10, 'batch_size': 16, 'lr': 0.0001}, 'config': {'max_position_embeddings': 514, 'hidden_size': 768, 'num_attention_heads': 12, 'num_hidden_layers': 6, 'type_vocab_size': 1, 'attn_mechanism': 'eager', 'num_labels': 2, 'position_embedding_type': 'relative_key'}}
2025-01-31 11:13 - WARNING - Processing cola.
2025-01-31 11:13 - WARNING - Initializing training.
2025-01-31 11:13 - WARNING - Beginning training process.
2025-01-31 11:15 - WARNING - ============ Finetuning test_rpeK. ============
2025-01-31 11:15 - INFO - Model Configuration: {'settings': {'model': 'FacebookAI/roberta-base', 'save_path': '/home/lvpoellhuber/storage/models/finetune_roberta/roberta', 'tokenizer': 'FacebookAI/roberta-base', 'dataset': '/home/lvpoellhuber/storage/datasets', 'task': 'glue', 'accelerate': True, 'logging': False, 'exp_name': 'test_rpeK', 'epochs': 10, 'batch_size': 16, 'lr': 0.0001}, 'config': {'max_position_embeddings': 514, 'hidden_size': 768, 'num_attention_heads': 12, 'num_hidden_layers': 6, 'type_vocab_size': 1, 'attn_mechanism': 'eager', 'num_labels': 2, 'position_embedding_type': 'relative_key'}}
2025-01-31 11:15 - WARNING - Processing cola.
2025-01-31 11:15 - WARNING - Initializing training.
2025-01-31 11:15 - WARNING - Beginning training process.
2025-01-31 11:17 - WARNING - ============ Finetuning test_rpeK. ============
2025-01-31 11:17 - INFO - Model Configuration: {'settings': {'model': 'FacebookAI/roberta-base', 'save_path': '/home/lvpoellhuber/storage/models/finetune_roberta/roberta', 'tokenizer': 'FacebookAI/roberta-base', 'dataset': '/home/lvpoellhuber/storage/datasets/wnli/wnli_train.pt', 'task': 'wnli', 'accelerate': True, 'logging': False, 'exp_name': 'test_rpeK', 'epochs': 1, 'batch_size': 16, 'lr': 0.0001}, 'config': {'max_position_embeddings': 514, 'hidden_size': 768, 'num_attention_heads': 12, 'num_hidden_layers': 6, 'type_vocab_size': 1, 'attn_mechanism': 'eager', 'num_labels': 2, 'position_embedding_type': 'relative_key'}}
2025-01-31 11:17 - WARNING - Initializing training.
2025-01-31 11:17 - WARNING - Beginning training process.
2025-01-31 11:26 - INFO - Model Configuration: {'config': {'max_position_embeddings': 514, 'hidden_size': 768, 'num_attention_heads': 12, 'num_hidden_layers': 6, 'type_vocab_size': 1, 'attn_mechanism': 'eager', 'num_labels': 2, 'position_embedding_type': 'relative_key'}, 'settings': {'task': 'glue', 'exp_name': 'test', 'save_path': '/home/lvpoellhuber/storage/models/new-attention/roberta/roberta_1', 'model': 'FacebookAI/roberta-base', 'train': True, 'validate': False, 'evaluate': False, 'logging': False, 'epochs': 10, 'batch_size': 16, 'lr': 0.0001, 'dataset_dir': '/home/lvpoellhuber/storage/datasets', 'tokenizer': 'FacebookAI/roberta-base', 'eval_strategy': 'no', 'accelerate': True, 'weight_decay': 0.01, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'lr_scheduler_type': 'linear', 'warmup_steps': 0, 'warmup_ratio': 0.06, 'save_strategy': 'epoch', 'save_total_limit': 1, 'optim': 'adamw_torch', 'auto_find_batch_size': False, 'resume_from_checkpoint': None}}
2025-01-31 11:27 - INFO - Model Configuration: {'config': {'max_position_embeddings': 514, 'hidden_size': 768, 'num_attention_heads': 12, 'num_hidden_layers': 6, 'type_vocab_size': 1, 'attn_mechanism': 'eager', 'num_labels': 2, 'position_embedding_type': 'relative_key'}, 'settings': {'task': 'glue', 'exp_name': 'test', 'save_path': '/home/lvpoellhuber/storage/models/new-attention/roberta/roberta_1', 'model': 'FacebookAI/roberta-base', 'train': True, 'validate': False, 'evaluate': False, 'logging': False, 'epochs': 10, 'batch_size': 32, 'lr': 0.0001, 'dataset_dir': '/home/lvpoellhuber/storage/datasets', 'tokenizer': 'FacebookAI/roberta-base', 'eval_strategy': 'no', 'accelerate': True, 'weight_decay': 0.01, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'lr_scheduler_type': 'linear', 'warmup_steps': 0, 'warmup_ratio': 0.06, 'save_strategy': 'epoch', 'save_total_limit': 1, 'optim': 'adamw_torch', 'auto_find_batch_size': False, 'resume_from_checkpoint': None}}
2025-01-31 11:28 - INFO - Model Configuration: {'config': {'max_position_embeddings': 514, 'hidden_size': 768, 'num_attention_heads': 12, 'num_hidden_layers': 6, 'type_vocab_size': 1, 'attn_mechanism': 'eager', 'num_labels': 2, 'position_embedding_type': 'relative_key'}, 'settings': {'task': 'glue', 'exp_name': 'test', 'save_path': '/home/lvpoellhuber/storage/models/new-attention/roberta/roberta_1', 'model': 'FacebookAI/roberta-base', 'train': True, 'validate': False, 'evaluate': False, 'logging': False, 'epochs': 10, 'batch_size': 32, 'lr': 0.0001, 'dataset_dir': '/home/lvpoellhuber/storage/datasets', 'tokenizer': 'FacebookAI/roberta-base', 'eval_strategy': 'no', 'accelerate': True, 'weight_decay': 0.01, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'lr_scheduler_type': 'linear', 'warmup_steps': 0, 'warmup_ratio': 0.06, 'save_strategy': 'epoch', 'save_total_limit': 1, 'optim': 'adamw_torch', 'auto_find_batch_size': False, 'resume_from_checkpoint': None}}
2025-01-31 11:44 - WARNING - ============ Finetuning test_rpeK. ============
2025-01-31 11:44 - INFO - Model Configuration: {'settings': {'model': 'FacebookAI/roberta-base', 'save_path': '/home/lvpoellhuber/storage/models/finetune_roberta/roberta', 'tokenizer': 'FacebookAI/roberta-base', 'dataset': '/home/lvpoellhuber/storage/datasets/wnli/wnli_train.pt', 'task': 'wnli', 'accelerate': True, 'logging': False, 'exp_name': 'test_rpeK', 'epochs': 1, 'batch_size': 16, 'lr': 0.0001}, 'config': {'max_position_embeddings': 514, 'hidden_size': 768, 'num_attention_heads': 12, 'num_hidden_layers': 6, 'type_vocab_size': 1, 'attn_mechanism': 'eager', 'num_labels': 2, 'position_embedding_type': 'relative_key'}}
2025-01-31 11:44 - WARNING - Initializing training.
2025-01-31 11:45 - WARNING - Beginning training process.
2025-01-31 11:45 - WARNING - ============ Finetuning test_rpeK. ============
2025-01-31 11:45 - INFO - Model Configuration: {'settings': {'model': 'FacebookAI/roberta-base', 'save_path': '/home/lvpoellhuber/storage/models/finetune_roberta/roberta', 'tokenizer': 'FacebookAI/roberta-base', 'dataset': '/home/lvpoellhuber/storage/datasets/wnli/wnli_train.pt', 'task': 'wnli', 'accelerate': True, 'logging': True, 'exp_name': 'test_rpeK', 'epochs': 1, 'batch_size': 16, 'lr': 0.0001}, 'config': {'max_position_embeddings': 514, 'hidden_size': 768, 'num_attention_heads': 12, 'num_hidden_layers': 6, 'type_vocab_size': 1, 'attn_mechanism': 'eager', 'num_labels': 2, 'position_embedding_type': 'relative_key'}}
2025-01-31 11:45 - WARNING - Initializing training.
2025-01-31 11:46 - WARNING - Beginning training process.
2025-01-31 11:46 - WARNING - ============ Finetuning test_rpeK. ============
2025-01-31 11:47 - INFO - Model Configuration: {'settings': {'model': 'FacebookAI/roberta-base', 'save_path': '/home/lvpoellhuber/storage/models/finetune_roberta/roberta', 'tokenizer': 'FacebookAI/roberta-base', 'dataset': '/home/lvpoellhuber/storage/datasets/wnli/wnli_train.pt', 'task': 'wnli', 'accelerate': True, 'logging': True, 'exp_name': 'test_rpeK', 'epochs': 1, 'batch_size': 16, 'lr': 0.0001}, 'config': {'max_position_embeddings': 514, 'hidden_size': 768, 'num_attention_heads': 12, 'num_hidden_layers': 6, 'type_vocab_size': 1, 'attn_mechanism': 'eager', 'num_labels': 2, 'position_embedding_type': 'relative_key'}}
2025-01-31 11:47 - WARNING - Initializing training.
2025-01-31 11:47 - WARNING - Beginning training process.
2025-01-31 11:47 - WARNING - ============ Finetuning test_rpeK. ============
2025-01-31 11:47 - INFO - Model Configuration: {'settings': {'model': 'FacebookAI/roberta-base', 'save_path': '/home/lvpoellhuber/storage/models/finetune_roberta/roberta', 'tokenizer': 'FacebookAI/roberta-base', 'dataset': '/home/lvpoellhuber/storage/datasets/wnli/wnli_train.pt', 'task': 'wnli', 'accelerate': True, 'logging': True, 'exp_name': 'test_rpeK', 'epochs': 1, 'batch_size': 16, 'lr': 0.0001}, 'config': {'max_position_embeddings': 514, 'hidden_size': 768, 'num_attention_heads': 12, 'num_hidden_layers': 6, 'type_vocab_size': 1, 'attn_mechanism': 'eager', 'num_labels': 2, 'position_embedding_type': 'relative_key'}}
2025-01-31 11:47 - WARNING - Initializing training.
2025-01-31 11:47 - WARNING - Beginning training process.
2025-01-31 13:04 - INFO - Model Configuration: {'config': {'max_position_embeddings': 514, 'hidden_size': 768, 'num_attention_heads': 12, 'num_hidden_layers': 6, 'type_vocab_size': 1, 'attn_mechanism': 'eager', 'num_labels': 2, 'position_embedding_type': 'relative_key'}, 'settings': {'task': 'glue', 'exp_name': 'test', 'save_path': '/home/lvpoellhuber/storage/models/new-attention/roberta/roberta_1', 'model': 'FacebookAI/roberta-base', 'train': True, 'validate': False, 'evaluate': False, 'logging': False, 'epochs': 10, 'batch_size': 16, 'lr': 0.0001, 'dataset_dir': '/home/lvpoellhuber/storage/datasets', 'tokenizer': 'FacebookAI/roberta-base', 'eval_strategy': 'no', 'accelerate': True, 'weight_decay': 0.01, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'lr_scheduler_type': 'linear', 'warmup_steps': 0, 'warmup_ratio': 0.06, 'save_strategy': 'epoch', 'save_total_limit': 1, 'optim': 'adamw_torch', 'auto_find_batch_size': False, 'resume_from_checkpoint': None}}
2025-01-31 13:04 - INFO - Model Configuration: {'config': {'max_position_embeddings': 514, 'hidden_size': 768, 'num_attention_heads': 12, 'num_hidden_layers': 6, 'type_vocab_size': 1, 'attn_mechanism': 'eager', 'num_labels': 2, 'position_embedding_type': 'relative_key'}, 'settings': {'task': 'glue', 'exp_name': 'test', 'save_path': '/home/lvpoellhuber/storage/models/new-attention/roberta/roberta_1', 'model': 'FacebookAI/roberta-base', 'train': True, 'validate': False, 'evaluate': False, 'logging': False, 'epochs': 10, 'batch_size': 32, 'lr': 0.0001, 'dataset_dir': '/home/lvpoellhuber/storage/datasets', 'tokenizer': 'FacebookAI/roberta-base', 'eval_strategy': 'no', 'accelerate': True, 'weight_decay': 0.01, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'lr_scheduler_type': 'linear', 'warmup_steps': 0, 'warmup_ratio': 0.06, 'save_strategy': 'epoch', 'save_total_limit': 1, 'optim': 'adamw_torch', 'auto_find_batch_size': False, 'resume_from_checkpoint': None}}
2025-01-31 13:05 - INFO - Model Configuration: {'config': {'max_position_embeddings': 514, 'hidden_size': 768, 'num_attention_heads': 12, 'num_hidden_layers': 6, 'type_vocab_size': 1, 'attn_mechanism': 'eager', 'num_labels': 2, 'position_embedding_type': 'relative_key'}, 'settings': {'task': 'glue', 'exp_name': 'test', 'save_path': '/home/lvpoellhuber/storage/models/new-attention/roberta/roberta_1', 'model': 'FacebookAI/roberta-base', 'train': True, 'validate': False, 'evaluate': False, 'logging': False, 'epochs': 10, 'batch_size': 32, 'lr': 0.0001, 'dataset_dir': '/home/lvpoellhuber/storage/datasets', 'tokenizer': 'FacebookAI/roberta-base', 'eval_strategy': 'no', 'accelerate': True, 'weight_decay': 0.01, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'lr_scheduler_type': 'linear', 'warmup_steps': 0, 'warmup_ratio': 0.06, 'save_strategy': 'epoch', 'save_total_limit': 1, 'optim': 'adamw_torch', 'auto_find_batch_size': False, 'resume_from_checkpoint': None}}
2025-01-31 14:12 - WARNING - ============ Finetuning test_rpeK. ============
2025-01-31 14:12 - INFO - Model Configuration: {'settings': {'model': 'FacebookAI/roberta-base', 'save_path': '/home/lvpoellhuber/storage/models/finetune_roberta/roberta', 'tokenizer': 'FacebookAI/roberta-base', 'dataset': '/home/lvpoellhuber/storage/datasets/wnli/wnli_train.pt', 'task': 'wnli', 'accelerate': True, 'logging': False, 'exp_name': 'test_rpeK', 'epochs': 1, 'batch_size': 16, 'lr': 0.0001}, 'config': {'max_position_embeddings': 514, 'hidden_size': 768, 'num_attention_heads': 12, 'num_hidden_layers': 6, 'type_vocab_size': 1, 'attn_mechanism': 'eager', 'num_labels': 2, 'position_embedding_type': 'relative_key'}}
2025-01-31 14:12 - WARNING - Initializing training.
2025-01-31 14:12 - WARNING - Beginning training process.
2025-01-31 14:12 - WARNING - ============ Finetuning test_rpeK. ============
2025-01-31 14:12 - INFO - Model Configuration: {'settings': {'model': 'FacebookAI/roberta-base', 'save_path': '/home/lvpoellhuber/storage/models/finetune_roberta/roberta', 'tokenizer': 'FacebookAI/roberta-base', 'dataset': '/home/lvpoellhuber/storage/datasets/wnli/wnli_train.pt', 'task': 'wnli', 'accelerate': True, 'logging': True, 'exp_name': 'test_rpeK', 'epochs': 1, 'batch_size': 16, 'lr': 0.0001}, 'config': {'max_position_embeddings': 514, 'hidden_size': 768, 'num_attention_heads': 12, 'num_hidden_layers': 6, 'type_vocab_size': 1, 'attn_mechanism': 'eager', 'num_labels': 2, 'position_embedding_type': 'relative_key'}}
2025-01-31 14:12 - WARNING - Initializing training.
2025-01-31 14:12 - WARNING - Beginning training process.
2025-01-31 14:15 - WARNING - Training done. Saving model.
2025-01-31 14:17 - WARNING - ============ Finetuning test_rpeK. ============
2025-01-31 14:17 - INFO - Model Configuration: {'settings': {'model': 'FacebookAI/roberta-base', 'save_path': '/home/lvpoellhuber/storage/models/finetune_roberta/roberta', 'tokenizer': 'FacebookAI/roberta-base', 'dataset': '/home/lvpoellhuber/storage/datasets/wnli/wnli_train.pt', 'task': 'wnli', 'accelerate': True, 'logging': True, 'exp_name': 'test_rpeK', 'epochs': 1, 'batch_size': 16, 'lr': 0.0001}, 'config': {'max_position_embeddings': 514, 'hidden_size': 768, 'num_attention_heads': 12, 'num_hidden_layers': 6, 'type_vocab_size': 1, 'attn_mechanism': 'eager', 'num_labels': 2, 'position_embedding_type': 'relative_key'}}
2025-01-31 14:17 - WARNING - Initializing training.
2025-01-31 14:17 - WARNING - Beginning training process.
2025-01-31 14:19 - WARNING - Training done. Saving model.
2025-02-03 10:13 - INFO - Model Configuration: {'config': {'max_position_embeddings': 514, 'hidden_size': 768, 'num_attention_heads': 12, 'num_hidden_layers': 6, 'type_vocab_size': 1, 'attn_mechanism': 'eager', 'num_labels': 2, 'position_embedding_type': 'relative_key'}, 'settings': {'task': 'glue', 'exp_name': 'test', 'save_path': '/home/lvpoellhuber/storage/models/new-attention/roberta/roberta_1', 'model': 'FacebookAI/roberta-base', 'train': True, 'validate': False, 'evaluate': False, 'logging': False, 'epochs': 10, 'batch_size': 16, 'lr': 0.0001, 'dataset_dir': '/home/lvpoellhuber/storage/datasets', 'tokenizer': 'FacebookAI/roberta-base', 'eval_strategy': 'no', 'accelerate': True, 'weight_decay': 0.01, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'lr_scheduler_type': 'linear', 'warmup_steps': 0, 'warmup_ratio': 0.06, 'save_strategy': 'epoch', 'save_total_limit': 1, 'optim': 'adamw_torch', 'auto_find_batch_size': False, 'resume_from_checkpoint': None}}
2025-02-03 10:34 - INFO - Model Configuration: {'config': {'max_position_embeddings': 514, 'hidden_size': 768, 'num_attention_heads': 12, 'num_hidden_layers': 6, 'type_vocab_size': 1, 'attn_mechanism': 'eager', 'num_labels': 2, 'position_embedding_type': 'relative_key'}, 'settings': {'task': 'glue', 'exp_name': 'test', 'save_path': '/home/lvpoellhuber/storage/models/new-attention/roberta/roberta_1', 'model': 'FacebookAI/roberta-base', 'train': True, 'validate': False, 'evaluate': False, 'logging': False, 'epochs': 10, 'batch_size': 16, 'lr': 0.0001, 'dataset_dir': '/home/lvpoellhuber/storage/datasets', 'tokenizer': 'FacebookAI/roberta-base', 'eval_strategy': 'no', 'accelerate': True, 'weight_decay': 0.01, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'lr_scheduler_type': 'linear', 'warmup_steps': 0, 'warmup_ratio': 0.06, 'save_strategy': 'epoch', 'save_total_limit': 1, 'optim': 'adamw_torch', 'auto_find_batch_size': False, 'resume_from_checkpoint': None}}
2025-02-03 10:38 - WARNING - ============ Finetuning test_rpeK. ============
2025-02-03 10:38 - INFO - Model Configuration: {'settings': {'model': 'FacebookAI/roberta-base', 'save_path': '/home/lvpoellhuber/storage/models/finetune_roberta/roberta', 'tokenizer': 'FacebookAI/roberta-base', 'dataset': '/home/lvpoellhuber/storage/datasets/wnli/wnli_train.pt', 'task': 'wnli', 'accelerate': True, 'logging': True, 'exp_name': 'test_rpeK', 'epochs': 1, 'batch_size': 16, 'lr': 0.0001}, 'config': {'max_position_embeddings': 514, 'hidden_size': 768, 'num_attention_heads': 12, 'num_hidden_layers': 6, 'type_vocab_size': 1, 'attn_mechanism': 'eager', 'num_labels': 2, 'position_embedding_type': 'relative_key'}}
2025-02-03 10:38 - WARNING - Initializing training.
2025-02-03 10:38 - WARNING - Beginning training process.
2025-02-03 10:39 - WARNING - ============ Finetuning test_rpeK. ============
2025-02-03 10:39 - INFO - Model Configuration: {'settings': {'model': 'FacebookAI/roberta-base', 'save_path': '/home/lvpoellhuber/storage/models/finetune_roberta/roberta', 'tokenizer': 'FacebookAI/roberta-base', 'dataset': '/home/lvpoellhuber/storage/datasets/wnli/wnli_train.pt', 'task': 'cola', 'accelerate': True, 'logging': False, 'exp_name': 'test_rpeK', 'epochs': 1, 'batch_size': 16, 'lr': 0.0001}, 'config': {'max_position_embeddings': 514, 'hidden_size': 768, 'num_attention_heads': 12, 'num_hidden_layers': 6, 'type_vocab_size': 1, 'attn_mechanism': 'eager', 'num_labels': 2, 'position_embedding_type': 'relative_key'}}
2025-02-03 10:39 - WARNING - Initializing training.
2025-02-03 10:39 - WARNING - Beginning training process.
2025-02-03 10:40 - WARNING - ============ Finetuning test_rpeK. ============
2025-02-03 10:40 - INFO - Model Configuration: {'settings': {'model': 'FacebookAI/roberta-base', 'save_path': '/home/lvpoellhuber/storage/models/finetune_roberta/roberta', 'tokenizer': 'FacebookAI/roberta-base', 'dataset': '/home/lvpoellhuber/storage/datasets/wnli/wnli_train.pt', 'task': 'cola', 'accelerate': True, 'logging': False, 'exp_name': 'test_rpeK', 'epochs': 1, 'batch_size': 16, 'lr': 0.0001}, 'config': {'max_position_embeddings': 514, 'hidden_size': 768, 'num_attention_heads': 12, 'num_hidden_layers': 6, 'type_vocab_size': 1, 'attn_mechanism': 'eager', 'num_labels': 2, 'position_embedding_type': 'relative_key'}}
2025-02-03 10:40 - WARNING - Initializing training.
2025-02-03 10:40 - WARNING - Beginning training process.
2025-02-03 10:40 - WARNING - ============ Finetuning test_rpeK. ============
2025-02-03 10:40 - INFO - Model Configuration: {'settings': {'model': 'FacebookAI/roberta-base', 'save_path': '/home/lvpoellhuber/storage/models/finetune_roberta/roberta', 'tokenizer': 'FacebookAI/roberta-base', 'dataset': '/home/lvpoellhuber/storage/datasets/wnli/cola_train.pt', 'task': 'cola', 'accelerate': True, 'logging': False, 'exp_name': 'test_rpeK', 'epochs': 1, 'batch_size': 16, 'lr': 0.0001}, 'config': {'max_position_embeddings': 514, 'hidden_size': 768, 'num_attention_heads': 12, 'num_hidden_layers': 6, 'type_vocab_size': 1, 'attn_mechanism': 'eager', 'num_labels': 2, 'position_embedding_type': 'relative_key'}}
2025-02-03 10:40 - WARNING - Initializing training.
2025-02-03 10:40 - WARNING - ============ Finetuning test_rpeK. ============
2025-02-03 10:40 - INFO - Model Configuration: {'settings': {'model': 'FacebookAI/roberta-base', 'save_path': '/home/lvpoellhuber/storage/models/finetune_roberta/roberta', 'tokenizer': 'FacebookAI/roberta-base', 'dataset': '/home/lvpoellhuber/storage/datasets/cola/cola_train.pt', 'task': 'cola', 'accelerate': True, 'logging': False, 'exp_name': 'test_rpeK', 'epochs': 1, 'batch_size': 16, 'lr': 0.0001}, 'config': {'max_position_embeddings': 514, 'hidden_size': 768, 'num_attention_heads': 12, 'num_hidden_layers': 6, 'type_vocab_size': 1, 'attn_mechanism': 'eager', 'num_labels': 2, 'position_embedding_type': 'relative_key'}}
2025-02-03 10:40 - WARNING - Initializing training.
2025-02-03 10:41 - WARNING - Beginning training process.
2025-02-03 11:15 - INFO - Model Configuration: {'config': {'max_position_embeddings': 514, 'hidden_size': 768, 'num_attention_heads': 12, 'num_hidden_layers': 6, 'type_vocab_size': 1, 'attn_mechanism': 'eager', 'num_labels': 2, 'position_embedding_type': 'relative_key'}, 'settings': {'task': 'glue', 'exp_name': 'test', 'save_path': '/home/lvpoellhuber/storage/models/new-attention/roberta/roberta_1', 'model': 'FacebookAI/roberta-base', 'train': True, 'validate': False, 'evaluate': False, 'logging': False, 'epochs': 10, 'batch_size': 16, 'lr': 0.0001, 'dataset_dir': '/home/lvpoellhuber/storage/datasets', 'tokenizer': 'FacebookAI/roberta-base', 'eval_strategy': 'no', 'accelerate': True, 'weight_decay': 0.01, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'lr_scheduler_type': 'linear', 'warmup_steps': 0, 'warmup_ratio': 0.06, 'save_strategy': 'epoch', 'save_total_limit': 1, 'optim': 'adamw_torch', 'auto_find_batch_size': False, 'resume_from_checkpoint': None}}
2025-02-03 11:29 - WARNING - ============ Finetuning test_rpeK. ============
2025-02-03 11:29 - INFO - Model Configuration: {'settings': {'model': 'FacebookAI/roberta-base', 'save_path': '/home/lvpoellhuber/storage/models/finetune_roberta/roberta', 'tokenizer': 'FacebookAI/roberta-base', 'dataset': '/home/lvpoellhuber/storage/datasets/cola/cola_train.pt', 'task': 'cola', 'accelerate': True, 'logging': False, 'exp_name': 'test_rpeK', 'epochs': 1, 'batch_size': 16, 'lr': 0.0001}, 'config': {'max_position_embeddings': 514, 'hidden_size': 768, 'num_attention_heads': 12, 'num_hidden_layers': 6, 'type_vocab_size': 1, 'attn_mechanism': 'eager', 'num_labels': 2, 'position_embedding_type': 'relative_key'}}
2025-02-03 11:29 - WARNING - Initializing training.
2025-02-03 11:29 - WARNING - Beginning training process.
2025-02-03 11:41 - INFO - Model Configuration: {'config': {'max_position_embeddings': 514, 'hidden_size': 768, 'num_attention_heads': 12, 'num_hidden_layers': 6, 'type_vocab_size': 1, 'attn_mechanism': 'eager', 'num_labels': 2, 'position_embedding_type': 'relative_key'}, 'settings': {'task': 'glue', 'exp_name': 'test', 'save_path': '/home/lvpoellhuber/storage/models/new-attention/roberta/roberta_1', 'model': 'FacebookAI/roberta-base', 'train': True, 'validate': False, 'evaluate': False, 'logging': False, 'epochs': 10, 'batch_size': 16, 'lr': 0.0001, 'dataset_dir': '/home/lvpoellhuber/storage/datasets', 'tokenizer': 'FacebookAI/roberta-base', 'eval_strategy': 'no', 'accelerate': True, 'weight_decay': 0.01, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'lr_scheduler_type': 'linear', 'warmup_steps': 0, 'warmup_ratio': 0.06, 'save_strategy': 'epoch', 'save_total_limit': 1, 'optim': 'adamw_torch', 'auto_find_batch_size': False, 'resume_from_checkpoint': None}}
2025-02-03 13:46 - INFO - Model Configuration: {'config': {'max_position_embeddings': 514, 'hidden_size': 768, 'num_attention_heads': 12, 'num_hidden_layers': 6, 'type_vocab_size': 1, 'attn_mechanism': 'eager', 'num_labels': 2, 'position_embedding_type': 'relative_key'}, 'settings': {'task': 'glue', 'exp_name': 'test', 'save_path': '/home/lvpoellhuber/storage/models/new-attention/roberta/roberta_1', 'model': 'FacebookAI/roberta-base', 'train': True, 'validate': False, 'evaluate': False, 'logging': False, 'epochs': 10, 'batch_size': 16, 'lr': 0.0001, 'dataset_dir': '/home/lvpoellhuber/storage/datasets', 'tokenizer': 'FacebookAI/roberta-base', 'eval_strategy': 'no', 'accelerate': True, 'weight_decay': 0.01, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'lr_scheduler_type': 'linear', 'warmup_steps': 0, 'warmup_ratio': 0.06, 'save_strategy': 'epoch', 'save_total_limit': 1, 'optim': 'adamw_torch', 'auto_find_batch_size': False, 'resume_from_checkpoint': None}}
2025-02-03 14:06 - WARNING - Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'SSLError(SSLEOFError(8, '[SSL: UNEXPECTED_EOF_WHILE_READING] EOF occurred in violation of protocol (_ssl.c:1007)'))': /clientlib/status-report/update
2025-02-03 14:15 - INFO - Model Configuration: {'config': {'max_position_embeddings': 514, 'hidden_size': 768, 'num_attention_heads': 12, 'num_hidden_layers': 6, 'type_vocab_size': 1, 'attn_mechanism': 'eager', 'num_labels': 2, 'position_embedding_type': 'relative_key'}, 'settings': {'task': 'glue', 'exp_name': 'test', 'save_path': '/home/lvpoellhuber/storage/models/new-attention/roberta/roberta_1', 'model': 'FacebookAI/roberta-base', 'train': True, 'validate': False, 'evaluate': False, 'logging': False, 'epochs': 10, 'batch_size': 16, 'lr': 0.0001, 'dataset_dir': '/home/lvpoellhuber/storage/datasets', 'tokenizer': 'FacebookAI/roberta-base', 'eval_strategy': 'no', 'accelerate': True, 'weight_decay': 0.01, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'lr_scheduler_type': 'linear', 'warmup_steps': 0, 'warmup_ratio': 0.06, 'save_strategy': 'epoch', 'save_total_limit': 1, 'optim': 'adamw_torch', 'auto_find_batch_size': False, 'resume_from_checkpoint': None}}
2025-02-03 14:37 - WARNING - Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'SSLError(SSLEOFError(8, '[SSL: UNEXPECTED_EOF_WHILE_READING] EOF occurred in violation of protocol (_ssl.c:1007)'))': /clientlib/status-report/update
2025-02-03 14:37 - WARNING - Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'SSLError(SSLEOFError(8, '[SSL: UNEXPECTED_EOF_WHILE_READING] EOF occurred in violation of protocol (_ssl.c:1007)'))': /clientlib/rest/v2/write/experiment/system-details
2025-02-03 14:42 - WARNING - Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'SSLError(SSLEOFError(8, '[SSL: UNEXPECTED_EOF_WHILE_READING] EOF occurred in violation of protocol (_ssl.c:1007)'))': /clientlib/status-report/update
2025-02-03 14:43 - INFO - Model Configuration: {'config': {'max_position_embeddings': 514, 'hidden_size': 768, 'num_attention_heads': 12, 'num_hidden_layers': 6, 'type_vocab_size': 1, 'attn_mechanism': 'eager', 'num_labels': 2, 'position_embedding_type': 'relative_key'}, 'settings': {'task': 'glue', 'exp_name': 'test', 'save_path': '/home/lvpoellhuber/storage/models/new-attention/roberta/roberta_1', 'model': 'FacebookAI/roberta-base', 'train': True, 'validate': False, 'evaluate': False, 'logging': False, 'epochs': 10, 'batch_size': 16, 'lr': 0.0001, 'dataset_dir': '/home/lvpoellhuber/storage/datasets', 'tokenizer': 'FacebookAI/roberta-base', 'eval_strategy': 'no', 'accelerate': True, 'weight_decay': 0.01, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'lr_scheduler_type': 'linear', 'warmup_steps': 0, 'warmup_ratio': 0.06, 'save_strategy': 'epoch', 'save_total_limit': 1, 'optim': 'adamw_torch', 'auto_find_batch_size': False, 'resume_from_checkpoint': None}}
2025-02-03 14:44 - INFO - Model Configuration: {'config': {'max_position_embeddings': 514, 'hidden_size': 768, 'num_attention_heads': 12, 'num_hidden_layers': 6, 'type_vocab_size': 1, 'attn_mechanism': 'eager', 'num_labels': 2, 'position_embedding_type': 'relative_key'}, 'settings': {'task': 'glue', 'exp_name': 'test', 'save_path': '/home/lvpoellhuber/storage/models/new-attention/roberta/roberta_1', 'model': 'FacebookAI/roberta-base', 'train': True, 'validate': False, 'evaluate': False, 'logging': False, 'epochs': 10, 'batch_size': 16, 'lr': 0.0001, 'dataset_dir': '/home/lvpoellhuber/storage/datasets', 'tokenizer': 'FacebookAI/roberta-base', 'eval_strategy': 'no', 'accelerate': True, 'weight_decay': 0.01, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'lr_scheduler_type': 'linear', 'warmup_steps': 0, 'warmup_ratio': 0.06, 'save_strategy': 'epoch', 'save_total_limit': 1, 'optim': 'adamw_torch', 'auto_find_batch_size': False, 'resume_from_checkpoint': None}}
2025-02-03 15:14 - WARNING - Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'SSLError(SSLEOFError(8, '[SSL: UNEXPECTED_EOF_WHILE_READING] EOF occurred in violation of protocol (_ssl.c:1007)'))': /clientlib/batch/logger/experiment/metric
2025-02-03 15:48 - INFO - Model Configuration: {'config': {'max_position_embeddings': 514, 'hidden_size': 768, 'num_attention_heads': 12, 'num_hidden_layers': 6, 'type_vocab_size': 1, 'attn_mechanism': 'eager', 'num_labels': 2, 'position_embedding_type': 'relative_key'}, 'settings': {'task': 'glue', 'exp_name': 'test', 'save_path': '/home/lvpoellhuber/storage/models/new-attention/roberta/roberta_1', 'model': 'FacebookAI/roberta-base', 'train': True, 'validate': False, 'evaluate': False, 'logging': False, 'epochs': 10, 'batch_size': 16, 'lr': 0.0001, 'dataset_dir': '/home/lvpoellhuber/storage/datasets', 'tokenizer': 'FacebookAI/roberta-base', 'eval_strategy': 'no', 'accelerate': True, 'weight_decay': 0.01, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'lr_scheduler_type': 'linear', 'warmup_steps': 0, 'warmup_ratio': 0.06, 'save_strategy': 'epoch', 'save_total_limit': 1, 'optim': 'adamw_torch', 'auto_find_batch_size': False, 'resume_from_checkpoint': None}}
2025-02-03 15:50 - INFO - Model Configuration: {'config': {'max_position_embeddings': 514, 'hidden_size': 768, 'num_attention_heads': 12, 'num_hidden_layers': 6, 'type_vocab_size': 1, 'attn_mechanism': 'eager', 'num_labels': 2, 'position_embedding_type': 'relative_key'}, 'settings': {'task': 'glue', 'exp_name': 'test', 'save_path': '/home/lvpoellhuber/storage/models/new-attention/roberta/roberta_1', 'model': 'FacebookAI/roberta-base', 'train': True, 'validate': False, 'evaluate': False, 'logging': False, 'epochs': 10, 'batch_size': 16, 'lr': 0.0001, 'dataset_dir': '/home/lvpoellhuber/storage/datasets', 'tokenizer': 'FacebookAI/roberta-base', 'eval_strategy': 'no', 'accelerate': True, 'weight_decay': 0.01, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'lr_scheduler_type': 'linear', 'warmup_steps': 0, 'warmup_ratio': 0.06, 'save_strategy': 'epoch', 'save_total_limit': 1, 'optim': 'adamw_torch', 'auto_find_batch_size': False, 'resume_from_checkpoint': None}}
2025-02-03 15:51 - WARNING - ============ Finetuning test_rpeK. ============
2025-02-03 15:51 - INFO - Model Configuration: {'settings': {'model': 'FacebookAI/roberta-base', 'save_path': '/home/lvpoellhuber/storage/models/finetune_roberta/roberta', 'tokenizer': 'FacebookAI/roberta-base', 'dataset': '/home/lvpoellhuber/storage/datasets/cola/cola_train.pt', 'task': 'cola', 'accelerate': True, 'logging': False, 'exp_name': 'test_rpeK', 'epochs': 1, 'batch_size': 16, 'lr': 0.0001}, 'config': {'max_position_embeddings': 514, 'hidden_size': 768, 'num_attention_heads': 12, 'num_hidden_layers': 6, 'type_vocab_size': 1, 'attn_mechanism': 'eager', 'num_labels': 2, 'position_embedding_type': 'relative_key'}}
2025-02-03 15:51 - WARNING - Initializing training.
2025-02-03 15:51 - WARNING - Beginning training process.
2025-02-03 15:52 - WARNING - ============ Finetuning test_rpeK. ============
2025-02-03 15:52 - INFO - Model Configuration: {'settings': {'model': 'FacebookAI/roberta-base', 'save_path': '/home/lvpoellhuber/storage/models/finetune_roberta/roberta', 'tokenizer': 'FacebookAI/roberta-base', 'dataset': '/home/lvpoellhuber/storage/datasets/cola/cola_train.pt', 'task': 'cola', 'accelerate': True, 'logging': False, 'exp_name': 'test_rpeK', 'epochs': 1, 'batch_size': 16, 'lr': 0.0001}, 'config': {'max_position_embeddings': 514, 'hidden_size': 768, 'num_attention_heads': 12, 'num_hidden_layers': 6, 'type_vocab_size': 1, 'attn_mechanism': 'eager', 'num_labels': 2, 'position_embedding_type': 'relative_key'}}
2025-02-03 15:52 - WARNING - Initializing training.
2025-02-03 15:54 - INFO - Model Configuration: {'config': {'max_position_embeddings': 514, 'hidden_size': 768, 'num_attention_heads': 12, 'num_hidden_layers': 6, 'type_vocab_size': 1, 'attn_mechanism': 'eager', 'num_labels': 2, 'position_embedding_type': 'relative_key'}, 'settings': {'task': 'glue', 'exp_name': 'test', 'save_path': '/home/lvpoellhuber/storage/models/new-attention/roberta/roberta_1', 'model': 'FacebookAI/roberta-base', 'train': True, 'validate': False, 'evaluate': False, 'logging': False, 'epochs': 10, 'batch_size': 16, 'lr': 0.0001, 'dataset_dir': '/home/lvpoellhuber/storage/datasets', 'tokenizer': 'FacebookAI/roberta-base', 'eval_strategy': 'no', 'accelerate': True, 'weight_decay': 0.01, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'lr_scheduler_type': 'linear', 'warmup_steps': 0, 'warmup_ratio': 0.06, 'save_strategy': 'epoch', 'save_total_limit': 1, 'optim': 'adamw_torch', 'auto_find_batch_size': False, 'resume_from_checkpoint': None}}
2025-02-03 16:13 - INFO - Model Configuration: {'config': {'max_position_embeddings': 514, 'hidden_size': 768, 'num_attention_heads': 12, 'num_hidden_layers': 6, 'type_vocab_size': 1, 'attn_mechanism': 'eager', 'num_labels': 2, 'position_embedding_type': 'relative_key'}, 'settings': {'task': 'glue', 'exp_name': 'test', 'save_path': '/home/lvpoellhuber/storage/models/new-attention/roberta/roberta_1', 'model': 'FacebookAI/roberta-base', 'train': True, 'validate': False, 'evaluate': False, 'logging': False, 'epochs': 10, 'batch_size': 16, 'lr': 0.0001, 'dataset_dir': '/home/lvpoellhuber/storage/datasets', 'tokenizer': 'FacebookAI/roberta-base', 'eval_strategy': 'no', 'accelerate': True, 'weight_decay': 0.01, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'lr_scheduler_type': 'linear', 'warmup_steps': 0, 'warmup_ratio': 0.06, 'save_strategy': 'epoch', 'save_total_limit': 1, 'optim': 'adamw_torch', 'auto_find_batch_size': False, 'resume_from_checkpoint': None}}
2025-02-03 16:15 - WARNING - ============ Finetuning test_rpeK. ============
2025-02-03 16:15 - INFO - Model Configuration: {'settings': {'model': 'FacebookAI/roberta-base', 'save_path': '/home/lvpoellhuber/storage/models/finetune_roberta/roberta', 'tokenizer': 'FacebookAI/roberta-base', 'dataset': '/home/lvpoellhuber/storage/datasets/cola/cola_train.pt', 'task': 'cola', 'accelerate': True, 'logging': False, 'exp_name': 'test_rpeK', 'epochs': 1, 'batch_size': 16, 'lr': 0.0001}, 'config': {'max_position_embeddings': 514, 'hidden_size': 768, 'num_attention_heads': 12, 'num_hidden_layers': 6, 'type_vocab_size': 1, 'attn_mechanism': 'eager', 'num_labels': 2, 'position_embedding_type': 'relative_key'}}
2025-02-03 16:15 - WARNING - Initializing training.
2025-02-03 16:15 - WARNING - Beginning training process.
2025-02-03 16:16 - WARNING - ============ Finetuning test_rpeK. ============
2025-02-03 16:16 - INFO - Model Configuration: {'settings': {'model': 'FacebookAI/roberta-base', 'save_path': '/home/lvpoellhuber/storage/models/finetune_roberta/roberta', 'tokenizer': 'FacebookAI/roberta-base', 'dataset': '/home/lvpoellhuber/storage/datasets/cola/cola_train.pt', 'task': 'cola', 'accelerate': True, 'logging': False, 'exp_name': 'test_rpeK', 'epochs': 1, 'batch_size': 16, 'lr': 0.0001}, 'config': {'max_position_embeddings': 514, 'hidden_size': 768, 'num_attention_heads': 12, 'num_hidden_layers': 6, 'type_vocab_size': 1, 'attn_mechanism': 'eager', 'num_labels': 2, 'position_embedding_type': 'relative_key'}}
2025-02-03 16:16 - WARNING - Initializing training.
2025-02-03 16:16 - WARNING - Beginning training process.
2025-02-04 10:30 - INFO - Model Configuration: {'config': {'max_position_embeddings': 514, 'hidden_size': 768, 'num_attention_heads': 12, 'num_hidden_layers': 6, 'type_vocab_size': 1, 'attn_mechanism': 'eager', 'num_labels': 2, 'position_embedding_type': 'relative_key'}, 'settings': {'task': 'glue', 'exp_name': 'test', 'save_path': '/home/lvpoellhuber/storage/models/new-attention/roberta/hfTrainer_test/cola', 'model': 'FacebookAI/roberta-base', 'train': False, 'validate': False, 'evaluate': True, 'logging': False, 'epochs': 10, 'batch_size': 16, 'lr': 0.0001, 'dataset_dir': '/home/lvpoellhuber/storage/datasets', 'tokenizer': 'FacebookAI/roberta-base', 'eval_strategy': 'no', 'accelerate': True, 'weight_decay': 0.01, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'lr_scheduler_type': 'linear', 'warmup_steps': 0, 'warmup_ratio': 0.06, 'save_strategy': 'epoch', 'save_total_limit': 1, 'optim': 'adamw_torch', 'auto_find_batch_size': False, 'resume_from_checkpoint': None}}
2025-02-04 10:30 - INFO - Model Configuration: {'config': {'max_position_embeddings': 514, 'hidden_size': 768, 'num_attention_heads': 12, 'num_hidden_layers': 6, 'type_vocab_size': 1, 'attn_mechanism': 'eager', 'num_labels': 2, 'position_embedding_type': 'relative_key'}, 'settings': {'task': 'glue', 'exp_name': 'test', 'save_path': '/home/lvpoellhuber/storage/models/new-attention/roberta/hfTrainer_test', 'model': 'FacebookAI/roberta-base', 'train': False, 'validate': False, 'evaluate': True, 'logging': False, 'epochs': 10, 'batch_size': 16, 'lr': 0.0001, 'dataset_dir': '/home/lvpoellhuber/storage/datasets', 'tokenizer': 'FacebookAI/roberta-base', 'eval_strategy': 'no', 'accelerate': True, 'weight_decay': 0.01, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'lr_scheduler_type': 'linear', 'warmup_steps': 0, 'warmup_ratio': 0.06, 'save_strategy': 'epoch', 'save_total_limit': 1, 'optim': 'adamw_torch', 'auto_find_batch_size': False, 'resume_from_checkpoint': None}}
2025-02-04 10:31 - WARNING - Using the latest cached version of the module from /home/lvpoellhuber/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--glue/05234ba7acc44554edcca0978db5fa3bc600eeee66229abe79ff9887eacaf3ed (last modified on Wed Jan 29 12:35:48 2025) since it couldn't be found locally at evaluate-metric--glue, or remotely on the Hugging Face Hub.
2025-02-05 12:08 - INFO - Model Configuration: {'config': {'max_position_embeddings': 514, 'hidden_size': 768, 'num_attention_heads': 12, 'num_hidden_layers': 6, 'type_vocab_size': 1, 'attn_mechanism': 'eager', 'num_labels': 2, 'position_embedding_type': 'relative_key'}, 'settings': {'task': 'glue', 'exp_name': 'test', 'save_path': '/home/lvpoellhuber/storage/models/new-attention/roberta/hfTrainer_test', 'model': 'FacebookAI/roberta-base', 'train': False, 'validate': False, 'evaluate': True, 'logging': False, 'epochs': 10, 'batch_size': 16, 'lr': 0.0001, 'dataset_dir': '/home/lvpoellhuber/storage/datasets', 'tokenizer': 'FacebookAI/roberta-base', 'eval_strategy': 'no', 'accelerate': True, 'weight_decay': 0.01, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'lr_scheduler_type': 'linear', 'warmup_steps': 0, 'warmup_ratio': 0.06, 'save_strategy': 'epoch', 'save_total_limit': 1, 'optim': 'adamw_torch', 'auto_find_batch_size': False, 'resume_from_checkpoint': None}}
